{
    "ggplot2": {
        "description": "A system for 'declaratively' creating graphics, based on \"The\n    Grammar of Graphics\". You provide the data, tell 'ggplot2' how to map\n    variables to aesthetics, what graphical primitives to use, and it\n    takes care of the details.",
        "examples": {
            "aes": "aes(x = mpg, y = wt)\naes(mpg, wt)\n\n# You can also map aesthetics to functions of variables\naes(x = mpg ^ 2, y = wt / cyl)\n\n# Or to constants\naes(x = 1, colour = \"smooth\")\n\n# Aesthetic names are automatically standardised\naes(col = x)\naes(fg = x)\naes(color = x)\naes(colour = x)\n\n# aes() is passed to either ggplot() or specific layer. Aesthetics supplied\n# to ggplot() are used as defaults for every layer.\nggplot(mpg, aes(displ, hwy)) + geom_point()\nggplot(mpg) + geom_point(aes(displ, hwy))\n\n# Tidy evaluation ----------------------------------------------------\n# aes() automatically quotes all its arguments, so you need to use tidy\n# evaluation to create wrappers around ggplot2 pipelines. The\n# simplest case occurs when your wrapper takes dots:\nscatter_by <- function(data, ...) {\n  ggplot(data) + geom_point(aes(...))\n}\nscatter_by(mtcars, disp, drat)\n\n# If your wrapper has a more specific interface with named arguments,\n# you need the \"embrace operator\":\nscatter_by <- function(data, x, y) {\n  ggplot(data) + geom_point(aes({{ x }}, {{ y }}))\n}\nscatter_by(mtcars, disp, drat)\n\n# Note that users of your wrapper can use their own functions in the\n# quoted expressions and all will resolve as it should!\ncut3 <- function(x) cut_number(x, 3)\nscatter_by(mtcars, cut3(disp), drat)",
            "aes_all": "aes_all(names(mtcars))\naes_all(c(\"x\", \"y\", \"col\", \"pch\"))",
            "aes_colour_fill_alpha": "\\donttest{\n\n# Bar chart example\np <- ggplot(mtcars, aes(factor(cyl)))\n# Default plotting\np + geom_bar()\n# To change the interior colouring use fill aesthetic\np + geom_bar(fill = \"red\")\n# Compare with the colour aesthetic which changes just the bar outline\np + geom_bar(colour = \"red\")\n# Combining both, you can see the changes more clearly\np + geom_bar(fill = \"white\", colour = \"red\")\n# Both colour and fill can take an rgb specification.\np + geom_bar(fill = \"#00abff\")\n# Use NA for a completely transparent colour.\np + geom_bar(fill = NA, colour = \"#00abff\")\n\n# Colouring scales differ depending on whether a discrete or\n# continuous variable is being mapped. For example, when mapping\n# fill to a factor variable, a discrete colour scale is used.\nggplot(mtcars, aes(factor(cyl), fill = factor(vs))) + geom_bar()\n\n# When mapping fill to continuous variable a continuous colour\n# scale is used.\nggplot(faithfuld, aes(waiting, eruptions)) +\n  geom_raster(aes(fill = density))\n\n# Some geoms only use the colour aesthetic but not the fill\n# aesthetic (e.g. geom_point() or geom_line()).\np <- ggplot(economics, aes(x = date, y = unemploy))\np + geom_line()\np + geom_line(colour = \"green\")\np + geom_point()\np + geom_point(colour = \"red\")\n\n# For large datasets with overplotting the alpha\n# aesthetic will make the points more transparent.\nset.seed(1)\ndf <- data.frame(x = rnorm(5000), y = rnorm(5000))\np  <- ggplot(df, aes(x,y))\np + geom_point()\np + geom_point(alpha = 0.5)\np + geom_point(alpha = 1/10)\n\n# Alpha can also be used to add shading.\np <- ggplot(economics, aes(x = date, y = unemploy)) + geom_line()\np\nyrng <- range(economics$unemploy)\np <- p +\n  geom_rect(\n    aes(NULL, NULL, xmin = start, xmax = end, fill = party),\n    ymin = yrng[1], ymax = yrng[2], data = presidential\n  )\np\np + scale_fill_manual(values = alpha(c(\"blue\", \"red\"), .3))\n}",
            "aes_eval": "# Default histogram display\nggplot(mpg, aes(displ)) +\n  geom_histogram(aes(y = after_stat(count)))\n\n# Scale tallest bin to 1\nggplot(mpg, aes(displ)) +\n  geom_histogram(aes(y = after_stat(count / max(count))))\n\n# Use a transparent version of colour for fill\nggplot(mpg, aes(class, hwy)) +\n  geom_boxplot(aes(colour = class, fill = after_scale(alpha(colour, 0.4))))\n\n# Use stage to modify the scaled fill\nggplot(mpg, aes(class, hwy)) +\n  geom_boxplot(aes(fill = stage(class, after_scale = alpha(fill, 0.4))))\n\n# Making a proportional stacked density plot\nggplot(mpg, aes(cty)) +\n  geom_density(\n    aes(\n      colour = factor(cyl),\n      fill = after_scale(alpha(colour, 0.3)),\n      y = after_stat(count / sum(n[!duplicated(group)]))\n    ),\n    position = \"stack\", bw = 1\n  ) +\n  geom_density(bw = 1)\n\n# Imitating a ridgeline plot\nggplot(mpg, aes(cty, colour = factor(cyl))) +\n  geom_ribbon(\n    stat = \"density\", outline.type = \"upper\",\n    aes(\n      fill = after_scale(alpha(colour, 0.3)),\n      ymin = after_stat(group),\n      ymax = after_stat(group + ndensity)\n    )\n  )\n\n# Labelling a bar plot\nggplot(mpg, aes(class)) +\n  geom_bar() +\n  geom_text(\n    aes(\n      y = after_stat(count + 2),\n      label = after_stat(count)\n    ),\n    stat = \"count\"\n  )\n\n# Labelling the upper hinge of a boxplot,\n# inspired by June Choe\nggplot(mpg, aes(displ, class)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_text(\n    aes(\n      label = after_stat(xmax),\n      x = stage(displ, after_stat = xmax)\n    ),\n    stat = \"boxplot\", hjust = -0.5\n  )",
            "aes_group_order": "\\donttest{\n\np <- ggplot(mtcars, aes(wt, mpg))\n# A basic scatter plot\np + geom_point(size = 4)\n# Using the colour aesthetic\np + geom_point(aes(colour = factor(cyl)), size = 4)\n# Using the shape aesthetic\np + geom_point(aes(shape = factor(cyl)), size = 4)\n\n# Using fill\np <- ggplot(mtcars, aes(factor(cyl)))\np + geom_bar()\np + geom_bar(aes(fill = factor(cyl)))\np + geom_bar(aes(fill = factor(vs)))\n\n# Using linetypes\nggplot(economics_long, aes(date, value01)) +\n  geom_line(aes(linetype = variable))\n\n# Multiple groups with one aesthetic\np <- ggplot(nlme::Oxboys, aes(age, height))\n# The default is not sufficient here. A single line tries to connect all\n# the observations.\np + geom_line()\n# To fix this, use the group aesthetic to map a different line for each\n# subject.\np + geom_line(aes(group = Subject))\n\n# Different groups on different layers\np <- p + geom_line(aes(group = Subject))\n# Using the group aesthetic with both geom_line() and geom_smooth()\n# groups the data the same way for both layers\np + geom_smooth(aes(group = Subject), method = \"lm\", se = FALSE)\n# Changing the group aesthetic for the smoother layer\n# fits a single line of best fit across all boys\np + geom_smooth(aes(group = 1), size = 2, method = \"lm\", se = FALSE)\n\n# Overriding the default grouping\n# Sometimes the plot has a discrete scale but you want to draw lines\n# that connect across groups. This is the strategy used in interaction\n# plots, profile plots, and parallel coordinate plots, among others.\n# For example, we draw boxplots of height at each measurement occasion.\np <- ggplot(nlme::Oxboys, aes(Occasion, height)) + geom_boxplot()\np\n# There is no need to specify the group aesthetic here; the default grouping\n# works because occasion is a discrete variable. To overlay individual\n# trajectories, we again need to override the default grouping for that layer\n# with aes(group = Subject)\np + geom_line(aes(group = Subject), colour = \"blue\")\n}",
            "aes_linetype_size_shape": "df <- data.frame(x = 1:10 , y = 1:10)\np <- ggplot(df, aes(x, y))\np + geom_line(linetype = 2)\np + geom_line(linetype = \"dotdash\")\n\n# An example with hex strings; the string \"33\" specifies three units on followed\n# by three off and \"3313\" specifies three units on followed by three off followed\n# by one on and finally three off.\np + geom_line(linetype = \"3313\")\n\n# Mapping line type from a grouping variable\nggplot(economics_long, aes(date, value01)) +\n  geom_line(aes(linetype = variable))\n\n# Linewidth examples\nggplot(economics, aes(date, unemploy)) +\n  geom_line(linewidth = 2, lineend = \"round\")\nggplot(economics, aes(date, unemploy)) +\n  geom_line(aes(linewidth = uempmed), lineend = \"round\")\n\n# Size examples\np <- ggplot(mtcars, aes(wt, mpg))\np + geom_point(size = 4)\np + geom_point(aes(size = qsec))\np + geom_point(size = 2.5) +\n  geom_hline(yintercept = 25, size = 3.5)\n\n# Shape examples\np + geom_point()\np + geom_point(shape = 5)\np + geom_point(shape = \"k\", size = 3)\np + geom_point(shape = \".\")\np + geom_point(shape = NA)\np + geom_point(aes(shape = factor(cyl)))\n\n# A look at all 25 symbols\ndf2 <- data.frame(x = 1:5 , y = 1:25, z = 1:25)\np <- ggplot(df2, aes(x, y))\np + geom_point(aes(shape = z), size = 4) +\n  scale_shape_identity()\n# While all symbols have a foreground colour, symbols 19-25 also take a\n# background colour (fill)\np + geom_point(aes(shape = z), size = 4, colour = \"Red\") +\n  scale_shape_identity()\np + geom_point(aes(shape = z), size = 4, colour = \"Red\", fill = \"Black\") +\n  scale_shape_identity()",
            "aes_position": "# Generate data: means and standard errors of means for prices\n# for each type of cut\ndmod <- lm(price ~ cut, data = diamonds)\ncut <- unique(diamonds$cut)\ncuts_df <- data.frame(\n  cut,\n  predict(dmod, data.frame(cut), se = TRUE)[c(\"fit\", \"se.fit\")]\n)\nggplot(cuts_df) +\n  aes(\n   x = cut,\n   y = fit,\n   ymin = fit - se.fit,\n   ymax = fit + se.fit,\n   colour = cut\n  ) +\n  geom_pointrange()\n\n# Using annotate\np <- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point()\np\np + annotate(\n  \"rect\", xmin = 2, xmax = 3.5, ymin = 2, ymax = 25,\n  fill = \"dark grey\", alpha = .5\n)\n\n# Geom_segment examples\np + geom_segment(\n  aes(x = 2, y = 15, xend = 2, yend = 25),\n  arrow = arrow(length = unit(0.5, \"cm\"))\n)\np + geom_segment(\n  aes(x = 2, y = 15, xend = 3, yend = 15),\n  arrow = arrow(length = unit(0.5, \"cm\"))\n)\np + geom_segment(\n  aes(x = 5, y = 30, xend = 3.5, yend = 25),\n  arrow = arrow(length = unit(0.5, \"cm\"))\n)\n\n# You can also use geom_segment() to recreate plot(type = \"h\")\n# from base R:\nset.seed(1)\ncounts <- as.data.frame(table(x = rpois(100, 5)))\ncounts$x <- as.numeric(as.character(counts$x))\nwith(counts, plot(x, Freq, type = \"h\", lwd = 10))\n\nggplot(counts, aes(x = x, y = Freq)) +\n  geom_segment(aes(yend = 0, xend = x), size = 10)",
            "annotate": "p <- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point()\np + annotate(\"text\", x = 4, y = 25, label = \"Some text\")\np + annotate(\"text\", x = 2:5, y = 25, label = \"Some text\")\np + annotate(\"rect\", xmin = 3, xmax = 4.2, ymin = 12, ymax = 21,\n  alpha = .2)\np + annotate(\"segment\", x = 2.5, xend = 4, y = 15, yend = 25,\n  colour = \"blue\")\np + annotate(\"pointrange\", x = 3.5, y = 20, ymin = 12, ymax = 28,\n  colour = \"red\", size = 2.5, linewidth = 1.5)\n\np + annotate(\"text\", x = 2:3, y = 20:21, label = c(\"my label\", \"label 2\"))\n\np + annotate(\"text\", x = 4, y = 25, label = \"italic(R) ^ 2 == 0.75\",\n  parse = TRUE)\np + annotate(\"text\", x = 4, y = 25,\n  label = \"paste(italic(R) ^ 2, \\\" = .75\\\")\", parse = TRUE)",
            "annotation_custom": "# Dummy plot\ndf <- data.frame(x = 1:10, y = 1:10)\nbase <- ggplot(df, aes(x, y)) +\n  geom_blank() +\n  theme_bw()\n\n# Full panel annotation\nbase + annotation_custom(\n  grob = grid::roundrectGrob(),\n  xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf\n)\n\n# Inset plot\ndf2 <- data.frame(x = 1 , y = 1)\ng <- ggplotGrob(ggplot(df2, aes(x, y)) +\n  geom_point() +\n  theme(plot.background = element_rect(colour = \"black\")))\nbase +\n  annotation_custom(grob = g, xmin = 1, xmax = 10, ymin = 8, ymax = 10)",
            "annotation_logticks": "# Make a log-log plot (without log ticks)\na <- ggplot(msleep, aes(bodywt, brainwt)) +\n geom_point(na.rm = TRUE) +\n scale_x_log10(\n   breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n   labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n ) +\n scale_y_log10(\n   breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n   labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n ) +\n theme_bw()\n\na + annotation_logticks()                # Default: log ticks on bottom and left\na + annotation_logticks(sides = \"lr\")    # Log ticks for y, on left and right\na + annotation_logticks(sides = \"trbl\")  # All four sides\n\na + annotation_logticks(sides = \"lr\", outside = TRUE) +\n coord_cartesian(clip = \"off\")  # Ticks outside plot\n\n# Hide the minor grid lines because they don't align with the ticks\na + annotation_logticks(sides = \"trbl\") + theme(panel.grid.minor = element_blank())\n\n# Another way to get the same results as 'a' above: log-transform the data before\n# plotting it. Also hide the minor grid lines.\nb <- ggplot(msleep, aes(log10(bodywt), log10(brainwt))) +\n geom_point(na.rm = TRUE) +\n scale_x_continuous(name = \"body\", labels = scales::label_math(10^.x)) +\n scale_y_continuous(name = \"brain\", labels = scales::label_math(10^.x)) +\n theme_bw() + theme(panel.grid.minor = element_blank())\n\nb + annotation_logticks()\n\n# Using a coordinate transform requires scaled = FALSE\nt <- ggplot(msleep, aes(bodywt, brainwt)) +\n  geom_point() +\n  coord_trans(x = \"log10\", y = \"log10\") +\n  theme_bw()\nt + annotation_logticks(scaled = FALSE)\n\n# Change the length of the ticks\na + annotation_logticks(\n  short = unit(.5,\"mm\"),\n  mid = unit(3,\"mm\"),\n  long = unit(4,\"mm\")\n)",
            "annotation_map": "\\dontrun{\nif (requireNamespace(\"maps\", quietly = TRUE)) {\n# location of cities in North Carolina\ndf <- data.frame(\n  name = c(\"Charlotte\", \"Raleigh\", \"Greensboro\"),\n  lat = c(35.227, 35.772, 36.073),\n  long = c(-80.843, -78.639, -79.792)\n)\n\np <- ggplot(df, aes(x = long, y = lat)) +\n  annotation_map(\n    map_data(\"state\"),\n    fill = \"antiquewhite\", colour = \"darkgrey\"\n  ) +\n  geom_point(color = \"blue\") +\n  geom_text(\n    aes(label = name),\n    hjust = 1.105, vjust = 1.05, color = \"blue\"\n  )\n\n# use without coord_sf() is possible but not recommended\np + xlim(-84, -76) + ylim(34, 37.2)\n\nif (requireNamespace(\"sf\", quietly = TRUE)) {\n# use with coord_sf() for appropriate projection\np +\n  coord_sf(\n    crs = sf::st_crs(3347),\n    default_crs = sf::st_crs(4326),  # data is provided as long-lat\n    xlim = c(-84, -76),\n    ylim = c(34, 37.2)\n  )\n\n# you can mix annotation_map() and geom_sf()\nnc <- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"), quiet = TRUE)\np +\n  geom_sf(\n    data = nc, inherit.aes = FALSE,\n    fill = NA, color = \"black\", linewidth = 0.1\n  ) +\n  coord_sf(crs = sf::st_crs(3347), default_crs = sf::st_crs(4326))\n}}}",
            "annotation_raster": "# Generate data\nrainbow <- matrix(hcl(seq(0, 360, length.out = 50 * 50), 80, 70), nrow = 50)\nggplot(mtcars, aes(mpg, wt)) +\n  geom_point() +\n  annotation_raster(rainbow, 15, 20, 3, 4)\n# To fill up whole plot\nggplot(mtcars, aes(mpg, wt)) +\n  annotation_raster(rainbow, -Inf, Inf, -Inf, Inf) +\n  geom_point()\n\nrainbow2 <- matrix(hcl(seq(0, 360, length.out = 10), 80, 70), nrow = 1)\nggplot(mtcars, aes(mpg, wt)) +\n  annotation_raster(rainbow2, -Inf, Inf, -Inf, Inf) +\n  geom_point()\nrainbow2 <- matrix(hcl(seq(0, 360, length.out = 10), 80, 70), nrow = 1)\nggplot(mtcars, aes(mpg, wt)) +\n  annotation_raster(rainbow2, -Inf, Inf, -Inf, Inf, interpolate = TRUE) +\n  geom_point()",
            "as_labeller": "p <- ggplot(mtcars, aes(disp, drat)) + geom_point()\np + facet_wrap(~am)\n\n# Rename labels on the fly with a lookup character vector\nto_string <- as_labeller(c(`0` = \"Zero\", `1` = \"One\"))\np + facet_wrap(~am, labeller = to_string)\n\n# Quickly transform a function operating on character vectors to a\n# labeller function:\nappender <- function(string, suffix = \"-foo\") paste0(string, suffix)\np + facet_wrap(~am, labeller = as_labeller(appender))\n\n# If you have more than one faceting variable, be sure to dispatch\n# your labeller to the right variable with labeller()\np + facet_grid(cyl ~ am, labeller = labeller(am = to_string))",
            "benchplot": "benchplot(ggplot(mtcars, aes(mpg, wt)) + geom_point())\nbenchplot(ggplot(mtcars, aes(mpg, wt)) + geom_point() + facet_grid(. ~ cyl))\n\n# With tidy eval:\np <- expr(ggplot(mtcars, aes(mpg, wt)) + geom_point())\nbenchplot(!!p)",
            "borders": "if (require(\"maps\")) {\n\nia <- map_data(\"county\", \"iowa\")\nmid_range <- function(x) mean(range(x))\nseats <- do.call(rbind, lapply(split(ia, ia$subregion), function(d) {\n  data.frame(lat = mid_range(d$lat), long = mid_range(d$long), subregion = unique(d$subregion))\n}))\n\nggplot(ia, aes(long, lat)) +\n  geom_polygon(aes(group = group), fill = NA, colour = \"grey60\") +\n  geom_text(aes(label = subregion), data = seats, size = 2, angle = 45)\n}\n\nif (require(\"maps\")) {\ndata(us.cities)\ncapitals <- subset(us.cities, capital == 2)\nggplot(capitals, aes(long, lat)) +\n  borders(\"state\") +\n  geom_point(aes(size = pop)) +\n  scale_size_area() +\n  coord_quickmap()\n}\n\nif (require(\"maps\")) {\n# Same map, with some world context\nggplot(capitals, aes(long, lat)) +\n  borders(\"world\", xlim = c(-130, -60), ylim = c(20, 50)) +\n  geom_point(aes(size = pop)) +\n  scale_size_area() +\n  coord_quickmap()\n}",
            "calc_element": "t <- theme_grey()\ncalc_element('text', t)\n\n# Compare the \"raw\" element definition to the element with calculated inheritance\nt$axis.text.x\ncalc_element('axis.text.x', t, verbose = TRUE)\n\n# This reports that axis.text.x inherits from axis.text,\n# which inherits from text. You can view each of them with:\nt$axis.text.x\nt$axis.text\nt$text",
            "check_device": "# Typically you'd run `check_device()` inside a function that might produce\n# advanced graphics.\n# The check is designed for use in control flow statements in the test mode\nif (check_device(\"patterns\", action = \"test\")) {\n  print(\"Yay\")\n} else {\n  print(\"Nay\")\n}\n\n# Automatically throw a warning when unavailable\nif (check_device(\"compositing\", action = \"warn\")) {\n  print(\"Yay\")\n} else {\n  print(\"Nay\")\n}\n\n# Possibly throw an error\ntry(check_device(\"glyphs\", action = \"abort\"))",
            "complete_theme": "my_theme <- theme(line = element_line(colour = \"red\"))\ncomplete_theme(my_theme)",
            "coord_cartesian": "# There are two ways of zooming the plot display: with scales or\n# with coordinate systems.  They work in two rather different ways.\n\np <- ggplot(mtcars, aes(disp, wt)) +\n  geom_point() +\n  geom_smooth()\np\n\n# Setting the limits on a scale converts all values outside the range to NA.\np + scale_x_continuous(limits = c(325, 500))\n\n# Setting the limits on the coordinate system performs a visual zoom.\n# The data is unchanged, and we just view a small portion of the original\n# plot. Note how smooth continues past the points visible on this plot.\np + coord_cartesian(xlim = c(325, 500))\n\n# By default, the same expansion factor is applied as when setting scale\n# limits. You can set the limits precisely by setting expand = FALSE\np + coord_cartesian(xlim = c(325, 500), expand = FALSE)\n\n# Similarly, we can use expand = FALSE to turn off expansion with the\n# default limits\np + coord_cartesian(expand = FALSE)\n\n# You can see the same thing with this 2d histogram\nd <- ggplot(diamonds, aes(carat, price)) +\n  stat_bin_2d(bins = 25, colour = \"white\")\nd\n\n# When zooming the scale, the we get 25 new bins that are the same\n# size on the plot, but represent smaller regions of the data space\nd + scale_x_continuous(limits = c(0, 1))\n\n# When zooming the coordinate system, we see a subset of original 50 bins,\n# displayed bigger\nd + coord_cartesian(xlim = c(0, 1))",
            "coord_fixed": "# ensures that the ranges of axes are equal to the specified ratio by\n# adjusting the plot aspect ratio\n\np <- ggplot(mtcars, aes(mpg, wt)) + geom_point()\np + coord_fixed(ratio = 1)\np + coord_fixed(ratio = 5)\np + coord_fixed(ratio = 1/5)\np + coord_fixed(xlim = c(15, 30))\n\n# Resize the plot to see that the specified aspect ratio is maintained",
            "coord_flip": "# The preferred method of creating horizontal instead of vertical boxplots\nggplot(diamonds, aes(price, cut)) +\n  geom_boxplot()\n\n# Using `coord_flip()` to make the same plot\nggplot(diamonds, aes(cut, price)) +\n  geom_boxplot() +\n  coord_flip()\n\n# With swapped aesthetics, the y-scale controls the left axis\nggplot(diamonds, aes(y = carat)) +\n  geom_histogram() +\n  scale_y_reverse()\n\n# In `coord_flip()`, the x-scale controls the left axis\nggplot(diamonds, aes(carat)) +\n  geom_histogram() +\n  coord_flip() +\n  scale_x_reverse()\n\n# In line and area plots, swapped aesthetics require an explicit orientation\ndf <- data.frame(a = 1:5, b = (1:5) ^ 2)\nggplot(df, aes(b, a)) +\n  geom_area(orientation = \"y\")\n\n# The same plot with `coord_flip()`\nggplot(df, aes(a, b)) +\n  geom_area() +\n  coord_flip()",
            "coord_map": "if (require(\"maps\")) {\nnz <- map_data(\"nz\")\n# Prepare a map of NZ\nnzmap <- ggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", colour = \"black\")\n\n# Plot it in cartesian coordinates\nnzmap\n}\n\nif (require(\"maps\")) {\n# With correct mercator projection\nnzmap + coord_map()\n}\n\nif (require(\"maps\")) {\n# With the aspect ratio approximation\nnzmap + coord_quickmap()\n}\n\nif (require(\"maps\")) {\n# Other projections\nnzmap + coord_map(\"azequalarea\", orientation = c(-36.92, 174.6, 0))\n}\n\nif (require(\"maps\")) {\nstates <- map_data(\"state\")\nusamap <- ggplot(states, aes(long, lat, group = group)) +\n  geom_polygon(fill = \"white\", colour = \"black\")\n\n# Use cartesian coordinates\nusamap\n}\n\nif (require(\"maps\")) {\n# With mercator projection\nusamap + coord_map()\n}\n\nif (require(\"maps\")) {\n# See ?mapproject for coordinate systems and their parameters\nusamap + coord_map(\"gilbert\")\n}\n\nif (require(\"maps\")) {\n# For most projections, you'll need to set the orientation yourself\n# as the automatic selection done by mapproject is not available to\n# ggplot\nusamap + coord_map(\"orthographic\")\n}\n\nif (require(\"maps\")) {\nusamap + coord_map(\"conic\", lat0 = 30)\n}\n\nif (require(\"maps\")) {\nusamap + coord_map(\"bonne\", lat0 = 50)\n}\n\n\\dontrun{\nif (require(\"maps\")) {\n# World map, using geom_path instead of geom_polygon\nworld <- map_data(\"world\")\nworldmap <- ggplot(world, aes(x = long, y = lat, group = group)) +\n  geom_path() +\n  scale_y_continuous(breaks = (-2:2) * 30) +\n  scale_x_continuous(breaks = (-4:4) * 45)\n\n# Orthographic projection with default orientation (looking down at North pole)\nworldmap + coord_map(\"ortho\")\n}\n\nif (require(\"maps\")) {\n# Looking up up at South Pole\nworldmap + coord_map(\"ortho\", orientation = c(-90, 0, 0))\n}\n\nif (require(\"maps\")) {\n# Centered on New York (currently has issues with closing polygons)\nworldmap + coord_map(\"ortho\", orientation = c(41, -74, 0))\n}\n}",
            "coord_polar": "# NOTE: Use these plots with caution - polar coordinates has\n# major perceptual problems.  The main point of these examples is\n# to demonstrate how these common plots can be described in the\n# grammar.  Use with EXTREME caution.\n\n# A pie chart = stacked bar chart + polar coordinates\npie <- ggplot(mtcars, aes(x = factor(1), fill = factor(cyl))) +\n geom_bar(width = 1)\npie + coord_polar(theta = \"y\")\n\n\\donttest{\n\n# A coxcomb plot = bar chart + polar coordinates\ncxc <- ggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(width = 1, colour = \"black\")\ncxc + coord_polar()\n# A new type of plot?\ncxc + coord_polar(theta = \"y\")\n\n# The bullseye chart\npie + coord_polar()\n\n# Hadley's favourite pie chart\ndf <- data.frame(\n  variable = c(\"does not resemble\", \"resembles\"),\n  value = c(20, 80)\n)\nggplot(df, aes(x = \"\", y = value, fill = variable)) +\n  geom_col(width = 1) +\n  scale_fill_manual(values = c(\"red\", \"yellow\")) +\n  coord_polar(\"y\", start = pi / 3) +\n  labs(title = \"Pac man\")\n\n# Windrose + doughnut plot\nif (require(\"ggplot2movies\")) {\nmovies$rrating <- cut_interval(movies$rating, length = 1)\nmovies$budgetq <- cut_number(movies$budget, 4)\n\ndoh <- ggplot(movies, aes(x = rrating, fill = budgetq))\n\n# Wind rose\ndoh + geom_bar(width = 1) + coord_polar()\n# Race track plot\ndoh + geom_bar(width = 0.9, position = \"fill\") + coord_polar(theta = \"y\")\n}\n}\n# A partial polar plot\nggplot(mtcars, aes(disp, mpg)) +\n  geom_point() +\n  coord_radial(start = -0.4 * pi, end = 0.4 * pi, inner.radius = 0.3)",
            "coord_trans": "\\donttest{\n# See ?geom_boxplot for other examples\n\n# Three ways of doing transformation in ggplot:\n#  * by transforming the data\nggplot(diamonds, aes(log10(carat), log10(price))) +\n  geom_point()\n#  * by transforming the scales\nggplot(diamonds, aes(carat, price)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10()\n#  * by transforming the coordinate system:\nggplot(diamonds, aes(carat, price)) +\n  geom_point() +\n  coord_trans(x = \"log10\", y = \"log10\")\n\n# The difference between transforming the scales and\n# transforming the coordinate system is that scale\n# transformation occurs BEFORE statistics, and coordinate\n# transformation afterwards.  Coordinate transformation also\n# changes the shape of geoms:\n\nd <- subset(diamonds, carat > 0.5)\n\nggplot(d, aes(carat, price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_x_log10() +\n  scale_y_log10()\n\nggplot(d, aes(carat, price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  coord_trans(x = \"log10\", y = \"log10\")\n\n# Here I used a subset of diamonds so that the smoothed line didn't\n# drop below zero, which obviously causes problems on the log-transformed\n# scale\n\n# With a combination of scale and coordinate transformation, it's\n# possible to do back-transformations:\nggplot(diamonds, aes(carat, price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  coord_trans(x = scales::transform_exp(10), y = scales::transform_exp(10))\n\n# cf.\nggplot(diamonds, aes(carat, price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n# Also works with discrete scales\nset.seed(1)\ndf <- data.frame(a = abs(rnorm(26)),letters)\nplot <- ggplot(df,aes(a,letters)) + geom_point()\n\nplot + coord_trans(x = \"log10\")\nplot + coord_trans(x = \"sqrt\")\n}",
            "cut_interval": "table(cut_interval(1:100, 10))\ntable(cut_interval(1:100, 11))\n\nset.seed(1)\n\ntable(cut_number(runif(1000), 10))\n\ntable(cut_width(runif(1000), 0.1))\ntable(cut_width(runif(1000), 0.1, boundary = 0))\ntable(cut_width(runif(1000), 0.1, center = 0))\ntable(cut_width(runif(1000), 0.1, labels = FALSE))",
            "draw_key": "p <- ggplot(economics, aes(date, psavert, color = \"savings rate\"))\n# key glyphs can be specified by their name\np + geom_line(key_glyph = \"timeseries\")\n\n# key glyphs can be specified via their drawing function\np + geom_line(key_glyph = draw_key_rect)",
            "element": "# A standard plot\nplot <- ggplot(mpg, aes(displ, hwy)) + geom_point()\n\n# Turning off theme elements by setting them to blank\nplot + theme(\n  panel.background = element_blank(),\n  axis.text = element_blank()\n)\n\n# Text adjustments\nplot + theme(\n  axis.text = element_text(colour = \"red\", size = rel(1.5))\n)\n\n# Turning on the axis line with an arrow\nplot + theme(\n  axis.line = element_line(arrow = arrow())\n)\n\nplot + theme(\n  panel.background = element_rect(fill = \"white\"),\n  plot.margin = margin_auto(2, unit = \"cm\"),\n  plot.background = element_rect(\n    fill = \"grey90\",\n    colour = \"black\",\n    linewidth = 1\n  )\n)\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method = \"lm\") +\n  theme(geom = element_geom(\n    ink = \"red\", accent = \"black\",\n    pointsize = 1, linewidth = 2\n  ))",
            "expand_limits": "p <- ggplot(mtcars, aes(mpg, wt)) + geom_point()\np + expand_limits(x = 0)\np + expand_limits(y = c(1, 9))\np + expand_limits(x = 0, y = 0)\n\nggplot(mtcars, aes(mpg, wt)) +\n  geom_point(aes(colour = cyl)) +\n  expand_limits(colour = seq(2, 10, by = 2))\nggplot(mtcars, aes(mpg, wt)) +\n  geom_point(aes(colour = factor(cyl))) +\n  expand_limits(colour = factor(seq(2, 10, by = 2)))",
            "expansion": "# No space below the bars but 10\\% above them\nggplot(mtcars) +\n  geom_bar(aes(x = factor(cyl))) +\n  scale_y_continuous(expand = expansion(mult = c(0, .1)))\n\n# Add 2 units of space on the left and right of the data\nggplot(subset(diamonds, carat > 2), aes(cut, clarity)) +\n  geom_jitter() +\n  scale_x_discrete(expand = expansion(add = 2))\n\n# Reproduce the default range expansion used\n# when the 'expand' argument is not specified\nggplot(subset(diamonds, carat > 2), aes(cut, price)) +\n  geom_jitter() +\n  scale_x_discrete(expand = expansion(add = .6)) +\n  scale_y_continuous(expand = expansion(mult = .05))",
            "facet_grid": "p <- ggplot(mpg, aes(displ, cty)) + geom_point()\n\n# Use vars() to supply variables from the dataset:\np + facet_grid(rows = vars(drv))\np + facet_grid(cols = vars(cyl))\np + facet_grid(vars(drv), vars(cyl))\n\n# To change plot order of facet grid,\n# change the order of variable levels with factor()\n\n# If you combine a facetted dataset with a dataset that lacks those\n# faceting variables, the data will be repeated across the missing\n# combinations:\ndf <- data.frame(displ = mean(mpg$displ), cty = mean(mpg$cty))\np +\n  facet_grid(cols = vars(cyl)) +\n  geom_point(data = df, colour = \"red\", size = 2)\n\n# When scales are constant, duplicated axes can be shown with\n# or without labels\nggplot(mpg, aes(cty, hwy)) +\n  geom_point() +\n  facet_grid(year ~ drv, axes = \"all\", axis.labels = \"all_x\")\n\n# Free scales -------------------------------------------------------\n# You can also choose whether the scales should be constant\n# across all panels (the default), or whether they should be allowed\n# to vary\nmt <- ggplot(mtcars, aes(mpg, wt, colour = factor(cyl))) +\n  geom_point()\n\nmt + facet_grid(vars(cyl), scales = \"free\")\n\n# If scales and space are free, then the mapping between position\n# and values in the data will be the same across all panels. This\n# is particularly useful for categorical axes\nggplot(mpg, aes(drv, model)) +\n  geom_point() +\n  facet_grid(manufacturer ~ ., scales = \"free\", space = \"free\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n# Margins ----------------------------------------------------------\n\\donttest{\n# Margins can be specified logically (all yes or all no) or for specific\n# variables as (character) variable names\nmg <- ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point()\nmg + facet_grid(vs + am ~ gear, margins = TRUE)\nmg + facet_grid(vs + am ~ gear, margins = \"am\")\n# when margins are made over \"vs\", since the facets for \"am\" vary\n# within the values of \"vs\", the marginal facet for \"vs\" is also\n# a margin over \"am\".\nmg + facet_grid(vs + am ~ gear, margins = \"vs\")\n}",
            "facet_null": "# facet_null is the default faceting specification if you\n# don't override it with facet_grid or facet_wrap\nggplot(mtcars, aes(mpg, wt)) + geom_point()",
            "facet_wrap": "p <- ggplot(mpg, aes(displ, hwy)) + geom_point()\n\n# Use vars() to supply faceting variables:\np + facet_wrap(vars(class))\n\n# Control the number of rows and columns with nrow and ncol\np + facet_wrap(vars(class), nrow = 4)\n\n\\donttest{\n# You can facet by multiple variables\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  facet_wrap(vars(cyl, drv))\n\n# Use the `labeller` option to control how labels are printed:\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  facet_wrap(vars(cyl, drv), labeller = \"label_both\")\n\n# To change the order in which the panels appear, change the levels\n# of the underlying factor.\nmpg$class2 <- reorder(mpg$class, mpg$displ)\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  facet_wrap(vars(class2))\n\n# By default, the same scales are used for all panels. You can allow\n# scales to vary across the panels with the `scales` argument.\n# Free scales make it easier to see patterns within each panel, but\n# harder to compare across panels.\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  facet_wrap(vars(class), scales = \"free\")\n\n# When scales are constant, duplicated axes can be shown with\n# or without labels\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  facet_wrap(vars(class), axes = \"all\", axis.labels = \"all_y\")\n\n# To repeat the same data in every panel, simply construct a data frame\n# that does not contain the faceting variable.\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(data = transform(mpg, class = NULL), colour = \"grey85\") +\n  geom_point() +\n  facet_wrap(vars(class))\n\n# Use `strip.position` to display the facet labels at the side of your\n# choice. Setting it to `bottom` makes it act as a subtitle for the axis.\n# This is typically used with free scales and a theme without boxes around\n# strip labels.\nggplot(economics_long, aes(date, value)) +\n  geom_line() +\n  facet_wrap(vars(variable), scales = \"free_y\", nrow = 2, strip.position = \"top\") +\n  theme(strip.background = element_blank(), strip.placement = \"outside\")\n}\n\n# The two letters determine the starting position, so 'tr' starts\n# in the top-right.\n# The first letter determines direction, so 'tr' fills top-to-bottom.\n# `dir = \"tr\"` is equivalent to `dir = \"v\", as.table = FALSE`\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  facet_wrap(vars(class), dir = \"tr\")",
            "fill_alpha": "# Typical colour input\nfill_alpha(\"red\", 0.5)\n\nif (utils::packageVersion(\"grid\") > \"4.2\") {\n  # Pattern input\n  fill_alpha(list(grid::linearGradient()), 0.5)\n}",
            "fortify-multcomp": "\\dontshow{if (require(\"multcomp\") && require(\"broom\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\namod <- aov(breaks ~ wool + tension, data = warpbreaks)\nwht <- multcomp::glht(amod, linfct = multcomp::mcp(tension = \"Tukey\"))\n\ntidy(wht) # recommended\nfortify(wht)\n\nggplot(tidy(wht), aes(contrast, estimate)) + geom_point()\n\nci <- confint(wht)\ntidy(ci) # recommended\nfortify(ci)\n\nggplot(tidy(confint(wht)),\n       aes(contrast, estimate, ymin = conf.low, ymax = conf.high)) +\n   geom_pointrange()\n\nsmry <- summary(wht)\ntidy(smry) # recommended\nfortify(smry)\n\nggplot(mapping = aes(contrast, estimate)) +\n   geom_linerange(aes(ymin = conf.low, ymax = conf.high), data = tidy(ci)) +\n   geom_point(aes(size = adj.p.value), data = tidy(smry)) +\n   scale_size(transform = \"reverse\")\n\ncld <- multcomp::cld(wht)\ntidy(cld) # recommended\nfortify(cld)\n\\dontshow{\\}) # examplesIf}",
            "fortify.lm": "\\dontshow{if (require(\"broom\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nmod <- lm(mpg ~ wt, data = mtcars)\n\n# Show augmented model\nhead(augment(mod))\nhead(fortify(mod))\n\n# Using augment to convert model to ready-to-plot data\nggplot(augment(mod), aes(.fitted, .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_smooth(se = FALSE)\n\n# Colouring by original data not included in the model\nggplot(augment(mod, mtcars), aes(.fitted, .std.resid, colour = factor(cyl))) +\n  geom_point()\n\\dontshow{\\}) # examplesIf}",
            "fortify.map": "if (require(\"maps\")) {\nca <- map(\"county\", \"ca\", plot = FALSE, fill = TRUE)\nhead(fortify(ca))\nggplot(ca, aes(long, lat)) +\n  geom_polygon(aes(group = group))\n}\n\nif (require(\"maps\")) {\ntx <- map(\"county\", \"texas\", plot = FALSE, fill = TRUE)\nhead(fortify(tx))\nggplot(tx, aes(long, lat)) +\n  geom_polygon(aes(group = group), colour = \"white\")\n}",
            "geom_abline": "p <- ggplot(mtcars, aes(wt, mpg)) + geom_point()\n\n# Fixed values\np + geom_vline(xintercept = 5)\np + geom_vline(xintercept = 1:5)\np + geom_hline(yintercept = 20)\n\np + geom_abline() # Can't see it - outside the range of the data\np + geom_abline(intercept = 20)\n\n# Calculate slope and intercept of line of best fit\ncoef(lm(mpg ~ wt, data = mtcars))\np + geom_abline(intercept = 37, slope = -5)\n# But this is easier to do with geom_smooth:\np + geom_smooth(method = \"lm\", se = FALSE)\n\n# To show different lines in different facets, use aesthetics\np <- ggplot(mtcars, aes(mpg, wt)) +\n  geom_point() +\n  facet_wrap(~ cyl)\n\nmean_wt <- data.frame(cyl = c(4, 6, 8), wt = c(2.28, 3.11, 4.00))\np + geom_hline(aes(yintercept = wt), mean_wt)\n\n# You can also control other aesthetics\nggplot(mtcars, aes(mpg, wt, colour = wt)) +\n  geom_point() +\n  geom_hline(aes(yintercept = wt, colour = wt), mean_wt) +\n  facet_wrap(~ cyl)",
            "geom_bar": "# geom_bar is designed to make it easy to create bar charts that show\n# counts (or sums of weights)\ng <- ggplot(mpg, aes(class))\n# Number of cars in each class:\ng + geom_bar()\n# Total engine displacement of each class\ng + geom_bar(aes(weight = displ))\n# Map class to y instead to flip the orientation\nggplot(mpg) + geom_bar(aes(y = class))\n\n# Bar charts are automatically stacked when multiple bars are placed\n# at the same location. The order of the fill is designed to match\n# the legend\ng + geom_bar(aes(fill = drv))\n\n# If you need to flip the order (because you've flipped the orientation)\n# call position_stack() explicitly:\nggplot(mpg, aes(y = class)) +\n geom_bar(aes(fill = drv), position = position_stack(reverse = TRUE)) +\n theme(legend.position = \"top\")\n\n# To show (e.g.) means, you need geom_col()\ndf <- data.frame(trt = c(\"a\", \"b\", \"c\"), outcome = c(2.3, 1.9, 3.2))\nggplot(df, aes(trt, outcome)) +\n  geom_col()\n# But geom_point() displays exactly the same information and doesn't\n# require the y-axis to touch zero.\nggplot(df, aes(trt, outcome)) +\n  geom_point()\n\n# You can also use geom_bar() with continuous data, in which case\n# it will show counts at unique locations\ndf <- data.frame(x = rep(c(2.9, 3.1, 4.5), c(5, 10, 4)))\nggplot(df, aes(x)) + geom_bar()\n# cf. a histogram of the same data\nggplot(df, aes(x)) + geom_histogram(binwidth = 0.5)\n\n# Use `just` to control how columns are aligned with axis breaks:\ndf <- data.frame(x = as.Date(c(\"2020-01-01\", \"2020-02-01\")), y = 1:2)\n# Columns centered on the first day of the month\nggplot(df, aes(x, y)) + geom_col(just = 0.5)\n# Columns begin on the first day of the month\nggplot(df, aes(x, y)) + geom_col(just = 1)",
            "geom_bin_2d": "d <- ggplot(diamonds, aes(x, y)) + xlim(4, 10) + ylim(4, 10)\nd + geom_bin_2d()\n\n# You can control the size of the bins by specifying the number of\n# bins in each direction:\nd + geom_bin_2d(bins = 10)\nd + geom_bin_2d(bins = list(x = 30, y = 10))\n\n# Or by specifying the width of the bins\nd + geom_bin_2d(binwidth = c(0.1, 0.1))",
            "geom_blank": "ggplot(mtcars, aes(wt, mpg))\n# Nothing to see here!",
            "geom_boxplot": "p <- ggplot(mpg, aes(class, hwy))\np + geom_boxplot()\n# Orientation follows the discrete axis\nggplot(mpg, aes(hwy, class)) + geom_boxplot()\n\np + geom_boxplot(notch = TRUE)\np + geom_boxplot(varwidth = TRUE)\np + geom_boxplot(fill = \"white\", colour = \"#3366FF\")\n# By default, outlier points match the colour of the box. Use\n# outlier.colour to override\np + geom_boxplot(outlier.colour = \"red\", outlier.shape = 1)\n# Remove outliers when overlaying boxplot with original data points\np + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2)\n\n# Boxplots are automatically dodged when any aesthetic is a factor\np + geom_boxplot(aes(colour = drv))\n\n# You can also use boxplots with continuous x, as long as you supply\n# a grouping variable. cut_width is particularly useful\nggplot(diamonds, aes(carat, price)) +\n  geom_boxplot()\nggplot(diamonds, aes(carat, price)) +\n  geom_boxplot(aes(group = cut_width(carat, 0.25)))\n# Adjust the transparency of outliers using outlier.alpha\nggplot(diamonds, aes(carat, price)) +\n  geom_boxplot(aes(group = cut_width(carat, 0.25)), outlier.alpha = 0.1)\n\n\\donttest{\n# It's possible to draw a boxplot with your own computations if you\n# use stat = \"identity\":\nset.seed(1)\ny <- rnorm(100)\ndf <- data.frame(\n  x = 1,\n  y0 = min(y),\n  y25 = quantile(y, 0.25),\n  y50 = median(y),\n  y75 = quantile(y, 0.75),\n  y100 = max(y)\n)\nggplot(df, aes(x)) +\n  geom_boxplot(\n   aes(ymin = y0, lower = y25, middle = y50, upper = y75, ymax = y100),\n   stat = \"identity\"\n )\n}",
            "geom_contour": "# Basic plot\nv <- ggplot(faithfuld, aes(waiting, eruptions, z = density))\nv + geom_contour()\n\n# Or compute from raw data\nggplot(faithful, aes(waiting, eruptions)) +\n  geom_density_2d()\n\n\\donttest{\n# use geom_contour_filled() for filled contours\nv + geom_contour_filled()\n\n# Setting bins creates evenly spaced contours in the range of the data\nv + geom_contour(bins = 3)\nv + geom_contour(bins = 5)\n\n# Setting binwidth does the same thing, parameterised by the distance\n# between contours\nv + geom_contour(binwidth = 0.01)\nv + geom_contour(binwidth = 0.001)\n\n# Other parameters\nv + geom_contour(aes(colour = after_stat(level)))\nv + geom_contour(colour = \"red\")\nv + geom_raster(aes(fill = density)) +\n  geom_contour(colour = \"white\")\n}",
            "geom_count": "ggplot(mpg, aes(cty, hwy)) +\n geom_point()\n\nggplot(mpg, aes(cty, hwy)) +\n geom_count()\n\n# Best used in conjunction with scale_size_area which ensures that\n# counts of zero would be given size 0. Doesn't make much different\n# here because the smallest count is already close to 0.\nggplot(mpg, aes(cty, hwy)) +\n geom_count() +\n scale_size_area()\n\n# Display proportions instead of counts -------------------------------------\n# By default, all categorical variables in the plot form the groups.\n# Specifying geom_count without a group identifier leads to a plot which is\n# not useful:\nd <- ggplot(diamonds, aes(x = cut, y = clarity))\nd + geom_count(aes(size = after_stat(prop)))\n# To correct this problem and achieve a more desirable plot, we need\n# to specify which group the proportion is to be calculated over.\nd + geom_count(aes(size = after_stat(prop), group = 1)) +\n  scale_size_area(max_size = 10)\n\n# Or group by x/y variables to have rows/columns sum to 1.\nd + geom_count(aes(size = after_stat(prop), group = cut)) +\n  scale_size_area(max_size = 10)\nd + geom_count(aes(size = after_stat(prop), group = clarity)) +\n  scale_size_area(max_size = 10)",
            "geom_density": "ggplot(diamonds, aes(carat)) +\n  geom_density()\n# Map the values to y to flip the orientation\nggplot(diamonds, aes(y = carat)) +\n  geom_density()\n\nggplot(diamonds, aes(carat)) +\n  geom_density(adjust = 1/5)\nggplot(diamonds, aes(carat)) +\n  geom_density(adjust = 5)\n\nggplot(diamonds, aes(depth, colour = cut)) +\n  geom_density() +\n  xlim(55, 70)\nggplot(diamonds, aes(depth, fill = cut, colour = cut)) +\n  geom_density(alpha = 0.1) +\n  xlim(55, 70)\n\n# Use `bounds` to adjust computation for known data limits\nbig_diamonds <- diamonds[diamonds$carat >= 1, ]\nggplot(big_diamonds, aes(carat)) +\n  geom_density(color = 'red') +\n  geom_density(bounds = c(1, Inf), color = 'blue')\n\n\\donttest{\n# Stacked density plots: if you want to create a stacked density plot, you\n# probably want to 'count' (density * n) variable instead of the default\n# density\n\n# Loses marginal densities\nggplot(diamonds, aes(carat, fill = cut)) +\n  geom_density(position = \"stack\")\n# Preserves marginal densities\nggplot(diamonds, aes(carat, after_stat(count), fill = cut)) +\n  geom_density(position = \"stack\")\n\n# You can use position=\"fill\" to produce a conditional density estimate\nggplot(diamonds, aes(carat, after_stat(count), fill = cut)) +\n  geom_density(position = \"fill\")\n}",
            "geom_density_2d": "m <- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n geom_point() +\n xlim(0.5, 6) +\n ylim(40, 110)\n\n# contour lines\nm + geom_density_2d()\n\n\\donttest{\n# contour bands\nm + geom_density_2d_filled(alpha = 0.5)\n\n# contour bands and contour lines\nm + geom_density_2d_filled(alpha = 0.5) +\n  geom_density_2d(linewidth = 0.25, colour = \"black\")\n\nset.seed(4393)\ndsmall <- diamonds[sample(nrow(diamonds), 1000), ]\nd <- ggplot(dsmall, aes(x, y))\n# If you map an aesthetic to a categorical variable, you will get a\n# set of contours for each value of that variable\nd + geom_density_2d(aes(colour = cut))\n\n# If you draw filled contours across multiple facets, the same bins are\n# used across all facets\nd + geom_density_2d_filled() + facet_wrap(vars(cut))\n# If you want to make sure the peak intensity is the same in each facet,\n# use `contour_var = \"ndensity\"`.\nd + geom_density_2d_filled(contour_var = \"ndensity\") + facet_wrap(vars(cut))\n# If you want to scale intensity by the number of observations in each group,\n# use `contour_var = \"count\"`.\nd + geom_density_2d_filled(contour_var = \"count\") + facet_wrap(vars(cut))\n\n# If we turn contouring off, we can use other geoms, such as tiles:\nd + stat_density_2d(\n  geom = \"raster\",\n  aes(fill = after_stat(density)),\n  contour = FALSE\n) + scale_fill_viridis_c()\n# Or points:\nd + stat_density_2d(geom = \"point\", aes(size = after_stat(density)), n = 20, contour = FALSE)\n}",
            "geom_dotplot": "ggplot(mtcars, aes(x = mpg)) +\n  geom_dotplot()\n\nggplot(mtcars, aes(x = mpg)) +\n  geom_dotplot(binwidth = 1.5)\n\n# Use fixed-width bins\nggplot(mtcars, aes(x = mpg)) +\n  geom_dotplot(method=\"histodot\", binwidth = 1.5)\n\n# Some other stacking methods\nggplot(mtcars, aes(x = mpg)) +\n  geom_dotplot(binwidth = 1.5, stackdir = \"center\")\n\nggplot(mtcars, aes(x = mpg)) +\n  geom_dotplot(binwidth = 1.5, stackdir = \"centerwhole\")\n\n# y axis isn't really meaningful, so hide it\nggplot(mtcars, aes(x = mpg)) + geom_dotplot(binwidth = 1.5) +\n  scale_y_continuous(NULL, breaks = NULL)\n\n# Overlap dots vertically\nggplot(mtcars, aes(x = mpg)) +\n  geom_dotplot(binwidth = 1.5, stackratio = .7)\n\n# Expand dot diameter\nggplot(mtcars, aes(x = mpg)) +\n  geom_dotplot(binwidth = 1.5, dotsize = 1.25)\n\n# Change dot fill colour, stroke width\nggplot(mtcars, aes(x = mpg)) +\n  geom_dotplot(binwidth = 1.5, fill = \"white\", stroke = 2)\n\n\\donttest{\n# Examples with stacking along y axis instead of x\nggplot(mtcars, aes(x = 1, y = mpg)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\")\n\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\")\n\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"centerwhole\")\n\nggplot(mtcars, aes(x = factor(vs), fill = factor(cyl), y = mpg)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", position = \"dodge\")\n\n# binpositions=\"all\" ensures that the bins are aligned between groups\nggplot(mtcars, aes(x = factor(am), y = mpg)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", binpositions=\"all\")\n\n# Stacking multiple groups, with different fill\nggplot(mtcars, aes(x = mpg, fill = factor(cyl))) +\n  geom_dotplot(stackgroups = TRUE, binwidth = 1, binpositions = \"all\")\n\nggplot(mtcars, aes(x = mpg, fill = factor(cyl))) +\n  geom_dotplot(stackgroups = TRUE, binwidth = 1, method = \"histodot\")\n\nggplot(mtcars, aes(x = 1, y = mpg, fill = factor(cyl))) +\n  geom_dotplot(binaxis = \"y\", stackgroups = TRUE, binwidth = 1, method = \"histodot\")\n}",
            "geom_function": "# geom_function() is useful for overlaying functions\nset.seed(1492)\nggplot(data.frame(x = rnorm(100)), aes(x)) +\n  geom_density() +\n  geom_function(fun = dnorm, colour = \"red\")\n\n# To plot functions without data, specify range of x-axis\nbase <-\n  ggplot() +\n  xlim(-5, 5)\n\nbase + geom_function(fun = dnorm)\n\nbase + geom_function(fun = dnorm, args = list(mean = 2, sd = .5))\n\n# The underlying mechanics evaluate the function at discrete points\n# and connect the points with lines\nbase + stat_function(fun = dnorm, geom = \"point\")\n\nbase + stat_function(fun = dnorm, geom = \"point\", n = 20)\n\nbase + stat_function(fun = dnorm, geom = \"polygon\", color = \"blue\", fill = \"blue\", alpha = 0.5)\n\nbase + geom_function(fun = dnorm, n = 20)\n\n# Two functions on the same plot\nbase +\n  geom_function(aes(colour = \"normal\"), fun = dnorm) +\n  geom_function(aes(colour = \"t, df = 1\"), fun = dt, args = list(df = 1))\n\n# Using a custom anonymous function\nbase + geom_function(fun = function(x) 0.5 * exp(-abs(x)))\n# or using lambda syntax:\n# base + geom_function(fun = ~ 0.5 * exp(-abs(.x)))\n# or in R4.1.0 and above:\n# base + geom_function(fun = \\(x) 0.5 * exp(-abs(x)))\n# or using a custom named function:\n# f <- function(x) 0.5 * exp(-abs(x))\n# base + geom_function(fun = f)\n\n# Using xlim to restrict the range of function\nggplot(data.frame(x = rnorm(100)), aes(x)) +\ngeom_density() +\ngeom_function(fun = dnorm, colour = \"red\", xlim=c(-1, 1))\n\n# Using xlim to widen the range of function\nggplot(data.frame(x = rnorm(100)), aes(x)) +\ngeom_density() +\ngeom_function(fun = dnorm, colour = \"red\", xlim=c(-7, 7))",
            "geom_hex": "d <- ggplot(diamonds, aes(carat, price))\nd + geom_hex()\n\n\\donttest{\n# You can control the size of the bins by specifying the number of\n# bins in each direction:\nd + geom_hex(bins = 10)\nd + geom_hex(bins = 30)\n\n# Or by specifying the width of the bins\nd + geom_hex(binwidth = c(1, 1000))\nd + geom_hex(binwidth = c(.1, 500))\n}",
            "geom_histogram": "ggplot(diamonds, aes(carat)) +\n  geom_histogram()\nggplot(diamonds, aes(carat)) +\n  geom_histogram(binwidth = 0.01)\nggplot(diamonds, aes(carat)) +\n  geom_histogram(bins = 200)\n# Map values to y to flip the orientation\nggplot(diamonds, aes(y = carat)) +\n  geom_histogram()\n\n# For histograms with tick marks between each bin, use `geom_bar()` with\n# `scale_x_binned()`.\nggplot(diamonds, aes(carat)) +\n  geom_bar() +\n  scale_x_binned()\n\n# Rather than stacking histograms, it's easier to compare frequency\n# polygons\nggplot(diamonds, aes(price, fill = cut)) +\n  geom_histogram(binwidth = 500)\nggplot(diamonds, aes(price, colour = cut)) +\n  geom_freqpoly(binwidth = 500)\n\n# To make it easier to compare distributions with very different counts,\n# put density on the y axis instead of the default count\nggplot(diamonds, aes(price, after_stat(density), colour = cut)) +\n  geom_freqpoly(binwidth = 500)\n\n\n# When using the non-equal-width bins, we should set the area of the bars to\n# represent the counts (not the height).\n# Here we're using 10 equi-probable bins:\nprice_bins <- quantile(diamonds$price, probs = seq(0, 1, length = 11))\n\nggplot(diamonds, aes(price)) +\n  geom_histogram(breaks = price_bins, color = \"black\") # misleading (height = count)\n\nggplot(diamonds, aes(price, after_stat(count / width))) +\n  geom_histogram(breaks = price_bins, color = \"black\") # area = count\n\nif (require(\"ggplot2movies\")) {\n# Often we don't want the height of the bar to represent the\n# count of observations, but the sum of some other variable.\n# For example, the following plot shows the number of movies\n# in each rating.\nm <- ggplot(movies, aes(rating))\nm + geom_histogram(binwidth = 0.1)\n\n# If, however, we want to see the number of votes cast in each\n# category, we need to weight by the votes variable\nm +\n  geom_histogram(aes(weight = votes), binwidth = 0.1) +\n  ylab(\"votes\")\n\n# For transformed scales, binwidth applies to the transformed data.\n# The bins have constant width on the transformed scale.\nm +\n geom_histogram() +\n scale_x_log10()\nm +\n  geom_histogram(binwidth = 0.05) +\n  scale_x_log10()\n\n# For transformed coordinate systems, the binwidth applies to the\n# raw data. The bins have constant width on the original scale.\n\n# Using log scales does not work here, because the first\n# bar is anchored at zero, and so when transformed becomes negative\n# infinity. This is not a problem when transforming the scales, because\n# no observations have 0 ratings.\nm +\n  geom_histogram(boundary = 0) +\n  coord_trans(x = \"log10\")\n# Use boundary = 0, to make sure we don't take sqrt of negative values\nm +\n  geom_histogram(boundary = 0) +\n  coord_trans(x = \"sqrt\")\n\n# You can also transform the y axis.  Remember that the base of the bars\n# has value 0, so log transformations are not appropriate\nm <- ggplot(movies, aes(x = rating))\nm +\n  geom_histogram(binwidth = 0.5) +\n  scale_y_sqrt()\n}\n\n# You can specify a function for calculating binwidth, which is\n# particularly useful when faceting along variables with\n# different ranges because the function will be called once per facet\nggplot(economics_long, aes(value)) +\n  facet_wrap(~variable, scales = 'free_x') +\n  geom_histogram(binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3)))",
            "geom_jitter": "p <- ggplot(mpg, aes(cyl, hwy))\np + geom_point()\np + geom_jitter()\n\n# Add aesthetic mappings\np + geom_jitter(aes(colour = class))\n\n# Use smaller width/height to emphasise categories\nggplot(mpg, aes(cyl, hwy)) +\n  geom_jitter()\nggplot(mpg, aes(cyl, hwy)) +\n  geom_jitter(width = 0.25)\n\n# Use larger width/height to completely smooth away discreteness\nggplot(mpg, aes(cty, hwy)) +\n  geom_jitter()\nggplot(mpg, aes(cty, hwy)) +\n  geom_jitter(width = 0.5, height = 0.5)",
            "geom_linerange": "# Create a simple example dataset\ndf <- data.frame(\n  trt = factor(c(1, 1, 2, 2)),\n  resp = c(1, 5, 3, 4),\n  group = factor(c(1, 2, 1, 2)),\n  upper = c(1.1, 5.3, 3.3, 4.2),\n  lower = c(0.8, 4.6, 2.4, 3.6)\n)\n\np <- ggplot(df, aes(trt, resp, colour = group))\np + geom_linerange(aes(ymin = lower, ymax = upper))\np + geom_pointrange(aes(ymin = lower, ymax = upper))\np + geom_crossbar(aes(ymin = lower, ymax = upper), width = 0.2)\np + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2)\n\n# Flip the orientation by changing mapping\nggplot(df, aes(resp, trt, colour = group)) +\n  geom_linerange(aes(xmin = lower, xmax = upper))\n\n# Draw lines connecting group means\np +\n  geom_line(aes(group = group)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2)\n\n# If you want to dodge bars and errorbars, you need to manually\n# specify the dodge width\np <- ggplot(df, aes(trt, resp, fill = group))\np +\n geom_col(position = \"dodge\") +\n geom_errorbar(aes(ymin = lower, ymax = upper), position = \"dodge\", width = 0.25)\n\n# Because the bars and errorbars have different widths\n# we need to specify how wide the objects we are dodging are\ndodge <- position_dodge(width=0.9)\np +\n  geom_col(position = dodge) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), position = dodge, width = 0.25)\n\n# When using geom_errorbar() with position_dodge2(), extra padding will be\n# needed between the error bars to keep them aligned with the bars.\np +\ngeom_col(position = \"dodge2\") +\ngeom_errorbar(\n  aes(ymin = lower, ymax = upper),\n  position = position_dodge2(width = 0.5, padding = 0.5)\n)",
            "geom_map": "# First, a made-up example containing a few polygons, to explain\n# how `geom_map()` works. It requires two data frames:\n# One contains the coordinates of each polygon (`positions`), and is\n# provided via the `map` argument. The other contains the\n# values associated with each polygon (`values`).  An id\n# variable links the two together.\n\nids <- factor(c(\"1.1\", \"2.1\", \"1.2\", \"2.2\", \"1.3\", \"2.3\"))\n\nvalues <- data.frame(\n  id = ids,\n  value = c(3, 3.1, 3.1, 3.2, 3.15, 3.5)\n)\n\npositions <- data.frame(\n  id = rep(ids, each = 4),\n  x = c(2, 1, 1.1, 2.2, 1, 0, 0.3, 1.1, 2.2, 1.1, 1.2, 2.5, 1.1, 0.3,\n  0.5, 1.2, 2.5, 1.2, 1.3, 2.7, 1.2, 0.5, 0.6, 1.3),\n  y = c(-0.5, 0, 1, 0.5, 0, 0.5, 1.5, 1, 0.5, 1, 2.1, 1.7, 1, 1.5,\n  2.2, 2.1, 1.7, 2.1, 3.2, 2.8, 2.1, 2.2, 3.3, 3.2)\n)\n\nggplot(values) +\n  geom_map(aes(map_id = id), map = positions) +\n  expand_limits(positions)\nggplot(values, aes(fill = value)) +\n  geom_map(aes(map_id = id), map = positions) +\n  expand_limits(positions)\nggplot(values, aes(fill = value)) +\n  geom_map(aes(map_id = id), map = positions) +\n  expand_limits(positions) + ylim(0, 3)\n\n# Now some examples with real maps\nif (require(maps)) {\n\n  crimes <- data.frame(state = tolower(rownames(USArrests)), USArrests)\n\n  # Equivalent to crimes \\%>\\% tidyr::pivot_longer(Murder:Rape)\n  vars <- lapply(names(crimes)[-1], function(j) {\n    data.frame(state = crimes$state, variable = j, value = crimes[[j]])\n  })\n  crimes_long <- do.call(\"rbind\", vars)\n\n  states_map <- map_data(\"state\")\n\n  # without geospatial coordinate system, the resulting plot\n  # looks weird\n  ggplot(crimes, aes(map_id = state)) +\n    geom_map(aes(fill = Murder), map = states_map) +\n    expand_limits(x = states_map$long, y = states_map$lat)\n\n  # in combination with `coord_sf()` we get an appropriate result\n  ggplot(crimes, aes(map_id = state)) +\n    geom_map(aes(fill = Murder), map = states_map) +\n    # crs = 5070 is a Conus Albers projection for North America,\n    #   see: https://epsg.io/5070\n    # default_crs = 4326 tells coord_sf() that the input map data\n    #   are in longitude-latitude format\n    coord_sf(\n      crs = 5070, default_crs = 4326,\n      xlim = c(-125, -70), ylim = c(25, 52)\n    )\n\n ggplot(crimes_long, aes(map_id = state)) +\n   geom_map(aes(fill = value), map = states_map) +\n   coord_sf(\n     crs = 5070, default_crs = 4326,\n     xlim = c(-125, -70), ylim = c(25, 52)\n   ) +\n   facet_wrap(~variable)\n}",
            "geom_path": "# geom_line() is suitable for time series\nggplot(economics, aes(date, unemploy)) + geom_line()\n# separate by colour and use \"timeseries\" legend key glyph\nggplot(economics_long, aes(date, value01, colour = variable)) +\n  geom_line(key_glyph = \"timeseries\")\n\n# You can get a timeseries that run vertically by setting the orientation\nggplot(economics, aes(unemploy, date)) + geom_line(orientation = \"y\")\n\n# geom_step() is useful when you want to highlight exactly when\n# the y value changes\nrecent <- economics[economics$date > as.Date(\"2013-01-01\"), ]\nggplot(recent, aes(date, unemploy)) + geom_line()\nggplot(recent, aes(date, unemploy)) + geom_step()\n\n# geom_path lets you explore how two variables are related over time,\n# e.g. unemployment and personal savings rate\nm <- ggplot(economics, aes(unemploy/pop, psavert))\nm + geom_path()\nm + geom_path(aes(colour = as.numeric(date)))\n\n# Changing parameters ----------------------------------------------\nggplot(economics, aes(date, unemploy)) +\n  geom_line(colour = \"red\")\n\n# Use the arrow parameter to add an arrow to the line\n# See ?arrow for more details\nc <- ggplot(economics, aes(x = date, y = pop))\nc + geom_line(arrow = arrow())\nc + geom_line(\n  arrow = arrow(angle = 15, ends = \"both\", type = \"closed\")\n)\n\n# Control line join parameters\ndf <- data.frame(x = 1:3, y = c(4, 1, 9))\nbase <- ggplot(df, aes(x, y))\nbase + geom_path(linewidth = 10)\nbase + geom_path(linewidth = 10, lineend = \"round\")\nbase + geom_path(linewidth = 10, linejoin = \"mitre\", lineend = \"butt\")\n\n# You can use NAs to break the line.\ndf <- data.frame(x = 1:5, y = c(1, 2, NA, 4, 5))\nggplot(df, aes(x, y)) + geom_point() + geom_line()\n\n\\donttest{\n# Setting line type vs colour/size\n# Line type needs to be applied to a line as a whole, so it can\n# not be used with colour or size that vary across a line\nx <- seq(0.01, .99, length.out = 100)\ndf <- data.frame(\n  x = rep(x, 2),\n  y = c(qlogis(x), 2 * qlogis(x)),\n  group = rep(c(\"a\",\"b\"),\n  each = 100)\n)\np <- ggplot(df, aes(x=x, y=y, group=group))\n# These work\np + geom_line(linetype = 2)\np + geom_line(aes(colour = group), linetype = 2)\np + geom_line(aes(colour = x))\n# But this doesn't\nshould_stop(p + geom_line(aes(colour = x), linetype=2))\n}",
            "geom_point": "p <- ggplot(mtcars, aes(wt, mpg))\np + geom_point()\n\n# Add aesthetic mappings\np + geom_point(aes(colour = factor(cyl)))\np + geom_point(aes(shape = factor(cyl)))\n# A \"bubblechart\":\np + geom_point(aes(size = qsec))\n\n# Set aesthetics to fixed value\nggplot(mtcars, aes(wt, mpg)) + geom_point(colour = \"red\", size = 3)\n\n\\donttest{\n# Varying alpha is useful for large datasets\nd <- ggplot(diamonds, aes(carat, price))\nd + geom_point(alpha = 1/10)\nd + geom_point(alpha = 1/20)\nd + geom_point(alpha = 1/100)\n}\n\n# For shapes that have a border (like 21), you can colour the inside and\n# outside separately. Use the stroke aesthetic to modify the width of the\n# border\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(shape = 21, colour = \"black\", fill = \"white\", size = 5, stroke = 5)\n\n# The default shape in legends is not filled, but you can override the shape\n# in the guide to reflect the fill in the legend\nggplot(mtcars, aes(wt, mpg, fill = factor(carb), shape = factor(cyl))) +\n  geom_point(size = 5, stroke = 1) +\n  scale_shape_manual(values = 21:25) +\n  scale_fill_ordinal(guide = guide_legend(override.aes = list(shape = 21)))\n\n\\donttest{\n# You can create interesting shapes by layering multiple points of\n# different sizes\np <- ggplot(mtcars, aes(mpg, wt, shape = factor(cyl)))\np +\n  geom_point(aes(colour = factor(cyl)), size = 4) +\n  geom_point(colour = \"grey90\", size = 1.5)\np +\n  geom_point(colour = \"black\", size = 4.5) +\n  geom_point(colour = \"pink\", size = 4) +\n  geom_point(aes(shape = factor(cyl)))\n\n# geom_point warns when missing values have been dropped from the data set\n# and not plotted, you can turn this off by setting na.rm = TRUE\nset.seed(1)\nmtcars2 <- transform(mtcars, mpg = ifelse(runif(32) < 0.2, NA, mpg))\nggplot(mtcars2, aes(wt, mpg)) +\n  geom_point()\nggplot(mtcars2, aes(wt, mpg)) +\n  geom_point(na.rm = TRUE)\n}",
            "geom_polygon": "# When using geom_polygon, you will typically need two data frames:\n# one contains the coordinates of each polygon (positions),  and the\n# other the values associated with each polygon (values).  An id\n# variable links the two together\n\nids <- factor(c(\"1.1\", \"2.1\", \"1.2\", \"2.2\", \"1.3\", \"2.3\"))\n\nvalues <- data.frame(\n  id = ids,\n  value = c(3, 3.1, 3.1, 3.2, 3.15, 3.5)\n)\n\npositions <- data.frame(\n  id = rep(ids, each = 4),\n  x = c(2, 1, 1.1, 2.2, 1, 0, 0.3, 1.1, 2.2, 1.1, 1.2, 2.5, 1.1, 0.3,\n  0.5, 1.2, 2.5, 1.2, 1.3, 2.7, 1.2, 0.5, 0.6, 1.3),\n  y = c(-0.5, 0, 1, 0.5, 0, 0.5, 1.5, 1, 0.5, 1, 2.1, 1.7, 1, 1.5,\n  2.2, 2.1, 1.7, 2.1, 3.2, 2.8, 2.1, 2.2, 3.3, 3.2)\n)\n\n# Currently we need to manually merge the two together\ndatapoly <- merge(values, positions, by = c(\"id\"))\n\np <- ggplot(datapoly, aes(x = x, y = y)) +\n  geom_polygon(aes(fill = value, group = id))\np\n\n# Which seems like a lot of work, but then it's easy to add on\n# other features in this coordinate system, e.g.:\n\nset.seed(1)\nstream <- data.frame(\n  x = cumsum(runif(50, max = 0.1)),\n  y = cumsum(runif(50,max = 0.1))\n)\n\np + geom_line(data = stream, colour = \"grey30\", linewidth = 5)\n\n# And if the positions are in longitude and latitude, you can use\n# coord_map to produce different map projections.\n\nif (packageVersion(\"grid\") >= \"3.6\") {\n  # As of R version 3.6 geom_polygon() supports polygons with holes\n  # Use the subgroup aesthetic to differentiate holes from the main polygon\n\n  holes <- do.call(rbind, lapply(split(datapoly, datapoly$id), function(df) {\n    df$x <- df$x + 0.5 * (mean(df$x) - df$x)\n    df$y <- df$y + 0.5 * (mean(df$y) - df$y)\n    df\n  }))\n  datapoly$subid <- 1L\n  holes$subid <- 2L\n  datapoly <- rbind(datapoly, holes)\n\n  p <- ggplot(datapoly, aes(x = x, y = y)) +\n    geom_polygon(aes(fill = value, group = id, subgroup = subid))\n  p\n}",
            "geom_qq": "\\donttest{\ndf <- data.frame(y = rt(200, df = 5))\np <- ggplot(df, aes(sample = y))\np + stat_qq() + stat_qq_line()\n\n# Use fitdistr from MASS to estimate distribution params:\n# if (requireNamespace(\"MASS\", quietly = TRUE)) {\n#   params <- as.list(MASS::fitdistr(df$y, \"t\")$estimate)\n# }\n# Here, we use pre-computed params\nparams <- list(m = -0.02505057194115, s = 1.122568610124, df = 6.63842653897)\nggplot(df, aes(sample = y)) +\n  stat_qq(distribution = qt, dparams = params[\"df\"]) +\n  stat_qq_line(distribution = qt, dparams = params[\"df\"])\n\n# Using to explore the distribution of a variable\nggplot(mtcars, aes(sample = mpg)) +\n  stat_qq() +\n  stat_qq_line()\nggplot(mtcars, aes(sample = mpg, colour = factor(cyl))) +\n  stat_qq() +\n  stat_qq_line()\n}",
            "geom_quantile": "m <-\n  ggplot(mpg, aes(displ, 1 / hwy)) +\n  geom_point()\nm + geom_quantile()\nm + geom_quantile(quantiles = 0.5)\nq10 <- seq(0.05, 0.95, by = 0.05)\nm + geom_quantile(quantiles = q10)\n\n# You can also use rqss to fit smooth quantiles\nm + geom_quantile(method = \"rqss\")\n# Note that rqss doesn't pick a smoothing constant automatically, so\n# you'll need to tweak lambda yourself\nm + geom_quantile(method = \"rqss\", lambda = 0.1)\n\n# Set aesthetics to fixed value\nm + geom_quantile(colour = \"red\", linewidth = 2, alpha = 0.5)",
            "geom_ribbon": "# Generate data\nhuron <- data.frame(year = 1875:1972, level = as.vector(LakeHuron))\nh <- ggplot(huron, aes(year))\n\nh + geom_ribbon(aes(ymin=0, ymax=level))\nh + geom_area(aes(y = level))\n\n# Orientation cannot be deduced by mapping, so must be given explicitly for\n# flipped orientation\nh + geom_area(aes(x = level, y = year), orientation = \"y\")\n\n# Add aesthetic mappings\nh +\n  geom_ribbon(aes(ymin = level - 1, ymax = level + 1), fill = \"grey70\") +\n  geom_line(aes(y = level))\n\n# The underlying stat_align() takes care of unaligned data points\ndf <- data.frame(\n  g = c(\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"),\n  x = c(1, 3, 5, 2, 4, 6),\n  y = c(2, 5, 1, 3, 6, 7)\n)\na <- ggplot(df, aes(x, y, fill = g)) +\n  geom_area()\n\n# Two groups have points on different X values.\na + geom_point(size = 8) + facet_grid(g ~ .)\n\n# stat_align() interpolates and aligns the value so that the areas can stack\n# properly.\na + geom_point(stat = \"align\", position = \"stack\", size = 8)\n\n# To turn off the alignment, the stat can be set to \"identity\"\nggplot(df, aes(x, y, fill = g)) +\n  geom_area(stat = \"identity\")",
            "geom_rug": "p <- ggplot(mtcars, aes(wt, mpg)) +\n  geom_point()\np\np + geom_rug()\np + geom_rug(sides=\"b\")    # Rug on bottom only\np + geom_rug(sides=\"trbl\") # All four sides\n\n# Use jittering to avoid overplotting for smaller datasets\nggplot(mpg, aes(displ, cty)) +\n  geom_point() +\n  geom_rug()\n\nggplot(mpg, aes(displ, cty)) +\n  geom_jitter() +\n  geom_rug(alpha = 1/2, position = \"jitter\")\n\n# move the rug tassels to outside the plot\n# remember to set clip = \"off\".\np +\n  geom_rug(outside = TRUE) +\n  coord_cartesian(clip = \"off\")\n\n# set sides to top right, and then move the margins\np +\n  geom_rug(outside = TRUE, sides = \"tr\") +\n  coord_cartesian(clip = \"off\") +\n  theme(plot.margin = margin_auto(1, unit = \"cm\"))\n\n# increase the line length and\n# expand axis to avoid overplotting\np +\n  geom_rug(length = unit(0.05, \"npc\")) +\n  scale_y_continuous(expand = c(0.1, 0.1))",
            "geom_segment": "b <- ggplot(mtcars, aes(wt, mpg)) +\n  geom_point()\n\ndf <- data.frame(x1 = 2.62, x2 = 3.57, y1 = 21.0, y2 = 15.0)\nb +\n geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2, colour = \"curve\"), data = df) +\n geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2, colour = \"segment\"), data = df)\n\nb + geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2), data = df, curvature = -0.2)\nb + geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2), data = df, curvature = 1)\nb + geom_curve(\n  aes(x = x1, y = y1, xend = x2, yend = y2),\n  data = df,\n  arrow = arrow(length = unit(0.03, \"npc\"))\n)\n\nif (requireNamespace('maps', quietly = TRUE)) {\nggplot(seals, aes(long, lat)) +\n  geom_segment(aes(xend = long + delta_long, yend = lat + delta_lat),\n    arrow = arrow(length = unit(0.1,\"cm\"))) +\n  borders(\"state\")\n}\n\n# Use lineend and linejoin to change the style of the segments\ndf2 <- expand.grid(\n  lineend = c('round', 'butt', 'square'),\n  linejoin = c('round', 'mitre', 'bevel'),\n  stringsAsFactors = FALSE\n)\ndf2 <- data.frame(df2, y = 1:9)\nggplot(df2, aes(x = 1, y = y, xend = 2, yend = y, label = paste(lineend, linejoin))) +\n  geom_segment(\n     lineend = df2$lineend, linejoin = df2$linejoin,\n     size = 3, arrow = arrow(length = unit(0.3, \"inches\"))\n  ) +\n  geom_text(hjust = 'outside', nudge_x = -0.2) +\n  xlim(0.5, 2)\n\n# You can also use geom_segment to recreate plot(type = \"h\") :\nset.seed(1)\ncounts <- as.data.frame(table(x = rpois(100,5)))\ncounts$x <- as.numeric(as.character(counts$x))\nwith(counts, plot(x, Freq, type = \"h\", lwd = 10))\n\nggplot(counts, aes(x, Freq)) +\n  geom_segment(aes(xend = x, yend = 0), linewidth = 10, lineend = \"butt\")",
            "geom_smooth": "ggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth()\n\n# If you need the fitting to be done along the y-axis set the orientation\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth(orientation = \"y\")\n\n# Use span to control the \"wiggliness\" of the default loess smoother.\n# The span is the fraction of points used to fit each local regression:\n# small numbers make a wigglier curve, larger numbers make a smoother curve.\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth(span = 0.3)\n\n# Instead of a loess smooth, you can use any other modelling function:\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE)\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth(method = lm, formula = y ~ splines::bs(x, 3), se = FALSE)\n\n# Smooths are automatically fit to each group (defined by categorical\n# aesthetics or the group aesthetic) and for each facet.\n\nggplot(mpg, aes(displ, hwy, colour = class)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = lm)\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth(span = 0.8) +\n  facet_wrap(~drv)\n\n\\donttest{\nbinomial_smooth <- function(...) {\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), ...)\n}\n# To fit a logistic regression, you need to coerce the values to\n# a numeric vector lying between 0 and 1.\nggplot(rpart::kyphosis, aes(Age, Kyphosis)) +\n  geom_jitter(height = 0.05) +\n  binomial_smooth()\n\nggplot(rpart::kyphosis, aes(Age, as.numeric(Kyphosis) - 1)) +\n  geom_jitter(height = 0.05) +\n  binomial_smooth()\n\nggplot(rpart::kyphosis, aes(Age, as.numeric(Kyphosis) - 1)) +\n  geom_jitter(height = 0.05) +\n  binomial_smooth(formula = y ~ splines::ns(x, 2))\n\n# But in this case, it's probably better to fit the model yourself\n# so you can exercise more control and see whether or not it's a good model.\n}",
            "geom_spoke": "df <- expand.grid(x = 1:10, y=1:10)\n\nset.seed(1)\ndf$angle <- runif(100, 0, 2*pi)\ndf$speed <- runif(100, 0, sqrt(0.1 * df$x))\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  geom_spoke(aes(angle = angle), radius = 0.5)\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  geom_spoke(aes(angle = angle, radius = speed))",
            "geom_text": "p <- ggplot(mtcars, aes(wt, mpg, label = rownames(mtcars)))\n\np + geom_text()\n# Avoid overlaps\np + geom_text(check_overlap = TRUE)\n# Labels with background\np + geom_label()\n# Change size of the label\np + geom_text(size = 10)\n\n# Set aesthetics to fixed value\np +\n  geom_point() +\n  geom_text(hjust = 0, nudge_x = 0.05)\np +\n  geom_point() +\n  geom_text(vjust = 0, nudge_y = 0.5)\np +\n  geom_point() +\n  geom_text(angle = 45)\n\\dontrun{\n# Doesn't work on all systems\np +\n  geom_text(family = \"Times New Roman\")\n}\n\n# Add aesthetic mappings\np + geom_text(aes(colour = factor(cyl)))\np + geom_text(aes(colour = factor(cyl))) +\n  scale_colour_hue(l = 40)\np + geom_label(aes(fill = factor(cyl)), colour = \"white\", fontface = \"bold\")\n\n# Scale size of text, and change legend key glyph from a to point\np + geom_text(aes(size = wt), key_glyph = \"point\")\n# Scale height of text, rather than sqrt(height)\np +\n  geom_text(aes(size = wt), key_glyph = \"point\") +\n  scale_radius(range = c(3,6))\n\n# You can display expressions by setting parse = TRUE.  The\n# details of the display are described in ?plotmath, but note that\n# geom_text uses strings, not expressions.\np +\n  geom_text(\n    aes(label = paste(wt, \"^(\", cyl, \")\", sep = \"\")),\n    parse = TRUE\n  )\n\n# Add a text annotation\np +\n  geom_text() +\n  annotate(\n    \"text\", label = \"plot mpg vs. wt\",\n    x = 2, y = 15, size = 8, colour = \"red\"\n  )\n\n\\donttest{\n# Aligning labels and bars --------------------------------------------------\ndf <- data.frame(\n  x = factor(c(1, 1, 2, 2)),\n  y = c(1, 3, 2, 1),\n  grp = c(\"a\", \"b\", \"a\", \"b\")\n)\n\n# ggplot2 doesn't know you want to give the labels the same virtual width\n# as the bars:\nggplot(data = df, aes(x, y, group = grp)) +\n  geom_col(aes(fill = grp), position = \"dodge\") +\n  geom_text(aes(label = y), position = \"dodge\")\n# So tell it:\nggplot(data = df, aes(x, y, group = grp)) +\n  geom_col(aes(fill = grp), position = \"dodge\") +\n  geom_text(aes(label = y), position = position_dodge(0.9))\n# You can't nudge and dodge text, so instead adjust the y position\nggplot(data = df, aes(x, y, group = grp)) +\n  geom_col(aes(fill = grp), position = \"dodge\") +\n  geom_text(\n    aes(label = y, y = y + 0.05),\n    position = position_dodge(0.9),\n    vjust = 0\n  )\n\n# To place text in the middle of each bar in a stacked barplot, you\n# need to set the vjust parameter of position_stack()\nggplot(data = df, aes(x, y, group = grp)) +\n geom_col(aes(fill = grp)) +\n geom_text(aes(label = y), position = position_stack(vjust = 0.5))\n\n# Justification -------------------------------------------------------------\ndf <- data.frame(\n  x = c(1, 1, 2, 2, 1.5),\n  y = c(1, 2, 1, 2, 1.5),\n  text = c(\"bottom-left\", \"top-left\", \"bottom-right\", \"top-right\", \"center\")\n)\nggplot(df, aes(x, y)) +\n  geom_text(aes(label = text))\nggplot(df, aes(x, y)) +\n  geom_text(aes(label = text), vjust = \"inward\", hjust = \"inward\")\n}",
            "geom_tile": "# The most common use for rectangles is to draw a surface. You always want\n# to use geom_raster here because it's so much faster, and produces\n# smaller output when saving to PDF\nggplot(faithfuld, aes(waiting, eruptions)) +\n geom_raster(aes(fill = density))\n\n# Interpolation smooths the surface & is most helpful when rendering images.\nggplot(faithfuld, aes(waiting, eruptions)) +\n geom_raster(aes(fill = density), interpolate = TRUE)\n\n# If you want to draw arbitrary rectangles, use geom_tile() or geom_rect()\ndf <- data.frame(\n  x = rep(c(2, 5, 7, 9, 12), 2),\n  y = rep(c(1, 2), each = 5),\n  z = factor(rep(1:5, each = 2)),\n  w = rep(diff(c(0, 4, 6, 8, 10, 14)), 2)\n)\nggplot(df, aes(x, y)) +\n  geom_tile(aes(fill = z), colour = \"grey50\")\nggplot(df, aes(x, y, width = w)) +\n  geom_tile(aes(fill = z), colour = \"grey50\")\nggplot(df, aes(xmin = x - w / 2, xmax = x + w / 2, ymin = y, ymax = y + 1)) +\n  geom_rect(aes(fill = z), colour = \"grey50\")\n\n\\donttest{\n# Justification controls where the cells are anchored\ndf <- expand.grid(x = 0:5, y = 0:5)\nset.seed(1)\ndf$z <- runif(nrow(df))\n# default is compatible with geom_tile()\nggplot(df, aes(x, y, fill = z)) +\n  geom_raster()\n# zero padding\nggplot(df, aes(x, y, fill = z)) +\n  geom_raster(hjust = 0, vjust = 0)\n\n# Inspired by the image-density plots of Ken Knoblauch\ncars <- ggplot(mtcars, aes(mpg, factor(cyl)))\ncars + geom_point()\ncars + stat_bin_2d(aes(fill = after_stat(count)), binwidth = c(3,1))\ncars + stat_bin_2d(aes(fill = after_stat(density)), binwidth = c(3,1))\n\ncars +\n  stat_density(\n    aes(fill = after_stat(density)),\n    geom = \"raster\",\n    position = \"identity\"\n   )\ncars +\n  stat_density(\n    aes(fill = after_stat(count)),\n    geom = \"raster\",\n    position = \"identity\"\n  )\n}",
            "geom_violin": "p <- ggplot(mtcars, aes(factor(cyl), mpg))\np + geom_violin()\n\n# Orientation follows the discrete axis\nggplot(mtcars, aes(mpg, factor(cyl))) +\n  geom_violin()\n\n\\donttest{\np + geom_violin() + geom_jitter(height = 0, width = 0.1)\n\n# Scale maximum width proportional to sample size:\np + geom_violin(scale = \"count\")\n\n# Scale maximum width to 1 for all violins:\np + geom_violin(scale = \"width\")\n\n# Default is to trim violins to the range of the data. To disable:\np + geom_violin(trim = FALSE)\n\n# Use a smaller bandwidth for closer density fit (default is 1).\np + geom_violin(adjust = .5)\n\n# Add aesthetic mappings\n# Note that violins are automatically dodged when any aesthetic is\n# a factor\np + geom_violin(aes(fill = cyl))\np + geom_violin(aes(fill = factor(cyl)))\np + geom_violin(aes(fill = factor(vs)))\np + geom_violin(aes(fill = factor(am)))\n\n# Set aesthetics to fixed value\np + geom_violin(fill = \"grey80\", colour = \"#3366FF\")\n\n# Show quartiles\np + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75))\n\n# Scales vs. coordinate transforms -------\nif (require(\"ggplot2movies\")) {\n# Scale transformations occur before the density statistics are computed.\n# Coordinate transformations occur afterwards.  Observe the effect on the\n# number of outliers.\nm <- ggplot(movies, aes(y = votes, x = rating, group = cut_width(rating, 0.5)))\nm + geom_violin()\nm +\n  geom_violin() +\n  scale_y_log10()\nm +\n  geom_violin() +\n  coord_trans(y = \"log10\")\nm +\n  geom_violin() +\n  scale_y_log10() + coord_trans(y = \"log10\")\n\n# Violin plots with continuous x:\n# Use the group aesthetic to group observations in violins\nggplot(movies, aes(year, budget)) +\n  geom_violin()\nggplot(movies, aes(year, budget)) +\n  geom_violin(aes(group = cut_width(year, 10)), scale = \"width\")\n}\n}",
            "get_alt_text": "p <- ggplot(mpg, aes(displ, hwy)) +\n  geom_point()\n\n# Returns an empty string\nget_alt_text(p)\n\n# A user provided alt text\np <- p + labs(\n  alt = paste(\"A scatterplot showing the negative correlation between engine\",\n              \"displacement as a function of highway miles per gallon\")\n)\n\nget_alt_text(p)",
            "get_geom_defaults": "# Using a function\nget_geom_defaults(geom_raster)\n\n# Using a layer includes static aesthetics as default\nget_geom_defaults(geom_tile(fill = \"white\"))\n\n# Using a class name\nget_geom_defaults(\"density_2d\")\n\n# Using a class\nget_geom_defaults(GeomPoint)\n\n# Changed theme\nget_geom_defaults(\"point\", theme(geom = element_geom(ink = \"purple\")))",
            "get_guide_data": "# A standard plot\np <- ggplot(mtcars) +\n  aes(mpg, disp, colour = drat, size = drat) +\n  geom_point() +\n  facet_wrap(vars(cyl), scales = \"free_x\")\n\n# Guide information for legends\nget_guide_data(p, \"size\")\n\n# Note that legend guides can be merged\nmerged <- p + guides(colour = \"legend\")\nget_guide_data(merged, \"size\")\n\n# Guide information for positions\nget_guide_data(p, \"x\", panel = 2)\n\n# Coord polar doesn't support proper guides, so we get a list\npolar <- p + coord_polar()\nget_guide_data(polar, \"theta\", panel = 2)",
            "get_strip_labels": "# Basic plot\np <- ggplot(mpg, aes(displ, hwy)) +\n  geom_point()\n\nget_strip_labels(p) # empty facets\nget_strip_labels(p + facet_wrap(year ~ cyl))\nget_strip_labels(p + facet_grid(year ~ cyl))",
            "get_theme": "p <- ggplot(mtcars, aes(mpg, wt)) +\n  geom_point()\np\n\n# Use set_theme() to completely override the current theme.\n# update_theme() and replace_theme() are similar except they\n# apply directly to the current/active theme.\n# update_theme() modifies a particular element of the current theme.\n# Here we have the old theme so we can later restore it.\n# Note that the theme is applied when the plot is drawn, not\n# when it is created.\nold <- set_theme(theme_bw())\np\n\nset_theme(old)\nupdate_theme(panel.grid.minor = element_line(colour = \"red\"))\np\n\nset_theme(old)\nreplace_theme(panel.grid.minor = element_line(colour = \"red\"))\np\n\nset_theme(old)\np\n\n\n# Modifying theme objects -----------------------------------------\n# You can use + and \\%+replace\\% to modify a theme object.\n# They differ in how they deal with missing arguments in\n# the theme elements.\n\nadd_el <- theme_grey() +\n  theme(text = element_text(family = \"Times\"))\nadd_el$text\n\nrep_el <- theme_grey() \\%+replace\\%\n  theme(text = element_text(family = \"Times\"))\nrep_el$text",
            "gg-add": "base <-\n ggplot(mpg, aes(displ, hwy)) +\n geom_point()\nbase + geom_smooth()\n\n# To override the data, you must use \\%+\\%\nbase \\%+\\% subset(mpg, fl == \"p\")\n\n# Alternatively, you can add multiple components with a list.\n# This can be useful to return from a function.\nbase + list(subset(mpg, fl == \"p\"), geom_smooth())",
            "ggplot": "# Create a data frame with some sample data, then create a data frame\n# containing the mean value for each group in the sample data.\nset.seed(1)\n\nsample_df <- data.frame(\n  group = factor(rep(letters[1:3], each = 10)),\n  value = rnorm(30)\n)\n\ngroup_means_df <- setNames(\n  aggregate(value ~ group, sample_df, mean),\n  c(\"group\", \"group_mean\")\n)\n\n# The following three code blocks create the same graphic, each using one\n# of the three patterns specified above. In each graphic, the sample data\n# are plotted in the first layer and the group means data frame is used to\n# plot larger red points on top of the sample data in the second layer.\n\n# Pattern 1\n# Both the `data` and `mapping` arguments are passed into the `ggplot()`\n# call. Those arguments are omitted in the first `geom_point()` layer\n# because they get passed along from the `ggplot()` call. Note that the\n# second `geom_point()` layer re-uses the `x = group` aesthetic through\n# that mechanism but overrides the y-position aesthetic.\nggplot(data = sample_df, mapping = aes(x = group, y = value)) +\n  geom_point() +\n  geom_point(\n    mapping = aes(y = group_mean), data = group_means_df,\n    colour = 'red', size = 3\n  )\n\n# Pattern 2\n# Same plot as above, passing only the `data` argument into the `ggplot()`\n# call. The `mapping` arguments are now required in each `geom_point()`\n# layer because there is no `mapping` argument passed along from the\n# `ggplot()` call.\nggplot(data = sample_df) +\n  geom_point(mapping = aes(x = group, y = value)) +\n  geom_point(\n    mapping = aes(x = group, y = group_mean), data = group_means_df,\n    colour = 'red', size = 3\n  )\n\n# Pattern 3\n# Same plot as above, passing neither the `data` or `mapping` arguments\n# into the `ggplot()` call. Both those arguments are now required in\n# each `geom_point()` layer. This pattern can be particularly useful when\n# creating more complex graphics with many layers using data from multiple\n# data frames.\nggplot() +\n  geom_point(mapping = aes(x = group, y = value), data = sample_df) +\n  geom_point(\n    mapping = aes(x = group, y = group_mean), data = group_means_df,\n    colour = 'red', size = 3\n  )"
        }
    },
    "rlang": {
        "description": "A toolbox for working with base types, core R features\n  like the condition system, and core 'Tidyverse' features like tidy\n  evaluation.",
        "examples": {
            "abort": "# These examples are guarded to avoid throwing errors\nif (FALSE) {\n\n# Signal an error with a message just like stop():\nabort(\"The error message.\")\n\n\n# Unhandled errors are saved automatically by `abort()` and can be\n# retrieved with `last_error()`. The error prints with a simplified\n# backtrace:\nf <- function() try(g())\ng <- function() evalq(h())\nh <- function() abort(\"Tilt.\")\nlast_error()\n\n# Use `summary()` to print the full backtrace and the condition fields:\nsummary(last_error())\n\n\n# Give a class to the error:\nabort(\"The error message\", \"mypkg_bad_error\")\n\n# This allows callers to handle the error selectively\ntryCatch(\n  mypkg_function(),\n  mypkg_bad_error = function(err) {\n    warn(conditionMessage(err)) # Demote the error to a warning\n    NA                          # Return an alternative value\n  }\n)\n\n# You can also specify metadata that will be stored in the condition:\nabort(\"The error message.\", \"mypkg_bad_error\", data = 1:10)\n\n# This data can then be consulted by user handlers:\ntryCatch(\n  mypkg_function(),\n  mypkg_bad_error = function(err) {\n    # Compute an alternative return value with the data:\n    recover_error(err$data)\n  }\n)\n\n\n# If you call low-level APIs it may be a good idea to create a\n# chained error with the low-level error wrapped in a more\n# user-friendly error. Use `try_fetch()` to fetch errors of a given\n# class and rethrow them with the `parent` argument of `abort()`:\nfile <- \"http://foo.bar/baz\"\ntry(\n  try_fetch(\n    download(file),\n    error = function(err) {\n      msg <- sprintf(\"Can't download `\\%s`\", file)\n      abort(msg, parent = err)\n  })\n)\n\n# You can also hard-code the call when it's not easy to\n# forward it from the caller\n f <- function() {\n  abort(\"my message\", call = call(\"my_function\"))\n}\ng <- function() {\n  f()\n}\n# Shows that the error occured in `my_function()`\ntry(g())\n\n}",
            "are_na": "# are_na() is vectorised and works regardless of the type\nare_na(c(1, 2, NA))\nare_na(c(1L, NA, 3L))\n\n# is_na() checks for scalar input and works for all types\nis_na(NA)\nis_na(na_dbl)\nis_na(character(0))\n\n# There are typed versions as well:\nis_lgl_na(NA)\nis_lgl_na(na_dbl)",
            "arg_match": "fn <- function(x = c(\"foo\", \"bar\")) arg_match(x)\nfn(\"bar\")\n\n# Throws an informative error for mismatches:\ntry(fn(\"b\"))\ntry(fn(\"baz\"))\n\n# Use the bare-bones version with explicit values for speed:\narg_match0(\"bar\", c(\"foo\", \"bar\", \"baz\"))\n\n# For convenience:\nfn1 <- function(x = c(\"bar\", \"baz\", \"foo\")) fn3(x)\nfn2 <- function(x = c(\"baz\", \"bar\", \"foo\")) fn3(x)\nfn3 <- function(x) arg_match0(x, c(\"foo\", \"bar\", \"baz\"))\nfn1()\nfn2(\"bar\")\ntry(fn3(\"zoo\"))",
            "as_closure": "# Primitive functions are regularised as closures\nas_closure(list)\nas_closure(\"list\")\n\n# Operators have `.x` and `.y` as arguments, just like lambda\n# functions created with the formula syntax:\nas_closure(`+`)\nas_closure(`~`)",
            "as_data_mask": "# Evaluating in a tidy evaluation environment enables all tidy\n# features:\nmask <- as_data_mask(mtcars)\neval_tidy(quo(letters), mask)\n\n# You can install new pronouns in the mask:\nmask$.pronoun <- as_data_pronoun(list(foo = \"bar\", baz = \"bam\"))\neval_tidy(quo(.pronoun$foo), mask)\n\n# In some cases the data mask can leak to the user, for example if\n# a function or formula is created in the data mask environment:\ncyl <- \"user variable from the context\"\nfn <- eval_tidy(quote(function() cyl), mask)\nfn()\n\n# If new objects are created in the mask, they persist in the\n# subsequent calls:\neval_tidy(quote(new <- cyl + am), mask)\neval_tidy(quote(new * 2), mask)\n\n\n# In some cases your data mask is a whole chain of environments\n# rather than a single environment. You'll have to use\n# `new_data_mask()` and let it know about the bottom of the mask\n# (the last child of the environment chain) and the topmost parent.\n\n# A common situation where you'll want a multiple-environment mask\n# is when you include functions in your mask. In that case you'll\n# put functions in the top environment and data in the bottom. This\n# will prevent the data from overwriting the functions.\ntop <- new_environment(list(`+` = base::paste, c = base::paste))\n\n# Let's add a middle environment just for sport:\nmiddle <- env(top)\n\n# And finally the bottom environment containing data:\nbottom <- env(middle, a = \"a\", b = \"b\", c = \"c\")\n\n# We can now create a mask by supplying the top and bottom\n# environments:\nmask <- new_data_mask(bottom, top = top)\n\n# This data mask can be passed to eval_tidy() instead of a list or\n# data frame:\neval_tidy(quote(a + b + c), data = mask)\n\n# Note how the function `c()` and the object `c` are looked up\n# properly because of the multi-level structure:\neval_tidy(quote(c(a, b, c)), data = mask)\n\n# new_data_mask() does not create data pronouns, but\n# data pronouns can be added manually:\nmask$.fns <- as_data_pronoun(top)\n\n# The `.data` pronoun should generally be created from the\n# mask. This will ensure data is looked up throughout the whole\n# ancestry. Only non-function objects are looked up from this\n# pronoun:\nmask$.data <- as_data_pronoun(mask)\nmask$.data$c\n\n# Now we can reference values with the pronouns:\neval_tidy(quote(c(.data$a, .data$b, .data$c)), data = mask)",
            "as_environment": "# Coerce a named vector to an environment:\nenv <- as_environment(mtcars)\n\n# By default it gets the empty environment as parent:\nidentical(env_parent(env), empty_env())\n\n\n# With strings it is a handy shortcut for pkg_env():\nas_environment(\"base\")\nas_environment(\"rlang\")\n\n# With NULL it returns the empty environment:\nas_environment(NULL)",
            "as_function": "f <- as_function(~ .x + 1)\nf(10)\n\ng <- as_function(~ -1 * .)\ng(4)\n\nh <- as_function(~ .x - .y)\nh(6, 3)\n\n# Functions created from a formula have a special class:\nis_lambda(f)\nis_lambda(as_function(function() \"foo\"))",
            "as_label": "# as_label() is useful with quoted expressions:\nas_label(expr(foo(bar)))\n\nas_label(expr(foobar))\n\n# It works with any R object. This is also useful for quoted\n# arguments because the user might unquote constant objects:\nas_label(1:3)\n\nas_label(base::list)",
            "as_name": "# Let's create some symbols:\nfoo <- quote(foo)\nbar <- sym(\"bar\")\n\n# as_name() converts symbols to strings:\nfoo\nas_name(foo)\n\ntypeof(bar)\ntypeof(as_name(bar))\n\n# as_name() unwraps quosured symbols automatically:\nas_name(quo(foo))",
            "as_string": "# Let's create some symbols:\nfoo <- quote(foo)\nbar <- sym(\"bar\")\n\n# as_string() converts symbols to strings:\nfoo\nas_string(foo)\n\ntypeof(bar)\ntypeof(as_string(bar))",
            "as_utf8_character": "# Let's create a string marked as UTF-8 (which is guaranteed by the\n# Unicode escaping in the string):\nutf8 <- \"caf\\uE9\"\nEncoding(utf8)\ncharToRaw(utf8)",
            "box": "boxed <- new_box(letters, \"mybox\")\nis_box(boxed)\nis_box(boxed, \"mybox\")\nis_box(boxed, \"otherbox\")\n\nunbox(boxed)\n\n# as_box() avoids double-boxing:\nboxed2 <- as_box(boxed, \"mybox\")\nboxed2\nunbox(boxed2)\n\n# Compare to:\nboxed_boxed <- new_box(boxed, \"mybox\")\nboxed_boxed\nunbox(unbox(boxed_boxed))\n\n# Use `as_box_if()` with a predicate if you need to ensure a box\n# only for a subset of values:\nas_box_if(NULL, is_null, \"null_box\")\nas_box_if(\"foo\", is_null, \"null_box\")",
            "bytes-class": "parse_bytes(\"1\")\nparse_bytes(\"1K\")\nparse_bytes(\"1Kb\")\nparse_bytes(\"1KiB\")\nparse_bytes(\"1MB\")\n\nparse_bytes(\"1KB\") < \"1MB\"\n\nsum(parse_bytes(c(\"1MB\", \"5MB\", \"500KB\")))",
            "call2": "# fn can either be a string, a symbol or a call\ncall2(\"f\", a = 1)\ncall2(quote(f), a = 1)\ncall2(quote(f()), a = 1)\n\n#' Can supply arguments individually or in a list\ncall2(quote(f), a = 1, b = 2)\ncall2(quote(f), !!!list(a = 1, b = 2))\n\n# Creating namespaced calls is easy:\ncall2(\"fun\", arg = quote(baz), .ns = \"mypkg\")\n\n# Empty arguments are preserved:\ncall2(\"[\", quote(x), , drop = )",
            "call_args": "call <- quote(f(a, b))\n\n# Subsetting a call returns the arguments converted to a language\n# object:\ncall[-1]\n\n# On the other hand, call_args() returns a regular list that is\n# often easier to work with:\nstr(call_args(call))\n\n# When the arguments are unnamed, a vector of empty strings is\n# supplied (rather than NULL):\ncall_args_names(call)",
            "call_inspect": "# When you call it directly, it simply returns what you typed\ncall_inspect(foo(bar), \"\" \\%>\\% identity())\n\n# Pass `call_inspect` to functionals like `lapply()` or `map()` to\n# inspect the calls they create around the supplied function\nlapply(1:3, call_inspect)",
            "call_match": "# `call_match()` supports matching missing arguments to their\n# defaults\nfn <- function(x = \"default\") fn\ncall_match(quote(fn()), fn)\ncall_match(quote(fn()), fn, defaults = TRUE)",
            "call_modify": "call <- quote(mean(x, na.rm = TRUE))\n\n# Modify an existing argument\ncall_modify(call, na.rm = FALSE)\ncall_modify(call, x = quote(y))\n\n# Remove an argument\ncall_modify(call, na.rm = zap())\n\n# Add a new argument\ncall_modify(call, trim = 0.1)\n\n# Add an explicit missing argument:\ncall_modify(call, na.rm = )\n\n# Supply a list of new arguments with `!!!`\nnewargs <- list(na.rm = zap(), trim = 0.1)\ncall <- call_modify(call, !!!newargs)\ncall\n\n# Remove multiple arguments by splicing zaps:\nnewargs <- rep_named(c(\"na.rm\", \"trim\"), list(zap()))\ncall <- call_modify(call, !!!newargs)\ncall\n\n\n# Modify the `...` arguments as if it were a named argument:\ncall <- call_modify(call, ... = )\ncall\n\ncall <- call_modify(call, ... = zap())\ncall\n\n\n# When you're working with a user-supplied call, standardise it\n# beforehand in case it includes unmatched arguments:\nuser_call <- quote(matrix(x, nc = 3))\ncall_modify(user_call, ncol = 1)\n\n# `call_match()` applies R's argument matching rules. Matching\n# ensures you're modifying the intended argument.\nuser_call <- call_match(user_call, matrix)\nuser_call\ncall_modify(user_call, ncol = 1)\n\n\n# By default, arguments with the same name are kept. This has\n# subtle implications, for instance you can move an argument to\n# last position by removing it and remapping it:\ncall <- quote(foo(bar = , baz))\ncall_modify(call, bar = zap(), bar = missing_arg())\n\n# You can also choose to keep only the first or last homonym\n# arguments:\nargs <-  list(bar = zap(), bar = missing_arg())\ncall_modify(call, !!!args, .homonyms = \"first\")\ncall_modify(call, !!!args, .homonyms = \"last\")",
            "call_name": "# Is the function named?\nis_call_simple(quote(foo()))\nis_call_simple(quote(foo[[1]]()))\n\n# Is the function namespaced?\nis_call_simple(quote(list()), ns = TRUE)\nis_call_simple(quote(base::list()), ns = TRUE)\n\n# Extract the function name from quoted calls:\ncall_name(quote(foo(bar)))\ncall_name(quo(foo(bar)))\n\n# Namespaced calls are correctly handled:\ncall_name(quote(base::matrix(baz)))\n\n# Anonymous and subsetted functions return NULL:\ncall_name(quote(foo$bar()))\ncall_name(quote(foo[[bar]]()))\ncall_name(quote(foo()()))\n\n# Extract namespace of a call with call_ns():\ncall_ns(quote(base::bar()))\n\n# If not namespaced, call_ns() returns NULL:\ncall_ns(quote(bar()))",
            "caller_arg": "arg_checker <- function(x, arg = caller_arg(x), call = caller_env()) {\n  cli::cli_abort(\"{.arg {arg}} must be a thingy.\", arg = arg, call = call)\n}\n\nmy_function <- function(my_arg) {\n  arg_checker(my_arg)\n}\n\ntry(my_function(NULL))",
            "catch_cnd": "catch_cnd(10)\ncatch_cnd(abort(\"an error\"))\ncatch_cnd(signal(\"my_condition\", message = \"a condition\"))",
            "check_dots_empty": "f <- function(x, ..., foofy = 8) {\n  check_dots_empty()\n  x + foofy\n}\n\n# This fails because `foofy` can't be matched positionally\ntry(f(1, 4))\n\n# This fails because `foofy` can't be matched partially by name\ntry(f(1, foof = 4))\n\n# Thanks to `...`, it must be matched exactly\nf(1, foofy = 4)",
            "check_dots_unnamed": "f <- function(..., foofy = 8) {\n  check_dots_unnamed()\n  c(...)\n}\n\nf(1, 2, 3, foofy = 4)\n\ntry(f(1, 2, 3, foof = 4))",
            "check_dots_used": "f <- function(...) {\n  check_dots_used()\n  g(...)\n}\n\ng <- function(x, y, ...) {\n  x + y\n}\nf(x = 1, y = 2)\n\ntry(f(x = 1, y = 2, z = 3))\n\ntry(f(x = 1, y = 2, 3, 4, 5))\n\n# Use an `error` handler to handle the error differently.\n# For instance to demote the error to a warning:\nfn <- function(...) {\n  check_dots_empty(\n    error = function(cnd) {\n      warning(cnd)\n    }\n  )\n  \"out\"\n}\nfn()",
            "check_exclusive": "f <- function(x, y) {\n  switch(\n    check_exclusive(x, y),\n    x = message(\"`x` was supplied.\"),\n    y = message(\"`y` was supplied.\")\n  )\n}\n\n# Supplying zero or multiple arguments is forbidden\ntry(f())\ntry(f(NULL, NULL))\n\n# The user must supply one of the mutually exclusive arguments\nf(NULL)\nf(y = NULL)\n\n\n# With `.require` you can allow zero arguments\nf <- function(x, y) {\n  switch(\n    check_exclusive(x, y, .require = FALSE),\n    x = message(\"`x` was supplied.\"),\n    y = message(\"`y` was supplied.\"),\n    message(\"No arguments were supplied\")\n  )\n}\nf()",
            "check_required": "f <- function(x)  {\n  check_required(x)\n}\n\n# Fails because `x` is not supplied\ntry(f())\n\n# Succeeds\nf(NULL)",
            "chr_unserialise_unicode": "ascii <- \"<U+5E78>\"\nchr_unserialise_unicode(ascii)\n\nidentical(chr_unserialise_unicode(ascii), \"\\u5e78\")",
            "cnd": "# Create a condition inheriting only from the S3 class \"foo\":\ncnd <- cnd(\"foo\")\n\n# Signal the condition to potential handlers. Since this is a bare\n# condition the signal has no effect if no handlers are set up:\ncnd_signal(cnd)\n\n# When a relevant handler is set up, the signal transfers control\n# to the handler\nwith_handlers(cnd_signal(cnd), foo = function(c) \"caught!\")\ntryCatch(cnd_signal(cnd), foo = function(c) \"caught!\")",
            "cnd_muffle": "fn <- function() {\n  inform(\"Beware!\", \"my_particular_msg\")\n  inform(\"On your guard!\")\n  \"foobar\"\n}\n\n# Let's install a muffling handler for the condition thrown by `fn()`.\n# This will suppress all `my_particular_wng` warnings but let other\n# types of warnings go through:\nwith_handlers(fn(),\n  my_particular_msg = calling(function(cnd) {\n    inform(\"Dealt with this particular message\")\n    cnd_muffle(cnd)\n  })\n)\n\n# Note how execution of `fn()` continued normally after dealing\n# with that particular message.\n\n# cnd_muffle() can also be passed to with_handlers() as a calling\n# handler:\nwith_handlers(fn(),\n  my_particular_msg = calling(cnd_muffle)\n)",
            "cnd_signal": "# The type of signal depends on the class. If the condition\n# inherits from \"warning\", a warning is issued:\ncnd <- warning_cnd(\"my_warning_class\", message = \"This is a warning\")\ncnd_signal(cnd)\n\n# If it inherits from \"error\", an error is raised:\ncnd <- error_cnd(\"my_error_class\", message = \"This is an error\")\ntry(cnd_signal(cnd))",
            "cnd_type": "cnd_type(catch_cnd(abort(\"Abort!\")))\ncnd_type(catch_cnd(interrupt()))",
            "defusing-advanced": "# `exprs()` is the plural variant of `expr()`\nexprs(foo, bar, bar)\n\n# `quo()` and `quos()` are the quosure variants of `expr()` and `exprs()`\nquo(foo)\nquos(foo, bar)\n\n# `enexpr()` and `enexprs()` are the naked variants of `enquo()` and `enquos()`\nmy_function1 <- function(arg) enexpr(arg)\nmy_function2 <- function(arg, ...) enexprs(arg, ...)\nmy_function1(1 + 1)\nmy_function2(1 + 1, 10 * 2)\n\n\n# `ensym()` and `ensyms()` are symbol variants of `enexpr()` and `enexprs()`\nmy_function3 <- function(arg) ensym(arg)\nmy_function4 <- function(arg, ...) ensyms(arg, ...)\n\n# The user must supply symbols\nmy_function3(foo)\nmy_function4(foo, bar)\n\n# Complex expressions are an error\ntry(my_function3(1 + 1))\ntry(my_function4(1 + 1, 10 * 2))\n\n\n# `enquo0()` and `enquos0()` disable injection operators\nautomatic_injection <- function(x) enquo(x)\nno_injection <- function(x) enquo0(x)\n\nautomatic_injection(foo(!!!1:3))\nno_injection(foo(!!!1:3))\n\n# Injection can still be done explicitly\ninject(no_injection(foo(!!!1:3)))",
            "done": "done(3)\n\nx <- done(3)\nis_done_box(x)",
            "dots_n": "fn <- function(...) dots_n(..., baz)\nfn(foo, bar)",
            "dots_values": "dots <- dots_values(!!! list(1, 2), 3)\ndots\n\n# Flatten the objects marked as spliced:\nflatten_if(dots, is_spliced)",
            "dyn-dots": "f <- function(...) {\n  out <- list2(...)\n  rev(out)\n}\n\n# Trailing commas are ignored\nf(this = \"that\", )\n\n# Splice lists of arguments with `!!!`\nx <- list(alpha = \"first\", omega = \"last\")\nf(!!!x)\n\n# Inject a name using glue syntax\nif (is_installed(\"glue\")) {\n  nm <- \"key\"\n  f(\"{nm}\" := \"value\")\n  f(\"prefix_{nm}\" := \"value\")\n}",
            "empty_env": "# Create environments with nothing in scope:\nchild_env(empty_env())",
            "englue": "g <- function(var) englue(\"{{ var }}\")\ng(cyl)\ng(1 + 1)\ng(!!letters)\n\n# These are equivalent to\nas_label(quote(cyl))\nas_label(quote(1 + 1))\nas_label(letters)",
            "enquo": "# `enquo()` defuses the expression supplied by your user\nf <- function(arg) {\n  enquo(arg)\n}\n\nf(1 + 1)\n\n# `enquos()` works with arguments and dots. It returns a list of\n# expressions\nf <- function(...) {\n  enquos(...)\n}\n\nf(1 + 1, 2 * 10)\n\n\n# `enquo()` and `enquos()` enable _injection_ and _embracing_ for\n# your users\ng <- function(arg) {\n  f({{ arg }} * 2)\n}\ng(100)\n\ncolumn <- sym(\"cyl\")\ng(!!column)",
            "entrace": "quote({  # Not run\n\n# Set `entrace()` globally in your RProfile\nglobalCallingHandlers(error = rlang::entrace)\n\n# On older R versions which don't feature `globalCallingHandlers`,\n# set the error handler like this:\noptions(error = rlang::entrace)\n\n})",
            "env": "# env() creates a new environment that inherits from the current\n# environment by default\nenv <- env(a = 1, b = \"foo\")\nenv$b\nidentical(env_parent(env), current_env())\n\n# Supply one unnamed argument to inherit from another environment:\nenv <- env(base_env(), a = 1, b = \"foo\")\nidentical(env_parent(env), base_env())\n\n\n# Both env() and child_env() support tidy dots features:\nobjs <- list(b = \"foo\", c = \"bar\")\nenv <- env(a = 1, !!! objs)\nenv$c\n\n# You can also unquote names with the definition operator `:=`\nvar <- \"a\"\nenv <- env(!!var := \"A\")\nenv$a\n\n\n# Use new_environment() to create containers with the empty\n# environment as parent:\nenv <- new_environment()\nenv_parent(env)\n\n# Like other new_ constructors, it takes an object rather than dots:\nnew_environment(list(a = \"foo\", b = \"bar\"))",
            "env_bind": "# env_bind() is a programmatic way of assigning values to symbols\n# with `<-`. We can add bindings in the current environment:\nenv_bind(current_env(), foo = \"bar\")\nfoo\n\n# Or modify those bindings:\nbar <- \"bar\"\nenv_bind(current_env(), bar = \"BAR\")\nbar\n\n# You can remove bindings by supplying zap sentinels:\nenv_bind(current_env(), foo = zap())\ntry(foo)\n\n# Unquote-splice a named list of zaps\nzaps <- rep_named(c(\"foo\", \"bar\"), list(zap()))\nenv_bind(current_env(), !!!zaps)\ntry(bar)\n\n# It is most useful to change other environments:\nmy_env <- env()\nenv_bind(my_env, foo = \"foo\")\nmy_env$foo\n\n# A useful feature is to splice lists of named values:\nvals <- list(a = 10, b = 20)\nenv_bind(my_env, !!!vals, c = 30)\nmy_env$b\nmy_env$c\n\n# You can also unquote a variable referring to a symbol or a string\n# as binding name:\nvar <- \"baz\"\nenv_bind(my_env, !!var := \"BAZ\")\nmy_env$baz\n\n\n# The old values of the bindings are returned invisibly:\nold <- env_bind(my_env, a = 1, b = 2, baz = \"baz\")\nold\n\n# You can restore the original environment state by supplying the\n# old values back:\nenv_bind(my_env, !!!old)\n\n# env_bind_lazy() assigns expressions lazily:\nenv <- env()\nenv_bind_lazy(env, name = { cat(\"forced!\\n\"); \"value\" })\n\n# Referring to the binding will cause evaluation:\nenv$name\n\n# But only once, subsequent references yield the final value:\nenv$name\n\n# You can unquote expressions:\nexpr <- quote(message(\"forced!\"))\nenv_bind_lazy(env, name = !!expr)\nenv$name\n\n\n# By default the expressions are evaluated in the current\n# environment. For instance we can create a local binding and refer\n# to it, even though the variable is bound in a different\n# environment:\nwho <- \"mickey\"\nenv_bind_lazy(env, name = paste(who, \"mouse\"))\nenv$name\n\n# You can specify another evaluation environment with `.eval_env`:\neval_env <- env(who = \"minnie\")\nenv_bind_lazy(env, name = paste(who, \"mouse\"), .eval_env = eval_env)\nenv$name\n\n# Or by unquoting a quosure:\nquo <- local({\n  who <- \"fievel\"\n  quo(paste(who, \"mouse\"))\n})\nenv_bind_lazy(env, name = !!quo)\nenv$name\n\n# You can create active bindings with env_bind_active(). Active\n# bindings execute a function each time they are evaluated:\nfn <- function() {\n  cat(\"I have been called\\n\")\n  rnorm(1)\n}\n\nenv <- env()\nenv_bind_active(env, symbol = fn)\n\n# `fn` is executed each time `symbol` is evaluated or retrieved:\nenv$symbol\nenv$symbol\neval_bare(quote(symbol), env)\neval_bare(quote(symbol), env)\n\n# All arguments are passed to as_function() so you can use the\n# formula shortcut:\nenv_bind_active(env, foo = ~ runif(1))\nenv$foo\nenv$foo",
            "env_binding_lock": "# Bindings are unlocked by default:\nenv <- env(a = \"A\", b = \"B\")\nenv_binding_are_locked(env)\n\n# But can optionally be locked:\nenv_binding_lock(env, \"a\")\nenv_binding_are_locked(env)\n\n# If run, the following would now return an error because `a` is locked:\n# env_bind(env, a = \"foo\")\n# with_env(env, a <- \"bar\")\n\n# Let's unlock it. Note that the return value indicate which\n# bindings were locked:\nwere_locked <- env_binding_unlock(env)\nwere_locked\n\n# Now that it is unlocked we can modify it again:\nenv_bind(env, a = \"foo\")\nwith_env(env, a <- \"bar\")\nenv$a",
            "env_bury": "orig_env <- env(a = 10)\nfn <- set_env(function() a, orig_env)\n\n# fn() currently sees `a` as the value `10`:\nfn()\n\n# env_bury() will bury the current scope of fn() behind a new\n# environment:\nfn <- env_bury(fn, a = 1000)\nfn()\n\n# Even though the symbol `a` is still defined deeper in the scope:\norig_env$a",
            "env_cache": "e <- env(a = \"foo\")\n\n# Returns existing binding\nenv_cache(e, \"a\", \"default\")\n\n# Creates a `b` binding and returns its default value\nenv_cache(e, \"b\", \"default\")\n\n# Now `b` is defined\ne$b",
            "env_clone": "# A clone initially contains the same bindings as the original\n# environment\nenv <- env(a = 1, b = 2)\nclone <- env_clone(env)\n\nenv_print(clone)\nenv_print(env)\n\n# But it can acquire new bindings or change existing ones without\n# impacting the original environment\nenv_bind(clone, a = \"foo\", c = 3)\n\nenv_print(clone)\nenv_print(env)\n\n\n# `env_coalesce()` copies bindings from one environment to another\nlhs <- env(a = 1)\nrhs <- env(a = \"a\", b = \"b\", c = \"c\")\nenv_coalesce(lhs, rhs)\nenv_print(lhs)\n\n# To copy all the bindings from `rhs` into `lhs`, first delete the\n# conflicting bindings from `rhs`\nenv_unbind(lhs, env_names(rhs))\nenv_coalesce(lhs, rhs)\nenv_print(lhs)",
            "env_depth": "env_depth(empty_env())\nenv_depth(pkg_env(\"rlang\"))",
            "env_get": "parent <- child_env(NULL, foo = \"foo\")\nenv <- child_env(parent, bar = \"bar\")\n\n# This throws an error because `foo` is not directly defined in env:\n# env_get(env, \"foo\")\n\n# However `foo` can be fetched in the parent environment:\nenv_get(env, \"foo\", inherit = TRUE)\n\n# You can also avoid an error by supplying a default value:\nenv_get(env, \"foo\", default = \"FOO\")",
            "env_has": "parent <- child_env(NULL, foo = \"foo\")\nenv <- child_env(parent, bar = \"bar\")\n\n# env does not own `foo` but sees it in its parent environment:\nenv_has(env, \"foo\")\nenv_has(env, \"foo\", inherit = TRUE)",
            "env_is_user_facing": "fn <- function() {\n  env_is_user_facing(caller_env())\n}\n\n# Direct call of `fn()` from the global env\nwith(global_env(), fn())\n\n# Indirect call of `fn()` from a package\nwith(ns_env(\"utils\"), fn())",
            "env_lock": "# New environments are unlocked by default:\nenv <- env(a = 1)\nenv_is_locked(env)\n\n# Use env_lock() to lock them:\nenv_lock(env)\nenv_is_locked(env)\n\n# Now that `env` is locked, it is no longer possible to remove or\n# add bindings. If run, the following would fail:\n# env_unbind(env, \"a\")\n# env_bind(env, b = 2)\n\n# Note that even though the environment as a container is locked,\n# the individual bindings are still unlocked and can be modified:\nenv$a <- 10",
            "env_name": "# Some environments have specific names:\nenv_name(global_env())\nenv_name(ns_env(\"rlang\"))\n\n# Anonymous environments don't have names but are labelled by their\n# address in memory:\nenv_name(env())\nenv_label(env())",
            "env_names": "env <- env(a = 1, b = 2)\nenv_names(env)",
            "env_parent": "# Get the parent environment with env_parent():\nenv_parent(global_env())\n\n# Or the tail environment with env_tail():\nenv_tail(global_env())\n\n# By default, env_parent() returns the parent environment of the\n# current evaluation frame. If called at top-level (the global\n# frame), the following two expressions are equivalent:\nenv_parent()\nenv_parent(base_env())\n\n# This default is more handy when called within a function. In this\n# case, the enclosure environment of the function is returned\n# (since it is the parent of the evaluation frame):\nenclos_env <- env()\nfn <- set_env(function() env_parent(), enclos_env)\nidentical(enclos_env, fn())",
            "env_unbind": "env <- env(foo = 1, bar = 2)\nenv_has(env, c(\"foo\", \"bar\"))\n\n# Remove bindings with `env_unbind()`\nenv_unbind(env, c(\"foo\", \"bar\"))\nenv_has(env, c(\"foo\", \"bar\"))\n\n# With inherit = TRUE, it removes bindings in parent environments\n# as well:\nparent <- env(empty_env(), foo = 1, bar = 2)\nenv <- env(parent, foo = \"b\")\n\nenv_unbind(env, \"foo\", inherit = TRUE)\nenv_has(env, c(\"foo\", \"bar\"))\nenv_has(env, c(\"foo\", \"bar\"), inherit = TRUE)",
            "eval_bare": "# eval_bare() works just like base::eval() but you have to create\n# the evaluation environment yourself:\neval_bare(quote(foo), env(foo = \"bar\"))\n\n# eval() has different evaluation semantics than eval_bare(). It\n# can return from the supplied environment even if its an\n# environment that is not on the call stack (i.e. because you've\n# created it yourself). The following would trigger an error with\n# eval_bare():\nret <- quote(return(\"foo\"))\neval(ret, env())\n# eval_bare(ret, env())  # \"no function to return from\" error\n\n# Another feature of eval() is that you can control surround loops:\nbail <- quote(break)\nwhile (TRUE) {\n  eval(bail)\n  # eval_bare(bail)  # \"no loop for break/next\" error\n}\n\n# To explore the consequences of stack inconsistent semantics, let's\n# create a function that evaluates `parent.frame()` deep in the call\n# stack, in an environment corresponding to a frame in the middle of\n# the stack. For consistency with R's lazy evaluation semantics, we'd\n# expect to get the caller of that frame as result:\nfn <- function(eval_fn) {\n  list(\n    returned_env = middle(eval_fn),\n    actual_env = current_env()\n  )\n}\nmiddle <- function(eval_fn) {\n  deep(eval_fn, current_env())\n}\ndeep <- function(eval_fn, eval_env) {\n  expr <- quote(parent.frame())\n  eval_fn(expr, eval_env)\n}\n\n# With eval_bare(), we do get the expected environment:\nfn(rlang::eval_bare)\n\n# But that's not the case with base::eval():\nfn(base::eval)",
            "eval_tidy": "# With simple defused expressions eval_tidy() works the same way as\n# eval():\nfruit <- \"apple\"\nvegetable <- \"potato\"\nexpr <- quote(paste(fruit, vegetable, sep = \" or \"))\nexpr\n\neval(expr)\neval_tidy(expr)\n\n# Both accept a data mask as argument:\ndata <- list(fruit = \"banana\", vegetable = \"carrot\")\neval(expr, data)\neval_tidy(expr, data)\n\n# The main difference is that eval_tidy() supports quosures:\nwith_data <- function(data, expr) {\n  quo <- enquo(expr)\n  eval_tidy(quo, data)\n}\nwith_data(NULL, fruit)\nwith_data(data, fruit)\n\n# eval_tidy() installs the `.data` and `.env` pronouns to allow\n# users to be explicit about variable references:\nwith_data(data, .data$fruit)\nwith_data(data, .env$fruit)",
            "exec": "args <- list(x = c(1:10, 100, NA), na.rm = TRUE)\nexec(\"mean\", !!!args)\nexec(\"mean\", !!!args, trim = 0.2)\n\nfs <- list(a = function() \"a\", b = function() \"b\")\nlapply(fs, exec)\n\n# Compare to do.call it will not automatically inline expressions\n# into the evaluated call.\nx <- 10\nargs <- exprs(x1 = x + 1, x2 = x * 2)\nexec(list, !!!args)\ndo.call(list, args)\n\n# exec() is not designed to generate pretty function calls. This is\n# most easily seen if you call a function that captures the call:\nf <- disp ~ cyl\nexec(\"lm\", f, data = mtcars)\n\n# If you need finer control over the generated call, you'll need to\n# construct it yourself. This may require creating a new environment\n# with carefully constructed bindings\ndata_env <- env(data = mtcars)\neval(expr(lm(!!f, data)), data_env)",
            "expr": "# R normally returns the result of an expression\n1 + 1\n\n# `expr()` defuses the expression that you have supplied and\n# returns it instead of its value\nexpr(1 + 1)\n\nexpr(toupper(letters))\n\n# It supports _injection_ with `!!` and `!!!`. This is a convenient\n# way of modifying part of an expression by injecting other\n# objects.\nvar <- \"cyl\"\nexpr(with(mtcars, mean(!!sym(var))))\n\nvars <- c(\"cyl\", \"am\")\nexpr(with(mtcars, c(!!!syms(vars))))\n\n# Compare to the normal way of building expressions\ncall(\"with\", call(\"mean\", sym(var)))\n\ncall(\"with\", call2(\"c\", !!!syms(vars)))",
            "expr_label": "# To labellise a function argument, first capture it with\n# substitute():\nfn <- function(x) expr_label(substitute(x))\nfn(x:y)\n\n# Strings are encoded\nexpr_label(\"a\\nb\")\n\n# Names and expressions are quoted with ``\nexpr_label(quote(x))\nexpr_label(quote(a + b + c))\n\n# Long expressions are collapsed\nexpr_label(quote(foo({\n  1 + 2\n  print(x)\n})))",
            "expr_print": "# It supports any object. Non-symbolic objects are always printed\n# within angular brackets:\nexpr_print(1:3)\nexpr_print(function() NULL)\n\n# Contrast this to how the code to create these objects is printed:\nexpr_print(quote(1:3))\nexpr_print(quote(function() NULL))\n\n# The main cause of non-symbolic objects in expressions is\n# quasiquotation:\nexpr_print(expr(foo(!!(1:3))))\n\n\n# Quosures from the global environment are printed normally:\nexpr_print(quo(foo))\nexpr_print(quo(foo(!!quo(bar))))\n\n# Quosures from local environments are colourised according to\n# their environments (if you have crayon installed):\nlocal_quo <- local(quo(foo))\nexpr_print(local_quo)\n\nwrapper_quo <- local(quo(bar(!!local_quo, baz)))\nexpr_print(wrapper_quo)",
            "f_rhs": "f_rhs(~ 1 + 2 + 3)\nf_rhs(~ x)\nf_rhs(~ \"A\")\nf_rhs(1 ~ 2)\n\nf_lhs(~ y)\nf_lhs(x ~ y)\n\nf_env(~ x)",
            "f_text": "f <- ~ a + b + bc\nf_text(f)\nf_label(f)\n\n# Names a quoted with ``\nf_label(~ x)\n# Strings are encoded\nf_label(~ \"a\\nb\")\n# Long expressions are collapsed\nf_label(~ foo({\n  1 + 2\n  print(x)\n}))",
            "flatten": "x <- replicate(2, sample(4), simplify = FALSE)\nx\n\nflatten(x)\nflatten_int(x)\n\n# With flatten(), only one level gets removed at a time:\ndeep <- list(1, list(2, list(3)))\nflatten(deep)\nflatten(flatten(deep))\n\n# But squash() removes all levels:\nsquash(deep)\nsquash_dbl(deep)\n\n# The typed flatten functions remove one level and coerce to an atomic\n# vector at the same time:\nflatten_dbl(list(1, list(2)))\n\n# Only bare lists are flattened, but you can splice S3 lists\n# explicitly:\nfoo <- set_attrs(list(\"bar\"), class = \"foo\")\nstr(flatten(list(1, foo, list(100))))\nstr(flatten(list(1, splice(foo), list(100))))\n\n# Instead of splicing manually, flatten_if() and squash_if() let\n# you specify a predicate function:\nis_foo <- function(x) inherits(x, \"foo\") || is_bare_list(x)\nstr(flatten_if(list(1, foo, list(100)), is_foo))\n\n# squash_if() does the same with deep lists:\ndeep_foo <- list(1, list(foo, list(foo, 100)))\nstr(deep_foo)\n\nstr(squash(deep_foo))\nstr(squash_if(deep_foo, is_foo))",
            "fn_body": "# fn_body() is like body() but always returns a block:\nfn <- function() do()\nbody(fn)\nfn_body(fn)\n\n# It also throws an error when used on a primitive function:\ntry(fn_body(base::list))",
            "fn_env": "env <- child_env(\"base\")\nfn <- with_env(env, function() NULL)\nidentical(fn_env(fn), env)\n\nother_env <- child_env(\"base\")\nfn_env(fn) <- other_env\nidentical(fn_env(fn), other_env)",
            "fn_fmls": "# Extract from current call:\nfn <- function(a = 1, b = 2) fn_fmls()\nfn()\n\n# fn_fmls_syms() makes it easy to forward arguments:\ncall2(\"apply\", !!! fn_fmls_syms(lapply))\n\n# You can also change the formals:\nfn_fmls(fn) <- list(A = 10, B = 20)\nfn()\n\nfn_fmls_names(fn) <- c(\"foo\", \"bar\")\nfn()",
            "format_error_bullets": "# All bullets\nwriteLines(format_error_bullets(c(\"foo\", \"bar\")))\n\n# This is equivalent to\nwriteLines(format_error_bullets(set_names(c(\"foo\", \"bar\"), \"*\")))\n\n# Supply named elements to format info, cross, and tick bullets\nwriteLines(format_error_bullets(c(i = \"foo\", x = \"bar\", v = \"baz\", \"*\" = \"quux\")))\n\n# An unnamed element breaks the line\nwriteLines(format_error_bullets(c(i = \"foo\\nbar\")))\n\n# A \" \" element breaks the line within a bullet (with indentation)\nwriteLines(format_error_bullets(c(i = \"foo\", \" \" = \"bar\")))",
            "format_error_call": "# Arguments are stripped\nwriteLines(format_error_call(quote(foo(bar, baz))))\n\n# Returns `NULL` with complex calls such as those that contain\n# inlined functions\nformat_error_call(call2(list))\n\n# Operators are formatted using their names rather than in\n# function call form\nwriteLines(format_error_call(quote(1 + 2)))"
        }
    },
    "magrittr": {
        "description": "Provides a mechanism for chaining commands with a new\n    forward-pipe operator, %>%. This operator will forward a value, or the\n    result of an expression, into the next function call/expression.\n    There is flexible support for the type of right-hand side expressions.\n    For more information, see package vignette.  To quote Rene Magritte,\n    \"Ceci n'est pas un pipe.\"",
        "examples": {
            "aliases": "iris \\%>\\%\n   extract(, 1:4) \\%>\\%\n   head\n\ngood.times <-\n  Sys.Date() \\%>\\%\n  as.POSIXct \\%>\\%\n  seq(by = \"15 mins\", length.out = 100) \\%>\\%\n  data.frame(timestamp = .)\n\ngood.times$quarter <-\n  good.times \\%>\\%\n  use_series(timestamp) \\%>\\%\n  format(\"\\%M\") \\%>\\%\n  as.numeric \\%>\\%\n  divide_by_int(15) \\%>\\%\n  add(1)",
            "compound": "iris$Sepal.Length \\%<>\\% sqrt\n\nx <- rnorm(100)\n\nx \\%<>\\% abs \\%>\\% sort\n\nis_weekend <- function(day)\n{\n   # day could be e.g. character a valid representation\n   day \\%<>\\% as.Date\n   \n   result <- day \\%>\\% format(\"\\%u\") \\%>\\% as.numeric \\%>\\% is_greater_than(5)\n   \n   if (result)\n     message(day \\%>\\% paste(\"is a weekend!\"))\n   else\n     message(day \\%>\\% paste(\"is not a weekend!\"))\n   \n   invisible(result)\n}",
            "exposition": "iris \\%>\\%\n  subset(Sepal.Length > mean(Sepal.Length)) \\%$\\%\n  cor(Sepal.Length, Sepal.Width)\n  \ndata.frame(z = rnorm(100)) \\%$\\% \n  ts.plot(z)",
            "magrittr-package": "\\dontrun{\n\nthe_data <-\n  read.csv('/path/to/data/file.csv') \\%>\\%\n  subset(variable_a > x) \\%>\\%\n  transform(variable_c = variable_a/variable_b) \\%>\\%\n  head(100)\n}",
            "pipe-eager": "f <- function(x) {\n  message(\"foo\")\n  x\n}\ng <- function(x) {\n  message(\"bar\")\n  x\n}\nh <- function(x) {\n  message(\"baz\")\n  invisible(x)\n}\n\n# The following lazy pipe sequence is equivalent to `h(g(f()))`.\n# Given R's lazy evaluation behaviour,`f()` and `g()` are lazily\n# evaluated when `h()` is already running. This causes the messages\n# to appear in reverse order:\nNULL \\%>\\% f() \\%>\\% g() \\%>\\% h()\n\n# Use the eager pipe to fix this:\nNULL \\%!>\\% f() \\%!>\\% g() \\%!>\\% h()\n\n# Or fix this by calling `force()` on the function arguments\nf <- function(x) {\n  force(x)\n  message(\"foo\")\n  x\n}\ng <- function(x) {\n  force(x)\n  message(\"bar\")\n  x\n}\nh <- function(x) {\n  force(x)\n  message(\"baz\")\n  invisible(x)\n}\n\n# With strict functions, the arguments are evaluated sequentially\nNULL \\%>\\% f() \\%>\\% g() \\%>\\% h()\n\n# Instead of forcing, you can also check the type of your functions.\n# Type-checking also has the effect of making your function lazy.",
            "pipe": "# Basic use:\niris \\%>\\% head\n\n# Use with lhs as first argument\niris \\%>\\% head(10)\n\n# Using the dot place-holder\n\"Ceci n'est pas une pipe\" \\%>\\% gsub(\"une\", \"un\", .)\n  \n# When dot is nested, lhs is still placed first:\nsample(1:10) \\%>\\% paste0(LETTERS[.])\n\n# This can be avoided:\nrnorm(100) \\%>\\% {c(min(.), mean(.), max(.))} \\%>\\% floor\n\n# Lambda expressions: \niris \\%>\\%\n{\n  size <- sample(1:10, size = 1)\n  rbind(head(., size), tail(., size))\n}\n\n# renaming in lambdas:\niris \\%>\\%\n{\n  my_data <- .\n  size <- sample(1:10, size = 1)\n  rbind(head(my_data, size), tail(my_data, size))\n}\n\n# Building unary functions with \\%>\\%\ntrig_fest <- . \\%>\\% tan \\%>\\% cos \\%>\\% sin\n\n1:10 \\%>\\% trig_fest\ntrig_fest(1:10)",
            "tee": "rnorm(200) \\%>\\%\nmatrix(ncol = 2) \\%T>\\%\nplot \\%>\\% # plot usually does not return anything. \ncolSums"
        }
    },
    "dplyr": {
        "description": "A fast, consistent tool for working with data frame like\n    objects, both in memory and out of memory.",
        "examples": {
            "across": "# For better printing\niris <- as_tibble(iris)\n\n# across() -----------------------------------------------------------------\n# Different ways to select the same set of columns\n# See <https://tidyselect.r-lib.org/articles/syntax.html> for details\niris \\%>\\%\n  mutate(across(c(Sepal.Length, Sepal.Width), round))\niris \\%>\\%\n  mutate(across(c(1, 2), round))\niris \\%>\\%\n  mutate(across(1:Sepal.Width, round))\niris \\%>\\%\n  mutate(across(where(is.double) & !c(Petal.Length, Petal.Width), round))\n\n# Using an external vector of names\ncols <- c(\"Sepal.Length\", \"Petal.Width\")\niris \\%>\\%\n  mutate(across(all_of(cols), round))\n\n# If the external vector is named, the output columns will be named according\n# to those names\nnames(cols) <- tolower(cols)\niris \\%>\\%\n  mutate(across(all_of(cols), round))\n\n# A purrr-style formula\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  summarise(across(starts_with(\"Sepal\"), ~ mean(.x, na.rm = TRUE)))\n\n# A named list of functions\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  summarise(across(starts_with(\"Sepal\"), list(mean = mean, sd = sd)))\n\n# Use the .names argument to control the output names\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  summarise(across(starts_with(\"Sepal\"), mean, .names = \"mean_{.col}\"))\n\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  summarise(\n    across(\n      starts_with(\"Sepal\"),\n      list(mean = mean, sd = sd),\n      .names = \"{.col}.{.fn}\"\n    )\n  )\n\n# If a named external vector is used for column selection, .names will use\n# those names when constructing the output names\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  summarise(across(all_of(cols), mean, .names = \"mean_{.col}\"))\n\n# When the list is not named, .fn is replaced by the function's position\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  summarise(\n    across(starts_with(\"Sepal\"), list(mean, sd), .names = \"{.col}.fn{.fn}\")\n  )\n\n# When the functions in .fns return a data frame, you typically get a\n# \"packed\" data frame back\nquantile_df <- function(x, probs = c(0.25, 0.5, 0.75)) {\n  tibble(quantile = probs, value = quantile(x, probs))\n}\n\niris \\%>\\%\n  reframe(across(starts_with(\"Sepal\"), quantile_df))\n\n# Use .unpack to automatically expand these packed data frames into their\n# individual columns\niris \\%>\\%\n  reframe(across(starts_with(\"Sepal\"), quantile_df, .unpack = TRUE))\n\n# .unpack can utilize a glue specification if you don't like the defaults\niris \\%>\\%\n  reframe(\n    across(starts_with(\"Sepal\"), quantile_df, .unpack = \"{outer}.{inner}\")\n  )\n\n# This is also useful inside mutate(), for example, with a multi-lag helper\nmultilag <- function(x, lags = 1:3) {\n  names(lags) <- as.character(lags)\n  purrr::map_dfr(lags, lag, x = x)\n}\n\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  mutate(across(starts_with(\"Sepal\"), multilag, .unpack = TRUE)) \\%>\\%\n  select(Species, starts_with(\"Sepal\"))\n\n# if_any() and if_all() ----------------------------------------------------\niris \\%>\\%\n  filter(if_any(ends_with(\"Width\"), ~ . > 4))\niris \\%>\\%\n  filter(if_all(ends_with(\"Width\"), ~ . > 2))",
            "all_equal": "scramble <- function(x) x[sample(nrow(x)), sample(ncol(x))]\n\n# `all_equal()` ignored row and column ordering by default,\n# but we now feel that that makes it too easy to make mistakes\nmtcars2 <- scramble(mtcars)\nall_equal(mtcars, mtcars2)\n\n# Instead, be explicit about the row and column ordering\nall.equal(\n  mtcars,\n  mtcars2[rownames(mtcars), names(mtcars)]\n)",
            "arrange": "arrange(mtcars, cyl, disp)\narrange(mtcars, desc(disp))\n\n# grouped arrange ignores groups\nby_cyl <- mtcars \\%>\\% group_by(cyl)\nby_cyl \\%>\\% arrange(desc(wt))\n# Unless you specifically ask:\nby_cyl \\%>\\% arrange(desc(wt), .by_group = TRUE)\n\n# use embracing when wrapping in a function;\n# see ?rlang::args_data_masking for more details\ntidy_eval_arrange <- function(.data, var) {\n  .data \\%>\\%\n    arrange({{ var }})\n}\ntidy_eval_arrange(mtcars, mpg)\n\n# Use `across()` or `pick()` to select columns with tidy-select\niris \\%>\\% arrange(pick(starts_with(\"Sepal\")))\niris \\%>\\% arrange(across(starts_with(\"Sepal\"), desc))",
            "arrange_all": "df <- as_tibble(mtcars)\narrange_all(df)\n# ->\narrange(df, pick(everything()))\n\narrange_all(df, desc)\n# ->\narrange(df, across(everything(), desc))",
            "band_members": "band_members\nband_instruments\nband_instruments2",
            "between": "between(1:12, 7, 9)\n\nx <- rnorm(1e2)\nx[between(x, -1, 1)]\n\n# On a tibble using `filter()`\nfilter(starwars, between(height, 100, 150))\n\n# Using the `ptype` argument with ordered factors, where otherwise everything\n# is cast to the common type of character before the comparison\nx <- ordered(\n  c(\"low\", \"medium\", \"high\", \"medium\"),\n  levels = c(\"low\", \"medium\", \"high\")\n)\nbetween(x, \"medium\", \"high\")\nbetween(x, \"medium\", \"high\", ptype = x)",
            "bind_cols": "df1 <- tibble(x = 1:3)\ndf2 <- tibble(y = 3:1)\nbind_cols(df1, df2)\n\n# Row sizes must be compatible when column-binding\ntry(bind_cols(tibble(x = 1:3), tibble(y = 1:2)))",
            "bind_rows": "df1 <- tibble(x = 1:2, y = letters[1:2])\ndf2 <- tibble(x = 4:5, z = 1:2)\n\n# You can supply individual data frames as arguments:\nbind_rows(df1, df2)\n\n# Or a list of data frames:\nbind_rows(list(df1, df2))\n\n# When you supply a column name with the `.id` argument, a new\n# column is created to link each row to its original data frame\nbind_rows(list(df1, df2), .id = \"id\")\nbind_rows(list(a = df1, b = df2), .id = \"id\")",
            "c_across": "df <- tibble(id = 1:4, w = runif(4), x = runif(4), y = runif(4), z = runif(4))\ndf \\%>\\%\n  rowwise() \\%>\\%\n  mutate(\n    sum = sum(c_across(w:z)),\n    sd = sd(c_across(w:z))\n  )",
            "case_match": "x <- c(\"a\", \"b\", \"a\", \"d\", \"b\", NA, \"c\", \"e\")\n\n# `case_match()` acts like a vectorized `switch()`.\n# Unmatched values \"fall through\" as a missing value.\ncase_match(\n  x,\n  \"a\" ~ 1,\n  \"b\" ~ 2,\n  \"c\" ~ 3,\n  \"d\" ~ 4\n)\n\n# Missing values can be matched exactly, and `.default` can be used to\n# control the value used for unmatched values of `.x`\ncase_match(\n  x,\n  \"a\" ~ 1,\n  \"b\" ~ 2,\n  \"c\" ~ 3,\n  \"d\" ~ 4,\n  NA ~ 0,\n  .default = 100\n)\n\n# Input values can be grouped into the same expression to map them to the\n# same output value\ncase_match(\n  x,\n  c(\"a\", \"b\") ~ \"low\",\n  c(\"c\", \"d\", \"e\") ~ \"high\"\n)\n\n# `case_match()` isn't limited to character input:\ny <- c(1, 2, 1, 3, 1, NA, 2, 4)\n\ncase_match(\n  y,\n  c(1, 3) ~ \"odd\",\n  c(2, 4) ~ \"even\",\n  .default = \"missing\"\n)\n\n# Setting `.default` to the original vector is a useful way to replace\n# selected values, leaving everything else as is\ncase_match(y, NA ~ 0, .default = y)\n\nstarwars \\%>\\%\n  mutate(\n    # Replace missings, but leave everything else alone\n    hair_color = case_match(hair_color, NA ~ \"unknown\", .default = hair_color),\n    # Replace some, but not all, of the species\n    species = case_match(\n      species,\n      \"Human\" ~ \"Humanoid\",\n      \"Droid\" ~ \"Robot\",\n      c(\"Wookiee\", \"Ewok\") ~ \"Hairy\",\n      .default = species\n    ),\n    .keep = \"used\"\n  )",
            "case_when": "x <- 1:70\ncase_when(\n  x \\%\\% 35 == 0 ~ \"fizz buzz\",\n  x \\%\\% 5 == 0 ~ \"fizz\",\n  x \\%\\% 7 == 0 ~ \"buzz\",\n  .default = as.character(x)\n)\n\n# Like an if statement, the arguments are evaluated in order, so you must\n# proceed from the most specific to the most general. This won't work:\ncase_when(\n  x \\%\\%  5 == 0 ~ \"fizz\",\n  x \\%\\%  7 == 0 ~ \"buzz\",\n  x \\%\\% 35 == 0 ~ \"fizz buzz\",\n  .default = as.character(x)\n)\n\n# If none of the cases match and no `.default` is supplied, NA is used:\ncase_when(\n  x \\%\\% 35 == 0 ~ \"fizz buzz\",\n  x \\%\\% 5 == 0 ~ \"fizz\",\n  x \\%\\% 7 == 0 ~ \"buzz\",\n)\n\n# Note that `NA` values on the LHS are treated like `FALSE` and will be\n# assigned the `.default` value. You must handle them explicitly if you\n# want to use a different value. The exact way to handle missing values is\n# dependent on the set of LHS conditions you use.\nx[2:4] <- NA_real_\ncase_when(\n  x \\%\\% 35 == 0 ~ \"fizz buzz\",\n  x \\%\\% 5 == 0 ~ \"fizz\",\n  x \\%\\% 7 == 0 ~ \"buzz\",\n  is.na(x) ~ \"nope\",\n  .default = as.character(x)\n)\n\n# `case_when()` evaluates all RHS expressions, and then constructs its\n# result by extracting the selected (via the LHS expressions) parts.\n# In particular `NaN`s are produced in this case:\ny <- seq(-2, 2, by = .5)\ncase_when(\n  y >= 0 ~ sqrt(y),\n  .default = y\n)\n\n# `case_when()` is particularly useful inside `mutate()` when you want to\n# create a new variable that relies on a complex combination of existing\n# variables\nstarwars \\%>\\%\n  select(name:mass, gender, species) \\%>\\%\n  mutate(\n    type = case_when(\n      height > 200 | mass > 200 ~ \"large\",\n      species == \"Droid\" ~ \"robot\",\n      .default = \"other\"\n    )\n  )\n\n\n# `case_when()` is not a tidy eval function. If you'd like to reuse\n# the same patterns, extract the `case_when()` call in a normal\n# function:\ncase_character_type <- function(height, mass, species) {\n  case_when(\n    height > 200 | mass > 200 ~ \"large\",\n    species == \"Droid\" ~ \"robot\",\n    .default = \"other\"\n  )\n}\n\ncase_character_type(150, 250, \"Droid\")\ncase_character_type(150, 150, \"Droid\")\n\n# Such functions can be used inside `mutate()` as well:\nstarwars \\%>\\%\n  mutate(type = case_character_type(height, mass, species)) \\%>\\%\n  pull(type)\n\n# `case_when()` ignores `NULL` inputs. This is useful when you'd\n# like to use a pattern only under certain conditions. Here we'll\n# take advantage of the fact that `if` returns `NULL` when there is\n# no `else` clause:\ncase_character_type <- function(height, mass, species, robots = TRUE) {\n  case_when(\n    height > 200 | mass > 200 ~ \"large\",\n    if (robots) species == \"Droid\" ~ \"robot\",\n    .default = \"other\"\n  )\n}\n\nstarwars \\%>\\%\n  mutate(type = case_character_type(height, mass, species, robots = FALSE)) \\%>\\%\n  pull(type)",
            "check_dbplyr": "\\dontshow{if (requireNamespace(\"dbplyr\", quietly = TRUE)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nwrap_dbplyr_obj(\"build_sql\")\nwrap_dbplyr_obj(\"base_agg\")\n\\dontshow{\\}) # examplesIf}",
            "coalesce": "# Use a single value to replace all missing values\nx <- sample(c(1:5, NA, NA, NA))\ncoalesce(x, 0L)\n\n# The equivalent to a missing value in a list is `NULL`\ncoalesce(list(1, 2, NULL), list(NA))\n\n# Or generate a complete vector from partially missing pieces\ny <- c(1, 2, NA, NA, 5)\nz <- c(NA, NA, 3, 4, 5)\ncoalesce(y, z)\n\n# Supply lists by splicing them into dots:\nvecs <- list(\n  c(1, 2, NA, NA, 5),\n  c(NA, NA, 3, 4, 5)\n)\ncoalesce(!!!vecs)",
            "combine": "f1 <- factor(\"a\")\nf2 <- factor(\"b\")\n\ncombine(f1, f2)\n# ->\nvctrs::vec_c(f1, f1)\n\ncombine(list(f1, f2))\n# ->\nvctrs::vec_c(!!!list(f1, f2))",
            "compute": "\\dontshow{if (requireNamespace(\"dbplyr\", quietly = TRUE) && requireNamespace(\"RSQLite\", quietly = TRUE)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nmtcars2 <- dbplyr::src_memdb() \\%>\\%\n  copy_to(mtcars, name = \"mtcars2-cc\", overwrite = TRUE)\n\nremote <- mtcars2 \\%>\\%\n  filter(cyl == 8) \\%>\\%\n  select(mpg:drat)\n\n# Compute query and save in remote table\ncompute(remote)\n\n# Compute query bring back to this session\ncollect(remote)\n\n# Creates a fresh query based on the generated SQL\ncollapse(remote)\n\\dontshow{\\}) # examplesIf}",
            "consecutive_id": "consecutive_id(c(TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, NA, NA))\nconsecutive_id(c(1, 1, 1, 2, 1, 1, 2, 2))\n\ndf <- data.frame(x = c(0, 0, 1, 0), y = c(2, 2, 2, 2))\ndf \\%>\\% group_by(x, y) \\%>\\% summarise(n = n())\ndf \\%>\\% group_by(id = consecutive_id(x, y), x, y) \\%>\\% summarise(n = n())",
            "context": "df <- tibble(\n  g = sample(rep(letters[1:3], 1:3)),\n  x = runif(6),\n  y = runif(6)\n)\ngf <- df \\%>\\% group_by(g)\n\ngf \\%>\\% summarise(n = n())\n\ngf \\%>\\% mutate(id = cur_group_id())\ngf \\%>\\% reframe(row = cur_group_rows())\ngf \\%>\\% summarise(data = list(cur_group()))\n\ngf \\%>\\% mutate(across(everything(), ~ paste(cur_column(), round(.x, 2))))",
            "copy_to": "\\dontrun{\niris2 <- dbplyr::src_memdb() \\%>\\% copy_to(iris, overwrite = TRUE)\niris2\n}",
            "count": "# count() is a convenient way to get a sense of the distribution of\n# values in a dataset\nstarwars \\%>\\% count(species)\nstarwars \\%>\\% count(species, sort = TRUE)\nstarwars \\%>\\% count(sex, gender, sort = TRUE)\nstarwars \\%>\\% count(birth_decade = round(birth_year, -1))\n\n# use the `wt` argument to perform a weighted count. This is useful\n# when the data has already been aggregated once\ndf <- tribble(\n  ~name,    ~gender,   ~runs,\n  \"Max\",    \"male\",       10,\n  \"Sandra\", \"female\",      1,\n  \"Susan\",  \"female\",      4\n)\n# counts rows:\ndf \\%>\\% count(gender)\n# counts runs:\ndf \\%>\\% count(gender, wt = runs)\n\n# When factors are involved, `.drop = FALSE` can be used to retain factor\n# levels that don't appear in the data\ndf2 <- tibble(\n  id = 1:5,\n  type = factor(c(\"a\", \"c\", \"a\", NA, \"a\"), levels = c(\"a\", \"b\", \"c\"))\n)\ndf2 \\%>\\% count(type)\ndf2 \\%>\\% count(type, .drop = FALSE)\n\n# Or, using `group_by()`:\ndf2 \\%>\\% group_by(type, .drop = FALSE) \\%>\\% count()\n\n# tally() is a lower-level function that assumes you've done the grouping\nstarwars \\%>\\% tally()\nstarwars \\%>\\% group_by(species) \\%>\\% tally()\n\n# both count() and tally() have add_ variants that work like\n# mutate() instead of summarise\ndf \\%>\\% add_count(gender, wt = runs)\ndf \\%>\\% add_tally(wt = runs)",
            "cross_join": "# Cross joins match each row in `x` to every row in `y`.\n# Data within the columns is not used in the matching process.\ncross_join(band_instruments, band_members)\n\n# Control the suffix added to variables duplicated in\n# `x` and `y` with `suffix`.\ncross_join(band_instruments, band_members, suffix = c(\"\", \"_y\"))",
            "cumall": "# `cummean()` returns a numeric/integer vector of the same length\n# as the input vector.\nx <- c(1, 3, 5, 2, 2)\ncummean(x)\ncumsum(x) / seq_along(x)\n\n# `cumall()` and `cumany()` return logicals\ncumall(x < 5)\ncumany(x == 3)\n\n# `cumall()` vs. `cumany()`\ndf <- data.frame(\n  date = as.Date(\"2020-01-01\") + 0:6,\n  balance = c(100, 50, 25, -25, -50, 30, 120)\n)\n# all rows after first overdraft\ndf \\%>\\% filter(cumany(balance < 0))\n# all rows until first overdraft\ndf \\%>\\% filter(cumall(!(balance < 0)))",
            "desc": "desc(1:10)\ndesc(factor(letters))\n\nfirst_day <- seq(as.Date(\"1910/1/1\"), as.Date(\"1920/1/1\"), \"years\")\ndesc(first_day)\n\nstarwars \\%>\\% arrange(desc(mass))",
            "dim_desc": "dim_desc(mtcars)",
            "distinct": "df <- tibble(\n  x = sample(10, 100, rep = TRUE),\n  y = sample(10, 100, rep = TRUE)\n)\nnrow(df)\nnrow(distinct(df))\nnrow(distinct(df, x, y))\n\ndistinct(df, x)\ndistinct(df, y)\n\n# You can choose to keep all other variables as well\ndistinct(df, x, .keep_all = TRUE)\ndistinct(df, y, .keep_all = TRUE)\n\n# You can also use distinct on computed variables\ndistinct(df, diff = abs(x - y))\n\n# Use `pick()` to select columns with tidy-select\ndistinct(starwars, pick(contains(\"color\")))\n\n# Grouping -------------------------------------------------\n\ndf <- tibble(\n  g = c(1, 1, 2, 2, 2),\n  x = c(1, 1, 2, 1, 2),\n  y = c(3, 2, 1, 3, 1)\n)\ndf <- df \\%>\\% group_by(g)\n\n# With grouped data frames, distinctness is computed within each group\ndf \\%>\\% distinct(x)\n\n# When `...` are omitted, `distinct()` still computes distinctness using\n# all variables in the data frame\ndf \\%>\\% distinct()",
            "distinct_all": "df <- tibble(x = rep(2:5, each = 2) / 2, y = rep(2:3, each = 4) / 2)\n\ndistinct_all(df)\n# ->\ndistinct(df, pick(everything()))\n\ndistinct_at(df, vars(x,y))\n# ->\ndistinct(df, pick(x, y))\n\ndistinct_if(df, is.numeric)\n# ->\ndistinct(df, pick(where(is.numeric)))\n\n# You can supply a function that will be applied before extracting the distinct values\n# The variables of the sorted tibble keep their original values.\ndistinct_all(df, round)\n# ->\ndistinct(df, across(everything(), round))",
            "do": "# do() with unnamed arguments becomes reframe() or summarise()\n# . becomes pick()\nby_cyl <- mtcars \\%>\\% group_by(cyl)\nby_cyl \\%>\\% do(head(., 2))\n# ->\nby_cyl \\%>\\% reframe(head(pick(everything()), 2))\nby_cyl \\%>\\% slice_head(n = 2)\n\n# Can refer to variables directly\nby_cyl \\%>\\% do(mean = mean(.$vs))\n# ->\nby_cyl \\%>\\% summarise(mean = mean(vs))\n\n# do() with named arguments becomes nest_by() + mutate() & list()\nmodels <- by_cyl \\%>\\% do(mod = lm(mpg ~ disp, data = .))\n# ->\nmodels <- mtcars \\%>\\%\n  nest_by(cyl) \\%>\\%\n  mutate(mod = list(lm(mpg ~ disp, data = data)))\nmodels \\%>\\% summarise(rsq = summary(mod)$r.squared)\n\n# use broom to turn models into data\nmodels \\%>\\% do(data.frame(\n  var = names(coef(.$mod)),\n  coef(summary(.$mod)))\n)\n\\dontshow{if (requireNamespace(\"broom\", quietly = TRUE)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# ->\nmodels \\%>\\% reframe(broom::tidy(mod))\n\\dontshow{\\}) # examplesIf}",
            "dplyr-locale": "\\dontshow{if (dplyr:::has_minimum_stringi()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\ndf <- tibble(x = c(\"a\", \"b\", \"C\", \"B\", \"c\"))\ndf\n\n# Default locale is C, which groups the English alphabet by case, placing\n# uppercase letters before lowercase letters.\narrange(df, x)\n\n# The American English locale groups the alphabet by letter.\n# Explicitly override `.locale` with `\"en\"` for this ordering.\narrange(df, x, .locale = \"en\")\n\n# This Danish letter is expected to sort after `z`\ndf <- tibble(x = c(\"o\", \"p\", \"\\u00F8\", \"z\"))\ndf\n\n# The American English locale sorts it right after `o`\narrange(df, x, .locale = \"en\")\n\n# Using `\"da\"` for Danish ordering gives the expected result\narrange(df, x, .locale = \"da\")\n\n# If you need the legacy behavior of `arrange()`, which respected the\n# system locale, then you can set the global option `dplyr.legacy_locale`,\n# but expect this to be removed in the future. We recommend that you use\n# the `.locale` argument instead.\nrlang::with_options(dplyr.legacy_locale = TRUE, {\n  arrange(df, x)\n})\n\\dontshow{\\}) # examplesIf}",
            "explain": "\\dontshow{if (requireNamespace(\"dbplyr\", quietly = TRUE) && requireNamespace(\"RSQLite\", quietly = TRUE)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\\donttest{\nlahman_s <- dbplyr::lahman_sqlite()\nbatting <- tbl(lahman_s, \"Batting\")\nbatting \\%>\\% show_query()\nbatting \\%>\\% explain()\n\n# The batting database has indices on all ID variables:\n# SQLite automatically picks the most restrictive index\nbatting \\%>\\% filter(lgID == \"NL\" & yearID == 2000L) \\%>\\% explain()\n\n# OR's will use multiple indexes\nbatting \\%>\\% filter(lgID == \"NL\" | yearID == 2000) \\%>\\% explain()\n\n# Joins will use indexes in both tables\nteams <- tbl(lahman_s, \"Teams\")\nbatting \\%>\\% left_join(teams, c(\"yearID\", \"teamID\")) \\%>\\% explain()\n}\n\\dontshow{\\}) # examplesIf}",
            "filter-joins": "# \"Filtering\" joins keep cases from the LHS\nband_members \\%>\\% semi_join(band_instruments)\nband_members \\%>\\% anti_join(band_instruments)\n\n# To suppress the message about joining variables, supply `by`\nband_members \\%>\\% semi_join(band_instruments, by = join_by(name))\n# This is good practice in production code",
            "filter": "# Filtering by one criterion\nfilter(starwars, species == \"Human\")\nfilter(starwars, mass > 1000)\n\n# Filtering by multiple criteria within a single logical expression\nfilter(starwars, hair_color == \"none\" & eye_color == \"black\")\nfilter(starwars, hair_color == \"none\" | eye_color == \"black\")\n\n# When multiple expressions are used, they are combined using &\nfilter(starwars, hair_color == \"none\", eye_color == \"black\")\n\n\n# The filtering operation may yield different results on grouped\n# tibbles because the expressions are computed within groups.\n#\n# The following filters rows where `mass` is greater than the\n# global average:\nstarwars \\%>\\% filter(mass > mean(mass, na.rm = TRUE))\n\n# Whereas this keeps rows with `mass` greater than the gender\n# average:\nstarwars \\%>\\% group_by(gender) \\%>\\% filter(mass > mean(mass, na.rm = TRUE))\n\n\n# To refer to column names that are stored as strings, use the `.data` pronoun:\nvars <- c(\"mass\", \"height\")\ncond <- c(80, 150)\nstarwars \\%>\\%\n  filter(\n    .data[[vars[[1]]]] > cond[[1]],\n    .data[[vars[[2]]]] > cond[[2]]\n  )\n# Learn more in ?rlang::args_data_masking",
            "filter_all": "# While filter() accepts expressions with specific variables, the\n# scoped filter verbs take an expression with the pronoun `.` and\n# replicate it over all variables. This expression should be quoted\n# with all_vars() or any_vars():\nall_vars(is.na(.))\nany_vars(is.na(.))\n\n\n# You can take the intersection of the replicated expressions:\nfilter_all(mtcars, all_vars(. > 150))\n# ->\nfilter(mtcars, if_all(everything(), ~ .x > 150))\n\n# Or the union:\nfilter_all(mtcars, any_vars(. > 150))\n# ->\nfilter(mtcars, if_any(everything(), ~ . > 150))\n\n\n# You can vary the selection of columns on which to apply the\n# predicate. filter_at() takes a vars() specification:\nfilter_at(mtcars, vars(starts_with(\"d\")), any_vars((. \\%\\% 2) == 0))\n# ->\nfilter(mtcars, if_any(starts_with(\"d\"), ~ (.x \\%\\% 2) == 0))\n\n# And filter_if() selects variables with a predicate function:\nfilter_if(mtcars, ~ all(floor(.) == .), all_vars(. != 0))\n# ->\nis_int <- function(x) all(floor(x) == x)\nfilter(mtcars, if_all(where(is_int), ~ .x != 0))",
            "funs": "funs(\"mean\", mean(., na.rm = TRUE))\n# ->\nlist(mean = mean, mean = ~ mean(.x, na.rm = TRUE))\n\nfuns(m1 = mean, m2 = \"mean\", m3 = mean(., na.rm = TRUE))\n# ->\nlist(m1 = mean, m2 = \"mean\", m3 = ~ mean(.x, na.rm = TRUE))",
            "glimpse": "glimpse(mtcars)\n\n# Note that original x is (invisibly) returned, allowing `glimpse()` to be\n# used within a pipeline.\nmtcars \\%>\\%\n  glimpse() \\%>\\%\n  select(1:3)\n\nglimpse(starwars)",
            "group_by": "by_cyl <- mtcars \\%>\\% group_by(cyl)\n\n# grouping doesn't change how the data looks (apart from listing\n# how it's grouped):\nby_cyl\n\n# It changes how it acts with the other dplyr verbs:\nby_cyl \\%>\\% summarise(\n  disp = mean(disp),\n  hp = mean(hp)\n)\nby_cyl \\%>\\% filter(disp == max(disp))\n\n# Each call to summarise() removes a layer of grouping\nby_vs_am <- mtcars \\%>\\% group_by(vs, am)\nby_vs <- by_vs_am \\%>\\% summarise(n = n())\nby_vs\nby_vs \\%>\\% summarise(n = sum(n))\n\n# To removing grouping, use ungroup\nby_vs \\%>\\%\n  ungroup() \\%>\\%\n  summarise(n = sum(n))\n\n# By default, group_by() overrides existing grouping\nby_cyl \\%>\\%\n  group_by(vs, am) \\%>\\%\n  group_vars()\n\n# Use add = TRUE to instead append\nby_cyl \\%>\\%\n  group_by(vs, am, .add = TRUE) \\%>\\%\n  group_vars()\n\n# You can group by expressions: this is a short-hand\n# for a mutate() followed by a group_by()\nmtcars \\%>\\%\n  group_by(vsam = vs + am)\n\n# The implicit mutate() step is always performed on the\n# ungrouped data. Here we get 3 groups:\nmtcars \\%>\\%\n  group_by(vs) \\%>\\%\n  group_by(hp_cut = cut(hp, 3))\n\n# If you want it to be performed by groups,\n# you have to use an explicit mutate() call.\n# Here we get 3 groups per value of vs\nmtcars \\%>\\%\n  group_by(vs) \\%>\\%\n  mutate(hp_cut = cut(hp, 3)) \\%>\\%\n  group_by(hp_cut)\n\n# when factors are involved and .drop = FALSE, groups can be empty\ntbl <- tibble(\n  x = 1:10,\n  y = factor(rep(c(\"a\", \"c\"), each  = 5), levels = c(\"a\", \"b\", \"c\"))\n)\ntbl \\%>\\%\n  group_by(y, .drop = FALSE) \\%>\\%\n  group_rows()",
            "group_by_all": "# Group a data frame by all variables:\ngroup_by_all(mtcars)\n# ->\nmtcars \\%>\\% group_by(pick(everything()))\n\n# Group by variables selected with a predicate:\ngroup_by_if(iris, is.factor)\n# ->\niris \\%>\\% group_by(pick(where(is.factor)))\n\n# Group by variables selected by name:\ngroup_by_at(mtcars, vars(vs, am))\n# ->\nmtcars \\%>\\% group_by(pick(vs, am))\n\n# Like group_by(), the scoped variants have optional mutate\n# semantics. This provide a shortcut for group_by() + mutate():\nd <- tibble(x=c(1,1,2,2), y=c(1,2,1,2))\ngroup_by_all(d, as.factor)\n# ->\nd \\%>\\% group_by(across(everything(), as.factor))\n\ngroup_by_if(iris, is.factor, as.character)\n# ->\niris \\%>\\% group_by(across(where(is.factor), as.character))",
            "group_by_drop_default": "group_by_drop_default(iris)\n\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  group_by_drop_default()\n\niris \\%>\\%\n  group_by(Species, .drop = FALSE) \\%>\\%\n  group_by_drop_default()",
            "group_cols": "gdf <- iris \\%>\\% group_by(Species)\ngdf \\%>\\% select(group_cols())\n\n# Remove the grouping variables from mutate selections:\ngdf \\%>\\% mutate_at(vars(-group_cols()), `/`, 100)\n# -> No longer necessary with across()\ngdf \\%>\\% mutate(across(everything(), ~ . / 100))",
            "group_data": "df <- tibble(x = c(1,1,2,2))\ngroup_vars(df)\ngroup_rows(df)\ngroup_data(df)\ngroup_indices(df)\n\ngf <- group_by(df, x)\ngroup_vars(gf)\ngroup_rows(gf)\ngroup_data(gf)\ngroup_indices(gf)",
            "group_map": "# return a list\nmtcars \\%>\\%\n  group_by(cyl) \\%>\\%\n  group_map(~ head(.x, 2L))\n\n# return a tibble grouped by `cyl` with 2 rows per group\n# the grouping data is recalculated\nmtcars \\%>\\%\n  group_by(cyl) \\%>\\%\n  group_modify(~ head(.x, 2L))\n\\dontshow{if (requireNamespace(\"broom\", quietly = TRUE)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# a list of tibbles\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  group_map(~ broom::tidy(lm(Petal.Length ~ Sepal.Length, data = .x)))\n\n# a restructured grouped tibble\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  group_modify(~ broom::tidy(lm(Petal.Length ~ Sepal.Length, data = .x)))\n\\dontshow{\\}) # examplesIf}\n\n# a list of vectors\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  group_map(~ quantile(.x$Petal.Length, probs = c(0.25, 0.5, 0.75)))\n\n# to use group_modify() the lambda must return a data frame\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  group_modify(~ {\n     quantile(.x$Petal.Length, probs = c(0.25, 0.5, 0.75)) \\%>\\%\n     tibble::enframe(name = \"prob\", value = \"quantile\")\n  })\n\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  group_modify(~ {\n    .x \\%>\\%\n      purrr::map_dfc(fivenum) \\%>\\%\n      mutate(nms = c(\"min\", \"Q1\", \"median\", \"Q3\", \"max\"))\n  })\n\n# group_walk() is for side effects\ndir.create(temp <- tempfile())\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  group_walk(~ write.csv(.x, file = file.path(temp, paste0(.y$Species, \".csv\"))))\nlist.files(temp, pattern = \"csv$\")\nunlink(temp, recursive = TRUE)\n\n# group_modify() and ungrouped data frames\nmtcars \\%>\\%\n  group_modify(~ head(.x, 2L))",
            "group_nest": "#----- use case 1: a grouped data frame\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  group_nest()\n\n# this can be useful if the grouped data has been altered before nesting\niris \\%>\\%\n  group_by(Species) \\%>\\%\n  filter(Sepal.Length > mean(Sepal.Length)) \\%>\\%\n  group_nest()\n\n#----- use case 2: using group_nest() on a ungrouped data frame with\n#                  a grouping specification that uses the data mask\nstarwars \\%>\\%\n  group_nest(species, homeworld)",
            "group_split": "ir <- iris \\%>\\% group_by(Species)\n\ngroup_split(ir)\ngroup_keys(ir)",
            "group_trim": "iris \\%>\\%\n  group_by(Species) \\%>\\%\n  filter(Species == \"setosa\", .preserve = TRUE) \\%>\\%\n  group_trim()",
            "ident": "# Identifiers are escaped with \"\n\\dontshow{if (requireNamespace(\"dbplyr\", quietly = TRUE)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nident(\"x\")\n\\dontshow{\\}) # examplesIf}",
            "if_else": "x <- c(-5:5, NA)\nif_else(x < 0, NA, x)\n\n# Explicitly handle `NA` values in the `condition` with `missing`\nif_else(x < 0, \"negative\", \"positive\", missing = \"missing\")\n\n# Unlike `ifelse()`, `if_else()` preserves types\nx <- factor(sample(letters[1:5], 10, replace = TRUE))\nifelse(x \\%in\\% c(\"a\", \"b\", \"c\"), x, NA)\nif_else(x \\%in\\% c(\"a\", \"b\", \"c\"), x, NA)\n\n# `if_else()` is often useful for creating new columns inside of `mutate()`\nstarwars \\%>\\%\n  mutate(category = if_else(height < 100, \"short\", \"tall\"), .keep = \"used\")",
            "join_by": "sales <- tibble(\n  id = c(1L, 1L, 1L, 2L, 2L),\n  sale_date = as.Date(c(\"2018-12-31\", \"2019-01-02\", \"2019-01-05\", \"2019-01-04\", \"2019-01-01\"))\n)\nsales\n\npromos <- tibble(\n  id = c(1L, 1L, 2L),\n  promo_date = as.Date(c(\"2019-01-01\", \"2019-01-05\", \"2019-01-02\"))\n)\npromos\n\n# Match `id` to `id`, and `sale_date` to `promo_date`\nby <- join_by(id, sale_date == promo_date)\nleft_join(sales, promos, by)\n\n# For each `sale_date` within a particular `id`,\n# find all `promo_date`s that occurred before that particular sale\nby <- join_by(id, sale_date >= promo_date)\nleft_join(sales, promos, by)\n\n# For each `sale_date` within a particular `id`,\n# find only the closest `promo_date` that occurred before that sale\nby <- join_by(id, closest(sale_date >= promo_date))\nleft_join(sales, promos, by)\n\n# If you want to disallow exact matching in rolling joins, use `>` rather\n# than `>=`. Note that the promo on `2019-01-05` is no longer considered the\n# closest match for the sale on the same date.\nby <- join_by(id, closest(sale_date > promo_date))\nleft_join(sales, promos, by)\n\n# Same as before, but also require that the promo had to occur at most 1\n# day before the sale was made. We'll use a full join to see that id 2's\n# promo on `2019-01-02` is no longer matched to the sale on `2019-01-04`.\nsales <- mutate(sales, sale_date_lower = sale_date - 1)\nby <- join_by(id, closest(sale_date >= promo_date), sale_date_lower <= promo_date)\nfull_join(sales, promos, by)\n\n# ---------------------------------------------------------------------------\n\nsegments <- tibble(\n  segment_id = 1:4,\n  chromosome = c(\"chr1\", \"chr2\", \"chr2\", \"chr1\"),\n  start = c(140, 210, 380, 230),\n  end = c(150, 240, 415, 280)\n)\nsegments\n\nreference <- tibble(\n  reference_id = 1:4,\n  chromosome = c(\"chr1\", \"chr1\", \"chr2\", \"chr2\"),\n  start = c(100, 200, 300, 415),\n  end = c(150, 250, 399, 450)\n)\nreference\n\n# Find every time a segment `start` falls between the reference\n# `[start, end]` range.\nby <- join_by(chromosome, between(start, start, end))\nfull_join(segments, reference, by)\n\n# If you wanted the reference columns first, supply `reference` as `x`\n# and `segments` as `y`, then explicitly refer to their columns using `x$`\n# and `y$`.\nby <- join_by(chromosome, between(y$start, x$start, x$end))\nfull_join(reference, segments, by)\n\n# Find every time a segment falls completely within a reference.\n# Sometimes using `x$` and `y$` makes your intentions clearer, even if they\n# match the default behavior.\nby <- join_by(chromosome, within(x$start, x$end, y$start, y$end))\ninner_join(segments, reference, by)\n\n# Find every time a segment overlaps a reference in any way.\nby <- join_by(chromosome, overlaps(x$start, x$end, y$start, y$end))\nfull_join(segments, reference, by)\n\n# It is common to have right-open ranges with bounds like `[)`, which would\n# mean an end value of `415` would no longer overlap a start value of `415`.\n# Setting `bounds` allows you to compute overlaps with those kinds of ranges.\nby <- join_by(chromosome, overlaps(x$start, x$end, y$start, y$end, bounds = \"[)\"))\nfull_join(segments, reference, by)",
            "lead-lag": "lag(1:5)\nlead(1:5)\n\nx <- 1:5\ntibble(behind = lag(x), x, ahead = lead(x))\n\n# If you want to look more rows behind or ahead, use `n`\nlag(1:5, n = 1)\nlag(1:5, n = 2)\n\nlead(1:5, n = 1)\nlead(1:5, n = 2)\n\n# If you want to define a value to pad with, use `default`\nlag(1:5)\nlag(1:5, default = 0)\n\nlead(1:5)\nlead(1:5, default = 6)\n\n# If the data are not already ordered, use `order_by`\nscrambled <- slice_sample(\n  tibble(year = 2000:2005, value = (0:5) ^ 2),\n  prop = 1\n)\n\nwrong <- mutate(scrambled, previous_year_value = lag(value))\narrange(wrong, year)\n\nright <- mutate(scrambled, previous_year_value = lag(value, order_by = year))\narrange(right, year)",
            "mutate-joins": "band_members \\%>\\% inner_join(band_instruments)\nband_members \\%>\\% left_join(band_instruments)\nband_members \\%>\\% right_join(band_instruments)\nband_members \\%>\\% full_join(band_instruments)\n\n# To suppress the message about joining variables, supply `by`\nband_members \\%>\\% inner_join(band_instruments, by = join_by(name))\n# This is good practice in production code\n\n# Use an equality expression if the join variables have different names\nband_members \\%>\\% full_join(band_instruments2, by = join_by(name == artist))\n# By default, the join keys from `x` and `y` are coalesced in the output; use\n# `keep = TRUE` to keep the join keys from both `x` and `y`\nband_members \\%>\\%\n  full_join(band_instruments2, by = join_by(name == artist), keep = TRUE)\n\n# If a row in `x` matches multiple rows in `y`, all the rows in `y` will be\n# returned once for each matching row in `x`.\ndf1 <- tibble(x = 1:3)\ndf2 <- tibble(x = c(1, 1, 2), y = c(\"first\", \"second\", \"third\"))\ndf1 \\%>\\% left_join(df2)\n\n# If a row in `y` also matches multiple rows in `x`, this is known as a\n# many-to-many relationship, which is typically a result of an improperly\n# specified join or some kind of messy data. In this case, a warning is\n# thrown by default:\ndf3 <- tibble(x = c(1, 1, 1, 3))\ndf3 \\%>\\% left_join(df2)\n\n# In the rare case where a many-to-many relationship is expected, set\n# `relationship = \"many-to-many\"` to silence this warning\ndf3 \\%>\\% left_join(df2, relationship = \"many-to-many\")\n\n# Use `join_by()` with a condition other than `==` to perform an inequality\n# join. Here we match on every instance where `df1$x > df2$x`.\ndf1 \\%>\\% left_join(df2, join_by(x > x))\n\n# By default, NAs match other NAs so that there are two\n# rows in the output of this join:\ndf1 <- data.frame(x = c(1, NA), y = 2)\ndf2 <- data.frame(x = c(1, NA), z = 3)\nleft_join(df1, df2)\n\n# You can optionally request that NAs don't match, giving a\n# a result that more closely resembles SQL joins\nleft_join(df1, df2, na_matches = \"never\")",
            "mutate": "# Newly created variables are available immediately\nstarwars \\%>\\%\n  select(name, mass) \\%>\\%\n  mutate(\n    mass2 = mass * 2,\n    mass2_squared = mass2 * mass2\n  )\n\n# As well as adding new variables, you can use mutate() to\n# remove variables and modify existing variables.\nstarwars \\%>\\%\n  select(name, height, mass, homeworld) \\%>\\%\n  mutate(\n    mass = NULL,\n    height = height * 0.0328084 # convert to feet\n  )\n\n# Use across() with mutate() to apply a transformation\n# to multiple columns in a tibble.\nstarwars \\%>\\%\n  select(name, homeworld, species) \\%>\\%\n  mutate(across(!name, as.factor))\n# see more in ?across\n\n# Window functions are useful for grouped mutates:\nstarwars \\%>\\%\n  select(name, mass, homeworld) \\%>\\%\n  group_by(homeworld) \\%>\\%\n  mutate(rank = min_rank(desc(mass)))\n# see `vignette(\"window-functions\")` for more details\n\n# By default, new columns are placed on the far right.\ndf <- tibble(x = 1, y = 2)\ndf \\%>\\% mutate(z = x + y)\ndf \\%>\\% mutate(z = x + y, .before = 1)\ndf \\%>\\% mutate(z = x + y, .after = x)\n\n# By default, mutate() keeps all columns from the input data.\ndf <- tibble(x = 1, y = 2, a = \"a\", b = \"b\")\ndf \\%>\\% mutate(z = x + y, .keep = \"all\") # the default\ndf \\%>\\% mutate(z = x + y, .keep = \"used\")\ndf \\%>\\% mutate(z = x + y, .keep = \"unused\")\ndf \\%>\\% mutate(z = x + y, .keep = \"none\")\n\n# Grouping ----------------------------------------\n# The mutate operation may yield different results on grouped\n# tibbles because the expressions are computed within groups.\n# The following normalises `mass` by the global average:\nstarwars \\%>\\%\n  select(name, mass, species) \\%>\\%\n  mutate(mass_norm = mass / mean(mass, na.rm = TRUE))\n\n# Whereas this normalises `mass` by the averages within species\n# levels:\nstarwars \\%>\\%\n  select(name, mass, species) \\%>\\%\n  group_by(species) \\%>\\%\n  mutate(mass_norm = mass / mean(mass, na.rm = TRUE))\n\n# Indirection ----------------------------------------\n# Refer to column names stored as strings with the `.data` pronoun:\nvars <- c(\"mass\", \"height\")\nmutate(starwars, prod = .data[[vars[[1]]]] * .data[[vars[[2]]]])\n# Learn more in ?rlang::args_data_masking",
            "mutate_all": "iris <- as_tibble(iris)\n\n# All variants can be passed functions and additional arguments,\n# purrr-style. The _at() variants directly support strings. Here\n# we'll scale the variables `height` and `mass`:\nscale2 <- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)\nstarwars \\%>\\% mutate_at(c(\"height\", \"mass\"), scale2)\n# ->\nstarwars \\%>\\% mutate(across(c(\"height\", \"mass\"), scale2))\n\n# You can pass additional arguments to the function:\nstarwars \\%>\\% mutate_at(c(\"height\", \"mass\"), scale2, na.rm = TRUE)\nstarwars \\%>\\% mutate_at(c(\"height\", \"mass\"), ~scale2(., na.rm = TRUE))\n# ->\nstarwars \\%>\\% mutate(across(c(\"height\", \"mass\"), ~ scale2(.x, na.rm = TRUE)))\n\n# You can also supply selection helpers to _at() functions but you have\n# to quote them with vars():\niris \\%>\\% mutate_at(vars(matches(\"Sepal\")), log)\niris \\%>\\% mutate(across(matches(\"Sepal\"), log))\n\n# The _if() variants apply a predicate function (a function that\n# returns TRUE or FALSE) to determine the relevant subset of\n# columns. Here we divide all the numeric columns by 100:\nstarwars \\%>\\% mutate_if(is.numeric, scale2, na.rm = TRUE)\nstarwars \\%>\\% mutate(across(where(is.numeric), ~ scale2(.x, na.rm = TRUE)))\n\n# mutate_if() is particularly useful for transforming variables from\n# one type to another\niris \\%>\\% mutate_if(is.factor, as.character)\niris \\%>\\% mutate_if(is.double, as.integer)\n# ->\niris \\%>\\% mutate(across(where(is.factor), as.character))\niris \\%>\\% mutate(across(where(is.double), as.integer))\n\n# Multiple transformations ----------------------------------------\n\n# If you want to apply multiple transformations, pass a list of\n# functions. When there are multiple functions, they create new\n# variables instead of modifying the variables in place:\niris \\%>\\% mutate_if(is.numeric, list(scale2, log))\niris \\%>\\% mutate_if(is.numeric, list(~scale2(.), ~log(.)))\niris \\%>\\% mutate_if(is.numeric, list(scale = scale2, log = log))\n# ->\niris \\%>\\%\n  as_tibble() \\%>\\%\n  mutate(across(where(is.numeric), list(scale = scale2, log = log)))\n\n# When there's only one function in the list, it modifies existing\n# variables in place. Give it a name to instead create new variables:\niris \\%>\\% mutate_if(is.numeric, list(scale2))\niris \\%>\\% mutate_if(is.numeric, list(scale = scale2))",
            "n_distinct": "x <- c(1, 1, 2, 2, 2)\nn_distinct(x)\n\ny <- c(3, 3, NA, 3, 3)\nn_distinct(y)\nn_distinct(y, na.rm = TRUE)\n\n# Pairs (1, 3), (2, 3), and (2, NA) are distinct\nn_distinct(x, y)\n\n# (2, NA) is dropped, leaving 2 distinct combinations\nn_distinct(x, y, na.rm = TRUE)\n\n# Also works with data frames\nn_distinct(data.frame(x, y))",
            "na_if": "na_if(1:5, 5:1)\n\nx <- c(1, -1, 0, 10)\n100 / x\n100 / na_if(x, 0)\n\ny <- c(\"abc\", \"def\", \"\", \"ghi\")\nna_if(y, \"\")\n\n# `na_if()` allows you to replace `NaN` with `NA`,\n# even though `NaN == NaN` returns `NA`\nz <- c(1, NaN, NA, 2, NaN)\nna_if(z, NaN)\n\n# `na_if()` is particularly useful inside `mutate()`,\n# and is meant for use with vectors rather than entire data frames\nstarwars \\%>\\%\n  select(name, eye_color) \\%>\\%\n  mutate(eye_color = na_if(eye_color, \"unknown\"))\n\n# `na_if()` can also be used with `mutate()` and `across()`\n# to alter multiple columns\nstarwars \\%>\\%\n   mutate(across(where(is.character), ~na_if(., \"unknown\")))",
            "near": "sqrt(2) ^ 2 == 2\nnear(sqrt(2) ^ 2, 2)",
            "nest_by": "# After nesting, you get one row per group\niris \\%>\\% nest_by(Species)\nstarwars \\%>\\% nest_by(species)\n\n# The output is grouped by row, which makes modelling particularly easy\nmodels <- mtcars \\%>\\%\n  nest_by(cyl) \\%>\\%\n  mutate(model = list(lm(mpg ~ wt, data = data)))\nmodels\n\nmodels \\%>\\% summarise(rsq = summary(model)$r.squared)\n\\dontshow{if (requireNamespace(\"broom\", quietly = TRUE)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# This is particularly elegant with the broom functions\nmodels \\%>\\% summarise(broom::glance(model))\nmodels \\%>\\% reframe(broom::tidy(model))\n\\dontshow{\\}) # examplesIf}\n\n# Note that you can also `reframe()` to unnest the data\nmodels \\%>\\% reframe(data)",
            "nest_join": "df1 <- tibble(x = 1:3)\ndf2 <- tibble(x = c(2, 3, 3), y = c(\"a\", \"b\", \"c\"))\n\nout <- nest_join(df1, df2)\nout\nout$df2",
            "new_grouped_df": "# 5 bootstrap samples\ntbl <- new_grouped_df(\n  tibble(x = rnorm(10)),\n  groups = tibble(\".rows\" := replicate(5, sample(1:10, replace = TRUE), simplify = FALSE))\n)\n# mean of each bootstrap sample\nsummarise(tbl, x = mean(x))",
            "nth": "x <- 1:10\ny <- 10:1\n\nfirst(x)\nlast(y)\n\nnth(x, 1)\nnth(x, 5)\nnth(x, -2)\n\n# `first()` and `last()` are often useful in `summarise()`\ndf <- tibble(x = x, y = y)\ndf \\%>\\%\n  summarise(\n    across(x:y, first, .names = \"{col}_first\"),\n    y_last = last(y)\n  )\n\n# Selecting a position that is out of bounds returns a default value\nnth(x, 11)\nnth(x, 0)\n\n# This out of bounds behavior also applies to empty vectors\nfirst(integer())\n\n# You can customize the default value with `default`\nnth(x, 11, default = -1L)\nfirst(integer(), default = 0L)\n\n# `order_by` provides optional ordering\nlast(x)\nlast(x, order_by = y)\n\n# `na_rm` removes missing values before extracting the value\nz <- c(NA, NA, 1, 3, NA, 5, NA)\nfirst(z)\nfirst(z, na_rm = TRUE)\nlast(z, na_rm = TRUE)\nnth(z, 3, na_rm = TRUE)\n\n# For data frames, these select entire rows\ndf <- tibble(a = 1:5, b = 6:10)\nfirst(df)\nnth(df, 4)",
            "ntile": "x <- c(5, 1, 3, 2, 2, NA)\nntile(x, 2)\nntile(x, 4)\n\n# If the bucket sizes are uneven, the larger buckets come first\nntile(1:8, 3)\n\n# Ties are ignored\nntile(rep(1, 8), 3)",
            "order_by": "order_by(10:1, cumsum(1:10))\nx <- 10:1\ny <- 1:10\norder_by(x, cumsum(y))\n\ndf <- data.frame(year = 2000:2005, value = (0:5) ^ 2)\nscrambled <- df[sample(nrow(df)), ]\n\nwrong <- mutate(scrambled, running = cumsum(value))\narrange(wrong, year)\n\nright <- mutate(scrambled, running = order_by(year, cumsum(value)))\narrange(right, year)",
            "percent_rank": "x <- c(5, 1, 3, 2, 2)\n\ncume_dist(x)\npercent_rank(x)\n\n# You can understand what's going on by computing it by hand\nsapply(x, function(xi) sum(x <= xi) / length(x))\nsapply(x, function(xi) sum(x < xi)  / (length(x) - 1))\n# The real computations are a little more complex in order to\n# correctly deal with missing values",
            "pick": "df <- tibble(\n  x = c(3, 2, 2, 2, 1),\n  y = c(0, 2, 1, 1, 4),\n  z1 = c(\"a\", \"a\", \"a\", \"b\", \"a\"),\n  z2 = c(\"c\", \"d\", \"d\", \"a\", \"c\")\n)\ndf\n\n# `pick()` provides a way to select a subset of your columns using\n# tidyselect. It returns a data frame.\ndf \\%>\\% mutate(cols = pick(x, y))\n\n# This is useful for functions that take data frames as inputs.\n# For example, you can compute a joint rank between `x` and `y`.\ndf \\%>\\% mutate(rank = dense_rank(pick(x, y)))\n\n# `pick()` is also useful as a bridge between data-masking functions (like\n# `mutate()` or `group_by()`) and functions with tidy-select behavior (like\n# `select()`). For example, you can use `pick()` to create a wrapper around\n# `group_by()` that takes a tidy-selection of columns to group on. For more\n# bridge patterns, see\n# https://rlang.r-lib.org/reference/topic-data-mask-programming.html#bridge-patterns.\nmy_group_by <- function(data, cols) {\n  group_by(data, pick({{ cols }}))\n}\n\ndf \\%>\\% my_group_by(c(x, starts_with(\"z\")))\n\n# Or you can use it to dynamically select columns to `count()` by\ndf \\%>\\% count(pick(starts_with(\"z\")))",
            "progress_estimated": "p <- progress_estimated(3)\np$tick()\np$tick()\np$tick()\n\np <- progress_estimated(3)\nfor (i in 1:3) p$pause(0.1)$tick()$print()\n\np <- progress_estimated(3)\np$tick()$print()$\n pause(1)$stop()\n\n# If min_time is set, progress bar not shown until that many\n# seconds have elapsed\np <- progress_estimated(3, min_time = 3)\nfor (i in 1:3) p$pause(0.1)$tick()$print()\n\n\\dontrun{\np <- progress_estimated(10, min_time = 3)\nfor (i in 1:10) p$pause(0.5)$tick()$print()\n}",
            "pull": "mtcars \\%>\\% pull(-1)\nmtcars \\%>\\% pull(1)\nmtcars \\%>\\% pull(cyl)\n\\dontshow{if (requireNamespace(\"dbplyr\", quietly = TRUE) && requireNamespace(\"RSQLite\", quietly = TRUE)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# Also works for remote sources\ndf <- dbplyr::memdb_frame(x = 1:10, y = 10:1, .name = \"pull-ex\")\ndf \\%>\\%\n  mutate(z = x * y) \\%>\\%\n  pull()\n\\dontshow{\\}) # examplesIf}\n\n# Pull a named vector\nstarwars \\%>\\% pull(height, name)",
            "recode": "char_vec <- sample(c(\"a\", \"b\", \"c\"), 10, replace = TRUE)\n\n# `recode()` is superseded by `case_match()`\nrecode(char_vec, a = \"Apple\", b = \"Banana\")\ncase_match(char_vec, \"a\" ~ \"Apple\", \"b\" ~ \"Banana\", .default = char_vec)\n\n# With `case_match()`, you don't need typed missings like `NA_character_`\nrecode(char_vec, a = \"Apple\", b = \"Banana\", .default = NA_character_)\ncase_match(char_vec, \"a\" ~ \"Apple\", \"b\" ~ \"Banana\", .default = NA)\n\n# Throws an error as `NA` is logical, not character.\ntry(recode(char_vec, a = \"Apple\", b = \"Banana\", .default = NA))\n\n# `case_match()` is easier to use with numeric vectors, because you don't\n# need to turn the numeric values into names\nnum_vec <- c(1:4, NA)\nrecode(num_vec, `2` = 20L, `4` = 40L)\ncase_match(num_vec, 2 ~ 20, 4 ~ 40, .default = num_vec)\n\n# `case_match()` doesn't have the ability to match by position like\n# `recode()` does with numeric vectors\nrecode(num_vec, \"a\", \"b\", \"c\", \"d\")\nrecode(c(1,5,3), \"a\", \"b\", \"c\", \"d\", .default = \"nothing\")\n\n# For `case_match()`, incompatible types are an error rather than a warning\nrecode(num_vec, `2` = \"b\", `4` = \"d\")\ntry(case_match(num_vec, 2 ~ \"b\", 4 ~ \"d\", .default = num_vec))\n\n# The factor method of `recode()` can generally be replaced with\n# `forcats::fct_recode()`\nfactor_vec <- factor(c(\"a\", \"b\", \"c\"))\nrecode(factor_vec, a = \"Apple\")\n\n# `recode_factor()` does not currently have a direct replacement, but we\n# plan to add one to forcats. In the meantime, you can use the `.ptype`\n# argument to `case_match()`.\nrecode_factor(\n  num_vec,\n  `1` = \"z\",\n  `2` = \"y\",\n  `3` = \"x\",\n  .default = \"D\",\n  .missing = \"M\"\n)\ncase_match(\n  num_vec,\n  1 ~ \"z\",\n  2 ~ \"y\",\n  3 ~ \"x\",\n  NA ~ \"M\",\n  .default = \"D\",\n  .ptype = factor(levels = c(\"z\", \"y\", \"x\", \"D\", \"M\"))\n)",
            "reframe": "table <- c(\"a\", \"b\", \"d\", \"f\")\n\ndf <- tibble(\n  g = c(1, 1, 1, 2, 2, 2, 2),\n  x = c(\"e\", \"a\", \"b\", \"c\", \"f\", \"d\", \"a\")\n)\n\n# `reframe()` allows you to apply functions that return\n# an arbitrary number of rows\ndf \\%>\\%\n  reframe(x = intersect(x, table))\n\n# Functions are applied per group, and each group can return a\n# different number of rows.\ndf \\%>\\%\n  reframe(x = intersect(x, table), .by = g)\n\n# The output is always ungrouped, even when using `group_by()`\ndf \\%>\\%\n  group_by(g) \\%>\\%\n  reframe(x = intersect(x, table))\n\n# You can add multiple columns at once using a single expression by returning\n# a data frame.\nquantile_df <- function(x, probs = c(0.25, 0.5, 0.75)) {\n  tibble(\n    val = quantile(x, probs, na.rm = TRUE),\n    quant = probs\n  )\n}\n\nx <- c(10, 15, 18, 12)\nquantile_df(x)\n\nstarwars \\%>\\%\n  reframe(quantile_df(height))\n\nstarwars \\%>\\%\n  reframe(quantile_df(height), .by = homeworld)\n\nstarwars \\%>\\%\n  reframe(\n    across(c(height, mass), quantile_df, .unpack = TRUE),\n    .by = homeworld\n  )",
            "relocate": "df <- tibble(a = 1, b = 1, c = 1, d = \"a\", e = \"a\", f = \"a\")\ndf \\%>\\% relocate(f)\ndf \\%>\\% relocate(a, .after = c)\ndf \\%>\\% relocate(f, .before = b)\ndf \\%>\\% relocate(a, .after = last_col())\n\n# relocated columns can change name\ndf \\%>\\% relocate(ff = f)\n\n# Can also select variables based on their type\ndf \\%>\\% relocate(where(is.character))\ndf \\%>\\% relocate(where(is.numeric), .after = last_col())\n# Or with any other select helper\ndf \\%>\\% relocate(any_of(c(\"a\", \"e\", \"i\", \"o\", \"u\")))\n\n# When .before or .after refers to multiple variables they will be\n# moved to be immediately before/after the selected variables.\ndf2 <- tibble(a = 1, b = \"a\", c = 1, d = \"a\")\ndf2 \\%>\\% relocate(where(is.numeric), .after = where(is.character))\ndf2 \\%>\\% relocate(where(is.numeric), .before = where(is.character))",
            "rename": "iris <- as_tibble(iris) # so it prints a little nicer\nrename(iris, petal_length = Petal.Length)\n\n# Rename using a named vector and `all_of()`\nlookup <- c(pl = \"Petal.Length\", sl = \"Sepal.Length\")\nrename(iris, all_of(lookup))\n\n# If your named vector might contain names that don't exist in the data,\n# use `any_of()` instead\nlookup <- c(lookup, new = \"unknown\")\ntry(rename(iris, all_of(lookup)))\nrename(iris, any_of(lookup))\n\nrename_with(iris, toupper)\nrename_with(iris, toupper, starts_with(\"Petal\"))\nrename_with(iris, ~ tolower(gsub(\".\", \"_\", .x, fixed = TRUE)))\n\n\\dontshow{if (getRversion() > \"4.0.1\") (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# If your renaming function uses `paste0()`, make sure to set\n# `recycle0 = TRUE` to ensure that empty selections are recycled correctly\ntry(rename_with(\n  iris,\n  ~ paste0(\"prefix_\", .x),\n  starts_with(\"nonexistent\")\n))\n\nrename_with(\n  iris,\n  ~ paste0(\"prefix_\", .x, recycle0 = TRUE),\n  starts_with(\"nonexistent\")\n)\n\\dontshow{\\}) # examplesIf}",
            "row_number": "x <- c(5, 1, 3, 2, 2, NA)\nrow_number(x)\nmin_rank(x)\ndense_rank(x)\n\n# Ranking functions can be used in `filter()` to select top/bottom rows\ndf <- data.frame(\n  grp = c(1, 1, 1, 2, 2, 2, 3, 3, 3),\n  x = c(3, 2, 1, 1, 2, 2, 1, 1, 1),\n  y = c(1, 3, 2, 3, 2, 2, 4, 1, 2),\n  id = 1:9\n)\n# Always gives exactly 1 row per group\ndf \\%>\\% group_by(grp) \\%>\\% filter(row_number(x) == 1)\n# May give more than 1 row if ties\ndf \\%>\\% group_by(grp) \\%>\\% filter(min_rank(x) == 1)\n# Rank by multiple columns (to break ties) by selecting them with `pick()`\ndf \\%>\\% group_by(grp) \\%>\\% filter(min_rank(pick(x, y)) == 1)\n# See slice_min() and slice_max() for another way to tackle the same problem\n\n# You can use row_number() without an argument to refer to the \"current\"\n# row number.\ndf \\%>\\% group_by(grp) \\%>\\% filter(row_number() == 1)\n\n# It's easiest to see what this does with mutate():\ndf \\%>\\% group_by(grp) \\%>\\% mutate(grp_id = row_number())",
            "rows": "data <- tibble(a = 1:3, b = letters[c(1:2, NA)], c = 0.5 + 0:2)\ndata\n\n# Insert\nrows_insert(data, tibble(a = 4, b = \"z\"))\n\n# By default, if a key in `y` matches a key in `x`, then it can't be inserted\n# and will throw an error. Alternatively, you can ignore rows in `y`\n# containing keys that conflict with keys in `x` with `conflict = \"ignore\"`,\n# or you can use `rows_append()` to ignore keys entirely.\ntry(rows_insert(data, tibble(a = 3, b = \"z\")))\nrows_insert(data, tibble(a = 3, b = \"z\"), conflict = \"ignore\")\nrows_append(data, tibble(a = 3, b = \"z\"))\n\n# Update\nrows_update(data, tibble(a = 2:3, b = \"z\"))\nrows_update(data, tibble(b = \"z\", a = 2:3), by = \"a\")\n\n# Variants: patch and upsert\nrows_patch(data, tibble(a = 2:3, b = \"z\"))\nrows_upsert(data, tibble(a = 2:4, b = \"z\"))\n\n# Delete and truncate\nrows_delete(data, tibble(a = 2:3))\nrows_delete(data, tibble(a = 2:3, b = \"b\"))\n\n# By default, for update, patch, and delete it is an error if a key in `y`\n# doesn't exist in `x`. You can ignore rows in `y` that have unmatched keys\n# with `unmatched = \"ignore\"`.\ny <- tibble(a = 3:4, b = \"z\")\ntry(rows_update(data, y, by = \"a\"))\nrows_update(data, y, by = \"a\", unmatched = \"ignore\")\nrows_patch(data, y, by = \"a\", unmatched = \"ignore\")\nrows_delete(data, y, by = \"a\", unmatched = \"ignore\")",
            "rowwise": "df <- tibble(x = runif(6), y = runif(6), z = runif(6))\n# Compute the mean of x, y, z in each row\ndf \\%>\\% rowwise() \\%>\\% mutate(m = mean(c(x, y, z)))\n# use c_across() to more easily select many variables\ndf \\%>\\% rowwise() \\%>\\% mutate(m = mean(c_across(x:z)))\n\n# Compute the minimum of x and y in each row\ndf \\%>\\% rowwise() \\%>\\% mutate(m = min(c(x, y, z)))\n# In this case you can use an existing vectorised function:\ndf \\%>\\% mutate(m = pmin(x, y, z))\n# Where these functions exist they'll be much faster than rowwise\n# so be on the lookout for them.\n\n# rowwise() is also useful when doing simulations\nparams <- tribble(\n ~sim, ~n, ~mean, ~sd,\n    1,  1,     1,   1,\n    2,  2,     2,   4,\n    3,  3,    -1,   2\n)\n# Here I supply variables to preserve after the computation\nparams \\%>\\%\n  rowwise(sim) \\%>\\%\n  reframe(z = rnorm(n, mean, sd))\n\n# If you want one row per simulation, put the results in a list()\nparams \\%>\\%\n  rowwise(sim) \\%>\\%\n  summarise(z = list(rnorm(n, mean, sd)), .groups = \"keep\")",
            "sample_n": "df <- tibble(x = 1:5, w = c(0.1, 0.1, 0.1, 2, 2))\n\n# sample_n() -> slice_sample() ----------------------------------------------\n# Was:\nsample_n(df, 3)\nsample_n(df, 10, replace = TRUE)\nsample_n(df, 3, weight = w)\n\n# Now:\nslice_sample(df, n = 3)\nslice_sample(df, n = 10, replace = TRUE)\nslice_sample(df, n = 3, weight_by = w)\n\n# Note that sample_n() would error if n was bigger than the group size\n# slice_sample() will just use the available rows for consistency with\n# the other slice helpers like slice_head()\ntry(sample_n(df, 10))\nslice_sample(df, n = 10)\n\n# sample_frac() -> slice_sample() -------------------------------------------\n# Was:\nsample_frac(df, 0.25)\nsample_frac(df, 2, replace = TRUE)\n\n# Now:\nslice_sample(df, prop = 0.25)\nslice_sample(df, prop = 2, replace = TRUE)",
            "select_all": "mtcars <- as_tibble(mtcars) # for nicer printing\n\nmtcars \\%>\\% rename_all(toupper)\n# ->\nmtcars \\%>\\% rename_with(toupper)\n\n# NB: the transformation comes first in rename_with\nis_whole <- function(x) all(floor(x) == x)\nmtcars \\%>\\% rename_if(is_whole, toupper)\n# ->\nmtcars \\%>\\% rename_with(toupper, where(is_whole))\n\nmtcars \\%>\\% rename_at(vars(mpg:hp), toupper)\n# ->\nmtcars \\%>\\% rename_with(toupper, mpg:hp)\n\n# You now must select() and then rename\n\nmtcars \\%>\\% select_all(toupper)\n# ->\nmtcars \\%>\\% rename_with(toupper)\n\n# Selection drops unselected variables:\nmtcars \\%>\\% select_if(is_whole, toupper)\n# ->\nmtcars \\%>\\% select(where(is_whole)) \\%>\\% rename_with(toupper)\n\nmtcars \\%>\\% select_at(vars(-contains(\"ar\"), starts_with(\"c\")), toupper)\n# ->\nmtcars \\%>\\%\n  select(!contains(\"ar\") | starts_with(\"c\")) \\%>\\%\n  rename_with(toupper)",
            "setops": "df1 <- tibble(x = 1:3)\ndf2 <- tibble(x = 3:5)\n\nintersect(df1, df2)\nunion(df1, df2)\nunion_all(df1, df2)\nsetdiff(df1, df2)\nsetdiff(df2, df1)\nsymdiff(df1, df2)\n\nsetequal(df1, df2)\nsetequal(df1, df1[3:1, ])\n\n# Note that the following functions remove pre-existing duplicates:\ndf1 <- tibble(x = c(1:3, 3, 3))\ndf2 <- tibble(x = c(3:5, 5))\n\nintersect(df1, df2)\nunion(df1, df2)\nsetdiff(df1, df2)\nsymdiff(df1, df2)",
            "slice": "# Similar to head(mtcars, 1):\nmtcars \\%>\\% slice(1L)\n# Similar to tail(mtcars, 1):\nmtcars \\%>\\% slice(n())\nmtcars \\%>\\% slice(5:n())\n# Rows can be dropped with negative indices:\nslice(mtcars, -(1:4))\n\n# First and last rows based on existing order\nmtcars \\%>\\% slice_head(n = 5)\nmtcars \\%>\\% slice_tail(n = 5)\n\n# Rows with minimum and maximum values of a variable\nmtcars \\%>\\% slice_min(mpg, n = 5)\nmtcars \\%>\\% slice_max(mpg, n = 5)\n\n# slice_min() and slice_max() may return more rows than requested\n# in the presence of ties.\nmtcars \\%>\\% slice_min(cyl, n = 1)\n# Use with_ties = FALSE to return exactly n matches\nmtcars \\%>\\% slice_min(cyl, n = 1, with_ties = FALSE)\n# Or use additional variables to break the tie:\nmtcars \\%>\\% slice_min(tibble(cyl, mpg), n = 1)\n\n# slice_sample() allows you to random select with or without replacement\nmtcars \\%>\\% slice_sample(n = 5)\nmtcars \\%>\\% slice_sample(n = 5, replace = TRUE)\n\n# You can optionally weight by a variable - this code weights by the\n# physical weight of the cars, so heavy cars are more likely to get\n# selected.\nmtcars \\%>\\% slice_sample(weight_by = wt, n = 5)\n\n# Group wise operation ----------------------------------------\ndf <- tibble(\n  group = rep(c(\"a\", \"b\", \"c\"), c(1, 2, 4)),\n  x = runif(7)\n)\n\n# All slice helpers operate per group, silently truncating to the group\n# size, so the following code works without error\ndf \\%>\\% group_by(group) \\%>\\% slice_head(n = 2)\n\n# When specifying the proportion of rows to include non-integer sizes\n# are rounded down, so group a gets 0 rows\ndf \\%>\\% group_by(group) \\%>\\% slice_head(prop = 0.5)\n\n# Filter equivalents --------------------------------------------\n# slice() expressions can often be written to use `filter()` and\n# `row_number()`, which can also be translated to SQL. For many databases,\n# you'll need to supply an explicit variable to use to compute the row number.\nfilter(mtcars, row_number() == 1L)\nfilter(mtcars, row_number() == n())\nfilter(mtcars, between(row_number(), 5, n()))",
            "src_dbi": "\\dontshow{if (requireNamespace(\"dbplyr\", quietly = TRUE) && requireNamespace(\"RSQLite\", quietly = TRUE)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\ncopy_to(con, mtcars)\n\n# To retrieve a single table from a source, use `tbl()`\nmtcars <- con \\%>\\% tbl(\"mtcars\")\nmtcars\n\n# You can also use pass raw SQL if you want a more sophisticated query\ncon \\%>\\% tbl(sql(\"SELECT * FROM mtcars WHERE cyl == 8\"))\n\\dontshow{\\}) # examplesIf}"
        }
    },
    "vctrs": {
        "description": "Defines new notions of prototype and size that are used to\n    provide tools for consistent and well-founded type-coercion and\n    size-recycling, and are in turn connected to ideas of type- and\n    size-stability useful for analysing function interfaces.",
        "examples": {
            "data_frame": "data_frame(x = 1, y = 2)\n\n# Inputs are recycled using tidyverse recycling rules\ndata_frame(x = 1, y = 1:3)\n\n# Strings are never converted to factors\nclass(data_frame(x = \"foo\")$x)\n\n# List columns can be easily created\ndf <- data_frame(x = list(1:2, 2, 3:4), y = 3:1)\n\n# However, the base print method is suboptimal for displaying them,\n# so it is recommended to convert them to tibble\nif (rlang::is_installed(\"tibble\")) {\n  tibble::as_tibble(df)\n}\n\n# Named data frame inputs create data frame columns\ndf <- data_frame(x = data_frame(y = 1:2, z = \"a\"))\n\n# The `x` column itself is another data frame\ndf$x\n\n# Again, it is recommended to convert these to tibbles for a better\n# print method\nif (rlang::is_installed(\"tibble\")) {\n  tibble::as_tibble(df)\n}\n\n# Unnamed data frame input is automatically unpacked\ndata_frame(x = 1, data_frame(y = 1:2, z = \"a\"))",
            "df_list": "# `new_data_frame()` can be used to create custom data frame constructors\nnew_fancy_df <- function(x = list(), n = NULL, ..., class = NULL) {\n  new_data_frame(x, n = n, ..., class = c(class, \"fancy_df\"))\n}\n\n# Combine this constructor with `df_list()` to create a safe,\n# consistent helper function for your data frame subclass\nfancy_df <- function(...) {\n  data <- df_list(...)\n  new_fancy_df(data)\n}\n\ndf <- fancy_df(x = 1)\nclass(df)",
            "fields": "x <- new_rcrd(list(x = 1:3, y = 3:1, z = letters[1:3]))\nn_fields(x)\nfields(x)\n\nfield(x, \"y\")\nfield(x, \"y\") <- runif(3)\nfield(x, \"y\")",
            "list_drop_empty": "x <- list(1, NULL, integer(), 2)\nlist_drop_empty(x)",
            "list_of": "x <- list_of(1:3, 5:6, 10:15)\nif (requireNamespace(\"tibble\", quietly = TRUE)) {\n  tibble::tibble(x = x)\n}\n\nvec_c(list_of(1, 2), list_of(FALSE, TRUE))",
            "missing": "x <- c(1, 2, NA, 4, NA)\n\nvec_detect_missing(x)\nvec_any_missing(x)\n\n# Data frames are iterated over rowwise, and only report a row as missing\n# if every element of that row is missing. If a row is only partially\n# missing, it is said to be incomplete, but not missing.\ny <- c(\"a\", \"b\", NA, \"d\", \"e\")\ndf <- data_frame(x = x, y = y)\n\ndf$missing <- vec_detect_missing(df)\ndf$incomplete <- !vec_detect_complete(df)\ndf",
            "name_spec": "# By default, named inputs must be length 1:\nvec_c(name = 1)         # ok\ntry(vec_c(name = 1:3))  # bad\n\n# They also can't have internal names, even if scalar:\ntry(vec_c(name = c(internal = 1)))  # bad\n\n# Pass a name specification to work around this. A specification\n# can be a glue string referring to `outer` and `inner`:\nvec_c(name = 1:3, other = 4:5, .name_spec = \"{outer}\")\nvec_c(name = 1:3, other = 4:5, .name_spec = \"{outer}_{inner}\")\n\n# They can also be functions:\nmy_spec <- function(outer, inner) paste(outer, inner, sep = \"_\")\nvec_c(name = 1:3, other = 4:5, .name_spec = my_spec)\n\n# Or purrr-style formulas for anonymous functions:\nvec_c(name = 1:3, other = 4:5, .name_spec = ~ paste0(.x, .y))",
            "new_data_frame": "new_data_frame(list(x = 1:10, y = 10:1))",
            "new_date": "new_date(0)\nnew_datetime(0, tzone = \"UTC\")\nnew_duration(1, \"hours\")",
            "obj_is_list": "obj_is_list(list())\nobj_is_list(list_of(1))\nobj_is_list(data.frame())\n\nlist_all_vectors(list(1, mtcars))\nlist_all_vectors(list(1, environment()))\n\nlist_all_size(list(1:2, 2:3), 2)\nlist_all_size(list(1:2, 2:4), 2)\n\n# `list_`-prefixed functions assume a list:\ntry(list_all_vectors(environment()))",
            "op-empty-default": "1:10 \\%0\\% 5\ninteger() \\%0\\% 5",
            "order-radix": "if (FALSE) {\n\nx <- round(sample(runif(5), 9, replace = TRUE), 3)\nx <- c(x, NA)\n\nvec_order_radix(x)\nvec_sort_radix(x)\nvec_sort_radix(x, direction = \"desc\")\n\n# Can also handle data frames\ndf <- data.frame(g = sample(2, 10, replace = TRUE), x = x)\nvec_order_radix(df)\nvec_sort_radix(df)\nvec_sort_radix(df, direction = \"desc\")\n\n# For data frames, `direction` and `na_value` are allowed to be vectors\n# with length equal to the number of columns in the data frame\nvec_sort_radix(\n  df,\n  direction = c(\"desc\", \"asc\"),\n  na_value = c(\"largest\", \"smallest\")\n)\n\n# Character vectors are ordered in the C locale, which orders capital letters\n# below lowercase ones\ny <- c(\"B\", \"A\", \"a\")\nvec_sort_radix(y)\n\n# To order in a case-insensitive manner, provide a `chr_proxy_collate`\n# function that transforms the strings to all lowercase\nvec_sort_radix(y, chr_proxy_collate = tolower)\n\n}",
            "partial_factor": "pf <- partial_factor(levels = c(\"x\", \"y\"))\npf\n\nvec_ptype_common(factor(\"v\"), factor(\"w\"), .ptype = pf)",
            "partial_frame": "pf <- partial_frame(x = double())\npf\n\nvec_rbind(\n  data.frame(x = 1L, y = \"a\"),\n  data.frame(x = FALSE, z = 10),\n  .ptype = partial_frame(x = double(), a = character())\n)",
            "runs": "x <- c(\"a\", \"z\", \"z\", \"c\", \"a\", \"a\")\n\nvec_identify_runs(x)\nvec_run_sizes(x)\nvec_unrep(x)\n\ny <- c(1, 1, 1, 2, 2, 3)\n\n# With multiple columns, the runs are constructed rowwise\ndf <- data_frame(\n  x = x,\n  y = y\n)\n\nvec_identify_runs(df)\nvec_run_sizes(df)\nvec_unrep(df)",
            "s3_register": "# A typical use case is to dynamically register tibble/pillar methods\n# for your class. That way you avoid creating a hard dependency on packages\n# that are not essential, while still providing finer control over\n# printing when they are used.\n\n.onLoad <- function(...) {\n  s3_register(\"pillar::pillar_shaft\", \"vctrs_vctr\")\n  s3_register(\"tibble::type_sum\", \"vctrs_vctr\")\n}",
            "unspecified": "vec_ptype_show()\nvec_ptype_show(NA)\n\nvec_c(NA, factor(\"x\"))\nvec_c(NA, Sys.Date())\nvec_c(NA, Sys.time())\nvec_c(NA, list(1:3, 4:5))",
            "vctrs-conditions": "# Most of the time, `maybe_lossy_cast()` returns its input normally:\nmaybe_lossy_cast(\n  c(\"foo\", \"bar\"),\n  NA,\n  \"\",\n  lossy = c(FALSE, FALSE),\n  x_arg = \"\",\n  to_arg = \"\"\n)\n\n# If `lossy` has any `TRUE`, an error is thrown:\ntry(maybe_lossy_cast(\n  c(\"foo\", \"bar\"),\n  NA,\n  \"\",\n  lossy = c(FALSE, TRUE),\n  x_arg = \"\",\n  to_arg = \"\"\n))\n\n# Unless lossy casts are allowed:\nallow_lossy_cast(\n  maybe_lossy_cast(\n    c(\"foo\", \"bar\"),\n    NA,\n    \"\",\n    lossy = c(FALSE, TRUE),\n    x_arg = \"\",\n    to_arg = \"\"\n  )\n)",
            "vec-rep": "# Repeat the entire vector\nvec_rep(1:2, 3)\n\n# Repeat within each vector\nvec_rep_each(1:2, 3)\nx <- vec_rep_each(1:2, c(3, 4))\nx\n\n# After using `vec_rep_each()`, you can recover the original vector\n# with `vec_unrep()`\nvec_unrep(x)\n\ndf <- data.frame(x = 1:2, y = 3:4)\n\n# `rep()` repeats columns of data frames, and returns lists\nrep(df, each = 2)\n\n# `vec_rep()` and `vec_rep_each()` repeat rows, and return data frames\nvec_rep(df, 2)\nvec_rep_each(df, 2)\n\n# `rle()` treats adjacent missing values as different\ny <- c(1, NA, NA, 2)\nrle(y)\n\n# `vec_unrep()` treats them as equivalent\nvec_unrep(y)",
            "vec-set": "x <- c(1, 2, 1, 4, 3)\ny <- c(2, 5, 5, 1)\n\n# All unique values in both `x` and `y`.\n# Duplicates in `x` and `y` are always removed.\nvec_set_intersect(x, y)\n\n# All unique values in `x` but not `y`\nvec_set_difference(x, y)\n\n# All unique values in either `x` or `y`\nvec_set_union(x, y)\n\n# All unique values in either `x` or `y` but not both\nvec_set_symmetric_difference(x, y)\n\n# These functions can also be used with data frames\nx <- data_frame(\n  a = c(2, 3, 2, 2),\n  b = c(\"j\", \"k\", \"j\", \"l\")\n)\ny <- data_frame(\n  a = c(1, 2, 2, 2, 3),\n  b = c(\"j\", \"l\", \"j\", \"l\", \"j\")\n)\n\nvec_set_intersect(x, y)\nvec_set_difference(x, y)\nvec_set_union(x, y)\nvec_set_symmetric_difference(x, y)\n\n# Vector names don't affect set membership, but if you'd like to force\n# them to, you can transform the vector into a two column data frame\nx <- c(a = 1, b = 2, c = 2, d = 3)\ny <- c(c = 2, b = 1, a = 3, d = 3)\n\nvec_set_intersect(x, y)\n\nx <- data_frame(name = names(x), value = unname(x))\ny <- data_frame(name = names(y), value = unname(y))\n\nvec_set_intersect(x, y)",
            "vec_arith": "d <- as.Date(\"2018-01-01\")\ndt <- as.POSIXct(\"2018-01-02 12:00\")\nt <- as.difftime(12, unit = \"hours\")\n\nvec_arith(\"-\", dt, 1)\nvec_arith(\"-\", dt, t)\nvec_arith(\"-\", dt, d)\n\nvec_arith(\"+\", dt, 86400)\nvec_arith(\"+\", dt, t)\nvec_arith(\"+\", t, t)\n\nvec_arith(\"/\", t, t)\nvec_arith(\"/\", t, 2)\n\nvec_arith(\"*\", t, 2)",
            "vec_as_location": "x <- array(1:6, c(2, 3))\ndimnames(x) <- list(c(\"r1\", \"r2\"), c(\"c1\", \"c2\", \"c3\"))\n\n# The most common use case validates row indices\nvec_as_location(1, vec_size(x))\n\n# Negative indices can be used to index from the back\nvec_as_location(-1, vec_size(x))\n\n# Character vectors can be used if `names` are provided\nvec_as_location(\"r2\", vec_size(x), rownames(x))\n\n# You can also construct an index for dimensions other than the first\nvec_as_location(c(\"c2\", \"c1\"), ncol(x), colnames(x))",
            "vec_as_names": "# By default, `vec_as_names()` returns minimal names:\nvec_as_names(c(NA, NA, \"foo\"))\n\n# You can make them unique:\nvec_as_names(c(NA, NA, \"foo\"), repair = \"unique\")\n\n# Universal repairing fixes any non-syntactic name:\nvec_as_names(c(\"_foo\", \"+\"), repair = \"universal\")",
            "vec_as_names_legacy": "if (rlang::is_installed(\"tibble\")) {\n\nlibrary(tibble)\n\n# Names repair is turned off by default in tibble:\ntry(tibble(a = 1, a = 2))\n\n# You can turn it on by supplying a repair method:\ntibble(a = 1, a = 2, .name_repair = \"universal\")\n\n# If you prefer the legacy method, use `vec_as_names_legacy()`:\ntibble(a = 1, a = 2, .name_repair = vec_as_names_legacy)\n\n}",
            "vec_bind": "# row binding -----------------------------------------\n\n# common columns are coerced to common class\nvec_rbind(\n  data.frame(x = 1),\n  data.frame(x = FALSE)\n)\n\n# unique columns are filled with NAs\nvec_rbind(\n  data.frame(x = 1),\n  data.frame(y = \"x\")\n)\n\n# null inputs are ignored\nvec_rbind(\n  data.frame(x = 1),\n  NULL,\n  data.frame(x = 2)\n)\n\n# bare vectors are treated as rows\nvec_rbind(\n  c(x = 1, y = 2),\n  c(x = 3)\n)\n\n# default names will be supplied if arguments are not named\nvec_rbind(\n  1:2,\n  1:3,\n  1:4\n)\n\n# column binding --------------------------------------\n\n# each input is recycled to have common length\nvec_cbind(\n  data.frame(x = 1),\n  data.frame(y = 1:3)\n)\n\n# bare vectors are treated as columns\nvec_cbind(\n  data.frame(x = 1),\n  y = letters[1:3]\n)\n\n# if you supply a named data frame, it is packed in a single column\ndata <- vec_cbind(\n  x = data.frame(a = 1, b = 2),\n  y = 1\n)\ndata\n\n# Packed data frames are nested in a single column. This makes it\n# possible to access it through a single name:\ndata$x\n\n# since the base print method is suboptimal with packed data\n# frames, it is recommended to use tibble to work with these:\nif (rlang::is_installed(\"tibble\")) {\n  vec_cbind(x = tibble::tibble(a = 1, b = 2), y = 1)\n}\n\n# duplicate names are flagged\nvec_cbind(x = 1, x = 2)",
            "vec_c": "vec_c(FALSE, 1L, 1.5)\n\n# Date/times --------------------------\nc(Sys.Date(), Sys.time())\nc(Sys.time(), Sys.Date())\n\nvec_c(Sys.Date(), Sys.time())\nvec_c(Sys.time(), Sys.Date())\n\n# Factors -----------------------------\nc(factor(\"a\"), factor(\"b\"))\nvec_c(factor(\"a\"), factor(\"b\"))\n\n\n# By default, named inputs must be length 1:\nvec_c(name = 1)\ntry(vec_c(name = 1:3))\n\n# Pass a name specification to work around this:\nvec_c(name = 1:3, .name_spec = \"{outer}_{inner}\")\n\n# See `?name_spec` for more examples of name specifications.",
            "vec_cast": "# x is a double, but no information is lost\nvec_cast(1, integer())\n\n# When information is lost the cast fails\ntry(vec_cast(c(1, 1.5), integer()))\ntry(vec_cast(c(1, 2), logical()))\n\n# You can suppress this error and get the partial results\nallow_lossy_cast(vec_cast(c(1, 1.5), integer()))\nallow_lossy_cast(vec_cast(c(1, 2), logical()))\n\n# By default this suppress all lossy cast errors without\n# distinction, but you can be specific about what cast is allowed\n# by supplying prototypes\nallow_lossy_cast(vec_cast(c(1, 1.5), integer()), to_ptype = integer())\ntry(allow_lossy_cast(vec_cast(c(1, 2), logical()), to_ptype = integer()))\n\n# No sensible coercion is possible so an error is generated\ntry(vec_cast(1.5, factor(\"a\")))\n\n# Cast to common type\nvec_cast_common(factor(\"a\"), factor(c(\"a\", \"b\")))",
            "vec_chop": "vec_chop(1:5)\n\n# These two are equivalent\nvec_chop(1:5, indices = list(1:2, 3:5))\nvec_chop(1:5, sizes = c(2, 3))\n\n# Can also be used on data frames\nvec_chop(mtcars, indices = list(1:3, 4:6))\n\n# If `indices` selects every value in `x` exactly once,\n# in any order, then `list_unchop()` inverts `vec_chop()`\nx <- c(\"a\", \"b\", \"c\", \"d\")\nindices <- list(2, c(3, 1), 4)\nvec_chop(x, indices = indices)\nlist_unchop(vec_chop(x, indices = indices), indices = indices)\n\n# When unchopping, size 1 elements of `x` are recycled\n# to the size of the corresponding index\nlist_unchop(list(1, 2:3), indices = list(c(1, 3, 5), c(2, 4)))\n\n# Names are retained, and outer names can be combined with inner\n# names through the use of a `name_spec`\nlst <- list(x = c(a = 1, b = 2), y = 1)\nlist_unchop(lst, indices = list(c(3, 2), c(1, 4)), name_spec = \"{outer}_{inner}\")\n\n# An alternative implementation of `ave()` can be constructed using\n# `vec_chop()` and `list_unchop()` in combination with `vec_group_loc()`\nave2 <- function(.x, .by, .f, ...) {\n  indices <- vec_group_loc(.by)$loc\n  chopped <- vec_chop(.x, indices = indices)\n  out <- lapply(chopped, .f, ...)\n  list_unchop(out, indices = indices)\n}\n\nbreaks <- warpbreaks$breaks\nwool <- warpbreaks$wool\n\nave2(breaks, wool, mean)\n\nidentical(\n  ave2(breaks, wool, mean),\n  ave(breaks, wool, FUN = mean)\n)\n\n# If you know your input is sorted and you'd like to split on the groups,\n# `vec_run_sizes()` can be efficiently combined with `sizes`\ndf <- data_frame(\n  g = c(2, 5, 5, 6, 6, 6, 6, 8, 9, 9),\n  x = 1:10\n)\nvec_chop(df, sizes = vec_run_sizes(df$g))\n\n# If you have a list of homogeneous vectors, sometimes it can be useful to\n# unchop, apply a function to the flattened vector, and then rechop according\n# to the original indices. This can be done efficiently with `list_sizes()`.\nx <- list(c(1, 2, 1), c(3, 1), 5, double())\nx_flat <- list_unchop(x)\nx_flat <- x_flat + max(x_flat)\nvec_chop(x_flat, sizes = list_sizes(x))",
            "vec_compare": "vec_compare(c(TRUE, FALSE, NA), FALSE)\nvec_compare(c(TRUE, FALSE, NA), FALSE, na_equal = TRUE)\n\nvec_compare(1:10, 5)\nvec_compare(runif(10), 0.5)\nvec_compare(letters[1:10], \"d\")\n\ndf <- data.frame(x = c(1, 1, 1, 2), y = c(0, 1, 2, 1))\nvec_compare(df, data.frame(x = 1, y = 1))",
            "vec_count": "vec_count(mtcars$vs)\nvec_count(iris$Species)\n\n# If you count a data frame you'll get a data frame\n# column in the output\nstr(vec_count(mtcars[c(\"vs\", \"am\")]))\n\n# Sorting ---------------------------------------\n\nx <- letters[rpois(100, 6)]\n# default is to sort by frequency\nvec_count(x)\n\n# by can sort by key\nvec_count(x, sort = \"key\")\n\n# or location of first value\nvec_count(x, sort = \"location\")\nhead(x)\n\n# or not at all\nvec_count(x, sort = \"none\")",
            "vec_detect_complete": "x <- c(1, 2, NA, 4, NA)\n\n# For most vectors, this is identical to `!vec_detect_missing(x)`\nvec_detect_complete(x)\n!vec_detect_missing(x)\n\ndf <- data_frame(\n  x = x,\n  y = c(\"a\", \"b\", NA, \"d\", \"e\")\n)\n\n# This returns `TRUE` where all elements of the row are non-missing.\n# Compare that with `!vec_detect_missing()`, which detects rows that have at\n# least one non-missing value.\ndf2 <- df\ndf2$all_non_missing <- vec_detect_complete(df)\ndf2$any_non_missing <- !vec_detect_missing(df)\ndf2",
            "vec_duplicate": "vec_duplicate_any(1:10)\nvec_duplicate_any(c(1, 1:10))\n\nx <- c(10, 10, 20, 30, 30, 40)\nvec_duplicate_detect(x)\n# Note that `duplicated()` doesn't consider the first instance to\n# be a duplicate\nduplicated(x)\n\n# Identify elements of a vector by the location of the first element that\n# they're equal to:\nvec_duplicate_id(x)\n# Location of the unique values:\nvec_unique_loc(x)\n# Equivalent to `duplicated()`:\nvec_duplicate_id(x) == seq_along(x)",
            "vec_equal": "vec_equal(c(TRUE, FALSE, NA), FALSE)\nvec_equal(c(TRUE, FALSE, NA), FALSE, na_equal = TRUE)\n\nvec_equal(5, 1:10)\nvec_equal(\"d\", letters[1:10])\n\ndf <- data.frame(x = c(1, 1, 2, 1), y = c(1, 2, 1, NA))\nvec_equal(df, data.frame(x = 1, y = 2))",
            "vec_expand_grid": "vec_expand_grid(x = 1:2, y = 1:3)\n\n# Use `.vary` to match `expand.grid()`:\nvec_expand_grid(x = 1:2, y = 1:3, .vary = \"fastest\")\n\n# Can also expand data frames\nvec_expand_grid(\n  x = data_frame(a = 1:2, b = 3:4),\n  y = 1:4\n)",
            "vec_fill_missing": "x <- c(NA, NA, 1, NA, NA, NA, 3, NA, NA)\n\n# Filling down replaces missing values with the previous non-missing value\nvec_fill_missing(x, direction = \"down\")\n\n# To also fill leading missing values, use `\"downup\"`\nvec_fill_missing(x, direction = \"downup\")\n\n# Limit the number of sequential missing values to fill with `max_fill`\nvec_fill_missing(x, max_fill = 1)\n\n# Data frames are filled rowwise. Rows are only considered missing\n# if all elements of that row are missing.\ny <- c(1, NA, 2, NA, NA, 3, 4, NA, 5)\ndf <- data_frame(x = x, y = y)\ndf\n\nvec_fill_missing(df)",
            "vec_group": "purrr <- c(\"p\", \"u\", \"r\", \"r\", \"r\")\nvec_group_id(purrr)\nvec_group_rle(purrr)\n\ngroups <- mtcars[c(\"vs\", \"am\")]\nvec_group_id(groups)\n\ngroup_rle <- vec_group_rle(groups)\ngroup_rle\n\n# Access fields with `field()`\nfield(group_rle, \"group\")\nfield(group_rle, \"length\")\n\n# `vec_group_id()` is equivalent to\nvec_match(groups, vec_unique(groups))\n\nvec_group_loc(mtcars$vs)\nvec_group_loc(mtcars[c(\"vs\", \"am\")])\n\nif (require(\"tibble\")) {\n  as_tibble(vec_group_loc(mtcars[c(\"vs\", \"am\")]))\n}",
            "vec_init": "vec_init(1:10, 3)\nvec_init(Sys.Date(), 5)\n\n# The \"missing\" value for a data frame is a row that is entirely missing\nvec_init(mtcars, 2)\n\n# The \"missing\" value for a list is `NULL`\nvec_init(list(), 3)",
            "vec_interleave": "# The most common case is to interleave two vectors\nvec_interleave(1:3, 4:6)\n\n# But you aren't restricted to just two\nvec_interleave(1:3, 4:6, 7:9, 10:12)\n\n# You can also interleave data frames\nx <- data_frame(x = 1:2, y = c(\"a\", \"b\"))\ny <- data_frame(x = 3:4, y = c(\"c\", \"d\"))\n\nvec_interleave(x, y)",
            "vec_locate_matches": "x <- c(1, 2, NA, 3, NaN)\ny <- c(2, 1, 4, NA, 1, 2, NaN)\n\n# By default, for each value of `x`, all matching locations in `y` are\n# returned\nmatches <- vec_locate_matches(x, y)\nmatches\n\n# The result can be used to slice the inputs to align them\ndata_frame(\n  x = vec_slice(x, matches$needles),\n  y = vec_slice(y, matches$haystack)\n)\n\n# If multiple matches are present, control which is returned with `multiple`\nvec_locate_matches(x, y, multiple = \"first\")\nvec_locate_matches(x, y, multiple = \"last\")\nvec_locate_matches(x, y, multiple = \"any\")\n\n# Use `relationship` to add constraints and error on multiple matches if\n# they aren't expected\ntry(vec_locate_matches(x, y, relationship = \"one-to-one\"))\n\n# In this case, the `NA` in `y` matches two rows in `x`\ntry(vec_locate_matches(x, y, relationship = \"one-to-many\"))\n\n# By default, `NA` is treated as being identical to `NaN`.\n# Using `nan_distinct = TRUE` treats `NA` and `NaN` as different values, so\n# `NA` can only match `NA`, and `NaN` can only match `NaN`.\nvec_locate_matches(x, y, nan_distinct = TRUE)\n\n# If you never want missing values to match, set `incomplete = NA` to return\n# `NA` in the `haystack` column anytime there was an incomplete value\n# in `needles`.\nvec_locate_matches(x, y, incomplete = NA)\n\n# Using `incomplete = NA` allows us to enforce the one-to-many relationship\n# that we couldn't before\nvec_locate_matches(x, y, relationship = \"one-to-many\", incomplete = NA)\n\n# `no_match` allows you to specify the returned value for a needle with\n# zero matches. Note that this is different from an incomplete value,\n# so specifying `no_match` allows you to differentiate between incomplete\n# values and unmatched values.\nvec_locate_matches(x, y, incomplete = NA, no_match = 0L)\n\n# If you want to require that every `needle` has at least 1 match, set\n# `no_match` to `\"error\"`:\ntry(vec_locate_matches(x, y, incomplete = NA, no_match = \"error\"))\n\n# By default, `vec_locate_matches()` detects equality between `needles` and\n# `haystack`. Using `condition`, you can detect where an inequality holds\n# true instead. For example, to find every location where `x[[i]] >= y`:\nmatches <- vec_locate_matches(x, y, condition = \">=\")\n\ndata_frame(\n  x = vec_slice(x, matches$needles),\n  y = vec_slice(y, matches$haystack)\n)\n\n# You can limit which matches are returned with a `filter`. For example,\n# with the above example you can filter the matches returned by `x[[i]] >= y`\n# down to only the ones containing the maximum `y` value of those matches.\nmatches <- vec_locate_matches(x, y, condition = \">=\", filter = \"max\")\n\n# Here, the matches for the `3` needle value have been filtered down to\n# only include the maximum haystack value of those matches, `2`. This is\n# often referred to as a rolling join.\ndata_frame(\n  x = vec_slice(x, matches$needles),\n  y = vec_slice(y, matches$haystack)\n)\n\n# In the very rare case that you need to generate locations for a\n# cross match, where every value of `x` is forced to match every\n# value of `y` regardless of what the actual values are, you can\n# replace `x` and `y` with integer vectors of the same size that contain\n# a single value and match on those instead.\nx_proxy <- vec_rep(1L, vec_size(x))\ny_proxy <- vec_rep(1L, vec_size(y))\nnrow(vec_locate_matches(x_proxy, y_proxy))\nvec_size(x) * vec_size(y)\n\n# By default, missing values will match other missing values when using\n# `==`, `>=`, or `<=` conditions, but not when using `>` or `<` conditions.\n# This is similar to how `vec_compare(x, y, na_equal = TRUE)` works.\nx <- c(1, NA)\ny <- c(NA, 2)\n\nvec_locate_matches(x, y, condition = \"<=\")\nvec_locate_matches(x, y, condition = \"<\")\n\n# You can force missing values to match regardless of the `condition`\n# by using `incomplete = \"match\"`\nvec_locate_matches(x, y, condition = \"<\", incomplete = \"match\")\n\n# You can also use data frames for `needles` and `haystack`. The\n# `condition` will be recycled to the number of columns in `needles`, or\n# you can specify varying conditions per column. In this example, we take\n# a vector of date `values` and find all locations where each value is\n# between lower and upper bounds specified by the `haystack`.\nvalues <- as.Date(\"2019-01-01\") + 0:9\nneedles <- data_frame(lower = values, upper = values)\n\nset.seed(123)\nlower <- as.Date(\"2019-01-01\") + sample(10, 10, replace = TRUE)\nupper <- lower + sample(3, 10, replace = TRUE)\nhaystack <- data_frame(lower = lower, upper = upper)\n\n# (values >= lower) & (values <= upper)\nmatches <- vec_locate_matches(needles, haystack, condition = c(\">=\", \"<=\"))\n\ndata_frame(\n  lower = vec_slice(lower, matches$haystack),\n  value = vec_slice(values, matches$needle),\n  upper = vec_slice(upper, matches$haystack)\n)",
            "vec_locate_sorted_groups": "df <- data.frame(\n  g = sample(2, 10, replace = TRUE),\n  x = c(NA, sample(5, 9, replace = TRUE))\n)\n\n# `vec_locate_sorted_groups()` is similar to `vec_group_loc()`, except keys\n# are returned ordered rather than by first appearance.\nvec_locate_sorted_groups(df)\n\nvec_group_loc(df)",
            "vec_match": "hadley <- strsplit(\"hadley\", \"\")[[1]]\nvec_match(hadley, letters)\n\nvowels <- c(\"a\", \"e\", \"i\", \"o\", \"u\")\nvec_match(hadley, vowels)\nvec_in(hadley, vowels)\n\n# Only the first index of duplicates is returned\nvec_match(c(\"a\", \"b\"), c(\"a\", \"b\", \"a\", \"b\"))",
            "vec_math": "x <- new_vctr(c(1, 2.5, 10))\nx\n\nabs(x)\nsum(x)\ncumsum(x)",
            "vec_names": "vec_names2(1:3)\nvec_names2(1:3, repair = \"unique\")\nvec_names2(c(a = 1, b = 2))\n\n# `vec_names()` consistently returns the rowwise names of data frames and arrays:\nvec_names(data.frame(a = 1, b = 2))\nnames(data.frame(a = 1, b = 2))\nvec_names(mtcars)\nnames(mtcars)\nvec_names(Titanic)\nnames(Titanic)\n\nvec_set_names(1:3, letters[1:3])\nvec_set_names(data.frame(a = 1:3), letters[1:3])",
            "vec_order": "x <- round(c(runif(9), NA), 3)\nvec_order(x)\nvec_sort(x)\nvec_sort(x, direction = \"desc\")\n\n# Can also handle data frames\ndf <- data.frame(g = sample(2, 10, replace = TRUE), x = x)\nvec_order(df)\nvec_sort(df)\nvec_sort(df, direction = \"desc\")\n\n# Missing values interpreted as largest values are last when\n# in increasing order:\nvec_order(c(1, NA), na_value = \"largest\", direction = \"asc\")\nvec_order(c(1, NA), na_value = \"largest\", direction = \"desc\")",
            "vec_proxy_compare": "# Lists are not comparable\nx <- list(1:2, 1, 1:2, 3)\ntry(vec_compare(x, x))\n\n# But lists are orderable by first appearance to allow for\n# ordering data frames with list-cols\ndf <- new_data_frame(list(x = x))\nvec_sort(df)",
            "vec_ptype": "# Unknown types ------------------------------------------\nvec_ptype_show()\nvec_ptype_show(NA)\nvec_ptype_show(NULL)\n\n# Vectors ------------------------------------------------\nvec_ptype_show(1:10)\nvec_ptype_show(letters)\nvec_ptype_show(TRUE)\n\nvec_ptype_show(Sys.Date())\nvec_ptype_show(Sys.time())\nvec_ptype_show(factor(\"a\"))\nvec_ptype_show(ordered(\"a\"))\n\n# Matrices -----------------------------------------------\n# The prototype of a matrix includes the number of columns\nvec_ptype_show(array(1, dim = c(1, 2)))\nvec_ptype_show(array(\"x\", dim = c(1, 2)))\n\n# Data frames --------------------------------------------\n# The prototype of a data frame includes the prototype of\n# every column\nvec_ptype_show(iris)\n\n# The prototype of multiple data frames includes the prototype\n# of every column that in any data frame\nvec_ptype_show(\n  data.frame(x = TRUE),\n  data.frame(y = 2),\n  data.frame(z = \"a\")\n)",
            "vec_ptype_full": "cat(vec_ptype_full(1:10))\ncat(vec_ptype_full(iris))\n\ncat(vec_ptype_abbr(1:10))",
            "vec_rank": "x <- c(5L, 6L, 3L, 3L, 5L, 3L)\n\nvec_rank(x, ties = \"min\")\nvec_rank(x, ties = \"max\")\n\n# Sequential ranks use an increasing sequence for duplicates\nvec_rank(x, ties = \"sequential\")\n\n# Dense ranks remove gaps between distinct values,\n# even if there are duplicates\nvec_rank(x, ties = \"dense\")\n\ny <- c(NA, x, NA, NaN)\n\n# Incomplete values match other incomplete values by default, and their\n# overall position can be adjusted with `na_value`\nvec_rank(y, na_value = \"largest\")\nvec_rank(y, na_value = \"smallest\")\n\n# NaN can be ranked separately from NA if required\nvec_rank(y, nan_distinct = TRUE)\n\n# Rank in descending order. Since missing values are the largest value,\n# they are given a rank of `1` when ranking in descending order.\nvec_rank(y, direction = \"desc\", na_value = \"largest\")\n\n# Give incomplete values a rank of `NA` by setting `incomplete = \"na\"`\nvec_rank(y, incomplete = \"na\")\n\n# Can also rank data frames, using columns after the first to break ties\nz <- c(2L, 3L, 4L, 4L, 5L, 2L)\ndf <- data_frame(x = x, z = z)\ndf\n\nvec_rank(df)",
            "vec_recycle": "# Inputs with 1 observation are recycled\nvec_recycle_common(1:5, 5)\nvec_recycle_common(integer(), 5)\n\\dontrun{\nvec_recycle_common(1:5, 1:2)\n}\n\n# Data frames and matrices are recycled along their rows\nvec_recycle_common(data.frame(x = 1), 1:5)\nvec_recycle_common(array(1:2, c(1, 2)), 1:5)\nvec_recycle_common(array(1:3, c(1, 3, 1)), 1:5)",
            "vec_seq_along": "vec_seq_along(mtcars)\nvec_init_along(head(mtcars))",
            "vec_size": "vec_size(1:100)\nvec_size(mtcars)\nvec_size(array(dim = c(3, 5, 10)))\n\nvec_size_common(1:10, 1:10)\nvec_size_common(1:10, 1)\nvec_size_common(integer(), 1)\n\nlist_sizes(list(\"a\", 1:5, letters))",
            "vec_slice": "x <- sample(10)\nx\nvec_slice(x, 1:3)\n\n# You can assign with the infix variant:\nvec_slice(x, 2) <- 100\nx\n\n# Or with the regular variant that doesn't modify the original input:\ny <- vec_assign(x, 3, 500)\ny\nx\n\n\n# Slicing objects of higher dimension:\nvec_slice(mtcars, 1:3)\n\n# Type stability --------------------------------------------------\n\n# The assign variant is type stable. It always returns the same\n# type as the input.\nx <- 1:5\nvec_slice(x, 2) <- 20.0\n\n# `x` is still an integer vector because the RHS was cast to the\n# type of the LHS:\nvec_ptype(x)\n\n# Compare to `[<-`:\nx[2] <- 20.0\nvec_ptype(x)\n\n\n# Note that the types must be coercible for the cast to happen.\n# For instance, you can cast a double vector of whole numbers to an\n# integer vector:\nvec_cast(1, integer())\n\n# But not fractional doubles:\ntry(vec_cast(1.5, integer()))\n\n# For this reason you can't assign fractional values in an integer\n# vector:\nx <- 1:3\ntry(vec_slice(x, 2) <- 1.5)",
            "vec_split": "vec_split(mtcars$cyl, mtcars$vs)\nvec_split(mtcars$cyl, mtcars[c(\"vs\", \"am\")])\n\nif (require(\"tibble\")) {\n  as_tibble(vec_split(mtcars$cyl, mtcars[c(\"vs\", \"am\")]))\n  as_tibble(vec_split(mtcars, mtcars[c(\"vs\", \"am\")]))\n}",
            "vec_unique": "x <- rpois(100, 8)\nvec_unique(x)\nvec_unique_loc(x)\nvec_unique_count(x)\n\n# `vec_unique()` returns values in the order that encounters them\n# use sort = \"location\" to match to the result of `vec_count()`\nhead(vec_unique(x))\nhead(vec_count(x, sort = \"location\"))\n\n# Normally missing values are not considered to be equal\nNA == NA\n\n# But they are for the purposes of considering uniqueness\nvec_unique(c(NA, NA, NA, NA, 1, 2, 1))",
            "vector-checks": "obj_is_vector(1)\n\n# Data frames are vectors\nobj_is_vector(data_frame())\n\n# Bare lists are vectors\nobj_is_vector(list())\n\n# S3 lists are vectors if they explicitly inherit from `\"list\"`\nx <- structure(list(), class = c(\"my_list\", \"list\"))\nobj_is_list(x)\nobj_is_vector(x)\n\n# But if they don't explicitly inherit from `\"list\"`, they aren't\n# automatically considered to be vectors. Instead, vctrs considers this\n# to be a scalar object, like a linear model returned from `lm()`.\ny <- structure(list(), class = \"my_list\")\nobj_is_list(y)\nobj_is_vector(y)\n\n# `obj_check_vector()` throws an informative error if the input\n# isn't a vector\ntry(obj_check_vector(y))\n\n# `vec_check_size()` throws an informative error if the size of the\n# input doesn't match `size`\nvec_check_size(1:5, size = 5)\ntry(vec_check_size(1:5, size = 4))"
        }
    },
    "cli": {
        "description": "A suite of tools to build attractive command line interfaces\n    ('CLIs'), from semantic elements: headings, lists, alerts, paragraphs,\n    etc. Supports custom themes via a 'CSS'-like language. It also\n    contains a number of lower level 'CLI' elements: rules, boxes, trees,\n    and 'Unicode' symbols with 'ASCII' alternatives. It support ANSI\n    colors and text styles as well.",
        "examples": {
            "ansi-styles": "col_blue(\"Hello \", \"world!\")\ncat(col_blue(\"Hello \", \"world!\"))\n\ncat(\"... to highlight the\", col_red(\"search term\"),\n    \"in a block of text\\n\")\n\n## Style stack properly\ncat(col_green(\n \"I am a green line \",\n col_blue(style_underline(style_bold(\"with a blue substring\"))),\n \" that becomes green again!\"\n))\n\nerror <- combine_ansi_styles(\"red\", \"bold\")\nwarn <- combine_ansi_styles(\"magenta\", \"underline\")\nnote <- col_cyan\ncat(error(\"Error: subscript out of bounds!\\n\"))\ncat(warn(\"Warning: shorter argument was recycled.\\n\"))\ncat(note(\"Note: no such directory.\\n\"))\n\n# style_no_* functions, note that the color is not removed\nstyle_italic(col_green(paste0(\n  \"italic before, \",\n  style_no_italic(\"normal here, \"),\n  \"italic after\"\n)))\n\n# avoiding  color for substring\nstyle_italic(col_red(paste(\n  \"red before\",\n  col_none(\"not red between\"),\n  \"red after\"\n)))",
            "ansi_collapse": "ansi_collapse(letters)\n\n# truncate\nansi_collapse(letters, trunc = 5)\n\n# head style\nansi_collapse(letters, trunc = 5, style = \"head\")",
            "ansi_grep": "red_needle <- col_red(\"needle\")\nhaystack <- c(\"foo\", \"needle\", \"foo\")\ngreen_haystack <- col_green(haystack)\nansi_grepl(red_needle, haystack)\nansi_grepl(red_needle, green_haystack)",
            "ansi_has_any": "## The second one has style if ANSI colors are supported\nansi_has_any(\"foobar\")\nansi_has_any(col_red(\"foobar\"))",
            "ansi_html": "\\dontshow{if (cli:::has_packages(c(\"htmltools\", \"withr\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n## Syntax highlight the source code of an R function with ANSI tags,\n## and export it to a HTML file.\ncode <- withr::with_options(\n  list(ansi.num_colors = 256),\n  code_highlight(format(ansi_html))\n)\nhcode <- paste(ansi_html(code), collapse = \"\\n\")\ncss <- paste(format(ansi_html_style()), collapse=  \"\\n\")\npage <- htmltools::tagList(\n  htmltools::tags$head(htmltools::tags$style(css)),\n  htmltools::tags$pre(htmltools::HTML(hcode))\n)\n\nif (interactive()) htmltools::html_print(page)\n\\dontshow{\\}) # examplesIf}",
            "ansi_html_style": "ansi_html_style(colors = FALSE)\nansi_html_style(colors = 8, palette = \"iterm-snazzy\")",
            "ansi_nchar": "str <- paste(\n  col_red(\"red\"),\n  \"default\",\n  col_green(\"green\")\n)\n\ncat(str, \"\\n\")\nnchar(str)\nansi_nchar(str)\nnchar(ansi_strip(str))",
            "ansi_nzchar": "ansi_nzchar(\"\")\nansi_nzchar(col_red(\"\"))",
            "ansi_palettes": "ansi_palettes\nansi_palette_show(\"dichro\", colors = truecolor)",
            "ansi_strip": "ansi_strip(col_red(\"foobar\")) == \"foobar\"",
            "ansi_strsplit": "str <- paste0(\n  col_red(\"I am red---\"),\n  col_green(\"and I am green-\"),\n  style_underline(\"I underlined\")\n)\n\ncat(str, \"\\n\")\n\n# split at dashes, keep color\ncat(ansi_strsplit(str, \"[-]+\")[[1]], sep = \"\\n\")\nstrsplit(ansi_strip(str), \"[-]+\")\n\n# split to characters, keep color\ncat(ansi_strsplit(str, \"\")[[1]], \"\\n\", sep = \" \")\nstrsplit(ansi_strip(str), \"\")",
            "ansi_strtrim": "text <- cli::col_red(cli:::lorem_ipsum())\nansi_strtrim(c(text, \"foobar\"), 40)",
            "ansi_strwrap": "text <- cli:::lorem_ipsum()\n# Highlight some words, that start with 's'\nrexp <- gregexpr(\"\\\\\\\\b([sS][a-zA-Z]+)\\\\\\\\b\", text)\nregmatches(text, rexp) <- lapply(regmatches(text, rexp), col_red)\ncat(text)\n\nwrp <- ansi_strwrap(text, width = 40)\ncat(wrp, sep = \"\\n\")",
            "ansi_substr": "str <- paste(\n  col_red(\"red\"),\n  \"default\",\n  col_green(\"green\")\n)\n\ncat(str, \"\\n\")\ncat(ansi_substr(str, 1, 5), \"\\n\")\ncat(ansi_substr(str, 1, 15), \"\\n\")\ncat(ansi_substr(str, 3, 7), \"\\n\")\n\nsubstr(ansi_strip(str), 1, 5)\nsubstr(ansi_strip(str), 1, 15)\nsubstr(ansi_strip(str), 3, 7)\n\nstr2 <- paste(\n  \"another\",\n  col_red(\"multi-\", style_underline(\"style\")),\n  \"text\"\n)\n\ncat(str2, \"\\n\")\ncat(ansi_substr(c(str, str2), c(3,5), c(7, 18)), sep = \"\\n\")\nsubstr(ansi_strip(c(str, str2)), c(3,5), c(7, 18))",
            "ansi_substring": "str <- paste(\n  col_red(\"red\"),\n  \"default\",\n  col_green(\"green\")\n)\n\ncat(str, \"\\n\")\ncat(ansi_substring(str, 1, 5), \"\\n\")\ncat(ansi_substring(str, 1, 15), \"\\n\")\ncat(ansi_substring(str, 3, 7), \"\\n\")\n\nsubstring(ansi_strip(str), 1, 5)\nsubstring(ansi_strip(str), 1, 15)\nsubstring(ansi_strip(str), 3, 7)\n\nstr2 <- paste(\n  \"another\",\n  col_red(\"multi-\", style_underline(\"style\")),\n  \"text\"\n)\n\ncat(str2, \"\\n\")\ncat(ansi_substring(str2, c(3,5), c(7, 18)), sep = \"\\n\")\nsubstring(ansi_strip(str2), c(3,5), c(7, 18))",
            "ansi_toupper": "ansi_toupper(col_red(\"Uppercase\"))\n\nansi_tolower(col_red(\"LowerCase\"))\n\nx <- paste0(col_green(\"MiXeD\"), col_red(\" cAsE 123\"))\nansi_chartr(\"iXs\", \"why\", x)",
            "ansi_trimws": "trimws(paste0(\"   \", col_red(\"I am red\"), \"   \"))\nansi_trimws(paste0(\"   \", col_red(\"I am red\"), \"   \"))\ntrimws(col_red(\"   I am red   \"))\nansi_trimws(col_red(\"   I am red   \"))",
            "cat_line": "cat_line(\"This is \", \"a \", \"line of text.\", col = \"red\")\ncat_bullet(letters[1:5])\ncat_bullet(letters[1:5], bullet = \"tick\", bullet_col = \"green\")\ncat_rule()",
            "cli_debug_doc": "\\dontrun{\ncli_debug_doc()\n\nolid <- cli_ol()\ncli_li()\ncli_debug_doc()\ncli_debug_doc()[]\n\ncli_end(olid)\ncli_debug_doc()\n}",
            "cli_fmt": "cli_fmt({\n  cli_alert_info(\"Loading data file\")\n  cli_alert_success(\"Loaded data file\")\n})",
            "cli_format_method": "# Let's create format and print methods for a new S3 class that\n# represents the an installed R package: `r_package`\n\n# An `r_package` will contain the DESCRIPTION metadata of the package\n# and also its installation path.\nnew_r_package <- function(pkg) {\n  tryCatch(\n    desc <- packageDescription(pkg),\n    warning = function(e) stop(\"Cannot find R package `\", pkg, \"`\")\n  )\n  file <- dirname(attr(desc, \"file\"))\n  if (basename(file) != pkg) file <- dirname(file)\n  structure(\n    list(desc = unclass(desc), lib = dirname(file)),\n    class = \"r_package\"\n  )\n}\n\nformat.r_package <- function(x, ...) {\n  cli_format_method({\n    cli_h1(\"{.pkg {x$desc$Package}} {cli::symbol$line} {x$desc$Title}\")\n    cli_text(\"{x$desc$Description}\")\n    cli_ul(c(\n      \"Version: {x$desc$Version}\",\n      if (!is.null(x$desc$Maintainer)) \"Maintainer: {x$desc$Maintainer}\",\n      \"License: {x$desc$License}\"\n    ))\n    if (!is.na(x$desc$URL)) cli_text(\"See more at {.url {x$desc$URL}}\")\n  })\n}\n\n# Now the print method is easy:\nprint.r_package <- function(x, ...) {\n  cat(format(x, ...), sep = \"\\n\")\n}\n\n# Try it out\nnew_r_package(\"cli\")\n\n# The formatting of the output depends on the current theme:\nopt <- options(cli.theme = simple_theme())\nprint(new_r_package(\"cli\"))\noptions(opt)  # <- restore theme",
            "cli_process_start": "## Failure by default\nfun <- function() {\n  cli_process_start(\"Calculating\")\n  if (interactive()) Sys.sleep(1)\n  if (runif(1) < 0.5) stop(\"Failed\")\n  cli_process_done()\n}\ntryCatch(fun(), error = function(err) err)\n\n## Success by default\nfun2 <- function() {\n  cli_process_start(\"Calculating\", on_exit = \"done\")\n  tryCatch({\n    if (interactive()) Sys.sleep(1)\n    if (runif(1) < 0.5) stop(\"Failed\")\n  }, error = function(err) cli_process_failed())\n}\nfun2()",
            "cli_sitrep": "cli_sitrep()",
            "code_highlight": "code_highlight(deparse(ls))\ncat(code_highlight(deparse(ls)), sep = \"\\n\")",
            "code_theme_list": "code_theme_list()\ncode_highlight(deparse(get), code_theme = \"Solarized Dark\")",
            "combine_ansi_styles": "## Use style names\nalert <- combine_ansi_styles(\"bold\", \"red4\")\ncat(alert(\"Warning!\"), \"\\n\")\n\n## Or style functions\nalert <- combine_ansi_styles(style_bold, col_red, bg_cyan)\ncat(alert(\"Warning!\"), \"\\n\")\n\n## Combine a composite style\nalert <- combine_ansi_styles(\n  \"bold\",\n  combine_ansi_styles(\"red\", bg_cyan))\ncat(alert(\"Warning!\"), \"\\n\")",
            "console_width": "console_width()",
            "diff_chr": "letters2 <- c(\"P\", \"R\", \"E\", letters, \"P\", \"O\", \"S\", \"T\")\nletters2[11:16] <- c(\"M\", \"I\", \"D\", \"D\", \"L\", \"E\")\ndiff_chr(letters, letters2)",
            "diff_str": "str1 <- \"abcdefghijklmnopqrstuvwxyz\"\nstr2 <- \"PREabcdefgMIDDLEnopqrstuvwxyzPOST\"\ndiff_str(str1, str2)",
            "format_inline": "format_inline(\"A message for {.emph later}, thanks {.fn format_inline}.\")",
            "has_keypress_support": "has_keypress_support()",
            "hash_animal": "hash_animal(c(\"foo\", \"bar\"))\n\n# if you increase `n_adj`, the shorter hash is a suffix of the longer:\nhash_animal(\"cli package\", 0)$hash\nhash_animal(\"cli package\", 1)$hash\nhash_animal(\"cli package\", 2)$hash\nhash_animal(\"cli package\", 3)$hash",
            "hash_emoji": "hash_emoji(c(\"foo\", NA, \"bar\", \"\"))$text\n\n# if you increase `size`, the shorter hash is a prefix of the longer:\nhash_emoji(\"foobar\", 1)$text\nhash_emoji(\"foobar\", 2)$text\nhash_emoji(\"foobar\", 3)$text\nhash_emoji(\"foobar\", 4)$text",
            "hash_md5": "hash_md5(c(\"foo\", NA, \"bar\", \"\"))",
            "hash_sha1": "hash_sha1(c(\"foo\", NA, \"bar\", \"\"))",
            "hash_sha256": "hash_sha256(c(\"foo\", NA, \"bar\", \"\"))",
            "hash_xxhash": "hash_xxhash(c(\"foo\", NA, \"bar\", \"\"))",
            "is_ansi_tty": "is_ansi_tty()",
            "is_dynamic_tty": "is_dynamic_tty()\nis_dynamic_tty(stdout())",
            "keypress": "\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nx <- keypress()\ncat(\"You pressed key\", x, \"\\n\")\n\\dontshow{\\}) # examplesIf}",
            "list_spinners": "list_spinners()\nget_spinner(list_spinners()[1])",
            "make_ansi_style": "make_ansi_style(\"orange\")\nmake_ansi_style(\"#123456\")\nmake_ansi_style(\"orange\", bg = TRUE)\n\norange <- make_ansi_style(\"orange\")\norange(\"foobar\")\ncat(orange(\"foobar\"))",
            "num_ansi_colors": "num_ansi_colors()"
        }
    },
    "tibble": {
        "description": "Provides a 'tbl_df' class (the 'tibble') with stricter checking and better formatting than the traditional\n    data frame.",
        "examples": {
            "add_column": "# add_column ---------------------------------\ndf <- tibble(x = 1:3, y = 3:1)\n\ndf \\%>\\% add_column(z = -1:1, w = 0)\ndf \\%>\\% add_column(z = -1:1, .before = \"y\")\n\n# You can't overwrite existing columns\ntry(df \\%>\\% add_column(x = 4:6))\n\n# You can't create new observations\ntry(df \\%>\\% add_column(z = 1:5))",
            "add_row": "# add_row ---------------------------------\ndf <- tibble(x = 1:3, y = 3:1)\n\ndf \\%>\\% add_row(x = 4, y = 0)\n\n# You can specify where to add the new rows\ndf \\%>\\% add_row(x = 4, y = 0, .before = 2)\n\n# You can supply vectors, to add multiple rows (this isn't\n# recommended because it's a bit hard to read)\ndf \\%>\\% add_row(x = 4:5, y = 0:-1)\n\n# Use tibble_row() to add one row only\ndf \\%>\\% add_row(tibble_row(x = 4, y = 0))\ntry(df \\%>\\% add_row(tibble_row(x = 4:5, y = 0:-1)))\n\n# Absent variables get missing values\ndf \\%>\\% add_row(x = 4)\n\n# You can't create new variables\ntry(df \\%>\\% add_row(z = 10))",
            "as_tibble": "m <- matrix(rnorm(50), ncol = 5)\ncolnames(m) <- c(\"a\", \"b\", \"c\", \"d\", \"e\")\ndf <- as_tibble(m)\n\nas_tibble_row(c(a = 1, b = 2))\nas_tibble_row(list(c = \"three\", d = list(4:5)))\nas_tibble_row(1:3, .name_repair = \"unique\")\n\nas_tibble_col(1:3)\nas_tibble_col(\n  list(c = \"three\", d = list(4:5)),\n  column_name = \"data\"\n)",
            "char": "# Display as a vector:\nchar(letters[1:3])\n\\dontshow{if ({ set.seed(20210331); rlang::is_installed(\"stringi\") }) withAutoprint(\\{ # examplesIf}\n# Space constraints:\nrand_strings <- stringi::stri_rand_strings(10, seq(40, 22, by = -2))\n\n# Plain character vectors get truncated if space is limited:\ndata_with_id <- function(id) {\n  tibble(\n    id,\n    some_number_1 = 1, some_number_2 = 2, some_number_3 = 3,\n    some_number_4 = 4, some_number_5 = 5, some_number_6 = 6,\n    some_number_7 = 7, some_number_8 = 8, some_number_9 = 9\n  )\n}\ndata_with_id(rand_strings)\n\n# Use char() to avoid or control truncation\ndata_with_id(char(rand_strings, min_chars = 24))\ndata_with_id(char(rand_strings, min_chars = Inf))\ndata_with_id(char(rand_strings, min_chars = 24, shorten = \"mid\"))\n\n# Lorem Ipsum, one sentence per row.\nlipsum <- unlist(strsplit(stringi::stri_rand_lipsum(1), \"(?<=[.]) +\", perl = TRUE))\ntibble(\n  back = char(lipsum, shorten = \"back\"),\n  front = char(lipsum, shorten = \"front\"),\n  mid = char(lipsum, shorten = \"mid\")\n)\ntibble(abbr = char(lipsum, shorten = \"abbreviate\"))\n\\dontshow{\\}) # examplesIf}",
            "enframe": "enframe(1:3)\nenframe(c(a = 5, b = 7))\nenframe(list(one = 1, two = 2:3, three = 4:6))\ndeframe(enframe(3:1))\ndeframe(tibble(a = 1:3))\ndeframe(tibble(a = as.list(1:3)))",
            "formatting": "print(as_tibble(mtcars))\nprint(as_tibble(mtcars), n = 1)\nprint(as_tibble(mtcars), n = 3)\n\nprint(as_tibble(trees), n = 100)\n\nprint(mtcars, width = 10)\n\nmtcars2 <- as_tibble(cbind(mtcars, mtcars), .name_repair = \"unique\")\nprint(mtcars2, n = 25, max_extra_cols = 3)\n\n\\dontshow{if (requireNamespace(\"nycflights13\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\nprint(nycflights13::flights, max_footer_lines = 1)\nprint(nycflights13::flights, width = Inf)\n\\dontshow{\\}) # examplesIf}",
            "frame_matrix": "frame_matrix(\n  ~col1, ~col2,\n  1,     3,\n  5,     2\n)",
            "lst": "# the value of n can be used immediately in the definition of x\nlst(n = 5, x = runif(n))\n\n# missing names are constructed from user's input\nlst(1:3, z = letters[4:6], runif(3))\n\na <- 1:3\nb <- letters[4:6]\nlst(a, b)\n\n# pre-formed quoted expressions can be used with lst() and then\n# unquoted (with !!) or unquoted and spliced (with !!!)\nn1 <- 2\nn2 <- 3\nn_stuff <- quote(n1 + n2)\nx_stuff <- quote(seq_len(n))\nlst(!!!list(n = n_stuff, x = x_stuff))\nlst(n = !!n_stuff, x = !!x_stuff)\nlst(n = 4, x = !!x_stuff)\nlst(!!!list(n = 2, x = x_stuff))",
            "new_tibble": "# The nrow argument can be omitted:\nnew_tibble(list(a = 1:3, b = 4:6))\n\n# Existing row.names attributes are ignored:\ntry(validate_tibble(new_tibble(trees, nrow = 3)))\n\n# The length of all columns must be compatible with the nrow argument:\ntry(validate_tibble(new_tibble(list(a = 1:3, b = 4:6), nrow = 2)))",
            "num": "# Display as a vector\nnum(9:11 * 100 + 0.5)\n\n# Significant figures\ntibble(\n  x3 = num(9:11 * 100 + 0.5, sigfig = 3),\n  x4 = num(9:11 * 100 + 0.5, sigfig = 4),\n  x5 = num(9:11 * 100 + 0.5, sigfig = 5),\n)\n\n# Maximum digits after the decimal points\ntibble(\n  x0 = num(9:11 * 100 + 0.5, digits = 0),\n  x1 = num(9:11 * 100 + 0.5, digits = -1),\n  x2 = num(9:11 * 100 + 0.5, digits = -2),\n)\n\n# Use fixed digits and a currency label\ntibble(\n  usd = num(9:11 * 100 + 0.5, digits = 2, label = \"USD\"),\n  gbp = num(9:11 * 100 + 0.5, digits = 2, label = \"\u00a3\"),\n  chf = num(9:11 * 100 + 0.5, digits = 2, label = \"SFr\")\n)\n\n# Scale\ntibble(\n  small  = num(9:11 / 1000 + 0.00005, label = \"\\%\", scale = 100),\n  medium = num(9:11 / 100 + 0.0005, label = \"\\%\", scale = 100),\n  large  = num(9:11 / 10 + 0.005, label = \"\\%\", scale = 100)\n)\n\n# Notation\ntibble(\n  sci = num(10^(-13:6), notation = \"sci\"),\n  eng = num(10^(-13:6), notation = \"eng\"),\n  si  = num(10^(-13:6), notation = \"si\"),\n  dec = num(10^(-13:6), notation = \"dec\")\n)\n\n# Fixed exponent\ntibble(\n  scimin = num(10^(-7:6) * 123, notation = \"sci\", fixed_exponent = -Inf),\n  engmin = num(10^(-7:6) * 123, notation = \"eng\", fixed_exponent = -Inf),\n  simin  = num(10^(-7:6) * 123, notation = \"si\", fixed_exponent = -Inf)\n)\n\ntibble(\n  scismall = num(10^(-7:6) * 123, notation = \"sci\", fixed_exponent = -3),\n  scilarge = num(10^(-7:6) * 123, notation = \"sci\", fixed_exponent = 3),\n  scimax   = num(10^(-7:6) * 123, notation = \"sci\", fixed_exponent = Inf)\n)\n\n#' Extra significant digits\ntibble(\n  default = num(100 + 1:3 * 0.001),\n  extra1 = num(100 + 1:3 * 0.001, extra_sigfig = TRUE),\n  extra2 = num(100 + 1:3 * 0.0001, extra_sigfig = TRUE),\n  extra3 = num(10000 + 1:3 * 0.00001, extra_sigfig = TRUE)\n)",
            "rownames": "# Detect row names ----------------------------------------------------\nhas_rownames(mtcars)\nhas_rownames(trees)\n\n# Remove row names ----------------------------------------------------\nremove_rownames(mtcars) \\%>\\% has_rownames()\n\n# Convert between row names and column --------------------------------\nmtcars_tbl <- rownames_to_column(mtcars, var = \"car\") \\%>\\% as_tibble()\nmtcars_tbl\ncolumn_to_rownames(mtcars_tbl, var = \"car\") \\%>\\% head()\n\n# Adding rowid as a column --------------------------------------------\nrowid_to_column(trees) \\%>\\% head()",
            "subsetting": "df <- data.frame(a = 1:3, bc = 4:6)\ntbl <- tibble(a = 1:3, bc = 4:6)\n\n# Subsetting single columns:\ndf[, \"a\"]\ntbl[, \"a\"]\ntbl[, \"a\", drop = TRUE]\nas.data.frame(tbl)[, \"a\"]\n\n# Subsetting single rows with the drop argument:\ndf[1, , drop = TRUE]\ntbl[1, , drop = TRUE]\nas.list(tbl[1, ])\n\n\\dontshow{if ((Sys.getenv(\"NOT_CRAN\") != \"true\" || Sys.getenv(\"IN_PKGDOWN\") == \"true\")) withAutoprint(\\{ # examplesIf}\n# Accessing non-existent columns:\ndf$b\ntbl$b\n\ndf[[\"b\", exact = FALSE]]\ntbl[[\"b\", exact = FALSE]]\n\ndf$bd <- c(\"n\", \"e\", \"w\")\ntbl$bd <- c(\"n\", \"e\", \"w\")\ndf$b\ntbl$b\n\\dontshow{\\}) # examplesIf}\n\ndf$b <- 7:9\ntbl$b <- 7:9\ndf$b\ntbl$b\n\n# Identical behavior:\ntbl[1, ]\ntbl[1, c(\"bc\", \"a\")]\ntbl[, c(\"bc\", \"a\")]\ntbl[c(\"bc\", \"a\")]\ntbl[\"a\"]\ntbl$a\ntbl[[\"a\"]]",
            "tibble": "# Unnamed arguments are named with their expression:\na <- 1:5\ntibble(a, a * 2)\n\n# Scalars (vectors of length one) are recycled:\ntibble(a, b = a * 2, c = 1)\n\n# Columns are available in subsequent expressions:\ntibble(x = runif(10), y = x * 2)\n\n# tibble() never coerces its inputs,\nstr(tibble(letters))\nstr(tibble(x = list(diag(1), diag(2))))\n\n# or munges column names (unless requested),\ntibble(`a + b` = 1:5)\n\n# but it forces you to take charge of names, if they need repair:\ntry(tibble(x = 1, x = 2))\ntibble(x = 1, x = 2, .name_repair = \"unique\")\ntibble(x = 1, x = 2, .name_repair = \"minimal\")\n\n## By default, non-syntactic names are allowed,\ndf <- tibble(`a 1` = 1, `a 2` = 2)\n## because you can still index by name:\ndf[[\"a 1\"]]\ndf$`a 1`\nwith(df, `a 1`)\n\n## Syntactic names are easier to work with, though, and you can request them:\ndf <- tibble(`a 1` = 1, `a 2` = 2, .name_repair = \"universal\")\ndf$a.1\n\n## You can specify your own name repair function:\ntibble(x = 1, x = 2, .name_repair = make.unique)\n\nfix_names <- function(x) gsub(\"\\\\\\\\s+\", \"_\", x)\ntibble(`year 1` = 1, `year 2` = 2, .name_repair = fix_names)\n\n## purrr-style anonymous functions and constants\n## are also supported\ntibble(x = 1, x = 2, .name_repair = ~ make.names(., unique = TRUE))\n\ntibble(x = 1, x = 2, .name_repair = ~ c(\"a\", \"b\"))\n\n# Tibbles can contain columns that are tibbles or matrices\n# if the number of rows is compatible. Unnamed tibbled are\n# spliced, i.e. the inner columns are inserted into the\n# tibble under construction.\ntibble(\n  a = 1:3,\n  tibble(\n    b = 4:6,\n    c = 7:9\n  ),\n  d = tibble(\n    e = tibble(\n      f = b\n    )\n  )\n)\ntibble(\n  a = 1:3,\n  b = diag(3),\n  c = cor(trees),\n  d = Titanic[1:3, , , ]\n)\n\n# Data can not contain tibbles or matrices with incompatible number of rows:\ntry(tibble(a = 1:3, b = tibble(c = 4:7)))\n\n# Use := to create columns with names that start with a dot:\ntibble(.dotted := 3)\n\n# This also works, but might break in the future:\ntibble(.dotted = 3)\n\n# You can unquote an expression:\nx <- 3\ntibble(x = 1, y = x)\ntibble(x = 1, y = !!x)\n\n# You can splice-unquote a list of quosures and expressions:\ntibble(!!!list(x = rlang::quo(1:10), y = quote(x * 2)))\n\n# Use .data, .env and !! to refer explicitly to columns or outside objects\na <- 1\ntibble(a = 2, b = a)\ntibble(a = 2, b = .data$a)\ntibble(a = 2, b = .env$a)\ntibble(a = 2, b = !!a)\ntry(tibble(a = 2, b = .env$bogus))\ntry(tibble(a = 2, b = !!bogus))\n\n# Use tibble_row() to construct a one-row tibble:\ntibble_row(a = 1, lm = lm(Height ~ Girth + Volume, data = trees))",
            "tibble_options": "# Default setting:\ngetOption(\"tibble.view_max\")\n\n# Change for the duration of the session:\nold <- options(tibble.view_max = 100)\n\n# view() would show only 100 rows e.g. for a lazy data frame\n\n# Change back to the original value:\noptions(old)\n\n# Local scope:\nlocal({\n  rlang::local_options(tibble.view_max = 100)\n  # view() would show only 100 rows e.g. for a lazy data frame\n})\n# view() would show the default 1000 rows e.g. for a lazy data frame",
            "tribble": "tribble(\n  ~colA, ~colB,\n  \"a\",   1,\n  \"b\",   2,\n  \"c\",   3\n)\n\n# tribble will create a list column if the value in any cell is\n# not a scalar\ntribble(\n  ~x,  ~y,\n  \"a\", 1:3,\n  \"b\", 4:6\n)\n\\dontshow{if (rlang::is_installed(\"dplyr\") && packageVersion(\"dplyr\") >= \"1.0.5\") withAutoprint(\\{ # examplesIf}\n\n# Use dplyr::mutate(dplyr::across(...)) to assign an explicit type\ntribble(\n  ~a, ~b, ~c,\n  1, \"2000-01-01\", \"1.5\"\n) \\%>\\%\n  dplyr::mutate(\n    dplyr::across(a, as.integer),\n    dplyr::across(b, as.Date)\n  )\n\\dontshow{\\}) # examplesIf}"
        }
    },
    "lifecycle": {
        "description": "Manage the life cycle of your exported functions with shared\n    conventions, documentation badges, and user-friendly deprecation\n    warnings.",
        "examples": {
            "deprecate_soft": "# A deprecated function `foo`:\ndeprecate_warn(\"1.0.0\", \"foo()\")\n\n# A deprecated argument `arg`:\ndeprecate_warn(\"1.0.0\", \"foo(arg)\")\n\n# A partially deprecated argument `arg`:\ndeprecate_warn(\"1.0.0\", \"foo(arg = 'must be a scalar integer')\")\n\n# A deprecated function with a function replacement:\ndeprecate_warn(\"1.0.0\", \"foo()\", \"bar()\")\n\n# A deprecated function with a function replacement from a\n# different package:\ndeprecate_warn(\"1.0.0\", \"foo()\", \"otherpackage::bar()\")\n\n# A deprecated function with custom message:\ndeprecate_warn(\n  when = \"1.0.0\",\n  what = \"foo()\",\n  details = \"Please use `otherpackage::bar(foo = TRUE)` instead\"\n)\n\n# A deprecated function with custom bulleted list:\ndeprecate_warn(\n  when = \"1.0.0\",\n  what = \"foo()\",\n  details = c(\n    x = \"This is dangerous\",\n    i = \"Did you mean `safe_foo()` instead?\"\n  )\n)",
            "deprecated": "foobar_adder <- function(foo, bar, baz = deprecated()) {\n  # Check if user has supplied `baz` instead of `bar`\n  if (lifecycle::is_present(baz)) {\n\n    # Signal the deprecation to the user\n    deprecate_warn(\"1.0.0\", \"foo::bar_adder(baz = )\", \"foo::bar_adder(bar = )\")\n\n    # Deal with the deprecated argument for compatibility\n    bar <- baz\n  }\n\n  foo + bar\n}\n\nfoobar_adder(1, 2)\nfoobar_adder(1, baz = 2)",
            "last_lifecycle_warnings": "# These examples are not run because `last_lifecycle_warnings()` does not\n# work well within knitr and pkgdown\n\\dontrun{\n\nf <- function() invisible(g())\ng <- function() list(h(), i())\nh <- function() deprecate_warn(\"1.0.0\", \"this()\")\ni <- function() deprecate_warn(\"1.0.0\", \"that()\")\nf()\n\n# Print all the warnings that occurred during the last command:\nlast_lifecycle_warnings()\n\n\n# By default, the backtraces are printed in their simplified form.\n# Use `simplify` to control the verbosity:\nprint(last_lifecycle_warnings(), simplify = \"none\")\n}",
            "lifecycle_linter": "lintr::lint(text = \"is_na(x)\", linters = lifecycle_linter(packages = \"rlang\"))\nlintr::lint(text = \"lapply(x, is_na)\", linters = lifecycle_linter(packages = \"rlang\", symbol_is_undesirable = TRUE))",
            "signal_stage": "foofy <- function(x, y, z) {\n  signal_stage(\"experimental\", \"foofy()\")\n  x + y / z\n}\nfoofy(1, 2, 3)",
            "verbosity": "if (rlang::is_installed(\"testthat\")) {\n  library(testthat)\n\n  mytool <- function() {\n    deprecate_soft(\"1.0.0\", \"mytool()\")\n    10 * 10\n  }\n\n  # Forcing the verbosity level is useful for unit testing. You can\n  # force errors to test that the function is indeed deprecated:\n  test_that(\"mytool is deprecated\", {\n    rlang::local_options(lifecycle_verbosity = \"error\")\n    expect_error(mytool(), class = \"defunctError\")\n  })\n\n  # Or you can enforce silence to safely test that the function\n  # still works:\n  test_that(\"mytool still works\", {\n    rlang::local_options(lifecycle_verbosity = \"quiet\")\n    expect_equal(mytool(), 100)\n  })\n}"
        }
    },
    "Rcpp": {
        "description": "The 'Rcpp' package provides R functions as well as C++ classes which\n offer a seamless integration of R and C++. Many R data types and objects can be\n mapped back and forth to C++ equivalents which facilitates both writing of new\n code as well as easier integration of third-party libraries. Documentation\n about 'Rcpp' is provided by several vignettes included in this package, via the\n 'Rcpp Gallery' site at <https://gallery.rcpp.org>, the paper by Eddelbuettel and\n Francois (2011, <doi:10.18637/jss.v040.i08>), the book by Eddelbuettel (2013,\n <doi:10.1007/978-1-4614-6868-4>) and the paper by Eddelbuettel and Balamuta (2018,\n <doi:10.1080/00031305.2017.1375990>); see 'citation(\"Rcpp\")' for details.",
        "examples": {
            "CppField-class": "showClass(\"C++Field\")",
            "CppFunction-class": "showClass(\"C++Function\")",
            "Rcpp-package": "\\dontrun{\n# introduction to Rcpp\nvignette(\"Rcpp-introduction\")\n\n# information on how to build a package that uses Rcpp\nvignette(\"Rcpp-package\")\n}",
            "Rcpp.package.skeleton": "\\dontrun{\n# simple package\nRcpp.package.skeleton( \"foobar\" )\n\n# package using attributes\nRcpp.package.skeleton( \"foobar\", attributes = TRUE )\n\n# package with a module\nRcpp.package.skeleton( \"testmod\", module = TRUE )\n\n# the Rcpp-package vignette\nvignette( \"Rcpp-package\" )\n\n# the Rcpp-modules vignette for information about modules\nvignette( \"Rcpp-modules\" )\n\n}",
            "RcppUnitTests": "# unit tests are in the unitTests directory of the package\nlist.files( system.file(\"unitTests\", package = \"Rcpp\" ),\n\tpattern = \"^runit\", full = TRUE )\n\n# trigger the unit tests preparation, follow printed instructions\n# on how to run them\n\\dontrun{\nsource( system.file(\"unitTests\", \"runTests.R\", package = \"Rcpp\" ) )\n}",
            "compileAttributes": "\\dontrun{\n\n# Compile attributes for package in the current working dir\ncompileAttributes()\n}",
            "cppFunction": "\\dontrun{\n\ncppFunction(\n    'int fibonacci(const int x) {\n        if (x == 0) return(0);\n        if (x == 1) return(1);\n        return (fibonacci(x - 1)) + fibonacci(x - 2);\n    }')\n\ncppFunction(depends = \"RcppArmadillo\",\n    'List fastLm(NumericVector yr, NumericMatrix Xr) {\n\n        int n = Xr.nrow(), k = Xr.ncol();\n\n        arma::mat X(Xr.begin(), n, k, false);\n        arma::colvec y(yr.begin(), yr.size(), false);\n\n        arma::colvec coef = arma::solve(X, y);\n        arma::colvec resid = y - X*coef;\n\n        double sig2 = arma::as_scalar(arma::trans(resid)*resid/(n-k) );\n        arma::colvec stderrest = arma::sqrt(\n            sig2 * arma::diagvec(arma::inv(arma::trans(X)*X)));\n\n        return List::create(Named(\"coefficients\") = coef,\n            Named(\"stderr\")       = stderrest\n        );\n    }')\n\ncppFunction(plugins=c(\"cpp11\"), '\n    int useCpp11() {\n        auto x = 10;\n        return x;\n    }')\n\n}",
            "demangle": "\\dontrun{\n    demangle( \"int64_t\" )\n    demangle( \"uint64_t\" )\n\n    demangle( \"NumericVector\" )\n    demangle( \"std::map<std::string,double>\" )\n    \n    sizeof( \"long\" )\n    sizeof( \"long long\" )\n    \n}",
            "dependsAttribute": "\\dontrun{\n\n// [[Rcpp::depends(RcppArmadillo)]]\n\n// [[Rcpp::depends(Matrix, RcppGSL)]]\n}",
            "evalCpp": "\\dontrun{\n\nevalCpp( \"__cplusplus\" )\nevalCpp( \"std::numeric_limits<double>::max()\" )\n    \nareMacrosDefined( c(\"__cplusplus\", \"HAS_TR1\" ) )\n\n}",
            "exportAttribute": "\\dontrun{\n\n#include <Rcpp.h>\n\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nint fibonacci(const int x) {\n\n   if (x == 0) return(0);\n   if (x == 1) return(1);\n\n   return (fibonacci(x - 1)) + fibonacci(x - 2);\n}\n\n// [[Rcpp::export(\"convolveCpp\")]]\nNumericVector convolve(NumericVector a, NumericVector b) {\n\n   int na = a.size(), nb = b.size();\n   int nab = na + nb - 1;\n   NumericVector xab(nab);\n\n   for (int i = 0; i < na; i++)\n      for (int j = 0; j < nb; j++)\n         xab[i + j] += a[i] * b[j];\n\n   return xab;\n}\n}",
            "exposeClass": "\\dontrun{\n### Given the following C++ class, defined in file PopBD.h,\n### the call to exposeClass() shown below will write a file\n### src/PopBDModule.cpp containing a corresponding module definition.\n###   class PopBD {\n###     public:\n###       PopBD(void);\n###       PopBD(NumericVector initBirth, NumericVector initDeath);\n###   \n###       std::vector<double> birth;\n###       std::vector<double> death;\n###       std::vector<int> lineage;\n###       std::vector<long> size;\n###       void evolve(int);\n###   \n###   };\n### A file R/PopBDClass.R will be written containing the one line:\n###   PopBD <- setRcppClass(\"PopBD\")\n###\n### The call below exposes the lineage and size fields, read-only,\n### and the evolve() method.\n\nexposeClass(\"PopBD\",\n      constructors =\n        list(\"\", c(\"NumericVector\", \"NumericVector\")),\n      fields = c(\"lineage\", \"size\"),\n      methods = \"evolve\",\n      header = '#include \"PopBD.h\"',\n      readOnly = c(\"lineage\", \"size\"))\n\n### Example with inheritance:  the class PopCount inherits from \n### the previous class, and adds a method table().  It has the same\n### constructors as the previous class.\n### To expose the table() method, and the inherited evolve() method and size field:\n\nexposeClass(\"PopCount\",\n      constructors =\n        list(\"\", c(\"NumericVector\", \"NumericVector\")),\n      fields = c(size = \"std::vector<long>\"),\n      methods = list(\"table\", evolve = c(\"void\", \"int\")),\n      header = '#include \"PopCount.h\"',\n      readOnly = \"size\")\n}",
            "getRcppVersion": "getRcppVersion()",
            "interfacesAttribute": "\\dontrun{\n\n// [[Rcpp::interfaces(r, cpp)]]\n}",
            "loadModule": "\\dontrun{\nloadModule(\"yada\", TRUE) # load all the objects from module \"yada\"\n}",
            "pluginsAttribute": "\\dontrun{\n\n// [[Rcpp::plugins(cpp11)]]\n\n// [[Rcpp::export]]\nint useCpp11() {\n    auto x = 10;\n    return x;\n}\n}",
            "setRcppClass": "\\dontrun{\nsetRcppClass(\"World\", \n    module = \"yada\", \n    fields = list(more = \"character\"),\n    methods = list(\n        test = function(what) message(\"Testing: \", what, \"; \", more)),\n    saveAs = \"genWorld\"\n         )\n}",
            "sourceCpp": "\\dontrun{\n\nsourceCpp(\"fibonacci.cpp\")\n\nsourceCpp(code='\n  #include <Rcpp.h>\n\n  // [[Rcpp::export]]\n  int fibonacci(const int x) {\n    if (x == 0) return(0);\n    if (x == 1) return(1);\n    return (fibonacci(x - 1)) + fibonacci(x - 2);\n  }'\n)\n\n}"
        }
    },
    "devtools": {
        "description": "Collection of package development tools.",
        "examples": {
            "check_man": "\\dontrun{\ncheck_man(\"mypkg\")\n}",
            "dev_mode": "\\dontrun{\ndev_mode()\ndev_mode()\n}",
            "dev_sitrep": "\\dontrun{\ndev_sitrep()\n}",
            "install_deps": "\\dontrun{install_deps(\".\")}",
            "load_all": "\\dontrun{\n# Load the package in the current directory\nload_all(\"./\")\n\n# Running again loads changed files\nload_all(\"./\")\n\n# With reset=TRUE, unload and reload the package for a clean start\nload_all(\"./\", TRUE)\n\n# With export_all=FALSE, only objects listed as exports in NAMESPACE\n# are exported\nload_all(\"./\", export_all = FALSE)\n}",
            "package_file": "\\dontrun{\npackage_file(\"figures\", \"figure_1\")\n}",
            "reload": "\\dontrun{\n# Reload package that is in current directory\nreload(\".\")\n\n# Reload package that is in ./ggplot2/\nreload(\"ggplot2/\")\n\n# Can use inst() to find the package path\n# This will reload the installed ggplot2 package\nreload(pkgload::inst(\"ggplot2\"))\n}",
            "revdep": "\\dontrun{\nrevdep(\"ggplot2\")\n\nrevdep(\"ggplot2\", ignore = c(\"xkcd\", \"zoo\"))\n}",
            "source_gist": "\\dontrun{\n# You can run gists given their id\nsource_gist(6872663)\nsource_gist(\"6872663\")\n\n# Or their html url\nsource_gist(\"https://gist.github.com/hadley/6872663\")\nsource_gist(\"gist.github.com/hadley/6872663\")\n\n# It's highly recommend that you run source_gist with the optional\n# sha1 argument - this will throw an error if the file has changed since\n# you first ran it\nsource_gist(6872663, sha1 = \"54f1db27e60\")\n# Wrong hash will result in error\nsource_gist(6872663, sha1 = \"54f1db27e61\")\n\n#' # You can speficy a particular R file in the gist\nsource_gist(6872663, filename = \"hi.r\")\nsource_gist(6872663, filename = \"hi.r\", sha1 = \"54f1db27e60\")\n}",
            "source_url": "\\dontrun{\n\nsource_url(\"https://gist.github.com/hadley/6872663/raw/hi.r\")\n\n# With a hash, to make sure the remote file hasn't changed\nsource_url(\"https://gist.github.com/hadley/6872663/raw/hi.r\",\n  sha1 = \"54f1db27e60bb7e0486d785604909b49e8fef9f9\")\n\n# With a truncated hash\nsource_url(\"https://gist.github.com/hadley/6872663/raw/hi.r\",\n  sha1 = \"54f1db27e60\")\n}"
        }
    },
    "pillar": {
        "description": "Provides 'pillar' and 'colonnade' generics designed\n    for formatting columns of data using the full range of colours\n    provided by modern terminals.",
        "examples": {
            "align": "align(c(\"abc\", \"de\"), align = \"left\")\nalign(c(\"abc\", \"de\"), align = \"right\")",
            "ctl_new_pillar": "\\dontshow{if (rlang::is_installed(c(\"palmerpenguins\", \"tibble\"))) withAutoprint(\\{ # examplesIf}\n# Create pillar objects\nctl_new_pillar(\n  palmerpenguins::penguins,\n  palmerpenguins::penguins$species[1:3],\n  width = 60\n)\n\nctl_new_pillar(\n  palmerpenguins::penguins,\n  palmerpenguins::penguins$bill_length_mm[1:3],\n  width = 60\n)\n\\dontshow{\\}) # examplesIf}\n\n# Customize output\nlines <- function(char = \"-\") {\n  stopifnot(nchar(char) == 1)\n  structure(char, class = \"lines\")\n}\n\nformat.lines <- function(x, width, ...) {\n  paste(rep(x, width), collapse = \"\")\n}\n\nctl_new_pillar.line_tbl <- function(controller, x, width, ...) {\n  out <- NextMethod()\n  new_pillar(list(\n    title = out$title,\n    type = out$type,\n    lines = new_pillar_component(list(lines(\"=\")), width = 1),\n    data = out$data\n  ))\n}\n\nctl_new_rowid_pillar.line_tbl <- function(controller, x, width, ...) {\n  out <- NextMethod()\n  new_pillar(\n    list(\n      title = out$title,\n      type = out$type,\n      lines = new_pillar_component(list(lines(\"=\")), width = 1),\n      data = out$data\n    ),\n    width = as.integer(floor(log10(max(nrow(x), 1))) + 1)\n  )\n}\n\nvctrs::new_data_frame(\n  list(a = 1:3, b = letters[1:3]),\n  class = c(\"line_tbl\", \"tbl\")\n)\n\nctl_new_rowid_pillar.roman_tbl <- function(controller, x, width, ...) {\n  out <- NextMethod()\n  rowid <- utils::as.roman(seq_len(nrow(x)))\n  width <- max(nchar(as.character(rowid)))\n  new_pillar(\n    list(\n      title = out$title,\n      type = out$type,\n      data = pillar_component(\n        new_pillar_shaft(list(row_ids = rowid),\n          width = width,\n          class = \"pillar_rif_shaft\"\n        )\n      )\n    ),\n    width = width\n  )\n}\n\nvctrs::new_data_frame(\n  list(a = 1:3, b = letters[1:3]),\n  class = c(\"roman_tbl\", \"tbl\")\n)",
            "ctl_new_pillar_list": "\\dontshow{if (rlang::is_installed(c(\"palmerpenguins\", \"tibble\")) && requireNamespace(\"tibble\")) withAutoprint(\\{ # examplesIf}\n# Simple column\nctl_new_pillar_list(\n  tibble::tibble(),\n  palmerpenguins::penguins$weight[1:3],\n  width = 10\n)\n\n# Packed data frame: unknown width\nctl_new_pillar_list(\n  tibble::tibble(),\n  palmerpenguins::penguins[1:3, ],\n  width = NULL\n)\n\n# Packed data frame: known width\nctl_new_pillar_list(\n  tibble::tibble(),\n  palmerpenguins::penguins,\n  width = 60\n)\n\n# Deeply packed data frame with known width:\n# showing only the first sub-column even if the width is sufficient\nctl_new_pillar_list(\n  tibble::tibble(),\n  tibble::tibble(x = tibble::tibble(b = 1, c = 2), y = 3),\n  width = 60\n)\n\n# Packed matrix: unknown width\nctl_new_pillar_list(tibble::tibble(), matrix(1:6, ncol = 2), width = NULL)\n\n# Packed matrix: known width\nctl_new_pillar_list(tibble::tibble(), matrix(1:6, ncol = 2), width = 60)\n\n# Packed array\nctl_new_pillar_list(tibble::tibble(), Titanic, width = 60)\n\\dontshow{\\}) # examplesIf}",
            "dim_desc": "dim_desc(1:10)\ndim_desc(Titanic)",
            "format_glimpse": "format_glimpse(1:3)\n\n# Lists use [], vectors inside lists use <>\nformat_glimpse(list(1:3))\nformat_glimpse(list(1, 2:3))\nformat_glimpse(list(list(1), list(2:3)))\nformat_glimpse(list(as.list(1), as.list(2:3)))\nformat_glimpse(list(character()))\nformat_glimpse(list(NULL))\n\n# Character strings are always quoted\nwriteLines(format_glimpse(letters[1:3]))\nwriteLines(format_glimpse(c(\"A\", \"B, C\")))\n\n# Factors are quoted only when needed\nwriteLines(format_glimpse(factor(letters[1:3])))\nwriteLines(format_glimpse(factor(c(\"A\", \"B, C\"))))",
            "format_type_sum": "# Default method: show the type with angle brackets\nwriteLines(format_type_sum(\"dbl\", width = NULL))\npillar(1)\n\n# AsIs method: show the type without angle brackets\ntype_sum.accel <- function(x) {\n  I(\"kg m/s^2\")\n}\n\n# Typically done through NAMESPACE\n# (perhaps with an @export directive in roxygen2)\nregisterS3method(\"type_sum\", \"accel\", type_sum.accel)\n\naccel <- structure(9.81, class = \"accel\")\npillar(accel)",
            "get_extent": "get_extent(c(\"abc\", \"de\"))\nget_extent(\"\\u904b\\u6c23\")\nget_max_extent(c(\"abc\", \"de\"))",
            "glimpse": "glimpse(mtcars)\n\n\\dontshow{if (rlang::is_installed(\"nycflights13\")) withAutoprint(\\{ # examplesIf}\nglimpse(nycflights13::flights)\n\\dontshow{\\}) # examplesIf}",
            "new_ornament": "new_ornament(c(\"abc\", \"de\"), align = \"right\")",
            "new_pillar": "lines <- function(char = \"-\") {\n  stopifnot(nchar(char) == 1)\n  structure(char, class = \"lines\")\n}\n\nformat.lines <- function(x, width, ...) {\n  paste(rep(x, width), collapse = \"\")\n}\n\nnew_pillar(list(\n  title = pillar_component(new_ornament(c(\"abc\", \"de\"), align = \"right\")),\n  lines = new_pillar_component(list(lines(\"=\")), width = 1)\n))",
            "new_pillar_component": "new_pillar_component(list(letters[1:3]), width = 1)\npillar_component(new_pillar_title(\"letters\"))\npillar_component(new_pillar_type(letters))\npillar_component(pillar_shaft(letters[1:3]))",
            "new_pillar_title": "format(new_pillar_title(names(trees)))",
            "new_pillar_type": "format(new_pillar_type(\"a\"))\nformat(new_pillar_type(factor(\"a\")))",
            "pillar-package": "\\dontshow{if (rlang::is_installed(\"tibble\")) withAutoprint(\\{ # examplesIf}\npillar(1:3)\npillar(c(1, 2, 3))\npillar(factor(letters[1:3]), title = \"letters\")\ntbl_format_setup(tibble::as_tibble(mtcars), width = 60)\n\\dontshow{\\}) # examplesIf}",
            "pillar": "x <- 123456789 * (10^c(-1, -3, -5, NA, -8, -10))\npillar(x)\npillar(-x)\npillar(runif(10))\npillar(rcauchy(20))\n\n# Special values are highlighted\npillar(c(runif(5), NA, NaN, Inf, -Inf))\n\n# Very wide ranges will be displayed in scientific format\npillar(c(1e10, 1e-10), width = 20)\npillar(c(1e10, 1e-10))\n\nx <- c(FALSE, NA, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE)\npillar(x)\n\nx <- c(\"This is string is rather long\", NA, \"?\", \"Short\")\npillar(x)\npillar(x, width = 30)\npillar(x, width = 5)\n\ndate <- as.Date(\"2017-05-15\")\npillar(date + c(1, NA, 3:5))\npillar(as.POSIXct(date) + c(30, NA, 600, 3600, 86400))",
            "pillar_options": "\\dontshow{if (rlang::is_installed(\"tibble\")) withAutoprint(\\{ # examplesIf}\ndf <- tibble::tibble(x = c(1.234567, NA, 5:10))\ndf\n\n# Change for the duration of the session:\nold <- options(\n  pillar.sigfig = 6,\n  pillar.print_max = 5,\n  pillar.print_min = 5,\n  pillar.advice = FALSE\n)\ndf\n\n# Change back to the original value:\noptions(old)\ndf\n\\dontshow{\\}) # examplesIf}",
            "pillar_shaft": "pillar_shaft(1:3)\npillar_shaft(1.5:3.5)\npillar_shaft(NA)\npillar_shaft(c(1:3, NA))",
            "style_subtle": "style_num(\n  c(\"123\", \"456\"),\n  negative = c(TRUE, FALSE)\n)\nstyle_num(\n  c(\"123\", \"456\"),\n  negative = c(TRUE, FALSE),\n  significant = c(FALSE, FALSE)\n)\nstyle_subtle(\"text\")\nstyle_subtle_num(0.01 * 1:3, c(TRUE, FALSE, TRUE))\nstyle_bold(\"Petal.Width\")\nstyle_na(\"NA\")\nstyle_neg(\"123\")",
            "tbl_format_body": "\\dontshow{if (rlang::is_installed(c(\"palmerpenguins\", \"tibble\"))) withAutoprint(\\{ # examplesIf}\nsetup <- tbl_format_setup(palmerpenguins::penguins)\ntbl_format_body(palmerpenguins::penguins, setup)\n\n# Shortcut for debugging\ntbl_format_body(setup)\n\\dontshow{\\}) # examplesIf}",
            "tbl_format_footer": "\\dontshow{if (rlang::is_installed(c(\"palmerpenguins\", \"tibble\"))) withAutoprint(\\{ # examplesIf}\nsetup <- tbl_format_setup(palmerpenguins::penguins)\ntbl_format_footer(palmerpenguins::penguins, setup)\n\n# Shortcut for debugging\ntbl_format_footer(setup)\n\\dontshow{\\}) # examplesIf}",
            "tbl_format_header": "\\dontshow{if (rlang::is_installed(c(\"palmerpenguins\", \"tibble\"))) withAutoprint(\\{ # examplesIf}\nsetup <- tbl_format_setup(palmerpenguins::penguins)\ntbl_format_header(palmerpenguins::penguins, setup)\n\n# Shortcut for debugging\ntbl_format_header(setup)\n\\dontshow{\\}) # examplesIf}",
            "tbl_format_setup": "\\dontshow{if (rlang::is_installed(c(\"palmerpenguins\", \"tibble\"))) withAutoprint(\\{ # examplesIf}\ntbl_format_setup(palmerpenguins::penguins)\n\\dontshow{\\}) # examplesIf}",
            "tbl_sum": "tbl_sum(1:10)\ntbl_sum(matrix(1:10))\ntbl_sum(data.frame(a = 1))\ntbl_sum(Sys.Date())\ntbl_sum(Sys.time())\ntbl_sum(mean)",
            "type_sum": "obj_sum(1:10)\nobj_sum(matrix(1:10))\nobj_sum(data.frame(a = 1))\nobj_sum(Sys.Date())\nobj_sum(Sys.time())\nobj_sum(mean)\n\nsize_sum(1:10)\nsize_sum(trees)\nsize_sum(Titanic)"
        }
    },
    "glue": {
        "description": "An implementation of interpreted string literals, inspired by\n    Python's Literal String Interpolation\n    <https://www.python.org/dev/peps/pep-0498/> and Docstrings\n    <https://www.python.org/dev/peps/pep-0257/> and Julia's Triple-Quoted\n    String Literals\n    <https://docs.julialang.org/en/v1.3/manual/strings/#Triple-Quoted-String-Literals-1>.",
        "examples": {
            "as_glue": "x <- as_glue(c(\"abc\", \"\\\"\\\\\\\\\\\\\\\\\", \"\\n\"))\nx\n\nx <- 1\ny <- 3\nglue(\"x + y\") + \" = {x + y}\"",
            "glue": "name <- \"Fred\"\nage <- 50\nanniversary <- as.Date(\"1991-10-12\")\nglue('My name is {name},',\n  'my age next year is {age + 1},',\n  'my anniversary is {format(anniversary, \"\\%A, \\%B \\%d, \\%Y\")}.')\n\n# single braces can be inserted by doubling them\nglue(\"My name is {name}, not {{name}}.\")\n\n# Named arguments can be used to assign temporary variables.\nglue('My name is {name},',\n  ' my age next year is {age + 1},',\n  ' my anniversary is {format(anniversary, \"\\%A, \\%B \\%d, \\%Y\")}.',\n  name = \"Joe\",\n  age = 40,\n  anniversary = as.Date(\"2001-10-12\"))\n\n# `glue()` can also be used in user defined functions\nintro <- function(name, profession, country){\n  glue(\"My name is {name}, a {profession}, from {country}\")\n}\nintro(\"Shelmith\", \"Senior Data Analyst\", \"Kenya\")\nintro(\"Cate\", \"Data Scientist\", \"Kenya\")\n\n# `glue_data()` is useful in magrittr pipes\nif (require(magrittr)) {\n\nmtcars \\%>\\% glue_data(\"{rownames(.)} has {hp} hp\")\n\n# Or within dplyr pipelines\nif (require(dplyr)) {\n\nhead(iris) \\%>\\%\n  mutate(description = glue(\"This {Species} has a petal length of {Petal.Length}\"))\n\n}}\n\n# Alternative delimiters can also be used if needed\none <- \"1\"\nglue(\"The value of $e^{2\\\\\\\\pi i}$ is $<<one>>$.\", .open = \"<<\", .close = \">>\")",
            "glue_col": "\\dontshow{if (require(crayon)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nlibrary(crayon)\n\nglue_col(\"{blue foo bar}\")\n\nglue_col(\"{blue 1 + 1 = {1 + 1}}\")\n\nglue_col(\"{blue 2 + 2 = {green {2 + 2}}}\")\n\nwhite_on_black <- bgBlack $ white\nglue_col(\"{white_on_black\n  Roses are {red {colors()[[552]]}},\n  Violets are {blue {colors()[[26]]}},\n  `glue_col()` can show \\\\\\\\\n  {red c}{yellow o}{green l}{cyan o}{blue r}{magenta s}\n  and {bold bold} and {underline underline} too!\n}\")\n\n# this would error due to an unterminated quote, if we did not specify\n# `.literal = TRUE`\nglue_col(\"{yellow It's} happening!\", .literal = TRUE)\n\n# `.literal = TRUE` also prevents an error here due to the `#` comment\nglue_col(\n  \"A URL: {magenta https://github.com/tidyverse/glue#readme}\",\n  .literal = TRUE\n)\n\n# `.literal = TRUE` does NOT prevent evaluation\nx <- \"world\"\ny <- \"day\"\nglue_col(\"hello {x}! {green it's a new {y}!}\", .literal = TRUE)\n\\dontshow{\\}) # examplesIf}",
            "glue_collapse": "glue_collapse(glue(\"{1:10}\"))\n\n# Wide values can be truncated\nglue_collapse(glue(\"{1:10}\"), width = 5)\n\nglue_collapse(1:4, \", \", last = \" and \")",
            "glue_safe": "\"1 + 1\" <- 5\n# glue actually executes the code\nglue(\"{1 + 1}\")\n\n# glue_safe just looks up the value\nglue_safe(\"{1 + 1}\")\n\nrm(\"1 + 1\")",
            "glue_sql": "\\dontshow{if (requireNamespace(\"DBI\", quietly = TRUE) && requireNamespace(\"RSQLite\", quietly = TRUE)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\niris2 <- iris\ncolnames(iris2) <- gsub(\"[.]\", \"_\", tolower(colnames(iris)))\nDBI::dbWriteTable(con, \"iris\", iris2)\nvar <- \"sepal_width\"\ntbl <- \"iris\"\nnum <- 2\nval <- \"setosa\"\nglue_sql(\"\n  SELECT {`var`}\n  FROM {`tbl`}\n  WHERE {`tbl`}.sepal_length > {num}\n    AND {`tbl`}.species = {val}\n  \", .con = con)\n\n# If sepal_length is store on the database as a character explicitly convert\n# the data to character to quote appropriately.\nglue_sql(\"\n  SELECT {`var`}\n  FROM {`tbl`}\n  WHERE {`tbl`}.sepal_length > {as.character(num)}\n    AND {`tbl`}.species = {val}\n  \", .con = con)\n\n\n# `glue_sql()` can be used in conjuction with parameterized queries using\n# `DBI::dbBind()` to provide protection for SQL Injection attacks\n sql <- glue_sql(\"\n    SELECT {`var`}\n    FROM {`tbl`}\n    WHERE {`tbl`}.sepal_length > ?\n  \", .con = con)\nquery <- DBI::dbSendQuery(con, sql)\nDBI::dbBind(query, list(num))\nDBI::dbFetch(query, n = 4)\nDBI::dbClearResult(query)\n\n# `glue_sql()` can be used to build up more complex queries with\n# interchangeable sub queries. It returns `DBI::SQL()` objects which are\n# properly protected from quoting.\nsub_query <- glue_sql(\"\n  SELECT *\n  FROM {`tbl`}\n  \", .con = con)\n\nglue_sql(\"\n  SELECT s.{`var`}\n  FROM ({sub_query}) AS s\n  \", .con = con)\n\n# If you want to input multiple values for use in SQL IN statements put `*`\n# at the end of the value and the values will be collapsed and quoted appropriately.\nglue_sql(\"SELECT * FROM {`tbl`} WHERE sepal_length IN ({vals*})\",\n  vals = 1, .con = con)\n\nglue_sql(\"SELECT * FROM {`tbl`} WHERE sepal_length IN ({vals*})\",\n  vals = 1:5, .con = con)\n\nglue_sql(\"SELECT * FROM {`tbl`} WHERE species IN ({vals*})\",\n  vals = \"setosa\", .con = con)\n\nglue_sql(\"SELECT * FROM {`tbl`} WHERE species IN ({vals*})\",\n  vals = c(\"setosa\", \"versicolor\"), .con = con)\n\n# If you need to reference variables from multiple tables use `DBI::Id()`.\n# Here we create a new table of nicknames, join the two tables together and\n# select columns from both tables. Using `DBI::Id()` and the special\n# `glue_sql()` syntax ensures all the table and column identifiers are quoted\n# appropriately.\n\niris_db <- \"iris\"\nnicknames_db <- \"nicknames\"\n\nnicknames <- data.frame(\n  species = c(\"setosa\", \"versicolor\", \"virginica\"),\n  nickname = c(\"Beachhead Iris\", \"Harlequin Blueflag\", \"Virginia Iris\"),\n  stringsAsFactors = FALSE\n)\n\nDBI::dbWriteTable(con, nicknames_db, nicknames)\n\ncols <- list(\n  DBI::Id(iris_db, \"sepal_length\"),\n  DBI::Id(iris_db, \"sepal_width\"),\n  DBI::Id(nicknames_db, \"nickname\")\n)\n\niris_species <- DBI::Id(iris_db, \"species\")\nnicknames_species <- DBI::Id(nicknames_db, \"species\")\n\nquery <- glue_sql(\"\n  SELECT {`cols`*}\n  FROM {`iris_db`}\n  JOIN {`nicknames_db`}\n  ON {`iris_species`}={`nicknames_species`}\",\n  .con = con\n)\nquery\n\nDBI::dbGetQuery(con, query, n = 5)\n\nDBI::dbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "quoting": "x <- 1:5\nglue('Values of x: {glue_collapse(backtick(x), sep = \", \", last = \" and \")}')",
            "trim": "glue(\"\n    A formatted string\n    Can have multiple lines\n      with additional indentation preserved\n    \")\n\nglue(\"\n  \\ntrailing or leading newlines can be added explicitly\\n\n  \")\n\nglue(\"\n    A formatted string \\\\\\\\\n    can also be on a \\\\\\\\\n    single line\n    \")"
        }
    },
    "ragg": {
        "description": "Anti-Grain Geometry (AGG) is a high-quality and\n    high-performance 2D drawing library. The 'ragg' package provides a set\n    of graphic devices based on AGG to use as alternative to the raster\n    devices provided through the 'grDevices' package.",
        "examples": {
            "agg_capture": "cap <- agg_capture()\nplot(1:10, 1:10)\n\n# Get the plot as a matrix\nraster <- cap()\n\n# Get the plot as a nativeRaster\nraster_n <- cap(native = TRUE)\n\ndev.off()\n\n# Look at the output\nplot(as.raster(raster))",
            "agg_jpeg": "file <- tempfile(fileext = '.jpeg')\nagg_jpeg(file, quality = 50)\nplot(sin, -pi, 2*pi)\ndev.off()",
            "agg_png": "file <- tempfile(fileext = '.png')\nagg_png(file)\nplot(sin, -pi, 2*pi)\ndev.off()",
            "agg_ppm": "file <- tempfile(fileext = '.ppm')\nagg_ppm(file)\nplot(sin, -pi, 2*pi)\ndev.off()",
            "agg_tiff": "file <- tempfile(fileext = '.tiff')\n# Use jpeg compression\nagg_tiff(file, compression = 'lzw+p')\nplot(sin, -pi, 2*pi)\ndev.off()"
        }
    },
    "textshaping": {
        "description": "Provides access to the text shaping functionality in the\n    'HarfBuzz' library and the bidirectional algorithm in the 'Fribidi'\n    library.  'textshaping' is a low-level utility package mainly for\n    graphic devices that expands upon the font tool-set provided by the\n    'systemfonts' package.",
        "examples": {
            "get_font_features": "# Select a random font on the system\nsys_fonts <- systemfonts::system_fonts()\nrandom_font <- sys_fonts$family[sample(nrow(sys_fonts), 1)]\n\n# Get the features\nget_font_features(random_font)",
            "lorem_text": "# Defaults to standard lorem ipsum\nlorem_text()\n\n# Get two paragraphs of hangul (Korean)\nlorem_text(\"hangul\", 2)\n\n# Get gibberish bi-directional text\nlorem_bidi()",
            "plot_shape": "arab_text <- lorem_text(\"arabic\", 2)\nshape <- shape_text(\n  arab_text,\n  max_width = 5,\n  indent = 0.2\n)\n\ntry(\n plot_shape(shape)\n)",
            "shape_text": "string <- \"This is a long string\\nLook; It spans multiple lines\\nand all\"\n\n# Shape with default settings\nshape_text(string)\n\n# Mix styles within the same string\nstring <- c(\n  \"This string will have\\na \",\n  \"very large\",\n  \" text style\\nin the middle\"\n)\n\nshape_text(string, id = c(1, 1, 1), size = c(12, 24, 12))",
            "text_width": "strings <- c('A short string', 'A very very looong string')\ntext_width(strings)"
        }
    },
    "stringr": {
        "description": "A consistent, simple and easy to use set of wrappers around\n    the fantastic 'stringi' package. All function and argument names (and\n    positions) are consistent, all functions deal with \"NA\"'s and zero\n    length vectors in the same way, and the output from one function is\n    easy to feed into the input of another.",
        "examples": {
            "case": "dog <- \"The quick brown dog\"\nstr_to_upper(dog)\nstr_to_lower(dog)\nstr_to_title(dog)\nstr_to_sentence(\"the quick brown dog\")\n\n# Locale matters!\nstr_to_upper(\"i\") # English\nstr_to_upper(\"i\", \"tr\") # Turkish",
            "invert_match": "numbers <- \"1 and 2 and 4 and 456\"\nnum_loc <- str_locate_all(numbers, \"[0-9]+\")[[1]]\nstr_sub(numbers, num_loc[, \"start\"], num_loc[, \"end\"])\n\ntext_loc <- invert_match(num_loc)\nstr_sub(numbers, text_loc[, \"start\"], text_loc[, \"end\"])",
            "modifiers": "pattern <- \"a.b\"\nstrings <- c(\"abb\", \"a.b\")\nstr_detect(strings, pattern)\nstr_detect(strings, fixed(pattern))\nstr_detect(strings, coll(pattern))\n\n# coll() is useful for locale-aware case-insensitive matching\ni <- c(\"I\", \"\\u0130\", \"i\")\ni\nstr_detect(i, fixed(\"i\", TRUE))\nstr_detect(i, coll(\"i\", TRUE))\nstr_detect(i, coll(\"i\", TRUE, locale = \"tr\"))\n\n# Word boundaries\nwords <- c(\"These are   some words.\")\nstr_count(words, boundary(\"word\"))\nstr_split(words, \" \")[[1]]\nstr_split(words, boundary(\"word\"))[[1]]\n\n# Regular expression variations\nstr_extract_all(\"The Cat in the Hat\", \"[a-z]+\")\nstr_extract_all(\"The Cat in the Hat\", regex(\"[a-z]+\", TRUE))\n\nstr_extract_all(\"a\\nb\\nc\", \"^.\")\nstr_extract_all(\"a\\nb\\nc\", regex(\"^.\", multiline = TRUE))\n\nstr_extract_all(\"a\\nb\\nc\", \"a.\")\nstr_extract_all(\"a\\nb\\nc\", regex(\"a.\", dotall = TRUE))",
            "str_c": "str_c(\"Letter: \", letters)\nstr_c(\"Letter\", letters, sep = \": \")\nstr_c(letters, \" is for\", \"...\")\nstr_c(letters[-26], \" comes before \", letters[-1])\n\nstr_c(letters, collapse = \"\")\nstr_c(letters, collapse = \", \")\n\n# Differences from paste() ----------------------\n# Missing inputs give missing outputs\nstr_c(c(\"a\", NA, \"b\"), \"-d\")\npaste0(c(\"a\", NA, \"b\"), \"-d\")\n# Use str_replace_NA to display literal NAs:\nstr_c(str_replace_na(c(\"a\", NA, \"b\")), \"-d\")\n\n# Uses tidyverse recycling rules\n\\dontrun{str_c(1:2, 1:3)} # errors\npaste0(1:2, 1:3)\n\nstr_c(\"x\", character())\npaste0(\"x\", character())",
            "str_conv": "# Example from encoding?stringi::stringi\nx <- rawToChar(as.raw(177))\nx\nstr_conv(x, \"ISO-8859-2\") # Polish \"a with ogonek\"\nstr_conv(x, \"ISO-8859-1\") # Plus-minus",
            "str_count": "fruit <- c(\"apple\", \"banana\", \"pear\", \"pineapple\")\nstr_count(fruit, \"a\")\nstr_count(fruit, \"p\")\nstr_count(fruit, \"e\")\nstr_count(fruit, c(\"a\", \"b\", \"p\", \"p\"))\n\nstr_count(c(\"a.\", \"...\", \".a.a\"), \".\")\nstr_count(c(\"a.\", \"...\", \".a.a\"), fixed(\".\"))",
            "str_detect": "fruit <- c(\"apple\", \"banana\", \"pear\", \"pineapple\")\nstr_detect(fruit, \"a\")\nstr_detect(fruit, \"^a\")\nstr_detect(fruit, \"a$\")\nstr_detect(fruit, \"b\")\nstr_detect(fruit, \"[aeiou]\")\n\n# Also vectorised over pattern\nstr_detect(\"aecfg\", letters)\n\n# Returns TRUE if the pattern do NOT match\nstr_detect(fruit, \"^p\", negate = TRUE)",
            "str_dup": "fruit <- c(\"apple\", \"pear\", \"banana\")\nstr_dup(fruit, 2)\nstr_dup(fruit, 2, sep = \" \")\nstr_dup(fruit, 1:3)\nstr_c(\"ba\", str_dup(\"na\", 0:5))",
            "str_equal": "# These two strings encode \"a\" with an accent in two different ways\na1 <- \"\\u00e1\"\na2 <- \"a\\u0301\"\nc(a1, a2)\n\na1 == a2\nstr_equal(a1, a2)\n\n# ohm and omega use different code points but should always be treated\n# as equal\nohm <- \"\\u2126\"\nomega <- \"\\u03A9\"\nc(ohm, omega)\n\nohm == omega\nstr_equal(ohm, omega)",
            "str_escape": "str_detect(c(\"a\", \".\"), \".\")\nstr_detect(c(\"a\", \".\"), str_escape(\".\"))",
            "str_extract": "shopping_list <- c(\"apples x4\", \"bag of flour\", \"bag of sugar\", \"milk x2\")\nstr_extract(shopping_list, \"\\\\\\\\d\")\nstr_extract(shopping_list, \"[a-z]+\")\nstr_extract(shopping_list, \"[a-z]{1,4}\")\nstr_extract(shopping_list, \"\\\\\\\\b[a-z]{1,4}\\\\\\\\b\")\n\nstr_extract(shopping_list, \"([a-z]+) of ([a-z]+)\")\nstr_extract(shopping_list, \"([a-z]+) of ([a-z]+)\", group = 1)\nstr_extract(shopping_list, \"([a-z]+) of ([a-z]+)\", group = 2)\n\n# Extract all matches\nstr_extract_all(shopping_list, \"[a-z]+\")\nstr_extract_all(shopping_list, \"\\\\\\\\b[a-z]+\\\\\\\\b\")\nstr_extract_all(shopping_list, \"\\\\\\\\d\")\n\n# Simplify results into character matrix\nstr_extract_all(shopping_list, \"\\\\\\\\b[a-z]+\\\\\\\\b\", simplify = TRUE)\nstr_extract_all(shopping_list, \"\\\\\\\\d\", simplify = TRUE)\n\n# Extract all words\nstr_extract_all(\"This is, suprisingly, a sentence.\", boundary(\"word\"))",
            "str_flatten": "str_flatten(letters)\nstr_flatten(letters, \"-\")\n\nstr_flatten(letters[1:3], \", \")\n\n# Use last to customise the last component\nstr_flatten(letters[1:3], \", \", \" and \")\n\n# this almost works if you want an Oxford (aka serial) comma\nstr_flatten(letters[1:3], \", \", \", and \")\n\n# but it will always add a comma, even when not necessary\nstr_flatten(letters[1:2], \", \", \", and \")\n\n# str_flatten_comma knows how to handle the Oxford comma\nstr_flatten_comma(letters[1:3], \", and \")\nstr_flatten_comma(letters[1:2], \", and \")",
            "str_glue": "name <- \"Fred\"\nage <- 50\nanniversary <- as.Date(\"1991-10-12\")\nstr_glue(\n  \"My name is {name}, \",\n  \"my age next year is {age + 1}, \",\n  \"and my anniversary is {format(anniversary, '\\%A, \\%B \\%d, \\%Y')}.\"\n)\n\n# single braces can be inserted by doubling them\nstr_glue(\"My name is {name}, not {{name}}.\")\n\n# You can also used named arguments\nstr_glue(\n  \"My name is {name}, \",\n  \"and my age next year is {age + 1}.\",\n  name = \"Joe\",\n  age = 40\n)\n\n# `str_glue_data()` is useful in data pipelines\nmtcars \\%>\\% str_glue_data(\"{rownames(.)} has {hp} hp\")",
            "str_interp": "# Using values from the environment, and some formats\nuser_name <- \"smbache\"\namount <- 6.656\naccount <- 1337\nstr_interp(\"User ${user_name} (account $[08d]{account}) has $$[.2f]{amount}.\")\n\n# Nested brace pairs work inside expressions too, and any braces can be\n# placed outside the expressions.\nstr_interp(\"Works with",
            "str_length": "str_length(letters)\nstr_length(NA)\nstr_length(factor(\"abc\"))\nstr_length(c(\"i\", \"like\", \"programming\", NA))\n\n# Some characters, like emoji and Chinese characters (hanzi), are square\n# which means they take up the width of two Latin characters\nx <- c(\"\\u6c49\\u5b57\", \"\\U0001f60a\")\nstr_view(x)\nstr_width(x)\nstr_length(x)\n\n# There are two ways of representing a u with an umlaut\nu <- c(\"\\u00fc\", \"u\\u0308\")\n# They have the same width\nstr_width(u)\n# But a different length\nstr_length(u)\n# Because the second element is made up of a u + an accent\nstr_sub(u, 1, 1)",
            "str_like": "fruit <- c(\"apple\", \"banana\", \"pear\", \"pineapple\")\nstr_like(fruit, \"app\")\nstr_like(fruit, \"app\\%\")\nstr_like(fruit, \"APP\\%\")\nstr_like(fruit, \"ba_ana\")\nstr_like(fruit, \"\\%apple\")\n\nstr_ilike(fruit, \"app\")\nstr_ilike(fruit, \"app\\%\")\nstr_ilike(fruit, \"APP\\%\")\nstr_ilike(fruit, \"ba_ana\")\nstr_ilike(fruit, \"\\%apple\")",
            "str_locate": "fruit <- c(\"apple\", \"banana\", \"pear\", \"pineapple\")\nstr_locate(fruit, \"$\")\nstr_locate(fruit, \"a\")\nstr_locate(fruit, \"e\")\nstr_locate(fruit, c(\"a\", \"b\", \"p\", \"p\"))\n\nstr_locate_all(fruit, \"a\")\nstr_locate_all(fruit, \"e\")\nstr_locate_all(fruit, c(\"a\", \"b\", \"p\", \"p\"))\n\n# Find location of every character\nstr_locate_all(fruit, \"\")",
            "str_match": "strings <- c(\" 219 733 8965\", \"329-293-8753 \", \"banana\", \"595 794 7569\",\n  \"387 287 6718\", \"apple\", \"233.398.9187  \", \"482 952 3315\",\n  \"239 923 8115 and 842 566 4692\", \"Work: 579-499-7527\", \"$1000\",\n  \"Home: 543.355.3679\")\nphone <- \"([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})\"\n\nstr_extract(strings, phone)\nstr_match(strings, phone)\n\n# Extract/match all\nstr_extract_all(strings, phone)\nstr_match_all(strings, phone)\n\n# You can also name the groups to make further manipulation easier\nphone <- \"(?<area>[2-9][0-9]{2})[- .](?<phone>[0-9]{3}[- .][0-9]{4})\"\nstr_match(strings, phone)\n\nx <- c(\"<a> <b>\", \"<a> <>\", \"<a>\", \"\", NA)\nstr_match(x, \"<(.*?)> <(.*?)>\")\nstr_match_all(x, \"<(.*?)>\")\n\nstr_extract(x, \"<.*?>\")\nstr_extract_all(x, \"<.*?>\")",
            "str_order": "x <- c(\"apple\", \"car\", \"happy\", \"char\")\nstr_sort(x)\n\nstr_order(x)\nx[str_order(x)]\n\nstr_rank(x)\n\n# In Czech, ch is a digraph that sorts after h\nstr_sort(x, locale = \"cs\")\n\n# Use numeric = TRUE to sort numbers in strings\nx <- c(\"100a10\", \"100a5\", \"2b\", \"2a\")\nstr_sort(x)\nstr_sort(x, numeric = TRUE)",
            "str_pad": "rbind(\n  str_pad(\"hadley\", 30, \"left\"),\n  str_pad(\"hadley\", 30, \"right\"),\n  str_pad(\"hadley\", 30, \"both\")\n)\n\n# All arguments are vectorised except side\nstr_pad(c(\"a\", \"abc\", \"abcdef\"), 10)\nstr_pad(\"a\", c(5, 10, 20))\nstr_pad(\"a\", 10, pad = c(\"-\", \"_\", \" \"))\n\n# Longer strings are returned unchanged\nstr_pad(\"hadley\", 3)",
            "str_remove": "fruits <- c(\"one apple\", \"two pears\", \"three bananas\")\nstr_remove(fruits, \"[aeiou]\")\nstr_remove_all(fruits, \"[aeiou]\")",
            "str_replace": "fruits <- c(\"one apple\", \"two pears\", \"three bananas\")\nstr_replace(fruits, \"[aeiou]\", \"-\")\nstr_replace_all(fruits, \"[aeiou]\", \"-\")\nstr_replace_all(fruits, \"[aeiou]\", toupper)\nstr_replace_all(fruits, \"b\", NA_character_)\n\nstr_replace(fruits, \"([aeiou])\", \"\")\nstr_replace(fruits, \"([aeiou])\", \"\\\\\\\\1\\\\\\\\1\")\n\n# Note that str_replace() is vectorised along text, pattern, and replacement\nstr_replace(fruits, \"[aeiou]\", c(\"1\", \"2\", \"3\"))\nstr_replace(fruits, c(\"a\", \"e\", \"i\"), \"-\")\n\n# If you want to apply multiple patterns and replacements to the same\n# string, pass a named vector to pattern.\nfruits \\%>\\%\n  str_c(collapse = \"---\") \\%>\\%\n  str_replace_all(c(\"one\" = \"1\", \"two\" = \"2\", \"three\" = \"3\"))\n\n# Use a function for more sophisticated replacement. This example\n# replaces colour names with their hex values.\ncolours <- str_c(\"\\\\\\\\b\", colors(), \"\\\\\\\\b\", collapse=\"|\")\ncol2hex <- function(col) {\n  rgb <- col2rgb(col)\n  rgb(rgb[\"red\", ], rgb[\"green\", ], rgb[\"blue\", ], maxColorValue = 255)\n}\n\nx <- c(\n  \"Roses are red, violets are blue\",\n  \"My favourite colour is green\"\n)\nstr_replace_all(x, colours, col2hex)",
            "str_replace_na": "str_replace_na(c(NA, \"abc\", \"def\"))",
            "str_split": "fruits <- c(\n  \"apples and oranges and pears and bananas\",\n  \"pineapples and mangos and guavas\"\n)\n\nstr_split(fruits, \" and \")\nstr_split(fruits, \" and \", simplify = TRUE)\n\n# If you want to split a single string, use `str_split_1`\nstr_split_1(fruits[[1]], \" and \")\n\n# Specify n to restrict the number of possible matches\nstr_split(fruits, \" and \", n = 3)\nstr_split(fruits, \" and \", n = 2)\n# If n greater than number of pieces, no padding occurs\nstr_split(fruits, \" and \", n = 5)\n\n# Use fixed to return a character matrix\nstr_split_fixed(fruits, \" and \", 3)\nstr_split_fixed(fruits, \" and \", 4)\n\n# str_split_i extracts only a single piece from a string\nstr_split_i(fruits, \" and \", 1)\nstr_split_i(fruits, \" and \", 4)\n# use a negative number to select from the end\nstr_split_i(fruits, \" and \", -1)",
            "str_starts": "fruit <- c(\"apple\", \"banana\", \"pear\", \"pineapple\")\nstr_starts(fruit, \"p\")\nstr_starts(fruit, \"p\", negate = TRUE)\nstr_ends(fruit, \"e\")\nstr_ends(fruit, \"e\", negate = TRUE)",
            "str_sub": "hw <- \"Hadley Wickham\"\n\nstr_sub(hw, 1, 6)\nstr_sub(hw, end = 6)\nstr_sub(hw, 8, 14)\nstr_sub(hw, 8)\n\n# Negative values index from end of string\nstr_sub(hw, -1)\nstr_sub(hw, -7)\nstr_sub(hw, end = -7)\n\n# str_sub() is vectorised by both string and position\nstr_sub(hw, c(1, 8), c(6, 14))\n\n# if you want to extract multiple positions from multiple strings,\n# use str_sub_all()\nx <- c(\"abcde\", \"ghifgh\")\nstr_sub(x, c(1, 2), c(2, 4))\nstr_sub_all(x, start = c(1, 2), end = c(2, 4))\n\n# Alternatively, you can pass in a two column matrix, as in the\n# output from str_locate_all\npos <- str_locate_all(hw, \"[aeio]\")[[1]]\npos\nstr_sub(hw, pos)\n\n# You can also use `str_sub()` to modify strings:\nx <- \"BBCDEF\"\nstr_sub(x, 1, 1) <- \"A\"; x\nstr_sub(x, -1, -1) <- \"K\"; x\nstr_sub(x, -2, -2) <- \"GHIJ\"; x\nstr_sub(x, 2, -2) <- \"\"; x",
            "str_subset": "fruit <- c(\"apple\", \"banana\", \"pear\", \"pineapple\")\nstr_subset(fruit, \"a\")\n\nstr_subset(fruit, \"^a\")\nstr_subset(fruit, \"a$\")\nstr_subset(fruit, \"b\")\nstr_subset(fruit, \"[aeiou]\")\n\n# Elements that don't match\nstr_subset(fruit, \"^p\", negate = TRUE)\n\n# Missings never match\nstr_subset(c(\"a\", NA, \"b\"), \".\")",
            "str_trim": "str_trim(\"  String with trailing and leading white space\\t\")\nstr_trim(\"\\n\\nString with trailing and leading white space\\n\\n\")\n\nstr_squish(\"  String with trailing,  middle, and leading white space\\t\")\nstr_squish(\"\\n\\nString with excess,  trailing and leading white   space\\n\\n\")",
            "str_trunc": "x <- \"This string is moderately long\"\nrbind(\n  str_trunc(x, 20, \"right\"),\n  str_trunc(x, 20, \"left\"),\n  str_trunc(x, 20, \"center\")\n)",
            "str_unique": "str_unique(c(\"a\", \"b\", \"c\", \"b\", \"a\"))\n\nstr_unique(c(\"a\", \"b\", \"c\", \"B\", \"A\"))\nstr_unique(c(\"a\", \"b\", \"c\", \"B\", \"A\"), ignore_case = TRUE)\n\n# Use ... to pass additional arguments to stri_unique()\nstr_unique(c(\"motley\", \"m\u00f6tley\", \"pinguino\", \"ping\u00fcino\"))\nstr_unique(c(\"motley\", \"m\u00f6tley\", \"pinguino\", \"ping\u00fcino\"), strength = 1)",
            "str_view": "# Show special characters\nstr_view(c(\"\\\"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\\\\\\\\\", \"fgh\", NA, \"NA\"))\n\n# A non-breaking space looks like a regular space:\nnbsp <- \"Hi\\u00A0you\"\nnbsp\n# But it doesn't behave like one:\nstr_detect(nbsp, \" \")\n# So str_view() brings it to your attention with a blue background\nstr_view(nbsp)\n\n# You can also use escapes to see all non-ASCII characters\nstr_view(nbsp, use_escapes = TRUE)\n\n# Supply a pattern to see where it matches\nstr_view(c(\"abc\", \"def\", \"fghi\"), \"[aeiou]\")\nstr_view(c(\"abc\", \"def\", \"fghi\"), \"^\")\nstr_view(c(\"abc\", \"def\", \"fghi\"), \"..\")\n\n# By default, only matching strings will be shown\nstr_view(c(\"abc\", \"def\", \"fghi\"), \"e\")\n# but you can show all:\nstr_view(c(\"abc\", \"def\", \"fghi\"), \"e\", match = NA)\n# or just those that don't match:\nstr_view(c(\"abc\", \"def\", \"fghi\"), \"e\", match = FALSE)",
            "str_which": "fruit <- c(\"apple\", \"banana\", \"pear\", \"pineapple\")\nstr_which(fruit, \"a\")\n\n# Elements that don't match\nstr_which(fruit, \"^p\", negate = TRUE)\n\n# Missings never match\nstr_which(c(\"a\", NA, \"b\"), \".\")",
            "str_wrap": "thanks_path <- file.path(R.home(\"doc\"), \"THANKS\")\nthanks <- str_c(readLines(thanks_path), collapse = \"\\n\")\nthanks <- word(thanks, 1, 3, fixed(\"\\n\\n\"))\ncat(str_wrap(thanks), \"\\n\")\ncat(str_wrap(thanks, width = 40), \"\\n\")\ncat(str_wrap(thanks, width = 60, indent = 2), \"\\n\")\ncat(str_wrap(thanks, width = 60, exdent = 2), \"\\n\")\ncat(str_wrap(thanks, width = 0, exdent = 2), \"\\n\")",
            "stringr-data": "length(sentences)\nsentences[1:5]\n\nlength(fruit)\nfruit[1:5]\n\nlength(words)\nwords[1:5]",
            "word": "sentences <- c(\"Jane saw a cat\", \"Jane sat down\")\nword(sentences, 1)\nword(sentences, 2)\nword(sentences, -1)\nword(sentences, 2, -1)\n\n# Also vectorised over start and end\nword(sentences[1], 1:3, -1)\nword(sentences[1], 1, 1:4)\n\n# Can define words by other separators\nstr <- 'abc.def..123.4568.999'\nword(str, 1, sep = fixed('..'))\nword(str, 2, sep = fixed('..'))"
        }
    },
    "stringi": {
        "description": "A collection of character string/text/natural language\n    processing tools for pattern searching (e.g., with 'Java'-like regular\n    expressions or the 'Unicode' collation algorithm), random string generation,\n    case mapping, string transliteration, concatenation, sorting, padding,\n    wrapping, Unicode normalisation, date-time formatting and parsing,\n    and many more. They are fast, consistent, convenient, and -\n    thanks to 'ICU' (International Components for Unicode) -\n    portable across all locales and platforms. Documentation about 'stringi' is\n    provided via its website at <https://stringi.gagolewski.com/> and\n    the paper by Gagolewski (2022, <doi:10.18637/jss.v103.i02>).",
        "examples": {
            "operator_add": "c('abc', '123', 'xy') \\%s+\\% letters[1:6]\n'ID_' \\%s+\\% 1:5",
            "operator_compare": "'a' \\%stri<\\% 'b'\nc('a', 'b', 'c') \\%stri>=\\% 'b'",
            "operator_dollar": "\"value='\\%d'\" \\%s$\\% 3\n\"value='\\%d'\" \\%s$\\% 1:3\n\"\\%s='\\%d'\" \\%s$\\% list(\"value\", 3)\n\"\\%s='\\%d'\" \\%s$\\% list(\"value\", 1:3)\n\"\\%s='\\%d'\" \\%s$\\% list(c(\"a\", \"b\", \"c\"), 1)\n\"\\%s='\\%d'\" \\%s$\\% list(c(\"a\", \"b\", \"c\"), 1:3)\n\nx <- c(\"abcd\", \"\\u00DF\\u00B5\\U0001F970\", \"abcdef\")\ncat(\"[\\%6s]\" \\%s$\\% x, sep=\"\\n\")  # width used, not the number of bytes",
            "stri_compare": "# in Polish, ch < h:\nstri_cmp_lt('hladny', 'chladny', locale='pl_PL')\n\n# in Slovak, ch > h:\nstri_cmp_lt('hladny', 'chladny', locale='sk_SK')\n\n# < or > (depends on locale):\nstri_cmp('hladny', 'chladny')\n\n# ignore case differences:\nstri_cmp_equiv('hladny', 'HLADNY', strength=2)\n\n# also ignore diacritical differences:\nstri_cmp_equiv('hladn\\u00FD', 'hladny', strength=1, locale='sk_SK')\n\nmarios <- c('Mario', 'mario', 'M\\\\\\\\u00e1rio', 'm\\\\\\\\u00e1rio')\nstri_cmp_equiv(marios, 'mario', case_level=TRUE, strength=2L)\nstri_cmp_equiv(marios, 'mario', case_level=TRUE, strength=1L)\nstri_cmp_equiv(marios, 'mario', strength=1L)\nstri_cmp_equiv(marios, 'mario', strength=2L)\n\n# non-Unicode-normalized vs normalized string:\nstri_cmp_equiv(stri_trans_nfkd('\\u0105'), '\\u105')\n\n# note the difference:\nstri_cmp_eq(stri_trans_nfkd('\\u0105'), '\\u105')\n\n# ligatures:\nstri_cmp_equiv('\\ufb00', 'ff', strength=2)\n\n# phonebook collation\nstri_cmp_equiv('G\\u00e4rtner', 'Gaertner', locale='de_DE@collation=phonebook', strength=1L)\nstri_cmp_equiv('G\\u00e4rtner', 'Gaertner', locale='de_DE', strength=1L)",
            "stri_count": "s <- 'Lorem ipsum dolor sit amet, consectetur adipisicing elit.'\nstri_count(s, fixed='dolor')\nstri_count(s, regex='\\\\\\\\p{L}+')\n\nstri_count_fixed(s, ' ')\nstri_count_fixed(s, 'o')\nstri_count_fixed(s, 'it')\nstri_count_fixed(s, letters)\nstri_count_fixed('babab', 'b')\nstri_count_fixed(c('stringi', '123'), 'string')\n\nstri_count_charclass(c('stRRRingi', 'STrrrINGI', '123'),\n   c('\\\\\\\\p{Ll}', '\\\\\\\\p{Lu}', '\\\\\\\\p{Zs}'))\nstri_count_charclass(' \\t\\n', '\\\\\\\\p{WHITE_SPACE}') # white space - binary property\nstri_count_charclass(' \\t\\n', '\\\\\\\\p{Z}') # white-space - general category (note the difference)\n\nstri_count_regex(s, '(s|el)it')\nstri_count_regex(s, 'i.i')\nstri_count_regex(s, '.it')\nstri_count_regex('bab baab baaab', c('b.*?b', 'b.b'))\nstri_count_regex(c('stringi', '123'), '^(s|1)')",
            "stri_count_boundaries": "test <- 'The\\u00a0above-mentioned    features are very useful. Spam, spam, eggs, bacon, and spam.'\nstri_count_boundaries(test, type='word')\nstri_count_boundaries(test, type='sentence')\nstri_count_boundaries(test, type='character')\nstri_count_words(test)\n\ntest2 <- stri_trans_nfkd('\\u03c0\\u0153\\u0119\\u00a9\\u00df\\u2190\\u2193\\u2192')\nstri_count_boundaries(test2, type='character')\nstri_length(test2)\nstri_numbytes(test2)",
            "stri_datetime_add": "x <- stri_datetime_now()\nstri_datetime_add(x, units='months') <- 2\nprint(x)\nstri_datetime_add(x, -2, units='months')\nstri_datetime_add(stri_datetime_create(2014, 4, 20), 1, units='years')\nstri_datetime_add(stri_datetime_create(2014, 4, 20), 1, units='years', locale='@calendar=hebrew')\n\nstri_datetime_add(stri_datetime_create(2016, 1, 31), 1, units='months')",
            "stri_datetime_create": "stri_datetime_create(2015, 12, 31, 23, 59, 59.999)\nstri_datetime_create(5775, 8, 1, locale='@calendar=hebrew')  # 1 Nisan 5775 -> 2015-03-21\nstri_datetime_create(2015, 02, 29)\nstri_datetime_create(2015, 02, 29, lenient=TRUE)\nstri_datetime_create(hour=15, minute=59)",
            "stri_datetime_fields": "stri_datetime_fields(stri_datetime_now())\nstri_datetime_fields(stri_datetime_now(), locale='@calendar=hebrew')\nstri_datetime_symbols(locale='@calendar=hebrew')$Month[\n   stri_datetime_fields(stri_datetime_now(), locale='@calendar=hebrew')$Month\n]",
            "stri_datetime_format": "x <- c('2015-02-28', '2015-02-29')\nstri_datetime_parse(x, 'yyyy-MM-dd')\nstri_datetime_parse(x, 'yyyy-MM-dd', lenient=TRUE)\nstri_datetime_parse(x \\%s+\\% \" 17:13\", \"yyyy-MM-dd HH:mm\")\nstri_datetime_parse('19 lipca 2015', 'date_long', locale='pl_PL')\nstri_datetime_format(stri_datetime_now(), 'datetime_relative_medium')",
            "stri_datetime_fstr": "stri_datetime_fstr('\\%Y-\\%m-\\%d \\%H:\\%M:\\%S')",
            "stri_datetime_symbols": "stri_datetime_symbols() # uses the Gregorian calendar in most locales\nstri_datetime_symbols('@calendar=hebrew')\nstri_datetime_symbols('he_IL@calendar=hebrew')\nstri_datetime_symbols('@calendar=islamic')\nstri_datetime_symbols('@calendar=persian')\nstri_datetime_symbols('@calendar=indian')\nstri_datetime_symbols('@calendar=coptic')\nstri_datetime_symbols('@calendar=japanese')\n\nstri_datetime_symbols('ja_JP_TRADITIONAL') # uses the Japanese calendar by default\nstri_datetime_symbols('th_TH_TRADITIONAL') # uses the Buddhist calendar\n\nstri_datetime_symbols('pl_PL', context='format')\nstri_datetime_symbols('pl_PL', context='standalone')\n\nstri_datetime_symbols(width='wide')\nstri_datetime_symbols(width='abbreviated')\nstri_datetime_symbols(width='narrow')",
            "stri_detect": "stri_detect_fixed(c('stringi R', 'R STRINGI', '123'), c('i', 'R', '0'))\nstri_detect_fixed(c('stringi R', 'R STRINGI', '123'), 'R')\n\nstri_detect_charclass(c('stRRRingi','R STRINGI', '123'),\n   c('\\\\\\\\p{Ll}', '\\\\\\\\p{Lu}', '\\\\\\\\p{Zs}'))\n\nstri_detect_regex(c('stringi R', 'R STRINGI', '123'), 'R.')\nstri_detect_regex(c('stringi R', 'R STRINGI', '123'), '[[:alpha:]]*?')\nstri_detect_regex(c('stringi R', 'R STRINGI', '123'), '[a-zC1]')\nstri_detect_regex(c('stringi R', 'R STRINGI', '123'), '( R|RE)')\nstri_detect_regex('stringi', 'STRING.', case_insensitive=TRUE)\n\nstri_detect_regex(c('abc', 'def', '123', 'ghi', '456', '789', 'jkl'),\n   '^[0-9]+$', max_count=1)\nstri_detect_regex(c('abc', 'def', '123', 'ghi', '456', '789', 'jkl'),\n   '^[0-9]+$', max_count=2)\nstri_detect_regex(c('abc', 'def', '123', 'ghi', '456', '789', 'jkl'),\n   '^[0-9]+$', negate=TRUE, max_count=3)",
            "stri_dup": "stri_dup('a', 1:5)\nstri_dup(c('a', NA, 'ba'), 4)\nstri_dup(c('abc', 'pqrst'), c(4, 2))\n\"a\" \\%s*\\% 5",
            "stri_duplicated": "# In the following examples, we have 3 duplicated values,\n# 'a' - 2 times, NA - 1 time\nstri_duplicated(c('a', 'b', 'a', NA, 'a', NA))\nstri_duplicated(c('a', 'b', 'a', NA, 'a', NA), from_last=TRUE)\nstri_duplicated_any(c('a', 'b', 'a', NA, 'a', NA))\n\n# compare the results:\nstri_duplicated(c('\\u0105', stri_trans_nfkd('\\u0105')))\nduplicated(c('\\u0105', stri_trans_nfkd('\\u0105')))\n\nstri_duplicated(c('gro\\u00df', 'GROSS', 'Gro\\u00df', 'Gross'), strength=1)\nduplicated(c('gro\\u00df', 'GROSS', 'Gro\\u00df', 'Gross'))",
            "stri_enc_detect": "## Not run:\n## f <- rawToChar(readBin('test.txt', 'raw', 100000))\n## stri_enc_detect(f)",
            "stri_enc_isascii": "stri_enc_isascii(letters[1:3])\nstri_enc_isascii('\\u0105\\u0104')",
            "stri_enc_isutf8": "stri_enc_isutf8(letters[1:3])\nstri_enc_isutf8('\\u0105\\u0104')\nstri_enc_isutf8('\\u1234\\u0222')",
            "stri_enc_list": "stri_enc_list()\nstri_enc_list(FALSE)",
            "stri_escape_unicode": "stri_escape_unicode('a\\u0105!')",
            "stri_extract": "stri_extract_all('XaaaaX', regex=c('\\\\\\\\p{Ll}', '\\\\\\\\p{Ll}+', '\\\\\\\\p{Ll}{2,3}', '\\\\\\\\p{Ll}{2,3}?'))\nstri_extract_all('Bartolini', coll='i')\nstri_extract_all('stringi is so good!', charclass='\\\\\\\\p{Zs}') # all white-spaces\n\nstri_extract_all_charclass(c('AbcdeFgHijK', 'abc', 'ABC'), '\\\\\\\\p{Ll}')\nstri_extract_all_charclass(c('AbcdeFgHijK', 'abc', 'ABC'), '\\\\\\\\p{Ll}', merge=FALSE)\nstri_extract_first_charclass('AaBbCc', '\\\\\\\\p{Ll}')\nstri_extract_last_charclass('AaBbCc', '\\\\\\\\p{Ll}')\n\n\\dontrun{\n# emoji support available since ICU 57\nstri_extract_all_charclass(stri_enc_fromutf32(32:55200), '\\\\\\\\p{EMOJI}')\n}\n\nstri_extract_all_coll(c('AaaaaaaA', 'AAAA'), 'a')\nstri_extract_first_coll(c('Yy\\u00FD', 'AAA'), 'y', strength=2, locale='sk_SK')\nstri_extract_last_coll(c('Yy\\u00FD', 'AAA'), 'y',  strength=1, locale='sk_SK')\n\nstri_extract_all_regex('XaaaaX', c('\\\\\\\\p{Ll}', '\\\\\\\\p{Ll}+', '\\\\\\\\p{Ll}{2,3}', '\\\\\\\\p{Ll}{2,3}?'))\nstri_extract_first_regex('XaaaaX', c('\\\\\\\\p{Ll}', '\\\\\\\\p{Ll}+', '\\\\\\\\p{Ll}{2,3}', '\\\\\\\\p{Ll}{2,3}?'))\nstri_extract_last_regex('XaaaaX', c('\\\\\\\\p{Ll}', '\\\\\\\\p{Ll}+', '\\\\\\\\p{Ll}{2,3}', '\\\\\\\\p{Ll}{2,3}?'))\n\nstri_list2matrix(stri_extract_all_regex('XaaaaX', c('\\\\\\\\p{Ll}', '\\\\\\\\p{Ll}+')))\nstri_extract_all_regex('XaaaaX', c('\\\\\\\\p{Ll}', '\\\\\\\\p{Ll}+'), simplify=TRUE)\nstri_extract_all_regex('XaaaaX', c('\\\\\\\\p{Ll}', '\\\\\\\\p{Ll}+'), simplify=NA)\n\nstri_extract_all_fixed('abaBAba', 'Aba', case_insensitive=TRUE)\nstri_extract_all_fixed('abaBAba', 'Aba', case_insensitive=TRUE, overlap=TRUE)\n\n# Searching for the last occurrence:\n# Note the difference - regex searches left to right, with no overlaps.\nstri_extract_last_fixed(\"agAGA\", \"aga\", case_insensitive=TRUE)\nstri_extract_last_regex(\"agAGA\", \"aga\", case_insensitive=TRUE)",
            "stri_extract_boundaries": "stri_extract_all_words('stringi: THE string processing package 123.48...')",
            "stri_flatten": "stri_flatten(LETTERS)\nstri_flatten(LETTERS, collapse=',')\nstri_flatten(stri_dup(letters[1:6], 1:3))\nstri_flatten(c(NA, '', 'A', '', 'B', NA, 'C'), collapse=',', na_empty=TRUE, omit_empty=TRUE)\nstri_flatten(c(NA, '', 'A', '', 'B', NA, 'C'), collapse=',', na_empty=NA)",
            "stri_isempty": "stri_isempty(letters[1:3])\nstri_isempty(c(',', '', 'abc', '123', '\\u0105\\u0104'))\nstri_isempty(character(1))",
            "stri_join": "stri_join(1:13, letters)\nstri_join(1:13, letters, sep=',')\nstri_join(1:13, letters, collapse='; ')\nstri_join(1:13, letters, sep=',', collapse='; ')\nstri_join(c('abc', '123', 'xyz'),'###', 1:6, sep=',')\nstri_join(c('abc', '123', 'xyz'),'###', 1:6, sep=',', collapse='; ')",
            "stri_join_list": "stri_join_list(\n   stri_extract_all_words(c('Lorem ipsum dolor sit amet.',\n   'Spam spam bacon sausage and spam.')),\nsep=', ')\n\nstri_join_list(\n   stri_extract_all_words(c('Lorem ipsum dolor sit amet.',\n   'Spam spam bacon sausage and spam.')),\nsep=', ', collapse='. ')\n\nstri_join_list(\n   stri_extract_all_regex(\n      c('spam spam bacon', '123 456', 'spam 789 sausage'), '\\\\\\\\p{L}+'\n   ),\nsep=',')\n\nstri_join_list(\n   stri_extract_all_regex(\n      c('spam spam bacon', '123 456', 'spam 789 sausage'), '\\\\\\\\p{L}+',\n      omit_no_match=TRUE\n   ),\nsep=',', collapse='; ')",
            "stri_length": "stri_length(LETTERS)\nstri_length(c('abc', '123', '\\u0105\\u0104'))\nstri_length('\\u0105') # length is one, but...\nstri_numbytes('\\u0105') # 2 bytes are used\nstri_numbytes(stri_trans_nfkd('\\u0105')) # 3 bytes here but...\nstri_length(stri_trans_nfkd('\\u0105')) # ...two code points (!)\nstri_count_boundaries(stri_trans_nfkd('\\u0105'), type='character') # ...and one Unicode character",
            "stri_list2matrix": "simplify2array(list(c('a', 'b'), c('c', 'd'), c('e', 'f')))\nstri_list2matrix(list(c('a', 'b'), c('c', 'd'), c('e', 'f')))\nstri_list2matrix(list(c('a', 'b'), c('c', 'd'), c('e', 'f')), byrow=TRUE)\n\nsimplify2array(list('a', c('b', 'c')))\nstri_list2matrix(list('a', c('b', 'c')))\nstri_list2matrix(list('a', c('b', 'c')), fill='')\nstri_list2matrix(list('a', c('b', 'c')), fill='', n_min=5)",
            "stri_locale_info": "stri_locale_info('pl_PL')\nstri_locale_info('Pl_pL') # the same result",
            "stri_locale_list": "stri_locale_list()",
            "stri_locale_set": "\\dontrun{\noldloc <- stri_locale_set('pt_BR')\n# ... some locale-dependent operations\n# ... note that you may always modify a locale per-call\n# ... changing the default locale is convenient if you perform\n# ... many operations\nstri_locale_set(oldloc) # restore the previous default locale\n}",
            "stri_locate": "stri_locate_all('stringi', fixed='i')\n\nstri_locate_first_coll('hladn\\u00FD', 'HLADNY', strength=1, locale='sk_SK')\n\nstri_locate_all_regex(\n    c('breakfast=eggs;lunch=pizza', 'breakfast=spam', 'no food here'),\n   '(?<when>\\\\\\\\w+)=(?<what>\\\\\\\\w+)',\n   capture_groups=TRUE\n)  # named capture groups\n\nstri_locate_all_fixed(\"abababa\", \"ABA\", case_insensitive=TRUE, overlap=TRUE)\nstri_locate_first_fixed(\"ababa\", \"aba\")\nstri_locate_last_fixed(\"ababa\", \"aba\")  # starts from end\nstri_locate_last_regex(\"ababa\", \"aba\")  # no overlaps, from left to right\n\nx <- c(\"yes yes\", \"no\", NA)\nstri_locate_all_fixed(x, \"yes\")\nstri_locate_all_fixed(x, \"yes\", omit_no_match=TRUE)\nstri_locate_all_fixed(x, \"yes\", get_length=TRUE)\nstri_locate_all_fixed(x, \"yes\", get_length=TRUE, omit_no_match=TRUE)\nstri_locate_first_fixed(x, \"yes\")\nstri_locate_first_fixed(x, \"yes\", get_length=TRUE)\n\n# Use regex positive-lookahead to locate overlapping pattern matches:\nstri_locate_all_regex('ACAGAGACTTTAGATAGAGAAGA', '(?=AGA)')\n# note that start > end here (match of length zero)",
            "stri_locate_boundaries": "test <- 'The\\u00a0above-mentioned    features are very useful. Spam, spam, eggs, bacon, and spam.'\nstri_locate_all_words(test)\nstri_locate_all_boundaries(\n    'Mr. Jones and Mrs. Brown are very happy. So am I, Prof. Smith.',\n    type='sentence',\n    locale='en_US@ss=standard' # ICU >= 56 only\n)",
            "stri_match": "stri_match_all_regex('breakfast=eggs, lunch=pizza, dessert=icecream',\n   '(\\\\\\\\w+)=(\\\\\\\\w+)')\nstri_match_all_regex(c('breakfast=eggs', 'lunch=pizza', 'no food here'),\n   '(\\\\\\\\w+)=(\\\\\\\\w+)')\nstri_match_all_regex(c('breakfast=eggs;lunch=pizza',\n   'breakfast=bacon;lunch=spaghetti', 'no food here'),\n   '(\\\\\\\\w+)=(\\\\\\\\w+)')\nstri_match_all_regex(c('breakfast=eggs;lunch=pizza',\n   'breakfast=bacon;lunch=spaghetti', 'no food here'),\n   '(?<when>\\\\\\\\w+)=(?<what>\\\\\\\\w+)')  # named capture groups\nstri_match_first_regex(c('breakfast=eggs;lunch=pizza',\n   'breakfast=bacon;lunch=spaghetti', 'no food here'),\n   '(\\\\\\\\w+)=(\\\\\\\\w+)')\nstri_match_last_regex(c('breakfast=eggs;lunch=pizza',\n   'breakfast=bacon;lunch=spaghetti', 'no food here'),\n   '(\\\\\\\\w+)=(\\\\\\\\w+)')\n\nstri_match_first_regex(c('abcd', ':abcd', ':abcd:'), '^(:)?([^:]*)(:)?$')\nstri_match_first_regex(c('abcd', ':abcd', ':abcd:'), '^(:)?([^:]*)(:)?$', cg_missing='')\n\n# Match all the pattern of the form XYX, including overlapping matches:\nstri_match_all_regex('ACAGAGACTTTAGATAGAGAAGA', '(?=(([ACGT])[ACGT]\\\\\\\\2))')[[1]][,2]\n# Compare the above to:\nstri_extract_all_regex('ACAGAGACTTTAGATAGAGAAGA', '([ACGT])[ACGT]\\\\\\\\1')",
            "stri_na2empty": "stri_na2empty(c('a', NA, '', 'b'))",
            "stri_numbytes": "stri_numbytes(letters)\nstri_numbytes(c('abc', '123', '\\u0105\\u0104'))\n\n\\dontrun{\n# this used to fail on Windows, where there were no native support\n# for 4-bytes Unicode characters; see, however, stri_unescape_unicode():\nstri_numbytes('\\U001F600') # compare stri_length('\\U001F600')\n}",
            "stri_opts_collator": "stri_cmp('number100', 'number2')\nstri_cmp('number100', 'number2', opts_collator=stri_opts_collator(numeric=TRUE))\nstri_cmp('number100', 'number2', numeric=TRUE) # equivalent\nstri_cmp('above mentioned', 'above-mentioned')\nstri_cmp('above mentioned', 'above-mentioned', alternate_shifted=TRUE)",
            "stri_opts_fixed": "stri_detect_fixed('ala', 'ALA') # case-sensitive by default\nstri_detect_fixed('ala', 'ALA', opts_fixed=stri_opts_fixed(case_insensitive=TRUE))\nstri_detect_fixed('ala', 'ALA', case_insensitive=TRUE) # equivalent",
            "stri_opts_regex": "stri_detect_regex('ala', 'ALA') # case-sensitive by default\nstri_detect_regex('ala', 'ALA', opts_regex=stri_opts_regex(case_insensitive=TRUE))\nstri_detect_regex('ala', 'ALA', case_insensitive=TRUE) # equivalent\nstri_detect_regex('ala', '(?i)ALA') # equivalent",
            "stri_order": "stri_order(c('hladny', 'chladny'), locale='pl_PL')\nstri_order(c('hladny', 'chladny'), locale='sk_SK')\n\nstri_order(c(1, 100, 2, 101, 11, 10))  # lexicographic order\nstri_order(c(1, 100, 2, 101, 11, 10), numeric=TRUE)  # OK for integers\nstri_order(c(0.25, 0.5, 1, -1, -2, -3), numeric=TRUE)  # incorrect",
            "stri_pad": "stri_pad_left('stringi', 10, pad='#')\nstri_pad_both('stringi', 8:12, pad='*')\n# center on screen:\ncat(stri_pad_both(c('the', 'string', 'processing', 'package'),\n   getOption('width')*0.9), sep='\\n')\ncat(stri_pad_both(c('\\ud6c8\\ubbfc\\uc815\\uc74c', # takes width into account\n   stri_trans_nfkd('\\ud6c8\\ubbfc\\uc815\\uc74c'), 'abcd'),\n   width=10), sep='\\n')",
            "stri_rand_lipsum": "cat(sapply(\n   stri_wrap(stri_rand_lipsum(10), 80, simplify=FALSE),\n   stri_flatten, collapse='\\n'), sep='\\n\\n')\ncat(stri_rand_lipsum(10), sep='\\n\\n')",
            "stri_rand_shuffle": "stri_rand_shuffle(c('abcdefghi', '0123456789'))\n# you can do better than this with stri_rand_strings:\nstri_rand_shuffle(rep(stri_paste(letters, collapse=''), 10))",
            "stri_rand_strings": "stri_rand_strings(5, 10) # 5 strings of length 10\nstri_rand_strings(5, sample(1:10, 5, replace=TRUE)) # 5 strings of random lengths\nstri_rand_strings(10, 5, '[\\\\\\\\p{script=latin}&\\\\\\\\p{Ll}]') # small letters from the Latin script\n\n# generate n random passwords of length in [8, 14]\n# consisting of at least one digit, small and big ASCII letter:\nn <- 10\nstri_rand_shuffle(stri_paste(\n   stri_rand_strings(n, 1, '[0-9]'),\n   stri_rand_strings(n, 1, '[a-z]'),\n   stri_rand_strings(n, 1, '[A-Z]'),\n   stri_rand_strings(n, sample(5:11, 5, replace=TRUE), '[a-zA-Z0-9]')\n))",
            "stri_rank": "stri_rank(c('hladny', 'chladny'), locale='pl_PL')\nstri_rank(c('hladny', 'chladny'), locale='sk_SK')\n\nstri_rank(\"a\" \\%s+\\% c(1, 100, 2, 101, 11, 10))  # lexicographic order\nstri_rank(\"a\" \\%s+\\% c(1, 100, 2, 101, 11, 10), numeric=TRUE)  # OK\nstri_rank(\"a\" \\%s+\\% c(0.25, 0.5, 1, -1, -2, -3), numeric=TRUE)  # incorrect\n\n# Ordering a data frame with respect to two criteria:\nX <- data.frame(a=c(\"b\", NA, \"b\", \"b\", NA, \"a\", \"a\", \"c\"), b=runif(8))\nX[order(stri_rank(X$a), X$b), ]",
            "stri_remove_empty": "stri_remove_empty(stri_na2empty(c('a', NA, '', 'b')))\nstri_remove_empty(c('a', NA, '', 'b'))\nstri_remove_empty(c('a', NA, '', 'b'), TRUE)\n\nstri_omit_empty_na(c('a', NA, '', 'b'))",
            "stri_replace": "stri_replace_all_charclass('aaaa', '[a]', 'b', merge=c(TRUE, FALSE))\n\nstri_replace_all_charclass('a\\nb\\tc   d', '\\\\\\\\p{WHITE_SPACE}', ' ')\nstri_replace_all_charclass('a\\nb\\tc   d', '\\\\\\\\p{WHITE_SPACE}', ' ', merge=TRUE)\n\ns <- 'Lorem ipsum dolor sit amet, consectetur adipisicing elit.'\nstri_replace_all_fixed(s, ' ', '#')\nstri_replace_all_fixed(s, 'o', '0')\n\nstri_replace_all_fixed(c('1', 'NULL', '3'), 'NULL', NA)\n\nstri_replace_all_regex(s, ' .*? ', '#')\nstri_replace_all_regex(s, '(el|s)it', '1234')\nstri_replace_all_regex('abaca', 'a', c('!', '*'))\nstri_replace_all_regex('123|456|789', '(\\\\\\\\p{N}).(\\\\\\\\p{N})', '$2-$1')\nstri_replace_all_regex(c('stringi R', 'REXAMINE', '123'), '( R|R.)', ' r ')\n\n# named capture groups are available since ICU 55\n\\dontrun{\nstri_replace_all_regex('words 123 and numbers 456',\n   '(?<numbers>[0-9]+)', '!${numbers}!')\n}\n\n# Compare the results:\nstri_replace_all_fixed('The quick brown fox jumped over the lazy dog.',\n     c('quick', 'brown', 'fox'), c('slow',  'black', 'bear'), vectorize_all=TRUE)\nstri_replace_all_fixed('The quick brown fox jumped over the lazy dog.',\n     c('quick', 'brown', 'fox'), c('slow',  'black', 'bear'), vectorize_all=FALSE)\n\n# Compare the results:\nstri_replace_all_fixed('The quicker brown fox jumped over the lazy dog.',\n     c('quick', 'brown', 'fox'), c('slow',  'black', 'bear'), vectorize_all=FALSE)\nstri_replace_all_regex('The quicker brown fox jumped over the lazy dog.',\n     '\\\\\\\\b'\\%s+\\%c('quick', 'brown', 'fox')\\%s+\\%'\\\\\\\\b', c('slow',  'black', 'bear'), vectorize_all=FALSE)\n\n# Searching for the last occurrence:\n# Note the difference - regex searches left to right, with no overlaps.\nstri_replace_last_fixed(\"agAGA\", \"aga\", \"*\", case_insensitive=TRUE)\nstri_replace_last_regex(\"agAGA\", \"aga\", \"*\", case_insensitive=TRUE)",
            "stri_replace_na": "x <- c('test', NA)\nstri_paste(x, 1:2)                           # 'test1' NA\npaste(x, 1:2)                                # 'test 1' 'NA 2'\nstri_paste(stri_replace_na(x), 1:2, sep=' ') # 'test 1' 'NA 2'",
            "stri_reverse": "stri_reverse(c('123', 'abc d e f'))\nstri_reverse('ZXY (\\u0105\\u0104123$^).')\nstri_reverse(stri_trans_nfd('\\u0105')) == stri_trans_nfd('\\u0105') # A, ogonek -> agonek, A",
            "stri_sort": "stri_sort(c('hladny', 'chladny'), locale='pl_PL')\nstri_sort(c('hladny', 'chladny'), locale='sk_SK')\nstri_sort(sample(LETTERS))\nstri_sort(c(1, 100, 2, 101, 11, 10))  # lexicographic order\nstri_sort(c(1, 100, 2, 101, 11, 10), numeric=TRUE)  # OK for integers\nstri_sort(c(0.25, 0.5, 1, -1, -2, -3), numeric=TRUE)  # incorrect",
            "stri_sort_key": "stri_sort_key(c('hladny', 'chladny'), locale='pl_PL')\nstri_sort_key(c('hladny', 'chladny'), locale='sk_SK')",
            "stri_split": "stri_split_fixed('a_b_c_d', '_')\nstri_split_fixed('a_b_c__d', '_')\nstri_split_fixed('a_b_c__d', '_', omit_empty=TRUE)\nstri_split_fixed('a_b_c__d', '_', n=2, tokens_only=FALSE) # 'a' & remainder\nstri_split_fixed('a_b_c__d', '_', n=2, tokens_only=TRUE) # 'a' & 'b' only\nstri_split_fixed('a_b_c__d', '_', n=4, omit_empty=TRUE, tokens_only=TRUE)\nstri_split_fixed('a_b_c__d', '_', n=4, omit_empty=FALSE, tokens_only=TRUE)\nstri_split_fixed('a_b_c__d', '_', omit_empty=NA)\nstri_split_fixed(c('ab_c', 'd_ef_g', 'h', ''), '_', n=1, tokens_only=TRUE, omit_empty=TRUE)\nstri_split_fixed(c('ab_c', 'd_ef_g', 'h', ''), '_', n=2, tokens_only=TRUE, omit_empty=TRUE)\nstri_split_fixed(c('ab_c', 'd_ef_g', 'h', ''), '_', n=3, tokens_only=TRUE, omit_empty=TRUE)\n\nstri_list2matrix(stri_split_fixed(c('ab,c', 'd,ef,g', ',h', ''), ',', omit_empty=TRUE))\nstri_split_fixed(c('ab,c', 'd,ef,g', ',h', ''), ',', omit_empty=FALSE, simplify=TRUE)\nstri_split_fixed(c('ab,c', 'd,ef,g', ',h', ''), ',', omit_empty=NA, simplify=TRUE)\nstri_split_fixed(c('ab,c', 'd,ef,g', ',h', ''), ',', omit_empty=TRUE, simplify=TRUE)\nstri_split_fixed(c('ab,c', 'd,ef,g', ',h', ''), ',', omit_empty=NA, simplify=NA)\n\nstri_split_regex(c('ab,c', 'd,ef  ,  g', ',  h', ''),\n   '\\\\\\\\p{WHITE_SPACE}*,\\\\\\\\p{WHITE_SPACE}*', omit_empty=NA, simplify=TRUE)\n\nstri_split_charclass('Lorem ipsum dolor sit amet', '\\\\\\\\p{WHITE_SPACE}')\nstri_split_charclass(' Lorem  ipsum dolor', '\\\\\\\\p{WHITE_SPACE}', n=3,\n   omit_empty=c(FALSE, TRUE))\n\nstri_split_regex('Lorem ipsum dolor sit amet',\n   '\\\\\\\\p{Z}+') # see also stri_split_charclass",
            "stri_split_boundaries": "test <- 'The\\u00a0above-mentioned    features are very useful. ' \\%s+\\%\n   'Spam, spam, eggs, bacon, and spam. 123 456 789'\nstri_split_boundaries(test, type='line')\nstri_split_boundaries(test, type='word')\nstri_split_boundaries(test, type='word', skip_word_none=TRUE)\nstri_split_boundaries(test, type='word', skip_word_none=TRUE, skip_word_letter=TRUE)\nstri_split_boundaries(test, type='word', skip_word_none=TRUE, skip_word_number=TRUE)\nstri_split_boundaries(test, type='sentence')\nstri_split_boundaries(test, type='sentence', skip_sentence_sep=TRUE)\nstri_split_boundaries(test, type='character')\n\n# a filtered break iterator with the new ICU:\nstri_split_boundaries('Mr. Jones and Mrs. Brown are very happy.\nSo am I, Prof. Smith.', type='sentence', locale='en_US@ss=standard') # ICU >= 56 only",
            "stri_sprintf": "stri_printf(\"\\%4s=\\%.3f\", c(\"e\", \"e\\u00b2\", \"\\u03c0\", \"\\u03c0\\u00b2\"),\n    c(exp(1), exp(2), pi, pi^2))\n\nx <- c(\n  \"xxabcd\",\n  \"xx\\u0105\\u0106\\u0107\\u0108\",\n  stri_paste(\n    \"\\u200b\\u200b\\u200b\\u200b\",\n    \"\\U0001F3F4\\U000E0067\\U000E0062\\U000E0073\\U000E0063\\U000E0074\\U000E007F\",\n    \"abcd\"\n  ))\nstri_printf(\"[\\%10s]\", x)  # minimum width = 10\nstri_printf(\"[\\%-10.3s]\", x)  # output of max width = 3, but pad to width of 10\nstri_printf(\"[\\%10s]\", x, use_length=TRUE)  # minimum number of Unicode code points = 10\n\n# vectorization wrt all arguments:\np <- runif(10)\nstri_sprintf(ifelse(p > 0.5, \"P(Y=1)=\\%1$.2f\", \"P(Y=0)=\\%2$.2f\"), p, 1-p)\n\n# using a \"preformatted\" logical vector:\nx <- c(TRUE, FALSE, FALSE, NA, TRUE, FALSE)\nstri_sprintf(\"\\%s) \\%s\", letters[seq_along(x)], c(\"\\u2718\", \"\\u2713\")[x+1])\n\n# custom NA/Inf/NaN strings:\nstri_printf(\"\\%+10.3f\", c(-Inf, -0, 0, Inf, NaN, NA_real_),\n    na_string=\"<NA>\", nan_string=\"\\U0001F4A9\", inf_string=\"\\u221E\")\n\nstri_sprintf(\"UNIX time \\%1$f is \\%1$s.\", Sys.time())\n\n# the following do not work in sprintf()\nstri_sprintf(\"\\%1$#- *2$.*3$f\", 1.23456, 10, 3)  # two asterisks\nstri_sprintf(c(\"\\%s\", \"\\%f\"), pi)  # re-coercion needed\nstri_sprintf(\"\\%1$s is \\%1$f UNIX time.\", Sys.time())  # re-coercion needed\nstri_sprintf(c(\"\\%d\", \"\\%s\"), factor(11:12))  # re-coercion needed\nstri_sprintf(c(\"\\%s\", \"\\%d\"), factor(11:12))  # re-coercion needed",
            "stri_startsendswith": "stri_startswith_charclass(' trim me! ', '\\\\\\\\p{WSpace}')\nstri_startswith_fixed(c('a1', 'a2', 'b3', 'a4', 'c5'), 'a')\nstri_detect_regex(c('a1', 'a2', 'b3', 'a4', 'c5'), '^a')\nstri_startswith_fixed('ababa', 'ba')\nstri_startswith_fixed('ababa', 'ba', from=2)\nstri_startswith_coll(c('a1', 'A2', 'b3', 'A4', 'C5'), 'a', strength=1)\npat <- stri_paste('\\u0635\\u0644\\u0649 \\u0627\\u0644\\u0644\\u0647 ',\n                  '\\u0639\\u0644\\u064a\\u0647 \\u0648\\u0633\\u0644\\u0645XYZ')\nstri_endswith_coll('\\ufdfa\\ufdfa\\ufdfaXYZ', pat, strength=1)",
            "stri_stats_general": "s <- c('Lorem ipsum dolor sit amet, consectetur adipisicing elit.',\n       'nibh augue, suscipit a, scelerisque sed, lacinia in, mi.',\n       'Cras vel lorem. Etiam pellentesque aliquet tellus.',\n       '')\nstri_stats_general(s)",
            "stri_stats_latex": "s <- c('Lorem \\\\\\\\textbf{ipsum} dolor sit \\\\\\\\textit{amet}, consectetur adipisicing elit.',\n       '\\\\\\\\begin{small}Proin nibh augue,\\\\\\\\end{small} suscipit a, scelerisque sed, lacinia in, mi.',\n       '')\nstri_stats_latex(s)",
            "stri_sub": "s <- c(\"spam, spam, bacon, and spam\", \"eggs and spam\")\nstri_sub(s, from=-4)\nstri_sub(s, from=1, length=c(10, 4))\n(stri_sub(s, 1, 4) <- 'stringi')\n\nx <- c('12 3456 789', 'abc', '', NA, '667')\nstri_sub(x, stri_locate_first_regex(x, '[0-9]+')) # see stri_extract_first\nstri_sub(x, stri_locate_last_regex(x, '[0-9]+'))  # see stri_extract_last\n\nstri_sub_replace(x, stri_locate_first_regex(x, '[0-9]+'),\n    omit_na=TRUE, replacement='***') # see stri_replace_first\nstri_sub_replace(x, stri_locate_last_regex(x, '[0-9]+'),\n    omit_na=TRUE, replacement='***') # see stri_replace_last\n\n\n\\dontrun{x |> stri_sub_replace(1, 5, replacement='new_substring')}",
            "stri_sub_all": "x <- c('12 3456 789', 'abc', '', NA, '667')\nstri_sub_all(x, stri_locate_all_regex(x, '[0-9]+')) # see stri_extract_all\nstri_sub_all(x, stri_locate_all_regex(x, '[0-9]+', omit_no_match=TRUE))\n\nstri_sub_all(x, stri_locate_all_regex(x, '[0-9]+', omit_no_match=TRUE)) <- '***'\nprint(x)\n\nstri_sub_replace_all('a b c', c(1, 3, 5), c(1, 3, 5), replacement=c('A', 'B', 'C'))",
            "stri_subset": "stri_subset_regex(c('stringi R', '123', 'ID456', ''), '^[0-9]+$')\n\nx <- c('stringi R', '123', 'ID456', '')\n`stri_subset_regex<-`(x, '[0-9]+$', negate=TRUE, value=NA)  # returns a copy\nstri_subset_regex(x, '[0-9]+$') <- NA  # modifies `x` in-place\nprint(x)",
            "stri_timezone_info": "stri_timezone_info()\nstri_timezone_info(locale='sk_SK')\nsapply(c('short', 'long', 'generic_short', 'generic_long',\n         'gmt_short', 'gmt_long', 'common', 'generic_location'),\n  function(e) stri_timezone_info('Europe/London', display_type=e))",
            "stri_timezone_list": "stri_timezone_list()\nstri_timezone_list(offset=1)\nstri_timezone_list(offset=5.5)\nstri_timezone_list(offset=5.75)\nstri_timezone_list(region='PL')\nstri_timezone_list(region='US', offset=-10)\n\n# Fetch information on all time zones\ndo.call(rbind.data.frame,\n   lapply(stri_timezone_list(), function(tz) stri_timezone_info(tz)))",
            "stri_timezone_set": "\\dontrun{\noldtz <- stri_timezone_set('Europe/Warsaw')\n# ... many time zone-dependent operations\nstri_timezone_set(oldtz) # restore previous default time zone\n}",
            "stri_trans_casemap": "stri_trans_toupper('\\u00DF', 'de_DE') # small German Eszett / scharfes S\nstri_cmp_eq(stri_trans_toupper('i', 'en_US'), stri_trans_toupper('i', 'tr_TR'))\nstri_trans_toupper(c('abc', '123', '\\u0105\\u0104'))\nstri_trans_tolower(c('AbC', '123', '\\u0105\\u0104'))\nstri_trans_totitle(c('AbC', '123', '\\u0105\\u0104'))\nstri_trans_casefold(c('AbC', '123', '\\u0105\\u0104'))\nstri_trans_totitle('stringi is a FREE R pAcKaGe. WItH NO StrinGS attached.') # word boundary\nstri_trans_totitle('stringi is a FREE R pAcKaGe. WItH NO StrinGS attached.', type='sentence')",
            "stri_trans_char": "stri_trans_char('id.123', '.', '_')\nstri_trans_char('babaab', 'ab', '01')\nstri_trans_char('GCUACGGAGCUUCGGAGCUAG', 'ACGT', 'TGCA')",
            "stri_trans_general": "stri_trans_general('gro\\u00df', 'latin-ascii')\nstri_trans_general('stringi', 'latin-greek')\nstri_trans_general('stringi', 'latin-cyrillic')\nstri_trans_general('stringi', 'upper') # see stri_trans_toupper\nstri_trans_general('\\u0104', 'nfd; lower') # compound id; see stri_trans_nfd\nstri_trans_general('Marek G\\u0105golewski', 'pl-pl_FONIPA')\nstri_trans_general('\\u2620', 'any-name') # character name\nstri_trans_general('\\\\\\\\N{latin small letter a}', 'name-any') # decode name\nstri_trans_general('\\u2620', 'hex/c') # to hex\nstri_trans_general(\"\\u201C\\u2026\\u201D \\u0105\\u015B\\u0107\\u017C\",\n    \"NFKD; NFC; [^\\\\\\\\p{L}] latin-ascii\")\n\nx <- \"\\uC885\\uB85C\\uAD6C \\uC0AC\\uC9C1\\uB3D9\"\nstringi::stri_trans_general(x, \"Hangul-Latin\")\n# Deviate from the ICU rules of romanisation of Korean,\n# see https://en.wikipedia.org/wiki/Romanization_of_Korean\nid <- \"\n    :: NFD;\n    \\u11A8 > k;\n    \\u11AE > t;\n    \\u11B8 > p;\n    \\u1105 > r;\n    :: Hangul-Latin;\n\"\nstringi::stri_trans_general(x, id, rules=TRUE)",
            "stri_trans_list": "stri_trans_list()",
            "stri_trans_nf": "stri_trans_nfd('\\u0105') # a with ogonek -> a, ogonek\nstri_trans_nfkc('\\ufdfa') # 1 codepoint -> 18 codepoints",
            "stri_trim": "stri_trim_left('               aaa')\nstri_trim_right('r-project.org/', '\\\\\\\\P{P}')\nstri_trim_both(' Total of 23.5 bitcoins. ', '\\\\\\\\p{N}')\nstri_trim_both(' Total of 23.5 bitcoins. ', '\\\\\\\\P{N}', negate=TRUE)",
            "stri_unescape_unicode": "stri_unescape_unicode('a\\\\\\\\u0105!\\\\\\\\u0032\\\\\\\\n')",
            "stri_unique": "# normalized and non-Unicode-normalized version of the same code point:\nstri_unique(c('\\u0105', stri_trans_nfkd('\\u0105')))\nunique(c('\\u0105', stri_trans_nfkd('\\u0105')))\n\nstri_unique(c('gro\\u00df', 'GROSS', 'Gro\\u00df', 'Gross'), strength=1)",
            "stri_width": "stri_width(LETTERS[1:5])\nstri_width(stri_trans_nfkd('\\u0105'))\nstri_width(stri_trans_nfkd('\\U0001F606'))\nstri_width( # Full-width equivalents of ASCII characters:\n   stri_enc_fromutf32(as.list(c(0x3000, 0xFF01:0xFF5E)))\n)\nstri_width(stri_trans_nfkd('\\ubc1f')) # includes Hangul Jamo medial vowels and final consonants"
        }
    },
    "tidyverse": {
        "description": "The 'tidyverse' is a set of packages that work in harmony\n    because they share common data representations and 'API' design. This\n    package is designed to make it easy to install and load multiple\n    'tidyverse' packages in a single step. Learn more about the\n    'tidyverse' at <https://www.tidyverse.org>.",
        "examples": {
            "tidyverse_conflicts": "tidyverse_conflicts()",
            "tidyverse_logo": "tidyverse_logo()",
            "tidyverse_packages": "tidyverse_packages()",
            "tidyverse_update": "\\dontrun{\ntidyverse_update()\n}"
        }
    },
    "digest": {
        "description": "Implementation of a function 'digest()' for the creation of hash\n digests of arbitrary R objects (using the 'md5', 'sha-1', 'sha-256', 'crc32',\n 'xxhash', 'murmurhash', 'spookyhash', 'blake3', 'crc32c', 'xxh3_64', and 'xxh3_128'\n algorithms) permitting easy comparison of R language objects, as well as functions\n such as'hmac()' to create hash-based message authentication code. Please note that\n this package is not meant to be deployed for cryptographic purposes for which more\n comprehensive (and widely tested) libraries such as 'OpenSSL' should be used.",
        "examples": {
            "AES": "# First in ECB mode: the repeated block is coded the same way each time\nmsg <- as.raw(c(1:16, 1:16))\nkey <- as.raw(1:16)\naes <- AES(key, mode=\"ECB\")\naes$encrypt(msg)\naes$decrypt(aes$encrypt(msg), raw=TRUE)\n\n# Now in CBC mode:  each encoding is different\niv <- sample(0:255, 16, replace=TRUE)\naes <- AES(key, mode=\"CBC\", iv)\ncode <- aes$encrypt(msg)\ncode\n\n# Need a new object for decryption in CBC mode\naes <- AES(key, mode=\"CBC\", iv)\naes$decrypt(code, raw=TRUE)\n\n# In CBC mode, the input length must be a multiple of 16 bytes.\n# You can use `padding = TRUE` to ensure the input length is always valid.\nAES(key, mode=\"CBC\", iv, padding = TRUE)$encrypt(as.raw(1:15))\n\n# CFB mode: IV must be the same length as the Block's block size\n# Two different instances of AES are required for encryption and decryption\niv <- sample(0:255, 16, replace=TRUE)\naes <- AES(key, mode=\"CFB\", iv)\ncode <- aes$encrypt(msg)\ncode\n#decrypt\naes <-  AES(key, mode=\"CFB\", iv)\naes$decrypt(code)\n\n\n# FIPS-197 examples\n\nhextextToRaw <- function(text) {\n  vals <- matrix(as.integer(as.hexmode(strsplit(text, \"\")[[1]])), ncol=2, byrow=TRUE)\n  vals <- vals \\%*\\% c(16, 1)\n  as.raw(vals)\n}\n\nplaintext       <- hextextToRaw(\"00112233445566778899aabbccddeeff\")\n\naes128key       <- hextextToRaw(\"000102030405060708090a0b0c0d0e0f\")\naes128output    <- hextextToRaw(\"69c4e0d86a7b0430d8cdb78070b4c55a\")\naes <- AES(aes128key)\naes128 <- aes$encrypt(plaintext)\nstopifnot(identical(aes128, aes128output))\nstopifnot(identical(plaintext, aes$decrypt(aes128, raw=TRUE)))\n\naes192key       <- hextextToRaw(\"000102030405060708090a0b0c0d0e0f1011121314151617\")\naes192output    <- hextextToRaw(\"dda97ca4864cdfe06eaf70a0ec0d7191\")\naes <- AES(aes192key)\naes192 <- aes$encrypt(plaintext)\nstopifnot(identical(aes192, aes192output))\nstopifnot(identical(plaintext, aes$decrypt(aes192, raw=TRUE)))\n\naes256key       <- hextextToRaw(\"000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f\")\naes256output     <- hextextToRaw(\"8ea2b7ca516745bfeafc49904b496089\")\naes <- AES(aes256key)\naes256 <- aes$encrypt(plaintext)\nstopifnot(identical(aes256, aes256output))\nstopifnot(identical(plaintext, aes$decrypt(aes256, raw=TRUE)))\n\n# SP800-38a examples\n\nplaintext <- hextextToRaw(paste(\"6bc1bee22e409f96e93d7e117393172a\",\n                                \"ae2d8a571e03ac9c9eb76fac45af8e51\",\n                                \"30c81c46a35ce411e5fbc1191a0a52ef\",\n                                \"f69f2445df4f9b17ad2b417be66c3710\",sep=\"\"))\nkey <- hextextToRaw(\"2b7e151628aed2a6abf7158809cf4f3c\")\n\necb128output <- hextextToRaw(paste(\"3ad77bb40d7a3660a89ecaf32466ef97\",\n                                   \"f5d3d58503b9699de785895a96fdbaaf\",\n                                   \"43b1cd7f598ece23881b00e3ed030688\",\n                                   \"7b0c785e27e8ad3f8223207104725dd4\",sep=\"\"))\naes <- AES(key)\necb128 <- aes$encrypt(plaintext)\nstopifnot(identical(ecb128, ecb128output))\nstopifnot(identical(plaintext, aes$decrypt(ecb128, raw=TRUE)))\n\ncbc128output <- hextextToRaw(paste(\"7649abac8119b246cee98e9b12e9197d\",\n                                   \"5086cb9b507219ee95db113a917678b2\",\n                                   \"73bed6b8e3c1743b7116e69e22229516\",\n                                   \"3ff1caa1681fac09120eca307586e1a7\",sep=\"\"))\niv <- hextextToRaw(\"000102030405060708090a0b0c0d0e0f\")\naes <- AES(key, mode=\"CBC\", IV=iv)\ncbc128 <- aes$encrypt(plaintext)\nstopifnot(identical(cbc128, cbc128output))\naes <- AES(key, mode=\"CBC\", IV=iv)\nstopifnot(identical(plaintext, aes$decrypt(cbc128, raw=TRUE)))\n\n\ncfb128output <- hextextToRaw(paste(\"3b3fd92eb72dad20333449f8e83cfb4a\",\n                                   \"c8a64537a0b3a93fcde3cdad9f1ce58b\",\n                                   \"26751f67a3cbb140b1808cf187a4f4df\",\n                                   \"c04b05357c5d1c0eeac4c66f9ff7f2e6\",sep=\"\"))\naes <- AES(key, mode=\"CFB\", IV=iv)\ncfb128 <- aes$encrypt(plaintext)\nstopifnot(identical(cfb128, cfb128output))\naes <- AES(key, mode=\"CFB\", IV=iv)\nstopifnot(identical(plaintext, aes$decrypt(cfb128, raw=TRUE)))\n\n\nctr128output <- hextextToRaw(paste(\"874d6191b620e3261bef6864990db6ce\",\n                                   \"9806f66b7970fdff8617187bb9fffdff\",\n                                   \"5ae4df3edbd5d35e5b4f09020db03eab\",\n                                   \"1e031dda2fbe03d1792170a0f3009cee\",sep=\"\"))\niv <- hextextToRaw(\"f0f1f2f3f4f5f6f7f8f9fafbfcfdfeff\")\naes <- AES(key, mode=\"CTR\", IV=iv)\nctr128 <- aes$encrypt(plaintext)\nstopifnot(identical(ctr128, ctr128output))\naes <- AES(key, mode=\"CTR\", IV=iv)\nstopifnot(identical(plaintext, aes$decrypt(ctr128, raw=TRUE)))",
            "digest": "## Standard RFC 1321 test vectors\nmd5Input <-\n  c(\"\",\n    \"a\",\n    \"abc\",\n    \"message digest\",\n    \"abcdefghijklmnopqrstuvwxyz\",\n    \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\",\n    paste(\"12345678901234567890123456789012345678901234567890123456789012\",\n          \"345678901234567890\", sep=\"\"))\nmd5Output <-\n  c(\"d41d8cd98f00b204e9800998ecf8427e\",\n    \"0cc175b9c0f1b6a831c399e269772661\",\n    \"900150983cd24fb0d6963f7d28e17f72\",\n    \"f96b697d7cb7938d525a2f31aaf161d0\",\n    \"c3fcd3d76192e4007dfb496cca67e13b\",\n    \"d174ab98d277d9f5a5611c2c9f419d9f\",\n    \"57edf4a22be3c955ac49da2e2107b67a\")\n\nfor (i in seq(along=md5Input)) {\n  md5 <- digest(md5Input[i], serialize=FALSE)\n  stopifnot(identical(md5, md5Output[i]))\n}\n\nsha1Input <-\n  c(\"abc\", \"abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq\")\nsha1Output <-\n  c(\"a9993e364706816aba3e25717850c26c9cd0d89d\",\n    \"84983e441c3bd26ebaae4aa1f95129e5e54670f1\")\n\nfor (i in seq(along=sha1Input)) {\n  sha1 <- digest(sha1Input[i], algo=\"sha1\", serialize=FALSE)\n  stopifnot(identical(sha1, sha1Output[i]))\n}\n\ncrc32Input <-\n  c(\"abc\",\n    \"abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq\")\ncrc32Output <-\n  c(\"352441c2\",\n    \"171a3f5f\")\n\nfor (i in seq(along=crc32Input)) {\n  crc32 <- digest(crc32Input[i], algo=\"crc32\", serialize=FALSE)\n  stopifnot(identical(crc32, crc32Output[i]))\n}\n\n\nsha256Input <-\n  c(\"abc\",\n    \"abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq\")\nsha256Output <-\n  c(\"ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad\",\n    \"248d6a61d20638b8e5c026930c3e6039a33ce45964ff2167f6ecedd419db06c1\")\n\nfor (i in seq(along=sha256Input)) {\n  sha256 <- digest(sha256Input[i], algo=\"sha256\", serialize=FALSE)\n  stopifnot(identical(sha256, sha256Output[i]))\n}\n\n# SHA 512 example\nsha512Input <-\n  c(\"abc\",\n    \"abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq\")\nsha512Output <-\n  c(paste(\"ddaf35a193617abacc417349ae20413112e6fa4e89a97ea20a9eeee64b55d39a\",\n          \"2192992a274fc1a836ba3c23a3feebbd454d4423643ce80e2a9ac94fa54ca49f\",\n          sep=\"\"),\n    paste(\"204a8fc6dda82f0a0ced7beb8e08a41657c16ef468b228a8279be331a703c335\",\n          \"96fd15c13b1b07f9aa1d3bea57789ca031ad85c7a71dd70354ec631238ca3445\",\n          sep=\"\"))\n\nfor (i in seq(along=sha512Input)) {\n  sha512 <- digest(sha512Input[i], algo=\"sha512\", serialize=FALSE)\n  stopifnot(identical(sha512, sha512Output[i]))\n}\n\n## xxhash32 example\nxxhash32Input <-\n    c(\"abc\",\n      \"abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq\",\n      \"\")\nxxhash32Output <-\n    c(\"32d153ff\",\n      \"89ea60c3\",\n      \"02cc5d05\")\n\nfor (i in seq(along=xxhash32Input)) {\n    xxhash32 <- digest(xxhash32Input[i], algo=\"xxhash32\", serialize=FALSE)\n    cat(xxhash32, \"\\n\")\n    stopifnot(identical(xxhash32, xxhash32Output[i]))\n}\n\n## xxhash64 example\nxxhash64Input <-\n    c(\"abc\",\n      \"abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq\",\n      \"\")\nxxhash64Output <-\n    c(\"44bc2cf5ad770999\",\n      \"f06103773e8585df\",\n      \"ef46db3751d8e999\")\n\nfor (i in seq(along=xxhash64Input)) {\n    xxhash64 <- digest(xxhash64Input[i], algo=\"xxhash64\", serialize=FALSE)\n    cat(xxhash64, \"\\n\")\n    stopifnot(identical(xxhash64, xxhash64Output[i]))\n}\n\n## these outputs were calculated using mmh3 python package\nmurmur32Input <-\n    c(\"abc\",\n      \"abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq\",\n      \"\")\nmurmur32Output <-\n    c(\"b3dd93fa\",\n      \"ee925b90\",\n      \"00000000\")\n\nfor (i in seq(along=murmur32Input)) {\n    murmur32 <- digest(murmur32Input[i], algo=\"murmur32\", serialize=FALSE)\n    cat(murmur32, \"\\n\")\n    stopifnot(identical(murmur32, murmur32Output[i]))\n}\n\n## these outputs were calculated using spooky python package\nspookyInput <-\n    c(\"a\",\n      \"abc\",\n      \"message digest\")\nspookyOutput <-\n    c(\"bdc9bba09181101a922a4161f0584275\",\n      \"67c93775f715ab8ab01178caf86713c6\",\n      \"9630c2a55c0987a0db44434f9d67a192\")\n\nfor (i in seq(along=spookyInput)) {\n    # skip = 30 skips the serialization header and just hashes the strings\n    spooky <- digest(spookyInput[i], algo=\"spookyhash\", skip = 30)\n    cat(spooky, \"\\n\")\n    ## we can only compare to reference output on little-endian systems\n    if (isTRUE(.Call(digest:::is_little_endian)))\n        stopifnot(identical(spooky, spookyOutput[i]))\n}\n\n## blake3 example\nblake3Input <-\n    c(\"abc\",\n      \"abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq\",\n      \"\")\nblake3Output <-\n    c(\"6437b3ac38465133ffb63b75273a8db548c558465d79db03fd359c6cd5bd9d85\",\n      \"c19012cc2aaf0dc3d8e5c45a1b79114d2df42abb2a410bf54be09e891af06ff8\",\n      \"af1349b9f5f9a1a6a0404dea36dcc9499bcb25c9adc112b7cc9a93cae41f3262\")\n\nfor (i in seq(along=blake3Input)) {\n    blake3 <- digest(blake3Input[i], algo=\"blake3\", serialize=FALSE)\n    cat(blake3, \"\\n\")\n    stopifnot(identical(blake3, blake3Output[i]))\n}\n\n## crc32c\ncrc32cInput <- c(\"123456789\", \"The quick brown fox jumps over the lazy dog\")\ncrc32cOutput <- c(\"e3069283\", \"22620404\")\nfor (i in seq_along(crc32cInput)) {\n    crc32c <- digest(crc32cInput[i], algo=\"crc32c\", serialize=FALSE)\n    cat(crc32c, \"\\n\")\n    stopifnot(identical(crc32c, crc32cOutput[i]))\n}\n\n# example of a digest of a standard R list structure\ndigest(list(LETTERS, data.frame(a=letters[1:5], b=matrix(1:10,ncol=2))))\n\n# test 'length' parameter and file input\nfname <- file.path(R.home(),\"COPYING\")\nx <- readChar(fname, file.info(fname)$size) # read file\nfor (alg in c(\"sha1\", \"md5\", \"crc32\")) {\n  # partial file\n  h1 <- digest(x    , length=18000, algo=alg, serialize=FALSE)\n  h2 <- digest(fname, length=18000, algo=alg, serialize=FALSE, file=TRUE)\n  h3 <- digest( substr(x,1,18000) , algo=alg, serialize=FALSE)\n  stopifnot( identical(h1,h2), identical(h1,h3) )\n  # whole file\n  h1 <- digest(x    , algo=alg, serialize=FALSE)\n  h2 <- digest(fname, algo=alg, serialize=FALSE, file=TRUE)\n  stopifnot( identical(h1,h2) )\n}\n\n# compare md5 algorithm to other tools\nlibrary(tools)\nfname <- file.path(R.home(),\"COPYING\")\nh1 <- as.character(md5sum(fname))\nh2 <- digest(fname, algo=\"md5\", file=TRUE)\nstopifnot( identical(h1,h2) )\n\n## digest is _designed_ to return one has summary per object to for a desired\n## For vectorised output see digest::getVDigest() which provides\n## better performance than base::Vectorize()\n\nmd5 <- getVDigest()\nv <- md5(1:5)     \t\t\t# digest integers 1 to 5\nstopifnot(identical(v[1], digest(1L)),\t# check first and third result\n          identical(v[3], digest(3L)))",
            "digest2int": "current <- digest2int(\"The quick brown fox jumps over the lazy dog\", 0L)\ntarget <- 1369346549L\nstopifnot(identical(target, current))",
            "hmac": "## Standard RFC 2104 test vectors\ncurrent <- hmac('Jefe', 'what do ya want for nothing?', \"md5\")\ntarget <- '750c783e6ab0b503eaa86e310a5db738'\nstopifnot(identical(target, as.character(current)))\n\ncurrent <- hmac(rep(0x0b, 16), 'Hi There', \"md5\")\ntarget <- '9294727a3638bb1c13f48ef8158bfc9d'\nstopifnot(identical(target, as.character(current)))\n\ncurrent <- hmac(rep(0xaa, 16), rep(0xdd, 50), \"md5\")\ntarget <- '56be34521d144c88dbb8c733f0e8b3f6'\nstopifnot(identical(target, as.character(current)))\n\n## SHA1 tests inspired to the RFC 2104 and checked against the python\n## hmac implementation.\ncurrent <- hmac('Jefe', 'what do ya want for nothing?', \"sha1\")\ntarget <- 'effcdf6ae5eb2fa2d27416d5f184df9c259a7c79'\nstopifnot(identical(target, as.character(current)))\n\ncurrent <- hmac(rep(0x0b, 16), 'Hi There', \"sha1\")\ntarget <- '675b0b3a1b4ddf4e124872da6c2f632bfed957e9'\nstopifnot(identical(target, as.character(current)))\n\ncurrent <- hmac(rep(0xaa, 16), rep(0xdd, 50), \"sha1\")\ntarget <- 'd730594d167e35d5956fd8003d0db3d3f46dc7bb'\nstopifnot(identical(target, as.character(current)))",
            "makeRaw": "makeRaw(\"1234567890ABCDE\")",
            "vdigest": "stretch_key <- function(d, n) {\n    md5 <- getVDigest()\n    for (i in seq_len(n))\n        d <- md5(d, serialize = FALSE)\n    d\n}\nstretch_key('abc123', 65e3)\nsha1 <- getVDigest(algo = 'sha1')\nsha1(letters)\n\nmd5Input <-\n    c(\"\",\n      \"a\",\n      \"abc\",\n      \"message digest\",\n      \"abcdefghijklmnopqrstuvwxyz\",\n      \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\",\n      paste(\"12345678901234567890123456789012345678901234567890123456789012\",\n            \"345678901234567890\", sep=\"\"))\nmd5Output <-\n    c(\"d41d8cd98f00b204e9800998ecf8427e\",\n      \"0cc175b9c0f1b6a831c399e269772661\",\n      \"900150983cd24fb0d6963f7d28e17f72\",\n      \"f96b697d7cb7938d525a2f31aaf161d0\",\n      \"c3fcd3d76192e4007dfb496cca67e13b\",\n      \"d174ab98d277d9f5a5611c2c9f419d9f\",\n      \"57edf4a22be3c955ac49da2e2107b67a\")\n\nmd5 <- getVDigest()\nstopifnot(identical(md5(md5Input, serialize = FALSE), md5Output))\nstopifnot(identical(digest(list(\"abc\")),\n                 md5(list(list(\"abc\")))))\n\nsha512Input <-c(\n    \"\",\n    \"The quick brown fox jumps over the lazy dog.\"\n    )\nsha512Output <- c(\n    paste0(\"cf83e1357eefb8bdf1542850d66d8007d620e4050b5715dc83f4a921d36ce9ce\",\n           \"47d0d13c5d85f2b0ff8318d2877eec2f63b931bd47417a81a538327af927da3e\"),\n    paste0(\"91ea1245f20d46ae9a037a989f54f1f790f0a47607eeb8a14d12890cea77a1bb\",\n           \"c6c7ed9cf205e67b7f2b8fd4c7dfd3a7a8617e45f3c463d481c7e586c39ac1ed\")\n    )\n\nsha512 <- getVDigest(algo = 'sha512')\nstopifnot(identical(sha512(sha512Input, serialize = FALSE), sha512Output))"
        }
    },
    "tidyselect": {
        "description": "A backend for the selecting functions of the 'tidyverse'.  It\n    makes it easy to implement select-like functions in your own packages\n    in a way that is consistent with other 'tidyverse' interfaces for\n    selection.",
        "examples": {
            "eval_relocate": "library(rlang)\n\n# Interpret defused code as a request to relocate\nx <- expr(c(mpg, disp))\nafter <- expr(wt)\neval_relocate(x, mtcars, after = after)\n\n# Supplying neither `before` nor `after` will move the selection to the\n# left-hand side\neval_relocate(x, mtcars)\n\n# Within a function, use `enquo()` to defuse a single argument.\n# Note that `before` and `after` must also be defused with `enquo()`.\nmy_relocator <- function(x, expr, before = NULL, after = NULL) {\n  eval_relocate(enquo(expr), x, before = enquo(before), after = enquo(after))\n}\n\nmy_relocator(mtcars, vs, before = hp)\n\n# Here is an example of using `eval_relocate()` to implement `relocate()`.\n# Note that the dots are passed on as a defused call to `c(...)`.\nrelocate <- function(.x, ..., .before = NULL, .after = NULL) {\n  pos <- eval_relocate(\n    expr(c(...)),\n    .x,\n    before = enquo(.before),\n    after = enquo(.after)\n  )\n  set_names(.x[pos], names(pos))\n}\n\nrelocate(mtcars, vs, .before = hp)\nrelocate(mtcars, starts_with(\"d\"), .after = last_col())",
            "eval_select": "library(rlang)\n\n# Interpret defused code as selection:\nx <- expr(mpg:cyl)\neval_select(x, mtcars)\n\n# Interpret defused code as a renaming selection. All inputs must\n# be named within `c()`:\ntry(eval_rename(expr(mpg), mtcars))\neval_rename(expr(c(foo = mpg)), mtcars)\n\n\n# Within a function, use `enquo()` to defuse one argument:\nmy_function <- function(x, expr) {\n  eval_select(enquo(expr), x)\n}\n\n# If your function takes dots, evaluate a defused call to `c(...)`\n# with `expr(c(...))`:\nmy_function <- function(.x, ...) {\n  eval_select(expr(c(...)), .x)\n}\n\n# If your function takes dots and a named argument, use `{{ }}`\n# inside the defused expression to tunnel it inside the tidyselect DSL:\nmy_function <- function(.x, .expr, ...) {\n  eval_select(expr(c({{ .expr }}, ...)), .x)\n}\n\n# Note that the trick above works because `expr({{ arg }})` is the\n# same as `enquo(arg)`.\n\n# Supply `error_arg` to improve the error message in case of\n# unexpected empty selection:\nselect_not_empty <- function(x, cols) {\n  eval_select(expr = enquo(cols), data = x, allow_empty = FALSE, error_arg = \"cols\")\n}\ntry(select_not_empty(mtcars, cols = starts_with(\"vs2\")))\n\n# The evaluators return a named vector of locations. Here are\n# examples of using these location vectors to implement `select()`\n# and `rename()`:\nselect <- function(.x, ...) {\n  pos <- eval_select(expr(c(...)), .x)\n  set_names(.x[pos], names(pos))\n}\nrename <- function(.x, ...) {\n  pos <- eval_rename(expr(c(...)), .x)\n  names(.x)[pos] <- names(pos)\n  .x\n}\n\nselect(mtcars, mpg:cyl)\nrename(mtcars, foo = mpg)",
            "poke_vars": "poke_vars(letters)\npeek_vars()\n\n# Now that the variables are registered, the helpers can figure out\n# the locations of elements within the variable vector:\nall_of(c(\"d\", \"z\"))\n\n# In a function be sure to restore the previous variables. An exit\n# hook is the best way to do it:\nfn <- function(vars) {\n  old <- poke_vars(vars)\n  on.exit(poke_vars(old))\n\n  all_of(\"d\")\n}\nfn(letters)\nfn(letters[3:5])\n\n# The previous variables are still registered after fn() was\n# called:\npeek_vars()\n\n\n# It is recommended to use the scoped variant as it restores the\n# state automatically when the function returns:\nfn <- function(vars) {\n  scoped_vars(vars)\n  starts_with(\"r\")\n}\nfn(c(\"red\", \"blue\", \"rose\"))\n\n# The with_vars() helper makes it easy to pass an expression that\n# should be evaluated in a variable context. Thanks to lazy\n# evaluation, you can just pass the expression argument from your\n# wrapper to with_vars():\nfn <- function(expr) {\n  vars <- c(\"red\", \"blue\", \"rose\")\n  with_vars(vars, expr)\n}\nfn(starts_with(\"r\"))",
            "vars_pull": "# It takes its argument by expression:\nvars_pull(letters, c)\n\n# Negative numbers select from the end:\nvars_pull(letters, -3)\n\n# You can unquote variables:\nvar <- 10\nvars_pull(letters, !!var)"
        }
    },
    "tidyr": {
        "description": "Tools to help to create tidy data, where each column is a\n    variable, each row is an observation, and each cell contains a single\n    value.  'tidyr' contains tools for changing the shape (pivoting) and\n    hierarchy (nesting and 'unnesting') of a dataset, turning deeply\n    nested lists into rectangular data frames ('rectangling'), and\n    extracting values out of string columns. It also includes tools for\n    working with missing values (both implicit and explicit).",
        "examples": {
            "check_pivot_spec": "# A valid spec\nspec <- tibble(.name = \"a\", .value = \"b\", foo = 1)\ncheck_pivot_spec(spec)\n\nspec <- tibble(.name = \"a\")\ntry(check_pivot_spec(spec))\n\n# `.name` and `.value` are forced to be the first two columns\nspec <- tibble(foo = 1, .value = \"b\", .name = \"a\")\ncheck_pivot_spec(spec)",
            "chop": "# Chop ----------------------------------------------------------------------\ndf <- tibble(x = c(1, 1, 1, 2, 2, 3), y = 1:6, z = 6:1)\n# Note that we get one row of output for each unique combination of\n# non-chopped variables\ndf \\%>\\% chop(c(y, z))\n# cf nest\ndf \\%>\\% nest(data = c(y, z))\n\n# Unchop --------------------------------------------------------------------\ndf <- tibble(x = 1:4, y = list(integer(), 1L, 1:2, 1:3))\ndf \\%>\\% unchop(y)\ndf \\%>\\% unchop(y, keep_empty = TRUE)\n\n# unchop will error if the types are not compatible:\ndf <- tibble(x = 1:2, y = list(\"1\", 1:3))\ntry(df \\%>\\% unchop(y))\n\n# Unchopping a list-col of data frames must generate a df-col because\n# unchop leaves the column names unchanged\ndf <- tibble(x = 1:3, y = list(NULL, tibble(x = 1), tibble(y = 1:2)))\ndf \\%>\\% unchop(y)\ndf \\%>\\% unchop(y, keep_empty = TRUE)",
            "cms_patient_experience": "cms_patient_experience \\%>\\%\n  dplyr::distinct(measure_cd, measure_title)\n\ncms_patient_experience \\%>\\%\n  pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n )\n\ncms_patient_care \\%>\\%\n  pivot_wider(\n    names_from = type,\n    values_from = score\n  )\n\ncms_patient_care \\%>\\%\n  pivot_wider(\n    names_from = measure_abbr,\n    values_from = score\n  )\n\ncms_patient_care \\%>\\%\n  pivot_wider(\n    names_from = c(measure_abbr, type),\n    values_from = score\n  )",
            "complete": "df <- tibble(\n  group = c(1:2, 1, 2),\n  item_id = c(1:2, 2, 3),\n  item_name = c(\"a\", \"a\", \"b\", \"b\"),\n  value1 = c(1, NA, 3, 4),\n  value2 = 4:7\n)\ndf\n\n# Combinations --------------------------------------------------------------\n# Generate all possible combinations of `group`, `item_id`, and `item_name`\n# (whether or not they appear in the data)\ndf \\%>\\% complete(group, item_id, item_name)\n\n# Cross all possible `group` values with the unique pairs of\n# `(item_id, item_name)` that already exist in the data\ndf \\%>\\% complete(group, nesting(item_id, item_name))\n\n# Within each `group`, generate all possible combinations of\n# `item_id` and `item_name` that occur in that group\ndf \\%>\\%\n  dplyr::group_by(group) \\%>\\%\n  complete(item_id, item_name)\n\n# Supplying values for new rows ---------------------------------------------\n# Use `fill` to replace NAs with some value. By default, affects both new\n# (implicit) and pre-existing (explicit) missing values.\ndf \\%>\\%\n  complete(\n    group,\n    nesting(item_id, item_name),\n    fill = list(value1 = 0, value2 = 99)\n  )\n\n# Limit the fill to only the newly created (i.e. previously implicit)\n# missing values with `explicit = FALSE`\ndf \\%>\\%\n  complete(\n    group,\n    nesting(item_id, item_name),\n    fill = list(value1 = 0, value2 = 99),\n    explicit = FALSE\n  )",
            "drop_na": "df <- tibble(x = c(1, 2, NA), y = c(\"a\", NA, \"b\"))\ndf \\%>\\% drop_na()\ndf \\%>\\% drop_na(x)\n\nvars <- \"y\"\ndf \\%>\\% drop_na(x, any_of(vars))",
            "expand": "# Finding combinations ------------------------------------------------------\nfruits <- tibble(\n  type = c(\"apple\", \"orange\", \"apple\", \"orange\", \"orange\", \"orange\"),\n  year = c(2010, 2010, 2012, 2010, 2011, 2012),\n  size = factor(\n    c(\"XS\", \"S\", \"M\", \"S\", \"S\", \"M\"),\n    levels = c(\"XS\", \"S\", \"M\", \"L\")\n  ),\n  weights = rnorm(6, as.numeric(size) + 2)\n)\n\n# All combinations, including factor levels that are not used\nfruits \\%>\\% expand(type)\nfruits \\%>\\% expand(size)\nfruits \\%>\\% expand(type, size)\nfruits \\%>\\% expand(type, size, year)\n\n# Only combinations that already appear in the data\nfruits \\%>\\% expand(nesting(type))\nfruits \\%>\\% expand(nesting(size))\nfruits \\%>\\% expand(nesting(type, size))\nfruits \\%>\\% expand(nesting(type, size, year))\n\n# Other uses ----------------------------------------------------------------\n# Use with `full_seq()` to fill in values of continuous variables\nfruits \\%>\\% expand(type, size, full_seq(year, 1))\nfruits \\%>\\% expand(type, size, 2010:2013)\n\n# Use `anti_join()` to determine which observations are missing\nall <- fruits \\%>\\% expand(type, size, year)\nall\nall \\%>\\% dplyr::anti_join(fruits)\n\n# Use with `right_join()` to fill in missing rows (like `complete()`)\nfruits \\%>\\% dplyr::right_join(all)\n\n# Use with `group_by()` to expand within each group\nfruits \\%>\\%\n  dplyr::group_by(type) \\%>\\%\n  expand(year, size)",
            "expand_grid": "# Default behavior varies the first column \"slowest\"\nexpand_grid(x = 1:3, y = 1:2)\n\n# Vary the first column \"fastest\", like `expand.grid()`\nexpand_grid(x = 1:3, y = 1:2, .vary = \"fastest\")\n\n# Can also expand data frames\nexpand_grid(df = tibble(x = 1:2, y = c(2, 1)), z = 1:3)\n\n# And matrices\nexpand_grid(x1 = matrix(1:4, nrow = 2), x2 = matrix(5:8, nrow = 2))",
            "extract": "df <- tibble(x = c(NA, \"a-b\", \"a-d\", \"b-c\", \"d-e\"))\ndf \\%>\\% extract(x, \"A\")\ndf \\%>\\% extract(x, c(\"A\", \"B\"), \"([[:alnum:]]+)-([[:alnum:]]+)\")\n\n# Now recommended\ndf \\%>\\%\n  separate_wider_regex(\n    x,\n    patterns = c(A = \"[[:alnum:]]+\", \"-\", B = \"[[:alnum:]]+\")\n  )\n\n# If no match, NA:\ndf \\%>\\% extract(x, c(\"A\", \"B\"), \"([a-d]+)-([a-d]+)\")",
            "fill": "# direction = \"down\" --------------------------------------------------------\n# Value (year) is recorded only when it changes\nsales <- tibble::tribble(\n  ~quarter, ~year, ~sales,\n  \"Q1\",    2000,    66013,\n  \"Q2\",      NA,    69182,\n  \"Q3\",      NA,    53175,\n  \"Q4\",      NA,    21001,\n  \"Q1\",    2001,    46036,\n  \"Q2\",      NA,    58842,\n  \"Q3\",      NA,    44568,\n  \"Q4\",      NA,    50197,\n  \"Q1\",    2002,    39113,\n  \"Q2\",      NA,    41668,\n  \"Q3\",      NA,    30144,\n  \"Q4\",      NA,    52897,\n  \"Q1\",    2004,    32129,\n  \"Q2\",      NA,    67686,\n  \"Q3\",      NA,    31768,\n  \"Q4\",      NA,    49094\n)\n# `fill()` defaults to replacing missing data from top to bottom\nsales \\%>\\% fill(year)\n\n# direction = \"up\" ----------------------------------------------------------\n# Value (pet_type) is missing above\ntidy_pets <- tibble::tribble(\n  ~rank, ~pet_type, ~breed,\n  1L,        NA,    \"Boston Terrier\",\n  2L,        NA,    \"Retrievers (Labrador)\",\n  3L,        NA,    \"Retrievers (Golden)\",\n  4L,        NA,    \"French Bulldogs\",\n  5L,        NA,    \"Bulldogs\",\n  6L,     \"Dog\",    \"Beagles\",\n  1L,        NA,    \"Persian\",\n  2L,        NA,    \"Maine Coon\",\n  3L,        NA,    \"Ragdoll\",\n  4L,        NA,    \"Exotic\",\n  5L,        NA,    \"Siamese\",\n  6L,     \"Cat\",    \"American Short\"\n)\n\n# For values that are missing above you can use `.direction = \"up\"`\ntidy_pets \\%>\\%\n  fill(pet_type, .direction = \"up\")\n\n# direction = \"downup\" ------------------------------------------------------\n# Value (n_squirrels) is missing above and below within a group\nsquirrels <- tibble::tribble(\n  ~group,    ~name,     ~role,     ~n_squirrels,\n  1,      \"Sam\",    \"Observer\",   NA,\n  1,     \"Mara\", \"Scorekeeper\",    8,\n  1,    \"Jesse\",    \"Observer\",   NA,\n  1,      \"Tom\",    \"Observer\",   NA,\n  2,     \"Mike\",    \"Observer\",   NA,\n  2,  \"Rachael\",    \"Observer\",   NA,\n  2,  \"Sydekea\", \"Scorekeeper\",   14,\n  2, \"Gabriela\",    \"Observer\",   NA,\n  3,  \"Derrick\",    \"Observer\",   NA,\n  3,     \"Kara\", \"Scorekeeper\",    9,\n  3,    \"Emily\",    \"Observer\",   NA,\n  3, \"Danielle\",    \"Observer\",   NA\n)\n\n# The values are inconsistently missing by position within the `group`.\n# Use `.direction = \"downup\"` to fill missing values in both directions\n# and `.by = group` to apply the fill per group.\nsquirrels \\%>\\%\n  fill(n_squirrels, .direction = \"downup\", .by = group)\n\n# If you want, you can also supply a data frame grouped with `group_by()`,\n# but don't forget to `ungroup()`!\nsquirrels \\%>\\%\n  dplyr::group_by(group) \\%>\\%\n  fill(n_squirrels, .direction = \"downup\") \\%>\\%\n  dplyr::ungroup()",
            "full_seq": "full_seq(c(1, 2, 4, 5, 10), 1)",
            "gather": "# From https://stackoverflow.com/questions/1181060\nstocks <- tibble(\n  time = as.Date(\"2009-01-01\") + 0:9,\n  X = rnorm(10, 0, 1),\n  Y = rnorm(10, 0, 2),\n  Z = rnorm(10, 0, 4)\n)\n\ngather(stocks, \"stock\", \"price\", -time)\nstocks \\%>\\% gather(\"stock\", \"price\", -time)\n\n# get first observation for each Species in iris data -- base R\nmini_iris <- iris[c(1, 51, 101), ]\n# gather Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\ngather(mini_iris, key = \"flower_att\", value = \"measurement\",\n       Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)\n# same result but less verbose\ngather(mini_iris, key = \"flower_att\", value = \"measurement\", -Species)",
            "hoist": "df <- tibble(\n  character = c(\"Toothless\", \"Dory\"),\n  metadata = list(\n    list(\n      species = \"dragon\",\n      color = \"black\",\n      films = c(\n        \"How to Train Your Dragon\",\n        \"How to Train Your Dragon 2\",\n        \"How to Train Your Dragon: The Hidden World\"\n      )\n    ),\n    list(\n      species = \"blue tang\",\n      color = \"blue\",\n      films = c(\"Finding Nemo\", \"Finding Dory\")\n    )\n  )\n)\ndf\n\n# Extract only specified components\ndf \\%>\\% hoist(metadata,\n  \"species\",\n  first_film = list(\"films\", 1L),\n  third_film = list(\"films\", 3L)\n)",
            "nest": "df <- tibble(x = c(1, 1, 1, 2, 2, 3), y = 1:6, z = 6:1)\n\n# Specify variables to nest using name-variable pairs.\n# Note that we get one row of output for each unique combination of\n# non-nested variables.\ndf \\%>\\% nest(data = c(y, z))\n\n# Specify variables to nest by (rather than variables to nest) using `.by`\ndf \\%>\\% nest(.by = x)\n\n# In this case, since `...` isn't used you can specify the resulting column\n# name with `.key`\ndf \\%>\\% nest(.by = x, .key = \"cols\")\n\n# Use tidyselect syntax and helpers, just like in `dplyr::select()`\ndf \\%>\\% nest(data = any_of(c(\"y\", \"z\")))\n\n# `...` and `.by` can be used together to drop columns you no longer need,\n# or to include the columns you are nesting by in the inner data frame too.\n# This drops `z`:\ndf \\%>\\% nest(data = y, .by = x)\n# This includes `x` in the inner data frame:\ndf \\%>\\% nest(data = everything(), .by = x)\n\n# Multiple nesting structures can be specified at once\niris \\%>\\%\n  nest(petal = starts_with(\"Petal\"), sepal = starts_with(\"Sepal\"))\niris \\%>\\%\n  nest(width = contains(\"Width\"), length = contains(\"Length\"))\n\n# Nesting a grouped data frame nests all variables apart from the group vars\nfish_encounters \\%>\\%\n  dplyr::group_by(fish) \\%>\\%\n  nest()\n\n# That is similar to `nest(.by = )`, except here the result isn't grouped\nfish_encounters \\%>\\%\n  nest(.by = fish)\n\n# Nesting is often useful for creating per group models\nmtcars \\%>\\%\n  nest(.by = cyl) \\%>\\%\n  dplyr::mutate(models = lapply(data, function(df) lm(mpg ~ wt, data = df)))",
            "nest_legacy": "# Nest and unnest are inverses\ndf <- tibble(x = c(1, 1, 2), y = 3:1)\ndf \\%>\\% nest_legacy(y)\ndf \\%>\\% nest_legacy(y) \\%>\\% unnest_legacy()\n\n# nesting -------------------------------------------------------------------\nas_tibble(iris) \\%>\\% nest_legacy(!Species)\nas_tibble(chickwts) \\%>\\% nest_legacy(weight)\n\n# unnesting -----------------------------------------------------------------\ndf <- tibble(\n  x = 1:2,\n  y = list(\n    tibble(z = 1),\n    tibble(z = 3:4)\n  )\n)\ndf \\%>\\% unnest_legacy(y)\n\n# You can also unnest multiple columns simultaneously\ndf <- tibble(\n  a = list(c(\"a\", \"b\"), \"c\"),\n  b = list(1:2, 3),\n  c = c(11, 22)\n)\ndf \\%>\\% unnest_legacy(a, b)\n# If you omit the column names, it'll unnest all list-cols\ndf \\%>\\% unnest_legacy()",
            "pack": "# Packing -------------------------------------------------------------------\n# It's not currently clear why you would ever want to pack columns\n# since few functions work with this sort of data.\ndf <- tibble(x1 = 1:3, x2 = 4:6, x3 = 7:9, y = 1:3)\ndf\ndf \\%>\\% pack(x = starts_with(\"x\"))\ndf \\%>\\% pack(x = c(x1, x2, x3), y = y)\n\n# .names_sep allows you to strip off common prefixes; this\n# acts as a natural inverse to name_sep in unpack()\niris \\%>\\%\n  as_tibble() \\%>\\%\n  pack(\n    Sepal = starts_with(\"Sepal\"),\n    Petal = starts_with(\"Petal\"),\n    .names_sep = \".\"\n  )\n\n# Unpacking -----------------------------------------------------------------\ndf <- tibble(\n  x = 1:3,\n  y = tibble(a = 1:3, b = 3:1),\n  z = tibble(X = c(\"a\", \"b\", \"c\"), Y = runif(3), Z = c(TRUE, FALSE, NA))\n)\ndf\ndf \\%>\\% unpack(y)\ndf \\%>\\% unpack(c(y, z))\ndf \\%>\\% unpack(c(y, z), names_sep = \"_\")",
            "pivot_longer": "# See vignette(\"pivot\") for examples and explanation\n\n# Simplest case where column names are character data\nrelig_income\nrelig_income \\%>\\%\n  pivot_longer(!religion, names_to = \"income\", values_to = \"count\")\n\n# Slightly more complex case where columns have common prefix,\n# and missing missings are structural so should be dropped.\nbillboard\nbillboard \\%>\\%\n  pivot_longer(\n    cols = starts_with(\"wk\"),\n    names_to = \"week\",\n    names_prefix = \"wk\",\n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n\n# Multiple variables stored in column names\nwho \\%>\\% pivot_longer(\n  cols = new_sp_m014:newrel_f65,\n  names_to = c(\"diagnosis\", \"gender\", \"age\"),\n  names_pattern = \"new_?(.*)_(.)(.*)\",\n  values_to = \"count\"\n)\n\n# Multiple observations per row. Since all columns are used in the pivoting\n# process, we'll use `cols_vary` to keep values from the original columns\n# close together in the output.\nanscombe\nanscombe \\%>\\%\n  pivot_longer(\n    everything(),\n    cols_vary = \"slowest\",\n    names_to = c(\".value\", \"set\"),\n    names_pattern = \"(.)(.)\"\n  )",
            "pivot_longer_spec": "# See vignette(\"pivot\") for examples and explanation\n\n# Use `build_longer_spec()` to build `spec` using similar syntax to `pivot_longer()`\n# and run `pivot_longer_spec()` based on `spec`.\nspec <- relig_income \\%>\\% build_longer_spec(\n  cols = !religion,\n  names_to = \"income\",\n  values_to = \"count\"\n)\nspec\n\npivot_longer_spec(relig_income, spec)\n\n# Is equivalent to:\nrelig_income \\%>\\% pivot_longer(\n  cols = !religion,\n  names_to = \"income\",\n  values_to = \"count\"\n)",
            "pivot_wider": "# See vignette(\"pivot\") for examples and explanation\n\nfish_encounters\nfish_encounters \\%>\\%\n  pivot_wider(names_from = station, values_from = seen)\n# Fill in missing values\nfish_encounters \\%>\\%\n  pivot_wider(names_from = station, values_from = seen, values_fill = 0)\n\n# Generate column names from multiple variables\nus_rent_income\nus_rent_income \\%>\\%\n  pivot_wider(\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )\n\n# You can control whether `names_from` values vary fastest or slowest\n# relative to the `values_from` column names using `names_vary`.\nus_rent_income \\%>\\%\n  pivot_wider(\n    names_from = variable,\n    values_from = c(estimate, moe),\n    names_vary = \"slowest\"\n  )\n\n# When there are multiple `names_from` or `values_from`, you can use\n# use `names_sep` or `names_glue` to control the output variable names\nus_rent_income \\%>\\%\n  pivot_wider(\n    names_from = variable,\n    names_sep = \".\",\n    values_from = c(estimate, moe)\n  )\nus_rent_income \\%>\\%\n  pivot_wider(\n    names_from = variable,\n    names_glue = \"{variable}_{.value}\",\n    values_from = c(estimate, moe)\n  )\n\n# Can perform aggregation with `values_fn`\nwarpbreaks <- as_tibble(warpbreaks[c(\"wool\", \"tension\", \"breaks\")])\nwarpbreaks\nwarpbreaks \\%>\\%\n  pivot_wider(\n    names_from = wool,\n    values_from = breaks,\n    values_fn = mean\n  )\n\n# Can pass an anonymous function to `values_fn` when you\n# need to supply additional arguments\nwarpbreaks$breaks[1] <- NA\nwarpbreaks \\%>\\%\n  pivot_wider(\n    names_from = wool,\n    values_from = breaks,\n    values_fn = ~ mean(.x, na.rm = TRUE)\n  )",
            "pivot_wider_spec": "# See vignette(\"pivot\") for examples and explanation\n\nus_rent_income\nspec1 <- us_rent_income \\%>\\%\n  build_wider_spec(names_from = variable, values_from = c(estimate, moe))\nspec1\n\nus_rent_income \\%>\\%\n  pivot_wider_spec(spec1)\n\n# Is equivalent to\nus_rent_income \\%>\\%\n  pivot_wider(names_from = variable, values_from = c(estimate, moe))\n\n# `pivot_wider_spec()` provides more control over column names and output format\n# instead of creating columns with estimate_ and moe_ prefixes,\n# keep original variable name for estimates and attach _moe as suffix\nspec2 <- tibble(\n  .name = c(\"income\", \"rent\", \"income_moe\", \"rent_moe\"),\n  .value = c(\"estimate\", \"estimate\", \"moe\", \"moe\"),\n  variable = c(\"income\", \"rent\", \"income\", \"rent\")\n)\n\nus_rent_income \\%>\\%\n  pivot_wider_spec(spec2)",
            "replace_na": "# Replace NAs in a data frame\ndf <- tibble(x = c(1, 2, NA), y = c(\"a\", NA, \"b\"))\ndf \\%>\\% replace_na(list(x = 0, y = \"unknown\"))\n\n# Replace NAs in a vector\ndf \\%>\\% dplyr::mutate(x = replace_na(x, 0))\n# OR\ndf$x \\%>\\% replace_na(0)\ndf$y \\%>\\% replace_na(\"unknown\")\n\n# Replace NULLs in a list: NULLs are the list-col equivalent of NAs\ndf_list <- tibble(z = list(1:5, NULL, 10:20))\ndf_list \\%>\\% replace_na(list(z = list(5)))",
            "separate": "# If you want to split by any non-alphanumeric value (the default):\ndf <- tibble(x = c(NA, \"x.y\", \"x.z\", \"y.z\"))\ndf \\%>\\% separate(x, c(\"A\", \"B\"))\n\n# If you just want the second variable:\ndf \\%>\\% separate(x, c(NA, \"B\"))\n\n# We now recommend separate_wider_delim() instead:\ndf \\%>\\% separate_wider_delim(x, \".\", names = c(\"A\", \"B\"))\ndf \\%>\\% separate_wider_delim(x, \".\", names = c(NA, \"B\"))\n\n# Controlling uneven splits -------------------------------------------------\n# If every row doesn't split into the same number of pieces, use\n# the extra and fill arguments to control what happens:\ndf <- tibble(x = c(\"x\", \"x y\", \"x y z\", NA))\ndf \\%>\\% separate(x, c(\"a\", \"b\"))\n# The same behaviour as previous, but drops the c without warnings:\ndf \\%>\\% separate(x, c(\"a\", \"b\"), extra = \"drop\", fill = \"right\")\n# Opposite of previous, keeping the c and filling left:\ndf \\%>\\% separate(x, c(\"a\", \"b\"), extra = \"merge\", fill = \"left\")\n# Or you can keep all three:\ndf \\%>\\% separate(x, c(\"a\", \"b\", \"c\"))\n\n# To only split a specified number of times use extra = \"merge\":\ndf <- tibble(x = c(\"x: 123\", \"y: error: 7\"))\ndf \\%>\\% separate(x, c(\"key\", \"value\"), \": \", extra = \"merge\")\n\n# Controlling column types --------------------------------------------------\n# convert = TRUE detects column classes:\ndf <- tibble(x = c(\"x:1\", \"x:2\", \"y:4\", \"z\", NA))\ndf \\%>\\% separate(x, c(\"key\", \"value\"), \":\") \\%>\\% str()\ndf \\%>\\% separate(x, c(\"key\", \"value\"), \":\", convert = TRUE) \\%>\\% str()",
            "separate_longer_delim": "df <- tibble(id = 1:4, x = c(\"x\", \"x y\", \"x y z\", NA))\ndf \\%>\\% separate_longer_delim(x, delim = \" \")\n\n# You can separate multiple columns at once if they have the same structure\ndf <- tibble(id = 1:3, x = c(\"x\", \"x y\", \"x y z\"), y = c(\"a\", \"a b\", \"a b c\"))\ndf \\%>\\% separate_longer_delim(c(x, y), delim = \" \")\n\n# Or instead split by a fixed length\ndf <- tibble(id = 1:3, x = c(\"ab\", \"def\", \"\"))\ndf \\%>\\% separate_longer_position(x, 1)\ndf \\%>\\% separate_longer_position(x, 2)\ndf \\%>\\% separate_longer_position(x, 2, keep_empty = TRUE)",
            "separate_rows": "df <- tibble(\n  x = 1:3,\n  y = c(\"a\", \"d,e,f\", \"g,h\"),\n  z = c(\"1\", \"2,3,4\", \"5,6\")\n)\nseparate_rows(df, y, z, convert = TRUE)\n\n# Now recommended\ndf \\%>\\%\n  separate_longer_delim(c(y, z), delim = \",\")",
            "separate_wider_delim": "df <- tibble(id = 1:3, x = c(\"m-123\", \"f-455\", \"f-123\"))\n# There are three basic ways to split up a string into pieces:\n# 1. with a delimiter\ndf \\%>\\% separate_wider_delim(x, delim = \"-\", names = c(\"gender\", \"unit\"))\n# 2. by length\ndf \\%>\\% separate_wider_position(x, c(gender = 1, 1, unit = 3))\n# 3. defining each component with a regular expression\ndf \\%>\\% separate_wider_regex(x, c(gender = \".\", \".\", unit = \"\\\\\\\\d+\"))\n\n# Sometimes you split on the \"last\" delimiter\ndf <- tibble(var = c(\"race_1\", \"race_2\", \"age_bucket_1\", \"age_bucket_2\"))\n# _delim won't help because it always splits on the first delimiter\ntry(df \\%>\\% separate_wider_delim(var, \"_\", names = c(\"var1\", \"var2\")))\ndf \\%>\\% separate_wider_delim(var, \"_\", names = c(\"var1\", \"var2\"), too_many = \"merge\")\n# Instead, you can use _regex\ndf \\%>\\% separate_wider_regex(var, c(var1 = \".*\", \"_\", var2 = \".*\"))\n# this works because * is greedy; you can mimic the _delim behaviour with .*?\ndf \\%>\\% separate_wider_regex(var, c(var1 = \".*?\", \"_\", var2 = \".*\"))\n\n# If the number of components varies, it's most natural to split into rows\ndf <- tibble(id = 1:4, x = c(\"x\", \"x y\", \"x y z\", NA))\ndf \\%>\\% separate_longer_delim(x, delim = \" \")\n# But separate_wider_delim() provides some tools to deal with the problem\n# The default behaviour tells you that there's a problem\ntry(df \\%>\\% separate_wider_delim(x, delim = \" \", names = c(\"a\", \"b\")))\n# You can get additional insight by using the debug options\ndf \\%>\\%\n  separate_wider_delim(\n    x,\n    delim = \" \",\n    names = c(\"a\", \"b\"),\n    too_few = \"debug\",\n    too_many = \"debug\"\n  )\n\n# But you can suppress the warnings\ndf \\%>\\%\n  separate_wider_delim(\n    x,\n    delim = \" \",\n    names = c(\"a\", \"b\"),\n    too_few = \"align_start\",\n    too_many = \"merge\"\n  )\n\n# Or choose to automatically name the columns, producing as many as needed\ndf \\%>\\% separate_wider_delim(x, delim = \" \", names_sep = \"\", too_few = \"align_start\")",
            "spread": "stocks <- tibble(\n  time = as.Date(\"2009-01-01\") + 0:9,\n  X = rnorm(10, 0, 1),\n  Y = rnorm(10, 0, 2),\n  Z = rnorm(10, 0, 4)\n)\nstocksm <- stocks \\%>\\% gather(stock, price, -time)\nstocksm \\%>\\% spread(stock, price)\nstocksm \\%>\\% spread(time, price)\n\n# Spread and gather are complements\ndf <- tibble(x = c(\"a\", \"b\"), y = c(3, 4), z = c(5, 6))\ndf \\%>\\%\n  spread(x, y) \\%>\\%\n  gather(\"x\", \"y\", a:b, na.rm = TRUE)\n\n# Use 'convert = TRUE' to produce variables of mixed type\ndf <- tibble(\n  row = rep(c(1, 51), each = 3),\n  var = rep(c(\"Sepal.Length\", \"Species\", \"Species_num\"), 2),\n  value = c(5.1, \"setosa\", 1, 7.0, \"versicolor\", 2)\n)\ndf \\%>\\% spread(var, value) \\%>\\% str()\ndf \\%>\\% spread(var, value, convert = TRUE) \\%>\\% str()",
            "tidyr_legacy": "df <- tibble(x = 1:2, y = list(tibble(x = 3:5), tibble(x = 4:7)))\n\n# Doesn't work because it would produce a data frame with two\n# columns called x\n\\dontrun{\nunnest(df, y)\n}\n\n# The new tidyverse standard:\nunnest(df, y, names_repair = \"universal\")\n\n# The old tidyr approach\nunnest(df, y, names_repair = tidyr_legacy)",
            "uncount": "df <- tibble(x = c(\"a\", \"b\"), n = c(1, 2))\nuncount(df, n)\nuncount(df, n, .id = \"id\")\n\n# You can also use constants\nuncount(df, 2)\n\n# Or expressions\nuncount(df, 2 / n)",
            "unite": "df <- expand_grid(x = c(\"a\", NA), y = c(\"b\", NA))\ndf\n\ndf \\%>\\% unite(\"z\", x:y, remove = FALSE)\n# To remove missing values:\ndf \\%>\\% unite(\"z\", x:y, na.rm = TRUE, remove = FALSE)\n\n# Separate is almost the complement of unite\ndf \\%>\\%\n  unite(\"xy\", x:y) \\%>\\%\n  separate(xy, c(\"x\", \"y\"))\n# (but note `x` and `y` contain now \"NA\" not NA)",
            "unnest": "# unnest() is designed to work with lists of data frames\ndf <- tibble(\n  x = 1:3,\n  y = list(\n    NULL,\n    tibble(a = 1, b = 2),\n    tibble(a = 1:3, b = 3:1, c = 4)\n  )\n)\n# unnest() recycles input rows for each row of the list-column\n# and adds a column for each column\ndf \\%>\\% unnest(y)\n\n# input rows with 0 rows in the list-column will usually disappear,\n# but you can keep them (generating NAs) with keep_empty = TRUE:\ndf \\%>\\% unnest(y, keep_empty = TRUE)\n\n# Multiple columns ----------------------------------------------------------\n# You can unnest multiple columns simultaneously\ndf <- tibble(\n  x = 1:2,\n  y = list(\n    tibble(a = 1, b = 2),\n    tibble(a = 3:4, b = 5:6)\n  ),\n  z = list(\n    tibble(c = 1, d = 2),\n    tibble(c = 3:4, d = 5:6)\n  )\n)\ndf \\%>\\% unnest(c(y, z))\n\n# Compare with unnesting one column at a time, which generates\n# the Cartesian product\ndf \\%>\\%\n  unnest(y) \\%>\\%\n  unnest(z)",
            "unnest_longer": "# `unnest_longer()` is useful when each component of the list should\n# form a row\ndf <- tibble(\n  x = 1:4,\n  y = list(NULL, 1:3, 4:5, integer())\n)\ndf \\%>\\% unnest_longer(y)\n\n# Note that empty values like `NULL` and `integer()` are dropped by\n# default. If you'd like to keep them, set `keep_empty = TRUE`.\ndf \\%>\\% unnest_longer(y, keep_empty = TRUE)\n\n# If the inner vectors are named, the names are copied to an `_id` column\ndf <- tibble(\n  x = 1:2,\n  y = list(c(a = 1, b = 2), c(a = 10, b = 11, c = 12))\n)\ndf \\%>\\% unnest_longer(y)\n\n# Multiple columns ----------------------------------------------------------\n# If columns are aligned, you can unnest simultaneously\ndf <- tibble(\n  x = 1:2,\n  y = list(1:2, 3:4),\n  z = list(5:6, 7:8)\n)\ndf \\%>\\%\n  unnest_longer(c(y, z))\n\n# This is important because sequential unnesting would generate the\n# Cartesian product of the rows\ndf \\%>\\%\n  unnest_longer(y) \\%>\\%\n  unnest_longer(z)",
            "unnest_wider": "df <- tibble(\n  character = c(\"Toothless\", \"Dory\"),\n  metadata = list(\n    list(\n      species = \"dragon\",\n      color = \"black\",\n      films = c(\n        \"How to Train Your Dragon\",\n        \"How to Train Your Dragon 2\",\n        \"How to Train Your Dragon: The Hidden World\"\n      )\n    ),\n    list(\n      species = \"blue tang\",\n      color = \"blue\",\n      films = c(\"Finding Nemo\", \"Finding Dory\")\n    )\n  )\n)\ndf\n\n# Turn all components of metadata into columns\ndf \\%>\\% unnest_wider(metadata)\n\n# Choose not to simplify list-cols of length-1 elements\ndf \\%>\\% unnest_wider(metadata, simplify = FALSE)\ndf \\%>\\% unnest_wider(metadata, simplify = list(color = FALSE))\n\n# You can also widen unnamed list-cols:\ndf <- tibble(\n  x = 1:3,\n  y = list(NULL, 1:3, 4:5)\n)\n# but you must supply `names_sep` to do so, which generates automatic names:\ndf \\%>\\% unnest_wider(y, names_sep = \"_\")\n\n# 0-length elements ---------------------------------------------------------\n# The defaults of `unnest_wider()` treat empty types (like `list()`) as `NULL`.\njson <- list(\n  list(x = 1:2, y = 1:2),\n  list(x = list(), y = 3:4),\n  list(x = 3L, y = list())\n)\n\ndf <- tibble(json = json)\ndf \\%>\\%\n  unnest_wider(json)\n\n# To instead enforce strict vctrs typing rules, use `strict`\ndf \\%>\\%\n  unnest_wider(json, strict = TRUE)"
        }
    },
    "R6": {
        "description": "Creates classes with reference semantics, similar to R's\n    built-in reference classes. Compared to reference classes, R6 classes\n    are simpler and lighter-weight, and they are not built on S4 classes\n    so they do not require the methods package. These classes allow public\n    and private members, and they support inheritance, even when the\n    classes are defined in different packages.",
        "examples": {
            "R6Class": "# A queue ---------------------------------------------------------\nQueue <- R6Class(\"Queue\",\n  public = list(\n    initialize = function(...) {\n      for (item in list(...)) {\n        self$add(item)\n      }\n    },\n    add = function(x) {\n      private$queue <- c(private$queue, list(x))\n      invisible(self)\n    },\n    remove = function() {\n      if (private$length() == 0) return(NULL)\n      # Can use private$queue for explicit access\n      head <- private$queue[[1]]\n      private$queue <- private$queue[-1]\n      head\n    }\n  ),\n  private = list(\n    queue = list(),\n    length = function() base::length(private$queue)\n  )\n)\n\nq <- Queue$new(5, 6, \"foo\")\n\n# Add and remove items\nq$add(\"something\")\nq$add(\"another thing\")\nq$add(17)\nq$remove()\n#> [1] 5\nq$remove()\n#> [1] 6\n\n# Private members can't be accessed directly\nq$queue\n#> NULL\n# q$length()\n#> Error: attempt to apply non-function\n\n# add() returns self, so it can be chained\nq$add(10)$add(11)$add(12)\n\n# remove() returns the value removed, so it's not chainable\nq$remove()\n#> [1] \"foo\"\nq$remove()\n#> [1] \"something\"\nq$remove()\n#> [1] \"another thing\"\nq$remove()\n#> [1] 17\n\n\n# Active bindings -------------------------------------------------\nNumbers <- R6Class(\"Numbers\",\n  public = list(\n    x = 100\n  ),\n  active = list(\n    x2 = function(value) {\n      if (missing(value)) return(self$x * 2)\n      else self$x <- value/2\n    },\n    rand = function() rnorm(1)\n  )\n)\n\nn <- Numbers$new()\nn$x\n#> [1] 100\nn$x2\n#> [1] 200\nn$x2 <- 1000\nn$x\n#> [1] 500\n\n# If the function takes no arguments, it's not possible to use it with <-:\nn$rand\n#> [1] 0.2648\nn$rand\n#> [1] 2.171\n# n$rand <- 3\n#> Error: unused argument (quote(3))\n\n\n# Inheritance -----------------------------------------------------\n# Note that this isn't very efficient - it's just for illustrating inheritance.\nHistoryQueue <- R6Class(\"HistoryQueue\",\n  inherit = Queue,\n  public = list(\n    show = function() {\n      cat(\"Next item is at index\", private$head_idx + 1, \"\\n\")\n      for (i in seq_along(private$queue)) {\n        cat(i, \": \", private$queue[[i]], \"\\n\", sep = \"\")\n      }\n    },\n    remove = function() {\n      if (private$length() - private$head_idx == 0) return(NULL)\n      private$head_idx <<- private$head_idx + 1\n      private$queue[[private$head_idx]]\n    }\n  ),\n  private = list(\n    head_idx = 0\n  )\n)\n\nhq <- HistoryQueue$new(5, 6, \"foo\")\nhq$show()\n#> Next item is at index 1\n#> 1: 5\n#> 2: 6\n#> 3: foo\nhq$remove()\n#> [1] 5\nhq$show()\n#> Next item is at index 2\n#> 1: 5\n#> 2: 6\n#> 3: foo\nhq$remove()\n#> [1] 6\n\n\n\n# Calling superclass methods with super$ --------------------------\nCountingQueue <- R6Class(\"CountingQueue\",\n  inherit = Queue,\n  public = list(\n    add = function(x) {\n      private$total <<- private$total + 1\n      super$add(x)\n    },\n    get_total = function() private$total\n  ),\n  private = list(\n    total = 0\n  )\n)\n\ncq <- CountingQueue$new(\"x\", \"y\")\ncq$get_total()\n#> [1] 2\ncq$add(\"z\")\ncq$remove()\n#> [1] \"x\"\ncq$remove()\n#> [1] \"y\"\ncq$get_total()\n#> [1] 3\n\n\n# Non-portable classes --------------------------------------------\n# By default, R6 classes are portable, which means they can be inherited\n# across different packages. Portable classes require using self$ and\n# private$ to access members.\n# When used in non-portable mode, members can be accessed without self$,\n# and assignments can be made with <<-.\n\nNP <- R6Class(\"NP\",\n  portable = FALSE,\n  public = list(\n    x = NA,\n    getx = function() x,\n    setx = function(value) x <<- value\n  )\n)\n\nnp <- NP$new()\nnp$setx(10)\nnp$getx()\n#> [1] 10\n\n# Setting new values ----------------------------------------------\n# It is possible to add new members to the class after it has been created,\n# by using the $set() method on the generator.\n\nSimple <- R6Class(\"Simple\",\n  public = list(\n    x = 1,\n    getx = function() self$x\n  )\n)\n\nSimple$set(\"public\", \"getx2\", function() self$x*2)\n\n# Use overwrite = TRUE to overwrite existing values\nSimple$set(\"public\", \"x\", 10, overwrite = TRUE)\n\ns <- Simple$new()\ns$x\ns$getx2()\n\n\n# Cloning objects -------------------------------------------------\na <- Queue$new(5, 6)\na$remove()\n#> [1] 5\n\n# Clone a. New object gets a's state.\nb <- a$clone()\n\n# Can add to each queue separately now.\na$add(10)\nb$add(20)\n\na$remove()\n#> [1] 6\na$remove()\n#> [1] 10\n\nb$remove()\n#> [1] 6\nb$remove()\n#> [1] 20\n\n\n# Deep clones -----------------------------------------------------\n\nSimple <- R6Class(\"Simple\",\n public = list(\n   x = NULL,\n   initialize = function(val) self$x <- val\n )\n)\n\nCloner <- R6Class(\"Cloner\",\n  public = list(\n    s = NULL,\n    y = 1,\n    initialize = function() self$s <- Simple$new(1)\n  )\n)\n\na <- Cloner$new()\nb <- a$clone()\nc <- a$clone(deep = TRUE)\n\n# Modify a\na$s$x <- 2\na$y <- 2\n\n# b is a shallow clone. b$s is the same as a$s because they are R6 objects.\nb$s$x\n#> [1] 2\n# But a$y and b$y are different, because y is just a value.\nb$y\n#> [1] 1\n\n# c is a deep clone, so c$s is not the same as a$s.\nc$s$x\n#> [1] 1\nc$y\n#> [1] 1\n\n\n# Deep clones with custom deep_clone method -----------------------\n\nCustomCloner <- R6Class(\"CustomCloner\",\n  public = list(\n    e = NULL,\n    s1 = NULL,\n    s2 = NULL,\n    s3 = NULL,\n    initialize = function() {\n      self$e <- new.env(parent = emptyenv())\n      self$e$x <- 1\n      self$s1 <- Simple$new(1)\n      self$s2 <- Simple$new(1)\n      self$s3 <- Simple$new(1)\n    }\n  ),\n  private = list(\n    # When x$clone(deep=TRUE) is called, the deep_clone gets invoked once for\n    # each field, with the name and value.\n    deep_clone = function(name, value) {\n      if (name == \"e\") {\n        # e1 is an environment, so use this quick way of copying\n        list2env(as.list.environment(value, all.names = TRUE),\n                 parent = emptyenv())\n\n      } else if (name \\%in\\% c(\"s1\", \"s2\")) {\n        # s1 and s2 are R6 objects which we can clone\n        value$clone()\n\n      } else {\n        # For everything else, just return it. This results in a shallow\n        # copy of s3.\n        value\n      }\n    }\n  )\n)\n\na <- CustomCloner$new()\nb <- a$clone(deep = TRUE)\n\n# Change some values in a's fields\na$e$x <- 2\na$s1$x <- 3\na$s2$x <- 4\na$s3$x <- 5\n\n# b has copies of e, s1, and s2, but shares the same s3\nb$e$x\n#> [1] 1\nb$s1$x\n#> [1] 1\nb$s2$x\n#> [1] 1\nb$s3$x\n#> [1] 5\n\n\n# Debugging -------------------------------------------------------\n\\dontrun{\n# This will enable debugging the getx() method for objects of the 'Simple'\n# class that are instantiated in the future.\nSimple$debug(\"getx\")\ns <- Simple$new()\ns$getx()\n\n# Disable debugging for future instances:\nSimple$undebug(\"getx\")\ns <- Simple$new()\ns$getx()\n\n# To enable and disable debugging for a method in a single instance of an\n# R6 object (this will not affect other objects):\ns <- Simple$new()\ndebug(s$getx)\ns$getx()\nundebug(s$getx)\n}",
            "is.R6": "class_generator <- R6Class()\nobject <- class_generator$new()\n\nis.R6Class(class_generator)\nis.R6(class_generator)\n\nis.R6Class(object)\nis.R6(object)"
        }
    },
    "scales": {
        "description": "Graphical scales map data to aesthetics, and provide methods\n    for automatically determining breaks and labels for axes and legends.",
        "examples": {
            "alpha": "alpha(\"red\", 0.1)\nalpha(colours(), 0.5)\nalpha(\"red\", seq(0, 1, length.out = 10))\nalpha(c(\"first\" = \"gold\", \"second\" = \"lightgray\", \"third\" = \"#cd7f32\"), .5)",
            "breaks_exp": "# Small range\ndemo_continuous(c(100, 102), transform = \"exp\", breaks = breaks_exp())\n# Large range\ndemo_continuous(c(0, 100), transform = \"exp\", breaks = breaks_exp(n = 4))",
            "breaks_extended": "demo_continuous(c(0, 10))\ndemo_continuous(c(0, 10), breaks = breaks_extended(3))\ndemo_continuous(c(0, 10), breaks = breaks_extended(10))",
            "breaks_log": "demo_log10(c(1, 1e5))\ndemo_log10(c(1, 1e6))\n\n# Request more breaks by setting n\ndemo_log10(c(1, 1e6), breaks = breaks_log(6))\n\n# Some tricky ranges\ndemo_log10(c(2000, 9000))\ndemo_log10(c(2000, 14000))\ndemo_log10(c(2000, 85000), expand = c(0, 0))\n\n# An even smaller range that requires falling back to linear breaks\ndemo_log10(c(1800, 2000))",
            "breaks_pretty": "one_month <- as.POSIXct(c(\"2020-05-01\", \"2020-06-01\"))\ndemo_datetime(one_month)\ndemo_datetime(one_month, breaks = breaks_pretty(2))\ndemo_datetime(one_month, breaks = breaks_pretty(4))\n\n# Tightly spaced date breaks often need custom labels too\ndemo_datetime(one_month, breaks = breaks_pretty(12))\ndemo_datetime(one_month,\n  breaks = breaks_pretty(12),\n  labels = label_date_short()\n)",
            "breaks_timespan": "demo_timespan(seq(0, 100), breaks = breaks_timespan())",
            "breaks_width": "demo_continuous(c(0, 100))\ndemo_continuous(c(0, 100), breaks = breaks_width(10))\ndemo_continuous(c(0, 100), breaks = breaks_width(20, -4))\ndemo_continuous(c(0, 100), breaks = breaks_width(20, 4))\n\n# This is also useful for dates\none_month <- as.POSIXct(c(\"2020-05-01\", \"2020-06-01\"))\ndemo_datetime(one_month)\ndemo_datetime(one_month, breaks = breaks_width(\"1 week\"))\ndemo_datetime(one_month, breaks = breaks_width(\"5 days\"))\n# This is so useful that scale_x_datetime() has a shorthand:\ndemo_datetime(one_month, date_breaks = \"5 days\")\n\n# hms times also work\none_hour <- hms::hms(hours = 0:1)\ndemo_time(one_hour)\ndemo_time(one_hour, breaks = breaks_width(\"15 min\"))\ndemo_time(one_hour, breaks = breaks_width(\"600 sec\"))\n\n# Offets are useful for years that begin on dates other than the 1st of\n# January, such as the UK financial year, which begins on the 1st of April.\nthree_years <- as.POSIXct(c(\"2020-01-01\", \"2021-01-01\", \"2022-01-01\"))\ndemo_datetime(\n  three_years,\n  breaks = breaks_width(\"1 year\", offset = \"3 months\")\n)\n\n# The offset can be a vector, to create offsets that have compound units,\n# such as the UK fiscal (tax) year, which begins on the 6th of April.\ndemo_datetime(\n  three_years,\n  breaks = breaks_width(\"1 year\", offset = c(\"3 months\", \"5 days\"))\n)",
            "cbreaks": "cbreaks(c(0, 100))\ncbreaks(c(0, 100), breaks_pretty(3))\ncbreaks(c(0, 100), breaks_pretty(10))\ncbreaks(c(1, 100), log_breaks())\ncbreaks(c(1, 1e4), log_breaks())\n\ncbreaks(c(0, 100), labels = math_format())\ncbreaks(c(0, 1), labels = percent_format())\ncbreaks(c(0, 1e6), labels = comma_format())\ncbreaks(c(0, 1e6), labels = dollar_format())\ncbreaks(c(0, 30), labels = dollar_format())\n\n# You can also specify them manually:\ncbreaks(c(0, 100), breaks = c(15, 20, 80))\ncbreaks(c(0, 100), breaks = c(15, 20, 80), labels = c(1.5, 2.0, 8.0))\ncbreaks(c(0, 100),\n  breaks = c(15, 20, 80),\n  labels = expression(alpha, beta, gamma)\n)",
            "col2hcl": "reds <- rep(\"red\", 6)\nshow_col(col2hcl(reds, h = seq(0, 180, length = 6)))\nshow_col(col2hcl(reds, c = seq(0, 80, length = 6)))\nshow_col(col2hcl(reds, l = seq(0, 100, length = 6)))\nshow_col(col2hcl(reds, alpha = seq(0, 1, length = 6)))",
            "col_mix": "col_mix(\"blue\", \"red\") # purple\ncol_mix(\"blue\", \"red\", amount = 1) # red\ncol_mix(\"blue\", \"red\", amount = 0) # blue\n\n# Not recommended:\ncol_mix(\"blue\", \"red\", space = \"hcl\") # green!",
            "col_numeric": "pal <- col_bin(\"Greens\", domain = 0:100)\nshow_col(pal(sort(runif(10, 60, 100))))\n\n# Exponential distribution, mapped continuously\nshow_col(col_numeric(\"Blues\", domain = NULL)(sort(rexp(16))))\n# Exponential distribution, mapped by interval\nshow_col(col_bin(\"Blues\", domain = NULL, bins = 4)(sort(rexp(16))))\n# Exponential distribution, mapped by quantile\nshow_col(col_quantile(\"Blues\", domain = NULL)(sort(rexp(16))))\n\n# Categorical data; by default, the values being coloured span the gamut...\nshow_col(col_factor(\"RdYlBu\", domain = NULL)(LETTERS[1:5]))\n# ...unless the data is a factor, without droplevels...\nshow_col(col_factor(\"RdYlBu\", domain = NULL)(factor(LETTERS[1:5], levels = LETTERS)))\n# ...or the domain is stated explicitly.\nshow_col(col_factor(\"RdYlBu\", levels = LETTERS)(LETTERS[1:5]))",
            "colour_manip": "col_shift(\"red\", 180) # teal\ncol_lighter(\"red\", 50) # light red\ncol_darker(\"red\", 50) # dark red\ncol_saturate(\"red\", -50) # brick-red",
            "colour_ramp": "ramp <- colour_ramp(c(\"red\", \"green\", \"blue\"))\nshow_col(ramp(seq(0, 1, length = 12)))",
            "compose_label": "demo_continuous(\n  c(-100, 100),\n  labels = compose_label(abs, number, ~paste0(.x, \" foobar\"), toupper)\n)\n\n# Same result\ndemo_continuous(\n  c(-100, 100),\n  labels = compose_label(abs, label_number(suffix = \" FOOBAR\"))\n)",
            "cscale": "with(mtcars, plot(disp, mpg, cex = cscale(hp, pal_rescale())))\nwith(mtcars, plot(disp, mpg, cex = cscale(hp, pal_rescale(),\n  trans = transform_sqrt()\n)))\nwith(mtcars, plot(disp, mpg, cex = cscale(hp, pal_area())))\nwith(mtcars, plot(disp, mpg,\n  pch = 20, cex = 5,\n  col = cscale(hp, pal_seq_gradient(\"grey80\", \"black\"))\n))",
            "dscale": "with(mtcars, plot(disp, mpg,\n  pch = 20, cex = 3,\n  col = dscale(factor(cyl), pal_brewer())\n))",
            "get_palette": "# Get one of the known palettes\nget_palette(\"hue\")\n\n# Set a new custom palette\ncols <- c(\"palegreen\", \"deepskyblue\", \"magenta\")\nset_palette(\"aurora\", palette = cols)\n\n# Palette is now known\n\"aurora\" \\%in\\% palette_names()\nas_continuous_pal(\"aurora\")\n\n# Resetting palettes\nreset_palettes()",
            "label_bytes": "demo_continuous(c(1, 1e6))\ndemo_continuous(c(1, 1e6), labels = label_bytes())\n\n# Auto units are particularly nice on log scales\ndemo_log10(c(1, 1e7), labels = label_bytes())\n\n# You can also set the units\ndemo_continuous(c(1, 1e6), labels = label_bytes(\"kB\"))\n\n# You can also use binary units where a megabyte is defined as\n# (1024) ^ 2 bytes rather than (1000) ^ 2. You'll need to override\n# the default breaks to make this more informative.\ndemo_continuous(c(1, 1024^2),\n  breaks = breaks_width(250 * 1024),\n  labels = label_bytes(\"auto_binary\")\n)",
            "label_currency": "demo_continuous(c(0, 1), labels = label_currency())\ndemo_continuous(c(1, 100), labels = label_currency())\n\n# Customise currency display with prefix and suffix\ndemo_continuous(c(1, 100), labels = label_currency(prefix = \"USD \"))\nyen <- label_currency(\n  prefix = \"\u00a5\",\n  suffix = \"\",\n  big.mark = \".\",\n  decimal.mark = \",\"\n)\ndemo_continuous(c(1000, 1100), labels = yen)\n\n# Use style_negative = \"parens\" for finance style display\ndemo_continuous(c(-100, 100), labels = label_currency(style_negative = \"parens\"))\n\n# Use scale_cut to use K/M/B where appropriate\ndemo_log10(c(1, 1e16),\n  breaks = log_breaks(7, 1e3),\n  labels = label_currency(scale_cut = cut_short_scale())\n)\n# cut_short_scale() uses B = one thousand million\n# cut_long_scale() uses B = one million million\ndemo_log10(c(1, 1e16),\n  breaks = log_breaks(7, 1e3),\n  labels = label_currency(scale_cut = cut_long_scale())\n)\n\n# You can also define your own breaks\ngbp <- label_currency(\n  prefix = \"\\u00a3\",\n  scale_cut = c(0, k = 1e3, m = 1e6, bn = 1e9, tn = 1e12)\n)\ndemo_log10(c(1, 1e12), breaks = log_breaks(5, 1e3), labels = gbp)",
            "label_date": "date_range <- function(start, days) {\n  start <- as.POSIXct(start)\n  c(start, start + days * 24 * 60 * 60)\n}\n\ntwo_months <- date_range(\"2020-05-01\", 60)\ndemo_datetime(two_months)\ndemo_datetime(two_months, labels = label_date(\"\\%m/\\%d\"))\ndemo_datetime(two_months, labels = label_date(\"\\%e \\%b\", locale = \"fr\"))\ndemo_datetime(two_months, labels = label_date(\"\\%e \\%B\", locale = \"es\"))\n# ggplot2 provides a short-hand:\ndemo_datetime(two_months, date_labels = \"\\%m/\\%d\")\n\n# An alternative labelling system is label_date_short()\ndemo_datetime(two_months, date_breaks = \"7 days\", labels = label_date_short())\n# This is particularly effective for dense labels\none_year <- date_range(\"2020-05-01\", 365)\ndemo_datetime(one_year, date_breaks = \"month\")\ndemo_datetime(one_year, date_breaks = \"month\", labels = label_date_short())",
            "label_dictionary": "# Example lookup table\nlut <- c(\n  \"4\" = \"four wheel drive\",\n  \"r\" = \"rear wheel drive\",\n  \"f\" = \"front wheel drive\"\n)\n\n# Typical usage\ndemo_discrete(c(\"4\", \"r\", \"f\"), labels = label_dictionary(lut))\n# By default, extra values ('w') will remain as-is\ndemo_discrete(c(\"4\", \"r\", \"f\", \"w\"), labels = label_dictionary(lut))\n# Alternatively, you can relabel extra values\ndemo_discrete(\n  c(\"4\", \"r\", \"f\", \"w\"),\n  labels = label_dictionary(lut, nomatch = \"unknown\")\n)",
            "label_glue": "# Example variables\nanimal  <- \"penguin\"\nspecies <- c(\"Adelie\",  \"Chinstrap\", \"Emperor\", \"Gentoo\")\n\n# Typical use, note that {x} will become the breaks\ndemo_discrete(species, labels = label_glue(\"The {x}\\n{animal}\"))\n# It adapts to the breaks that are present\ndemo_discrete(species[-3], labels =  label_glue(\"The {x}\\n{animal}\"))\n# Contrary to directly glueing species + animal, which results in mislabelling!\ndemo_discrete(species[-3], labels = glue::glue(\"The {species}\\n{animal}\"))",
            "label_log": "demo_log10(c(1, 1e5), labels = label_log())\ndemo_log10(c(1, 1e5), breaks = breaks_log(base = 2), labels = label_log(base = 2))\nformat_log(c(0.1, 1, 10))",
            "label_number": "\\dontshow{if (getRversion() >= \"3.5\") (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\ndemo_continuous(c(-1e6, 1e6))\ndemo_continuous(c(-1e6, 1e6), labels = label_number())\ndemo_continuous(c(-1e6, 1e6), labels = label_comma())\n\n# Use scale to rescale very small or large numbers to generate\n# more readable labels\ndemo_continuous(c(0, 1e6), labels = label_number())\ndemo_continuous(c(0, 1e6), labels = label_number(scale = 1 / 1e3))\ndemo_continuous(c(0, 1e-6), labels = label_number())\ndemo_continuous(c(0, 1e-6), labels = label_number(scale = 1e6))\n\n#' Use scale_cut to automatically add prefixes for large/small numbers\ndemo_log10(\n  c(1, 1e9),\n  breaks = log_breaks(10),\n  labels = label_number(scale_cut = cut_short_scale())\n)\ndemo_log10(\n  c(1, 1e9),\n  breaks = log_breaks(10),\n  labels = label_number(scale_cut = cut_si(\"m\"))\n)\ndemo_log10(\n  c(1e-9, 1),\n  breaks = log_breaks(10),\n  labels = label_number(scale_cut = cut_si(\"g\"))\n)\n# use scale and scale_cut when data already uses SI prefix\n# for example, if data was stored in kg\ndemo_log10(\n  c(1e-9, 1),\n  breaks = log_breaks(10),\n  labels = label_number(scale_cut = cut_si(\"g\"), scale = 1e3)\n)\n\n#' # Use style arguments to vary the appearance of positive and negative numbers\ndemo_continuous(c(-1e3, 1e3), labels = label_number(\n  style_positive = \"plus\",\n  style_negative = \"minus\"\n))\ndemo_continuous(c(-1e3, 1e3), labels = label_number(style_negative = \"parens\"))\n\n# You can use prefix and suffix for other types of display\ndemo_continuous(c(32, 212), labels = label_number(suffix = \"\\u00b0F\"))\ndemo_continuous(c(0, 100), labels = label_number(suffix = \"\\u00b0C\"))\n\\dontshow{\\}) # examplesIf}",
            "label_number_auto": "# Very small and very large numbers get scientific notation\ndemo_continuous(c(0, 1e-6), labels = label_number_auto())\ndemo_continuous(c(0, 1e9), labels = label_number_auto())\n\n# Other ranges get the numbers printed in full\ndemo_continuous(c(0, 1e-3), labels = label_number_auto())\ndemo_continuous(c(0, 1), labels = label_number_auto())\ndemo_continuous(c(0, 1e3), labels = label_number_auto())\ndemo_continuous(c(0, 1e6), labels = label_number_auto())\n\n# Transformation is applied individually so you get as little\n# scientific notation as possible\ndemo_log10(c(1, 1e7), labels = label_number_auto())",
            "label_ordinal": "demo_continuous(c(1, 5))\ndemo_continuous(c(1, 5), labels = label_ordinal())\ndemo_continuous(c(1, 5), labels = label_ordinal(rules = ordinal_french()))\n\n# The rules are just a set of regular expressions that are applied in turn\nordinal_french()\nordinal_english()\n\n# Note that ordinal rounds values, so you may need to adjust the breaks too\ndemo_continuous(c(1, 10))\ndemo_continuous(c(1, 10), labels = label_ordinal())\ndemo_continuous(c(1, 10),\n  labels = label_ordinal(),\n  breaks = breaks_width(2)\n)",
            "label_parse": "# Use label_parse() with discrete scales\ngreek <- c(\"alpha\", \"beta\", \"gamma\")\ndemo_discrete(greek)\ndemo_discrete(greek, labels = label_parse())\n\n# Use label_math() with continuous scales\ndemo_continuous(c(1, 5))\ndemo_continuous(c(1, 5), labels = label_math(alpha[.x]))\ndemo_continuous(c(1, 5), labels = label_math())",
            "label_percent": "demo_continuous(c(0, 1))\ndemo_continuous(c(0, 1), labels = label_percent())\n\n# Use prefix and suffix to create your own variants\nfrench_percent <- label_percent(\n  decimal.mark = \",\",\n  suffix = \" \\%\"\n)\ndemo_continuous(c(0, .01), labels = french_percent)",
            "label_pvalue": "demo_continuous(c(0, 1))\ndemo_continuous(c(0, 1), labels = label_pvalue())\ndemo_continuous(c(0, 1), labels = label_pvalue(accuracy = 0.1))\ndemo_continuous(c(0, 1), labels = label_pvalue(add_p = TRUE))\n\n# Or provide your own prefixes\nprefix <- c(\"p < \", \"p = \", \"p > \")\ndemo_continuous(c(0, 1), labels = label_pvalue(prefix = prefix))",
            "label_scientific": "demo_continuous(c(1, 10))\ndemo_continuous(c(1, 10), labels = label_scientific())\ndemo_continuous(c(1, 10), labels = label_scientific(digits = 3))\n\ndemo_log10(c(1, 1e9))",
            "label_wrap": "x <- c(\n  \"this is a long label\",\n  \"this is another long label\",\n  \"this a label this is even longer\"\n)\ndemo_discrete(x)\ndemo_discrete(x, labels = label_wrap(10))\ndemo_discrete(x, labels = label_wrap(20))",
            "minor_breaks_log": "# Standard usage with log10 scale\ndemo_log10(c(1, 1e10), minor_breaks = minor_breaks_log())\n# Increasing detail over many powers\ndemo_log10(c(1, 1e10), minor_breaks = minor_breaks_log(detail = 1))\n# Adjusting until where to draw minor breaks\ndemo_continuous(\n  c(-1000, 1000),\n  transform = asinh_trans(),\n  minor_breaks = minor_breaks_log(smallest = 1)\n)",
            "minor_breaks_width": "demo_log10(c(1, 1e6))\nif (FALSE) {\n  # Requires https://github.com/tidyverse/ggplot2/pull/3591\n  demo_log10(c(1, 1e6), minor_breaks = minor_breaks_n(10))\n}",
            "muted": "muted(\"red\")\nmuted(\"blue\")\nshow_col(c(\"red\", \"blue\", muted(\"red\"), muted(\"blue\")))",
            "new_continuous_palette": "# Creating a new discrete palette\nnew_discrete_palette(\n  fun = grDevices::terrain.colors,\n  type = \"colour\", nlevels = 255\n)\n\n# Creating a new continuous palette\nnew_continuous_palette(\n  fun = function(x) rescale(x, to = c(1, 0)),\n  type = \"numeric\", na_safe = FALSE\n)\n\n# Testing palette properties\nis_continuous_pal(pal_seq_gradient())\nis_discrete_pal(pal_viridis())\nis_numeric_pal(pal_area())\nis_colour_pal(pal_manual(c(\"red\", \"green\")))\nis_pal(transform_log10())\n\n# Extracting properties\npalette_nlevels(pal_viridis())\npalette_na_safe(colour_ramp(c(\"red\", \"green\"), na.color = \"grey50\"))\npalette_type(pal_shape())\n\n# Switching discrete to continuous\npal <- as_continuous_pal(pal_viridis())\nshow_col(pal(c(0, 0.1, 0.2, 0.4, 1)))\n\n# Switching continuous to discrete\npal <- as_discrete_pal(pal_div_gradient())\nshow_col(pal(9))",
            "number_options": "# Default number formatting\nx <- c(0.1, 1, 1000)\nlabel_number()(x)\n\n# Now again with new options set\nnumber_options(style_positive = \"plus\", decimal.mark = \",\")\nlabel_number()(x)\n\n# The options are the argument names with a 'scales.'-prefix\noptions(\"scales.style_positive\")\n\n# Resetting the options to their defaults\nnumber_options()\nlabel_number()(x)",
            "oob": "# Censoring replaces out of bounds values with NAs\noob_censor(c(-Inf, -1, 0.5, 1, 2, NA, Inf))\noob_censor_any(c(-Inf, -1, 0.5, 1, 2, NA, Inf))\n\n# Squishing replaces out of bounds values with the nearest range limit\noob_squish(c(-Inf, -1, 0.5, 1, 2, NA, Inf))\noob_squish_any(c(-Inf, -1, 0.5, 1, 2, NA, Inf))\noob_squish_infinite(c(-Inf, -1, 0.5, 1, 2, NA, Inf))\n\n# Keeping does not alter values\noob_keep(c(-Inf, -1, 0.5, 1, 2, NA, Inf))\n\n# Discarding will remove out of bounds values\noob_discard(c(-Inf, -1, 0.5, 1, 2, NA, Inf))",
            "pal_brewer": "show_col(pal_brewer()(10))\nshow_col(pal_brewer(\"div\")(5))\nshow_col(pal_brewer(palette = \"Greens\")(5))\n\n# Can use with gradient_n to create a continuous gradient\ncols <- pal_brewer(\"div\")(5)\nshow_col(pal_gradient_n(cols)(seq(0, 1, length.out = 30)))",
            "pal_dichromat": "if (requireNamespace(\"dichromat\", quietly = TRUE)) {\n  show_col(pal_dichromat(\"BluetoOrange.10\")(10))\n  show_col(pal_dichromat(\"BluetoOrange.10\")(5))\n\n  # Can use with gradient_n to create a continuous gradient\n  cols <- pal_dichromat(\"DarkRedtoBlue.12\")(12)\n  show_col(pal_gradient_n(cols)(seq(0, 1, length.out = 30)))\n}",
            "pal_div_gradient": "x <- seq(-1, 1, length.out = 100)\nr <- sqrt(outer(x^2, x^2, \"+\"))\nimage(r, col = pal_div_gradient()(seq(0, 1, length.out = 12)))\nimage(r, col = pal_div_gradient()(seq(0, 1, length.out = 30)))\nimage(r, col = pal_div_gradient()(seq(0, 1, length.out = 100)))\n\npal <- pal_div_gradient(low = \"#2E6A70\")\nimage(r, col = pal(seq(0, 1, length.out = 100)))",
            "pal_grey": "show_col(pal_grey()(25))\nshow_col(pal_grey(0, 1)(25))",
            "pal_hue": "show_col(pal_hue()(4))\nshow_col(pal_hue()(9))\nshow_col(pal_hue(l = 90)(9))\nshow_col(pal_hue(l = 30)(9))\n\nshow_col(pal_hue()(9))\nshow_col(pal_hue(direction = -1)(9))\nshow_col(pal_hue(h.start = 30)(9))\nshow_col(pal_hue(h.start = 90)(9))\n\nshow_col(pal_hue()(9))\nshow_col(pal_hue(h = c(0, 90))(9))\nshow_col(pal_hue(h = c(90, 180))(9))\nshow_col(pal_hue(h = c(180, 270))(9))\nshow_col(pal_hue(h = c(270, 360))(9))",
            "pal_seq_gradient": "x <- seq(0, 1, length.out = 25)\nshow_col(pal_seq_gradient()(x))\nshow_col(pal_seq_gradient(\"white\", \"black\")(x))\n\nshow_col(pal_seq_gradient(\"white\", \"#90503F\")(x))",
            "pal_viridis": "show_col(pal_viridis()(10))\nshow_col(pal_viridis(direction = -1)(6))\nshow_col(pal_viridis(begin = 0.2, end = 0.8)(4))\nshow_col(pal_viridis(option = \"plasma\")(6))",
            "regular_minor_breaks": "m <- extended_breaks()(c(1, 10))\nregular_minor_breaks()(m, c(1, 10), n = 2)\n\nn <- extended_breaks()(c(0, -9))\nregular_minor_breaks(reverse = TRUE)(n, c(0, -9), n = 2)",
            "rescale": "rescale(1:100)\nrescale(runif(50))\nrescale(1)",
            "rescale_max": "rescale_max(1:100)\nrescale_max(runif(50))\nrescale_max(1)",
            "rescale_mid": "rescale_mid(1:100, mid = 50.5)\nrescale_mid(runif(50), mid = 0.5)\nrescale_mid(1)",
            "rescale_none": "rescale_none(1:100)",
            "show_col": "show_col(pal_hue()(9))\nshow_col(pal_hue()(9), borders = NA)\n\nshow_col(pal_viridis()(16))\nshow_col(pal_viridis()(16), labels = FALSE)",
            "trans_breaks": "trans_breaks(\"log10\", function(x) 10^x)(c(1, 1e6))\ntrans_breaks(\"sqrt\", function(x) x^2)(c(1, 100))\ntrans_breaks(function(x) 1 / x, function(x) 1 / x)(c(1, 100))\ntrans_breaks(function(x) -x, function(x) -x)(c(1, 100))",
            "trans_format": "tf <- trans_format(\"log10\", scientific_format())\ntf(10^1:6)",
            "transform_asinh": "plot(transform_asinh(), xlim = c(-1e2, 1e2))",
            "transform_asn": "plot(transform_asn(), xlim = c(0, 1))",
            "transform_atanh": "plot(transform_atanh(), xlim = c(-1, 1))",
            "transform_boxcox": "plot(transform_boxcox(-1), xlim = c(0, 10))\nplot(transform_boxcox(0), xlim = c(0, 10))\nplot(transform_boxcox(1), xlim = c(0, 10))\nplot(transform_boxcox(2), xlim = c(0, 10))\n\nplot(transform_modulus(-1), xlim = c(-10, 10))\nplot(transform_modulus(0), xlim = c(-10, 10))\nplot(transform_modulus(1), xlim = c(-10, 10))\nplot(transform_modulus(2), xlim = c(-10, 10))",
            "transform_compose": "demo_continuous(10^c(-2:4), trans = \"log10\", labels = label_log())\ndemo_continuous(10^c(-2:4), trans = c(\"log10\", \"reverse\"), labels = label_log())",
            "transform_date": "years <- seq(as.Date(\"1910/1/1\"), as.Date(\"1999/1/1\"), \"years\")\nt <- transform_date()\nt$transform(years)\nt$inverse(t$transform(years))\nt$format(t$breaks(range(years)))",
            "transform_exp": "plot(transform_exp(0.5), xlim = c(-2, 2))\nplot(transform_exp(1), xlim = c(-2, 2))\nplot(transform_exp(2), xlim = c(-2, 2))\nplot(transform_exp(), xlim = c(-2, 2))",
            "transform_identity": "plot(transform_identity(), xlim = c(-1, 1))",
            "transform_log": "plot(transform_log2(), xlim = c(0, 5))\nplot(transform_log(), xlim = c(0, 5))\nplot(transform_log10(), xlim = c(0, 5))\n\nplot(transform_log(), xlim = c(0, 2))\nplot(transform_log1p(), xlim = c(-1, 1))\n\n# The pseudo-log is defined for all real numbers\nplot(transform_pseudo_log(), xlim = c(-5, 5))\nlines(transform_log(), xlim = c(0, 5), col = \"red\")\n\n# For large positives numbers it's very close to log\nplot(transform_pseudo_log(), xlim = c(1, 20))\nlines(transform_log(), xlim = c(1, 20), col = \"red\")",
            "transform_probability": "plot(transform_logit(), xlim = c(0, 1))\nplot(transform_probit(), xlim = c(0, 1))",
            "transform_reciprocal": "plot(transform_reciprocal(), xlim = c(0, 1))",
            "transform_reverse": "plot(transform_reverse(), xlim = c(-1, 1))",
            "transform_sqrt": "plot(transform_sqrt(), xlim = c(0, 5))",
            "transform_time": "hours <- seq(ISOdate(2000, 3, 20, tz = \"\"), by = \"hour\", length.out = 10)\nt <- transform_time()\nt$transform(hours)\nt$inverse(t$transform(hours))\nt$format(t$breaks(range(hours)))",
            "transform_timespan": "# transform_timespan allows you to specify the time unit numeric data is\n# interpreted in\ntrans_min <- transform_timespan(\"mins\")\ndemo_timespan(seq(0, 100), trans = trans_min)\n# Input already in difftime format is interpreted correctly\ndemo_timespan(as.difftime(seq(0, 100), units = \"secs\"), trans = trans_min)\n\nif (require(\"hms\")) {\n  # transform_hms always assumes seconds\n  hms <- round(runif(10) * 86400)\n  t <- transform_hms()\n  t$transform(hms)\n  t$inverse(t$transform(hms))\n  t$breaks(hms)\n  # The break labels also follow the hms format\n  demo_timespan(hms, trans = t)\n}",
            "transform_yj": "plot(transform_yj(-1), xlim = c(-10, 10))\nplot(transform_yj(0), xlim = c(-10, 10))\nplot(transform_yj(1), xlim = c(-10, 10))\nplot(transform_yj(2), xlim = c(-10, 10))",
            "unit_format": "# Label with units\ndemo_continuous(c(0, 1), labels = unit_format(unit = \"m\"))\n# Labels in kg, but original data in g\nkm <- unit_format(unit = \"km\", scale = 1e-3, digits = 2)\ndemo_continuous(c(0, 2500), labels = km)"
        }
    },
    "sf": {
        "description": "Support for simple features, a standardized way to\n    encode spatial vector data. Binds to 'GDAL' for reading and writing\n    data, to 'GEOS' for geometrical operations, and to 'PROJ' for\n    projection conversions and datum transformations. Uses by default the 's2'\n    package for spherical geometry operations on ellipsoidal (long/lat) coordinates.",
        "examples": {
            "Ops": "st_point(c(1,2,3)) + 4\nst_point(c(1,2,3)) * 3 + 4\nm = matrix(0, 2, 2)\ndiag(m) = c(1, 3)\n# affine:\nst_point(c(1,2)) * m + c(2,5)\n# world in 0-360 range:\nif (require(maps, quietly = TRUE)) {\n w = st_as_sf(map('world', plot = FALSE, fill = TRUE))\n w2 = (st_geometry(w) + c(360,90)) \\%\\% c(360) - c(0,90)\n w3 = st_wrap_dateline(st_set_crs(w2 - c(180,0), 4326)) + c(180,0)\n plot(st_set_crs(w3, 4326), axes = TRUE)\n}\n(mp <- st_point(c(1,2)) + st_point(c(3,4))) # MULTIPOINT (1 2, 3 4)\nmp - st_point(c(3,4)) # POINT (1 2)\nopar = par(mfrow = c(2,2), mar = c(0, 0, 1, 0))\na = st_buffer(st_point(c(0,0)), 2)\nb = a + c(2, 0)\np = function(m) { plot(c(a,b)); plot(eval(parse(text=m)), col=grey(.9), add = TRUE); title(m) }\no = lapply(c('a | b', 'a / b', 'a & b', 'a \\%/\\% b'), p)\npar(opar)\nsfc = st_sfc(st_point(0:1), st_point(2:3))\nsfc + c(2,3) # added to EACH geometry\nsfc * c(2,3) # first geometry multiplied by 2, second by 3\nnc = st_transform(st_read(system.file(\"gpkg/nc.gpkg\", package=\"sf\")), 32119) # nc state plane, m\nb = st_buffer(st_centroid(st_union(nc)), units::set_units(50, km)) # shoot a hole in nc:\nplot(st_geometry(nc) / b, col = grey(.9))",
            "aggregate.sf": "m1 = cbind(c(0, 0, 1, 0), c(0, 1, 1, 0))\nm2 = cbind(c(0, 1, 1, 0), c(0, 0, 1, 0))\npol = st_sfc(st_polygon(list(m1)), st_polygon(list(m2)))\nset.seed(1985)\nd = data.frame(matrix(runif(15), ncol = 3))\np = st_as_sf(x = d, coords = 1:2)\nplot(pol)\nplot(p, add = TRUE)\n(p_ag1 = aggregate(p, pol, mean))\nplot(p_ag1) # geometry same as pol\n# works when x overlaps multiple objects in 'by':\np_buff = st_buffer(p, 0.2)\nplot(p_buff, add = TRUE)\n(p_ag2 = aggregate(p_buff, pol, mean)) # increased mean of second\n# with non-matching features\nm3 = cbind(c(0, 0, -0.1, 0), c(0, 0.1, 0.1, 0))\npol = st_sfc(st_polygon(list(m3)), st_polygon(list(m1)), st_polygon(list(m2)))\n(p_ag3 = aggregate(p, pol, mean))\nplot(p_ag3)\n# In case we need to pass an argument to the join function:\n(p_ag4 = aggregate(p, pol, mean, \n     join = function(x, y) st_is_within_distance(x, y, dist = 0.3)))",
            "bind": "crs = st_crs(3857)\na = st_sf(a=1, geom = st_sfc(st_point(0:1)), crs = crs)\nb = st_sf(a=1, geom = st_sfc(st_linestring(matrix(1:4,2))), crs = crs)\nc = st_sf(a=4, geom = st_sfc(st_multilinestring(list(matrix(1:4,2)))), crs = crs)\nrbind(a,b,c)\nrbind(a,b)\nrbind(a,b)\nrbind(b,c)\ncbind(a,b,c) # warns\nif (require(dplyr, quietly = TRUE))\n  dplyr::bind_cols(a,b)\nc = st_sf(a=4, geomc = st_sfc(st_multilinestring(list(matrix(1:4,2)))), crs = crs)\ncbind(a,b,c, sf_column_name = \"geomc\")\ndf = data.frame(x=3)\nst_sf(data.frame(c, df))\nif (require(dplyr, quietly = TRUE))\n  dplyr::bind_cols(c, df)",
            "coerce-methods": "nc <- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\nif (require(sp, quietly = TRUE)) {\n# convert to SpatialPolygonsDataFrame\nspdf <- as_Spatial(nc)\n# identical to\nspdf <- as(nc, \"Spatial\")\n# convert to SpatialPolygons\nas(st_geometry(nc), \"Spatial\")\n# back to sf\nas(spdf, \"sf\")\n}",
            "gdal": "\\dontrun{\n  f = system.file(\"tif/L7_ETMs.tif\", package=\"stars\")\n  f = system.file(\"nc/avhrr-only-v2.19810901.nc\", package = \"stars\")\n  gdal_metadata(f)\n  gdal_metadata(f, NA_character_)\n  try(gdal_metadata(f, \"wrongDomain\"))\n  gdal_metadata(f, c(\"\", \"AREA_OR_POINT\"))\n}",
            "gdal_utils": "if (sf_extSoftVersion()[\"GDAL\"] > \"2.1.0\") {\n# info utils can be used to list information about a raster\n# dataset. More info: https://gdal.org/programs/gdalinfo.html\nin_file <- system.file(\"tif/geomatrix.tif\", package = \"sf\")\ngdal_utils(\"info\", in_file, options = c(\"-mm\", \"-proj4\"))\n\n# vectortranslate utils can be used to convert simple features data between\n# file formats. More info: https://gdal.org/programs/ogr2ogr.html\nin_file <- system.file(\"shape/storms_xyz.shp\", package=\"sf\")\nout_file <- paste0(tempfile(), \".gpkg\")\ngdal_utils(\n  util = \"vectortranslate\",\n  source = in_file,\n  destination = out_file, # output format must be specified for GDAL < 2.3\n  options = c(\"-f\", \"GPKG\")\n)\n# The parameters can be specified as c(\"name\") or c(\"name\", \"value\"). The\n# vectortranslate utils can perform also various operations during the\n# conversion process. For example, we can reproject the features during the\n# translation.\ngdal_utils(\n  util = \"vectortranslate\",\n  source = in_file,\n  destination = out_file,\n  options = c(\n  \"-f\", \"GPKG\", # output file format for GDAL < 2.3\n  \"-s_srs\", \"EPSG:4326\", # input file SRS\n  \"-t_srs\", \"EPSG:2264\", # output file SRS\n  \"-overwrite\"\n  )\n)\nst_read(out_file)\n# The parameter s_srs had to be specified because, in this case, the in_file\n# has no associated SRS.\nst_read(in_file)\n}",
            "geos_binary_ops": "set.seed(131)\nlibrary(sf)\nm = rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0))\np = st_polygon(list(m))\nn = 100\nl = vector(\"list\", n)\nfor (i in 1:n)\n  l[[i]] = p + 10 * runif(2)\ns = st_sfc(l)\nplot(s, col = sf.colors(categorical = TRUE, alpha = .5))\ntitle(\"overlapping squares\")\nd = st_difference(s) # sequential differences: s1, s2-s1, s3-s2-s1, ...\nplot(d, col = sf.colors(categorical = TRUE, alpha = .5))\ntitle(\"non-overlapping differences\")\ni = st_intersection(s) # all intersections\nplot(i, col = sf.colors(categorical = TRUE, alpha = .5))\ntitle(\"non-overlapping intersections\")\nsummary(lengths(st_overlaps(s, s))) # includes self-counts!\nsummary(lengths(st_overlaps(d, d)))\nsummary(lengths(st_overlaps(i, i)))\nsf = st_sf(s)\ni = st_intersection(sf) # all intersections\nplot(i[\"n.overlaps\"])\nsummary(i$n.overlaps - lengths(i$origins))\n# A helper function that erases all of y from x:\nst_erase = function(x, y) st_difference(x, st_union(st_combine(y)))\npoly = st_polygon(list(cbind(c(0, 0, 1, 1, 0), c(0, 1, 1, 0, 0))))\nlines = st_multilinestring(list(\n cbind(c(0, 1), c(1, 1.05)),\n cbind(c(0, 1), c(0, -.05)),\n cbind(c(1, .95, 1), c(1.05, .5, -.05))\n))\nsnapped = st_snap(poly, lines, tolerance=.1)\nplot(snapped, col='red')\nplot(poly, border='green', add=TRUE)\nplot(lines, lwd=2, col='blue', add=TRUE)",
            "geos_binary_pred": "pts = st_sfc(st_point(c(.5,.5)), st_point(c(1.5, 1.5)), st_point(c(2.5, 2.5)))\npol = st_polygon(list(rbind(c(0,0), c(2,0), c(2,2), c(0,2), c(0,0))))\n(lst = st_intersects(pts, pol))\n(mat = st_intersects(pts, pol, sparse = FALSE))\n# which points fall inside a polygon?\napply(mat, 1, any)\nlengths(lst) > 0\n# which points fall inside the first polygon?\nst_intersects(pol, pts)[[1]]\n# remove duplicate geometries:\np1 = st_point(0:1)\np2 = st_point(2:1)\np = st_sf(a = letters[1:8], geom = st_sfc(p1, p1, p2, p1, p1, p2, p2, p1))\nst_equals(p)\nst_equals(p, remove_self = TRUE)\n(u = st_equals(p, retain_unique = TRUE))\n# retain the records with unique geometries:\np[-unlist(u),]",
            "geos_combine": "nc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\nst_combine(nc)\nplot(st_union(nc))",
            "geos_measures": "b0 = st_polygon(list(rbind(c(-1,-1), c(1,-1), c(1,1), c(-1,1), c(-1,-1))))\nb1 = b0 + 2\nb2 = b0 + c(-0.2, 2)\nx = st_sfc(b0, b1, b2)\nst_area(x)\nline = st_sfc(st_linestring(rbind(c(30,30), c(40,40))), crs = 4326)\nst_length(line)\n\nouter = matrix(c(0,0,10,0,10,10,0,10,0,0),ncol=2, byrow=TRUE)\nhole1 = matrix(c(1,1,1,2,2,2,2,1,1,1),ncol=2, byrow=TRUE)\nhole2 = matrix(c(5,5,5,6,6,6,6,5,5,5),ncol=2, byrow=TRUE)\n\npoly = st_polygon(list(outer, hole1, hole2))\nmpoly = st_multipolygon(list(\n\tlist(outer, hole1, hole2),\n\tlist(outer + 12, hole1 + 12)\n))\n\nst_length(st_sfc(poly, mpoly))\nst_perimeter(poly)\nst_perimeter(mpoly)\np = st_sfc(st_point(c(0,0)), st_point(c(0,1)), st_point(c(0,2)))\nst_distance(p, p)\nst_distance(p, p, by_element = TRUE)",
            "geos_query": "x = st_sfc(\n\tst_point(0:1),\n\tst_linestring(rbind(c(0,0),c(1,1))),\n\tst_polygon(list(rbind(c(0,0),c(1,0),c(0,1),c(0,0)))),\n\tst_multipoint(),\n\tst_linestring(),\n\tst_geometrycollection())\nst_dimension(x)\nst_dimension(x, FALSE)\nls = st_linestring(rbind(c(0,0), c(1,1), c(1,0), c(0,1)))\nst_is_simple(st_sfc(ls, st_point(c(0,0))))\nls = st_linestring(rbind(c(0,0), c(1,1), c(1,0), c(0,1)))\nst_is_empty(st_sfc(ls, st_point(), st_linestring()))",
            "geos_unary": "## st_buffer, style options (taken from rgeos gBuffer)\nl1 = st_as_sfc(\"LINESTRING(0 0,1 5,4 5,5 2,8 2,9 4,4 6.5)\")\nop = par(mfrow=c(2,3))\nplot(st_buffer(l1, dist = 1, endCapStyle=\"ROUND\"), reset = FALSE, main = \"endCapStyle: ROUND\")\nplot(l1,col='blue',add=TRUE)\nplot(st_buffer(l1, dist = 1, endCapStyle=\"FLAT\"), reset = FALSE, main = \"endCapStyle: FLAT\")\nplot(l1,col='blue',add=TRUE)\nplot(st_buffer(l1, dist = 1, endCapStyle=\"SQUARE\"), reset = FALSE, main = \"endCapStyle: SQUARE\")\nplot(l1,col='blue',add=TRUE)\nplot(st_buffer(l1, dist = 1, nQuadSegs=1), reset = FALSE, main = \"nQuadSegs: 1\")\nplot(l1,col='blue',add=TRUE)\nplot(st_buffer(l1, dist = 1, nQuadSegs=2), reset = FALSE, main = \"nQuadSegs: 2\")\nplot(l1,col='blue',add=TRUE)\nplot(st_buffer(l1, dist = 1, nQuadSegs= 5), reset = FALSE, main = \"nQuadSegs: 5\")\nplot(l1,col='blue',add=TRUE)\npar(op)\n\n\nl2 = st_as_sfc(\"LINESTRING(0 0,1 5,3 2)\")\nop = par(mfrow = c(2, 3))\nplot(st_buffer(l2, dist = 1, joinStyle=\"ROUND\"), reset = FALSE, main = \"joinStyle: ROUND\")\nplot(l2, col = 'blue', add = TRUE)\nplot(st_buffer(l2, dist = 1, joinStyle=\"MITRE\"), reset = FALSE, main = \"joinStyle: MITRE\")\nplot(l2, col= 'blue', add = TRUE)\nplot(st_buffer(l2, dist = 1, joinStyle=\"BEVEL\"), reset = FALSE, main = \"joinStyle: BEVEL\")\nplot(l2, col= 'blue', add=TRUE)\nplot(st_buffer(l2, dist = 1, joinStyle=\"MITRE\" , mitreLimit=0.5), reset = FALSE,\n   main = \"mitreLimit: 0.5\")\nplot(l2, col = 'blue', add = TRUE)\nplot(st_buffer(l2, dist = 1, joinStyle=\"MITRE\",mitreLimit=1), reset = FALSE,\n   main = \"mitreLimit: 1\")\nplot(l2, col = 'blue', add = TRUE)\nplot(st_buffer(l2, dist = 1, joinStyle=\"MITRE\",mitreLimit=3), reset = FALSE,\n   main = \"mitreLimit: 3\")\nplot(l2, col = 'blue', add = TRUE)\npar(op)\nnc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\nnc_g = st_geometry(nc)\nplot(st_convex_hull(nc_g))\nplot(nc_g, border = grey(.5), add = TRUE)\npt = st_combine(st_sfc(st_point(c(0,80)), st_point(c(120,80)), st_point(c(240,80))))\nst_convex_hull(pt) # R2\nst_convex_hull(st_set_crs(pt, 'OGC:CRS84')) # S2\nset.seed(131)\nif (compareVersion(sf_extSoftVersion()[[\"GEOS\"]], \"3.11.0\") > -1) {\n pts = cbind(runif(100), runif(100))\n m = st_multipoint(pts)\n co = sf:::st_concave_hull(m, 0.3)\n coh = sf:::st_concave_hull(m, 0.3, allow_holes = TRUE)\n plot(co, col = 'grey')\n plot(coh, add = TRUE, border = 'red')\n plot(m, add = TRUE)\n}\n\n# st_simplify examples:\nop = par(mfrow = c(2, 3), mar = rep(0, 4))\nplot(nc_g[1])\nplot(st_simplify(nc_g[1], dTolerance = 1e3)) # 1000m\nplot(st_simplify(nc_g[1], dTolerance = 5e3)) # 5000m\nnc_g_planar = st_transform(nc_g, 2264) # planar coordinates, US foot\nplot(nc_g_planar[1])\nplot(st_simplify(nc_g_planar[1], dTolerance = 1e3)) # 1000 foot\nplot(st_simplify(nc_g_planar[1], dTolerance = 5e3)) # 5000 foot\npar(op)\n\nif (compareVersion(sf_extSoftVersion()[[\"GEOS\"]], \"3.10.0\") > -1) {\n pts = rbind(c(0,0), c(1,0), c(1,1), c(.5,.5), c(0,1), c(0,0))\n po = st_polygon(list(pts))\n co = st_triangulate_constrained(po)\n tr = st_triangulate(po)\n plot(po, col = NA, border = 'grey', lwd = 15)\n plot(tr, border = 'green', col = NA, lwd = 5, add = TRUE)\n plot(co, border = 'red', col = 'NA', add = TRUE)\n}\nif (compareVersion(sf_extSoftVersion()[[\"GEOS\"]], \"3.9.0\") > -1) {\n  nc_t = st_transform(nc, 'EPSG:2264')\n  x = st_inscribed_circle(st_geometry(nc_t))\n  plot(st_geometry(nc_t), asp = 1, col = grey(.9))\n  plot(x, add = TRUE, col = '#ff9999')\n}\nset.seed(1)\nx = st_multipoint(matrix(runif(10),,2))\nbox = st_polygon(list(rbind(c(0,0),c(1,0),c(1,1),c(0,1),c(0,0))))\nif (compareVersion(sf_extSoftVersion()[[\"GEOS\"]], \"3.5.0\") > -1) {\n v = st_sfc(st_voronoi(x, st_sfc(box)))\n plot(v, col = 0, border = 1, axes = TRUE)\n plot(box, add = TRUE, col = 0, border = 1) # a larger box is returned, as documented\n plot(x, add = TRUE, col = 'red', cex=2, pch=16)\n plot(st_intersection(st_cast(v), box)) # clip to smaller box\n plot(x, add = TRUE, col = 'red', cex=2, pch=16)\n # matching Voronoi polygons to data points:\n # https://github.com/r-spatial/sf/issues/1030\n # generate 50 random unif points:\n n = 100\n pts = st_as_sf(data.frame(matrix(runif(n), , 2), id = 1:(n/2)), coords = c(\"X1\", \"X2\"))\n # compute Voronoi polygons:\n pols = st_collection_extract(st_voronoi(do.call(c, st_geometry(pts))))\n # match them to points:\n pts_pol = st_intersects(pts, pols)\n pts$pols = pols[unlist(pts_pol)] # re-order\n if (isTRUE(try(compareVersion(sf_extSoftVersion()[\"GEOS\"], \"3.12.0\") > -1,\n   silent = TRUE))) {\n   pols_po = st_collection_extract(st_voronoi(do.call(c, st_geometry(pts)),\n     point_order = TRUE)) # GEOS >= 3.12 can preserve order of inputs\n   pts_pol_po = st_intersects(pts, pols_po)\n   print(all(unlist(pts_pol_po) == 1:(n/2)))\n }\n plot(pts[\"id\"], pch = 16) # ID is color\n plot(st_set_geometry(pts, \"pols\")[\"id\"], xlim = c(0,1), ylim = c(0,1), reset = FALSE)\n plot(st_geometry(pts), add = TRUE)\n layout(matrix(1)) # reset plot layout\n}\nmls = st_multilinestring(list(matrix(c(0,0,0,1,1,1,0,0),,2,byrow=TRUE)))\nst_polygonize(st_sfc(mls))\nmls = st_multilinestring(list(rbind(c(0,0), c(1,1)), rbind(c(2,0), c(1,1))))\nst_line_merge(st_sfc(mls))\nplot(nc_g, axes = TRUE)\nplot(st_centroid(nc_g), add = TRUE, pch = 3, col = 'red')\nmp = st_combine(st_buffer(st_sfc(lapply(1:3, function(x) st_point(c(x,x)))), 0.2 * 1:3))\nplot(mp)\nplot(st_centroid(mp), add = TRUE, col = 'red') # centroid of combined geometry\nplot(st_centroid(mp, of_largest_polygon = TRUE), add = TRUE, col = 'blue', pch = 3)\nplot(nc_g, axes = TRUE)\nplot(st_point_on_surface(nc_g), add = TRUE, pch = 3, col = 'red')\nif (compareVersion(sf_extSoftVersion()[[\"GEOS\"]], \"3.7.0\") > -1) {\n  st_reverse(st_linestring(rbind(c(1,1), c(2,2), c(3,3))))\n}\n(l = st_linestring(rbind(c(0,0), c(1,1), c(0,1), c(1,0), c(0,0))))\nst_polygonize(st_node(l))\nst_node(st_multilinestring(list(rbind(c(0,0), c(1,1), c(0,1), c(1,0), c(0,0)))))\nsf = st_sf(a=1, geom=st_sfc(st_linestring(rbind(c(0,0),c(1,1)))), crs = 4326)\nif (require(lwgeom, quietly = TRUE)) {\n seg = st_segmentize(sf, units::set_units(100, km))\n seg = st_segmentize(sf, units::set_units(0.01, rad))\n nrow(seg$geom[[1]])\n}",
            "interpolate_aw": "nc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\ng = st_make_grid(nc, n = c(10, 5))\na1 = st_interpolate_aw(nc[\"BIR74\"], g, extensive = FALSE)\nsum(a1$BIR74) / sum(nc$BIR74) # not close to one: property is assumed spatially intensive\na2 = st_interpolate_aw(nc[\"BIR74\"], g, extensive = TRUE)\n# verify mass preservation (pycnophylactic) property:\nsum(a2$BIR74) / sum(nc$BIR74)\na1$intensive = a1$BIR74\na1$extensive = a2$BIR74\n\\donttest{plot(a1[c(\"intensive\", \"extensive\")], key.pos = 4)}",
            "merge.sf": "a = data.frame(a = 1:3, b = 5:7)\nst_geometry(a) = st_sfc(st_point(c(0,0)), st_point(c(1,1)), st_point(c(2,2)))\nb = data.frame(x = c(\"a\", \"b\", \"c\"), b = c(2,5,6))\nmerge(a, b)\nmerge(a, b, all = TRUE)",
            "nc": "\\donttest{\nnc <- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n}",
            "plot": "nc = st_read(system.file(\"gpkg/nc.gpkg\", package=\"sf\"), quiet = TRUE)\n# plot single attribute, auto-legend:\nplot(nc[\"SID74\"])\n# plot multiple:\nplot(nc[c(\"SID74\", \"SID79\")]) # better use ggplot2::geom_sf to facet and get a single legend!\n# adding to a plot of an sf object only works when using reset=FALSE in the first plot:\nplot(nc[\"SID74\"], reset = FALSE)\nplot(st_centroid(st_geometry(nc)), add = TRUE)\n# log10 z-scale:\nplot(nc[\"SID74\"], logz = TRUE, breaks = c(0,.5,1,1.5,2), at = c(0,.5,1,1.5,2))\n# and we need to reset the plotting device after that, e.g. by\nlayout(1)\n# when plotting only geometries, the reset=FALSE is not needed:\nplot(st_geometry(nc))\nplot(st_geometry(nc)[1], col = 'red', add = TRUE)\n# add a custom legend to an arbitray plot:\nlayout(matrix(1:2, ncol = 2), widths = c(1, lcm(2)))\nplot(1)\n.image_scale(1:10, col = sf.colors(9), key.length = lcm(8), key.pos = 4, at = 1:10)\n# manipulate plotting order, plot largest polygons first:\np = st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0))))\nx = st_sf(a=1:4, st_sfc(p, p * 2, p * 3, p * 4)) # plot(x, col=2:5) only shows the largest polygon!\nplot(x[order(st_area(x), decreasing = TRUE),], col = 2:5) # plot largest polygons first\n\nsf.colors(10)\ntext(nc, labels = substring(nc$NAME,1,1))",
            "s2": "m = rbind(c(-1,-1), c(1,-1), c(1,1), c(-1,1), c(-1,-1))\nm1 = rbind(c(-1,-1), c(1,-1), c(1,1), c(-1,1), c(-1,0), c(-1,-1))\nm0 = m[5:1,]\nmp = st_multipolygon(list(\nlist(m, 0.8 * m0, 0.01 * m1 + 0.9),\nlist(0.7* m, 0.6*m0),\nlist(0.5 * m0),\nlist(m+2),\nlist(m+4,(.9*m0)+4)\n))\nsf = st_sfc(mp, mp, crs = 'EPSG:4326')\ns2 = st_as_s2(sf)",
            "sf": "g = st_sfc(st_point(1:2))\nst_sf(a=3,g)\nst_sf(g, a=3)\nst_sf(a=3, st_sfc(st_point(1:2))) # better to name it!\n# create empty structure with preallocated empty geometries:\nnrows <- 10\ngeometry = st_sfc(lapply(1:nrows, function(x) st_geometrycollection()))\ndf <- st_sf(id = 1:nrows, geometry = geometry)\ng = st_sfc(st_point(1:2), st_point(3:4))\ns = st_sf(a=3:4, g)\ns[1,]\nclass(s[1,])\ns[,1]\nclass(s[,1])\ns[,2]\nclass(s[,2])\ng = st_sf(a=2:3, g)\npol = st_sfc(st_polygon(list(cbind(c(0,3,3,0,0),c(0,0,3,3,0)))))\nh = st_sf(r = 5, pol)\ng[h,]\nh[g,]",
            "sf_project": "sf_add_proj_units()",
            "sfc": "pt1 = st_point(c(0,1))\npt2 = st_point(c(1,1))\n(sfc = st_sfc(pt1, pt2))\nsfc[sfc[1], op = st_is_within_distance, dist = 0.5]\nd = st_sf(data.frame(a=1:2, geom=sfc))",
            "st": "(p1 = st_point(c(1,2)))\nclass(p1)\nst_bbox(p1)\n(p2 = st_point(c(1,2,3)))\nclass(p2)\n(p3 = st_point(c(1,2,3), \"XYM\"))\npts = matrix(1:10, , 2)\n(mp1 = st_multipoint(pts))\npts = matrix(1:15, , 3)\n(mp2 = st_multipoint(pts))\n(mp3 = st_multipoint(pts, \"XYM\"))\npts = matrix(1:20, , 4)\n(mp4 = st_multipoint(pts))\npts = matrix(1:10, , 2)\n(ls1 = st_linestring(pts))\npts = matrix(1:15, , 3)\n(ls2 = st_linestring(pts))\n(ls3 = st_linestring(pts, \"XYM\"))\npts = matrix(1:20, , 4)\n(ls4 = st_linestring(pts))\nouter = matrix(c(0,0,10,0,10,10,0,10,0,0),ncol=2, byrow=TRUE)\nhole1 = matrix(c(1,1,1,2,2,2,2,1,1,1),ncol=2, byrow=TRUE)\nhole2 = matrix(c(5,5,5,6,6,6,6,5,5,5),ncol=2, byrow=TRUE)\npts = list(outer, hole1, hole2)\n(ml1 = st_multilinestring(pts))\npts3 = lapply(pts, function(x) cbind(x, 0))\n(ml2 = st_multilinestring(pts3))\n(ml3 = st_multilinestring(pts3, \"XYM\"))\npts4 = lapply(pts3, function(x) cbind(x, 0))\n(ml4 = st_multilinestring(pts4))\nouter = matrix(c(0,0,10,0,10,10,0,10,0,0),ncol=2, byrow=TRUE)\nhole1 = matrix(c(1,1,1,2,2,2,2,1,1,1),ncol=2, byrow=TRUE)\nhole2 = matrix(c(5,5,5,6,6,6,6,5,5,5),ncol=2, byrow=TRUE)\npts = list(outer, hole1, hole2)\n(pl1 = st_polygon(pts))\npts3 = lapply(pts, function(x) cbind(x, 0))\n(pl2 = st_polygon(pts3))\n(pl3 = st_polygon(pts3, \"XYM\"))\npts4 = lapply(pts3, function(x) cbind(x, 0))\n(pl4 = st_polygon(pts4))\npol1 = list(outer, hole1, hole2)\npol2 = list(outer + 12, hole1 + 12)\npol3 = list(outer + 24)\nmp = list(pol1,pol2,pol3)\n(mp1 = st_multipolygon(mp))\npts3 = lapply(mp, function(x) lapply(x, function(y) cbind(y, 0)))\n(mp2 = st_multipolygon(pts3))\n(mp3 = st_multipolygon(pts3, \"XYM\"))\npts4 = lapply(mp2, function(x) lapply(x, function(y) cbind(y, 0)))\n(mp4 = st_multipolygon(pts4))\n(gc = st_geometrycollection(list(p1, ls1, pl1, mp1)))\nst_geometrycollection() # empty geometry\nc(st_point(1:2), st_point(5:6))\nc(st_point(1:2), st_multipoint(matrix(5:8,2)))\nc(st_multipoint(matrix(1:4,2)), st_multipoint(matrix(5:8,2)))\nc(st_linestring(matrix(1:6,3)), st_linestring(matrix(11:16,3)))\nc(st_multilinestring(list(matrix(1:6,3))), st_multilinestring(list(matrix(11:16,3))))\npl = list(rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0)))\nc(st_polygon(pl), st_polygon(pl))\nc(st_polygon(pl), st_multipolygon(list(pl)))\nc(st_linestring(matrix(1:6,3)), st_point(1:2))\nc(st_geometrycollection(list(st_point(1:2), st_linestring(matrix(1:6,3)))),\n  st_geometrycollection(list(st_multilinestring(list(matrix(11:16,3))))))\nc(st_geometrycollection(list(st_point(1:2), st_linestring(matrix(1:6,3)))),\n  st_multilinestring(list(matrix(11:16,3))), st_point(5:6),\n  st_geometrycollection(list(st_point(10:11))))",
            "st_as_binary": "# examples of setting precision:\nst_point(c(1/3, 1/6)) \\%>\\% st_sfc(precision = 1000) \\%>\\% st_as_binary \\%>\\% st_as_sfc\nst_point(c(1/3, 1/6)) \\%>\\% st_sfc(precision =  100) \\%>\\% st_as_binary \\%>\\% st_as_sfc\nst_point(1e6 * c(1/3, 1/6)) \\%>\\% st_sfc(precision = 0.01) \\%>\\% st_as_binary \\%>\\% st_as_sfc\nst_point(1e6 * c(1/3, 1/6)) \\%>\\% st_sfc(precision = 0.001) \\%>\\% st_as_binary \\%>\\% st_as_sfc",
            "st_as_sf": "pt1 = st_point(c(0,1))\npt2 = st_point(c(1,1))\nst_sfc(pt1, pt2)\nd = data.frame(a = 1:2)\nd$geom = st_sfc(pt1, pt2)\ndf = st_as_sf(d)\nd$geom = c(\"POINT(0 0)\", \"POINT(0 1)\")\ndf = st_as_sf(d, wkt = \"geom\")\nd$geom2 = st_sfc(pt1, pt2)\nst_as_sf(d) # should warn\nif (require(sp, quietly = TRUE)) {\n data(meuse, package = \"sp\")\n meuse_sf = st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992, agr = \"constant\")\n meuse_sf[1:3,]\n summary(meuse_sf)\n}\nif (require(sp, quietly = TRUE)) {\nx = rbind(c(-1,-1), c(1,-1), c(1,1), c(-1,1), c(-1,-1))\nx1 = 0.1 * x + 0.1\nx2 = 0.1 * x + 0.4\nx3 = 0.1 * x + 0.7\ny = x + 3\ny1 = x1 + 3\ny3 = x3 + 3\nm = matrix(c(3, 0), 5, 2, byrow = TRUE)\nz = x + m\nz1 = x1 + m\nz2 = x2 + m\nz3 = x3 + m\np1 = Polygons(list( Polygon(x[5:1,]), Polygon(x2), Polygon(x3),\n   Polygon(y[5:1,]), Polygon(y1), Polygon(x1), Polygon(y3)), \"ID1\")\np2 = Polygons(list( Polygon(z[5:1,]), Polygon(z2), Polygon(z3), Polygon(z1)),\n  \"ID2\")\nr = SpatialPolygons(list(p1,p2))\na = suppressWarnings(st_as_sf(r))\nsummary(a)\ndemo(meuse, ask = FALSE, echo = FALSE)\nsummary(st_as_sf(meuse))\nsummary(st_as_sf(meuse.grid))\nsummary(st_as_sf(meuse.area))\nsummary(st_as_sf(meuse.riv))\nsummary(st_as_sf(as(meuse.riv, \"SpatialLines\")))\npol.grd = as(meuse.grid, \"SpatialPolygonsDataFrame\")\n# summary(st_as_sf(pol.grd))\n# summary(st_as_sf(as(pol.grd, \"SpatialLinesDataFrame\")))\n}\nif (require(spatstat.geom)) {\n  g = st_as_sf(gorillas)\n  # select only the points:\n  g[st_is(g, \"POINT\"),]\n}\nif (require(spatstat.linnet)) {\n data(chicago)\n plot(st_as_sf(chicago)[\"label\"])\n plot(st_as_sf(chicago)[-1,\"label\"])\n}",
            "st_as_sfc": "wkb = structure(list(\"01010000204071000000000000801A064100000000AC5C1441\"), class = \"WKB\")\nst_as_sfc(wkb, EWKB = TRUE)\nwkb = structure(list(\"0x01010000204071000000000000801A064100000000AC5C1441\"), class = \"WKB\")\nst_as_sfc(wkb, EWKB = TRUE)\nst_as_sfc(st_as_binary(st_sfc(st_point(0:1)))[[1]], crs = 4326)\nst_as_sfc(\"SRID=3978;LINESTRING(1663106 -105415,1664320 -104617)\")",
            "st_as_text": "st_as_text(st_point(1:2))\nst_as_text(st_sfc(st_point(c(-90,40)), crs = 4326), EWKT = TRUE)",
            "st_bbox": "a = st_sf(a = 1:2, geom = st_sfc(st_point(0:1), st_point(1:2)), crs = 4326)\nst_bbox(a)\nst_as_sfc(st_bbox(a))\nst_bbox(c(xmin = 16.1, xmax = 16.6, ymax = 48.6, ymin = 47.9), crs = st_crs(4326))",
            "st_break_antimeridian": "\\donttest{\nif (require(\"maps\", quietly=TRUE)) {\n opar = par(mfrow=c(3, 2))\n wld = st_as_sf(map(fill=FALSE, interior=FALSE, plot=FALSE), fill=FALSE)\n for (lon_0 in c(-170, -90, -10, 10, 90, 170)) {\n  wld |> st_break_antimeridian(lon_0=lon_0) |>\n   st_transform(paste0(\"+proj=natearth +lon_0=\", lon_0)) |>\n   st_geometry() |> plot(main=lon_0)\n }\n par(opar)\n}\n}",
            "st_cast": "# example(st_read)\nnc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\nmpl <- st_geometry(nc)[[4]]\n#st_cast(x) ## error 'argument \"to\" is missing, with no default'\ncast_all <- function(xg) {\n  lapply(c(\"MULTIPOLYGON\", \"MULTILINESTRING\", \"MULTIPOINT\", \"POLYGON\", \"LINESTRING\", \"POINT\"), \n      function(x) st_cast(xg, x))\n}\nst_sfc(cast_all(mpl))\n## no closing coordinates should remain for multipoint\nany(duplicated(unclass(st_cast(mpl, \"MULTIPOINT\"))))  ## should be FALSE\n## number of duplicated coordinates in the linestrings should equal the number of polygon rings \n## (... in this case, won't always be true)\nsum(duplicated(do.call(rbind, unclass(st_cast(mpl, \"MULTILINESTRING\"))))\n     ) == sum(unlist(lapply(mpl, length)))  ## should be TRUE\n\np1 <- structure(c(0, 1, 3, 2, 1, 0, 0, 0, 2, 4, 4, 0), .Dim = c(6L, 2L))\np2 <- structure(c(1, 1, 2, 1, 1, 2, 2, 1), .Dim = c(4L, 2L))\nst_polygon(list(p1, p2))\nmls <- st_cast(st_geometry(nc)[[4]], \"MULTILINESTRING\")\nst_sfc(cast_all(mls))\nmpt <- st_cast(st_geometry(nc)[[4]], \"MULTIPOINT\")\nst_sfc(cast_all(mpt))\npl <- st_cast(st_geometry(nc)[[4]], \"POLYGON\")\nst_sfc(cast_all(pl))\nls <- st_cast(st_geometry(nc)[[4]], \"LINESTRING\")\nst_sfc(cast_all(ls))\npt <- st_cast(st_geometry(nc)[[4]], \"POINT\")\n## st_sfc(cast_all(pt))  ## Error: cannot create MULTIPOLYGON from POINT \nst_sfc(lapply(c(\"POINT\", \"MULTIPOINT\"), function(x) st_cast(pt, x)))\ns = st_multipoint(rbind(c(1,0)))\nst_cast(s, \"POINT\")\n# https://github.com/r-spatial/sf/issues/1930:\npt1 <- st_point(c(0,1))\npt23 <- st_multipoint(matrix(c(1,2,3,4), ncol = 2, byrow = TRUE))\nd <- st_sf(geom = st_sfc(pt1, pt23))\nst_cast(d, \"POINT\") # will not convert the entire MULTIPOINT, and warns\nst_cast(d, \"MULTIPOINT\") \\%>\\% st_cast(\"POINT\")",
            "st_collection_extract": "pt <- st_point(c(1, 0))\nls <- st_linestring(matrix(c(4, 3, 0, 0), ncol = 2))\npoly1 <- st_polygon(list(matrix(c(5.5, 7, 7, 6, 5.5, 0, 0, -0.5, -0.5, 0), ncol = 2)))\npoly2 <- st_polygon(list(matrix(c(6.6, 8, 8, 7, 6.6, 1, 1, 1.5, 1.5, 1), ncol = 2)))\nmultipoly <- st_multipolygon(list(poly1, poly2))\n\ni <- st_geometrycollection(list(pt, ls, poly1, poly2))\nj <- st_geometrycollection(list(pt, ls, poly1, poly2, multipoly))\n\nst_collection_extract(i, \"POLYGON\")\nst_collection_extract(i, \"POINT\")\nst_collection_extract(i, \"LINESTRING\")\n\n## A GEOMETRYCOLLECTION\naa <- rbind(st_sf(a=1, geom = st_sfc(i)),\n\t\t\tst_sf(a=2, geom = st_sfc(j)))\n\n## With sf objects\nst_collection_extract(aa, \"POLYGON\")\nst_collection_extract(aa, \"LINESTRING\")\nst_collection_extract(aa, \"POINT\")\n\n## With sfc objects\nst_collection_extract(st_geometry(aa), \"POLYGON\")\nst_collection_extract(st_geometry(aa), \"LINESTRING\")\nst_collection_extract(st_geometry(aa), \"POINT\")\n\n## A GEOMETRY of single types\nbb <- rbind(\n\tst_sf(a = 1, geom = st_sfc(pt)),\n\tst_sf(a = 2, geom = st_sfc(ls)),\n\tst_sf(a = 3, geom = st_sfc(poly1)),\n\tst_sf(a = 4, geom = st_sfc(multipoly))\n)\n\nst_collection_extract(bb, \"POLYGON\")\n\n## A GEOMETRY of mixed single types and GEOMETRYCOLLECTIONS\ncc <- rbind(aa, bb)\n\nst_collection_extract(cc, \"POLYGON\")",
            "st_crop": "box = c(xmin = 0, ymin = 0, xmax = 1, ymax = 1)\npol = st_sfc(st_buffer(st_point(c(.5, .5)), .6))\npol_sf = st_sf(a=1, geom=pol)\nplot(st_crop(pol, box))\nplot(st_crop(pol_sf, st_bbox(box)))\n# alternative:\nplot(st_crop(pol, xmin = 0, ymin = 0, xmax = 1, ymax = 1))",
            "st_crs": "sfc = st_sfc(st_point(c(0,0)), st_point(c(1,1)))\nsf = st_sf(a = 1:2, geom = sfc)\nst_crs(sf) = 4326\nst_geometry(sf)\nsfc = st_sfc(st_point(c(0,0)), st_point(c(1,1)))\nst_crs(sfc) = 4326\nsfc\nsfc = st_sfc(st_point(c(0,0)), st_point(c(1,1)))\nsfc \\%>\\% st_set_crs(4326) \\%>\\% st_transform(3857)\nst_crs(\"EPSG:3857\")$input\nst_crs(3857)$proj4string\npt = st_sfc(st_point(c(0, 60)), crs = 4326)\n# st_axis_order() only has effect in GDAL >= 2.5.0:\nst_axis_order() # query default: FALSE means interpret pt as (longitude latitude)\nst_transform(pt, 3857)[[1]]\nold_value = FALSE\nif (sf_extSoftVersion()[\"GDAL\"] >= \"2.5.0\")\n   (old_value = st_axis_order(TRUE))\n# now interpret pt as (latitude longitude), as EPSG:4326 prescribes:\nst_axis_order() # query current value\nst_transform(pt, 3857)[[1]]\nst_axis_order(old_value) # set back to old value",
            "st_drivers": "# The following driver lists depend on the GDAL setup and platform used:\nst_drivers()\nst_drivers(\"raster\", \"GeoT\")",
            "st_geometry": "df = data.frame(a = 1:2)\nsfc = st_sfc(st_point(c(3,4)), st_point(c(10,11)))\nst_geometry(sfc)\nst_geometry(df) <- sfc\nclass(df)\nst_geometry(df)\nst_geometry(df) <- sfc # replaces\nst_geometry(df) <- NULL # remove geometry, coerce to data.frame\nsf <- st_set_geometry(df, sfc) # set geometry, return sf\nst_set_geometry(sf, NULL) # remove geometry, coerce to data.frame",
            "st_graticule": "library(sf)\nif (require(maps, quietly = TRUE)) {\n\nusa = st_as_sf(map('usa', plot = FALSE, fill = TRUE))\nlaea = st_crs(\"+proj=laea +lat_0=30 +lon_0=-95\") # Lambert equal area\nusa <- st_transform(usa, laea)\n\nbb = st_bbox(usa)\nbbox = st_linestring(rbind(c( bb[1],bb[2]),c( bb[3],bb[2]),\n   c( bb[3],bb[4]),c( bb[1],bb[4]),c( bb[1],bb[2])))\n\ng = st_graticule(usa)\nplot(usa, xlim = 1.2 * c(-2450853.4, 2186391.9), reset = FALSE)\nplot(g[1], add = TRUE, col = 'grey')\nplot(bbox, add = TRUE)\npoints(g$x_start, g$y_start, col = 'red')\npoints(g$x_end, g$y_end, col = 'blue')\n\ninvisible(lapply(seq_len(nrow(g)), function(i) {\nif (g$type[i] == \"N\" && g$x_start[i] - min(g$x_start) < 1000)\n\ttext(g$x_start[i], g$y_start[i], labels = parse(text = g$degree_label[i]), \n\t\tsrt = g$angle_start[i], pos = 2, cex = .7)\nif (g$type[i] == \"E\" && g$y_start[i] - min(g$y_start) < 1000)\n\ttext(g$x_start[i], g$y_start[i], labels = parse(text = g$degree_label[i]), \n\t\tsrt = g$angle_start[i] - 90, pos = 1, cex = .7)\nif (g$type[i] == \"N\" && g$x_end[i] - max(g$x_end) > -1000)\n\ttext(g$x_end[i], g$y_end[i], labels = parse(text = g$degree_label[i]), \n\t\tsrt = g$angle_end[i], pos = 4, cex = .7)\nif (g$type[i] == \"E\" && g$y_end[i] - max(g$y_end) > -1000)\n\ttext(g$x_end[i], g$y_end[i], labels = parse(text = g$degree_label[i]), \n\t\tsrt = g$angle_end[i] - 90, pos = 3, cex = .7)\n}))\nplot(usa, graticule = st_crs(4326), axes = TRUE, lon = seq(-60,-130,by=-10))\n}",
            "st_is": "st_is(st_point(0:1), \"POINT\")\nsfc = st_sfc(st_point(0:1), st_linestring(matrix(1:6,,2)))\nst_is(sfc, \"POINT\")\nst_is(sfc, \"POLYGON\")\nst_is(sfc, \"LINESTRING\")\nst_is(st_sf(a = 1:2, sfc), \"LINESTRING\")\nst_is(sfc, c(\"POINT\", \"LINESTRING\"))",
            "st_jitter": "nc = st_read(system.file(\"gpkg/nc.gpkg\", package=\"sf\"))\npts = st_centroid(st_geometry(nc))\nplot(pts)\nplot(st_jitter(pts, .05), add = TRUE, col = 'red')\nplot(st_geometry(nc))\nplot(st_jitter(st_geometry(nc), factor = .01), add = TRUE, col = '#ff8888')",
            "st_join": "a = st_sf(a = 1:3,\n geom = st_sfc(st_point(c(1,1)), st_point(c(2,2)), st_point(c(3,3))))\nb = st_sf(a = 11:14,\n geom = st_sfc(st_point(c(10,10)), st_point(c(2,2)), st_point(c(2,2)), st_point(c(3,3))))\nst_join(a, b)\nst_join(a, b, left = FALSE)\n# two ways to aggregate y's attribute values outcome over x's geometries:\nst_join(a, b) \\%>\\% aggregate(list(.$a.x), mean)\nif (require(dplyr, quietly = TRUE)) {\n st_join(a, b) \\%>\\% group_by(a.x) \\%>\\% summarise(mean(a.y))\n}\n# example of largest = TRUE:\nnc <- st_transform(st_read(system.file(\"shape/nc.shp\", package=\"sf\")), 2264)                \ngr = st_sf(\n    label = apply(expand.grid(1:10, LETTERS[10:1])[,2:1], 1, paste0, collapse = \" \"),\n    geom = st_make_grid(st_as_sfc(st_bbox(nc))))\ngr$col = sf.colors(10, categorical = TRUE, alpha = .3)\n# cut, to check, NA's work out:\ngr = gr[-(1:30),]\nnc_j <- st_join(nc, gr, largest = TRUE)\n# the two datasets:\nopar = par(mfrow = c(2,1), mar = rep(0,4))\nplot(st_geometry(nc_j))\nplot(st_geometry(gr), add = TRUE, col = gr$col)\ntext(st_coordinates(st_centroid(gr)), labels = gr$label)\n# the joined dataset:\nplot(st_geometry(nc_j), border = 'black', col = nc_j$col)\ntext(st_coordinates(st_centroid(nc_j)), labels = nc_j$label, cex = .8)\nplot(st_geometry(gr), border = 'green', add = TRUE)\npar(opar)\n# st_filter keeps the geometries in x where .predicate(x,y) returns any match in y for x\nst_filter(a, b)\n# for an anti-join, use the union of y\nst_filter(a, st_union(b), .predicate = st_disjoint)",
            "st_line_project_point": "st_line_project(st_as_sfc(\"LINESTRING (0 0, 10 10)\"), st_as_sfc(c(\"POINT (0 0)\", \"POINT (5 5)\")))\nst_line_project(st_as_sfc(\"LINESTRING (0 0, 10 10)\"), st_as_sfc(\"POINT (5 5)\"), TRUE)\nst_line_interpolate(st_as_sfc(\"LINESTRING (0 0, 1 1)\"), 1)\nst_line_interpolate(st_as_sfc(\"LINESTRING (0 0, 1 1)\"), 1, TRUE)",
            "st_line_sample": "ls = st_sfc(st_linestring(rbind(c(0,0),c(0,1))),\n\tst_linestring(rbind(c(0,0),c(10,0))))\nst_line_sample(ls, density = 1)\nls = st_sfc(st_linestring(rbind(c(0,0),c(0,1))),\n st_linestring(rbind(c(0,0),c(.1,0))), crs = 4326)\ntry(st_line_sample(ls, density = 1/1000)) # error\nst_line_sample(st_transform(ls, 3857), n = 5) # five points for each line\nst_line_sample(st_transform(ls, 3857), n = c(1, 3)) # one and three points\nst_line_sample(st_transform(ls, 3857), density = 1/1000) # one per km\nst_line_sample(st_transform(ls, 3857), density = c(1/1000, 1/10000)) # one per km, one per 10 km\nst_line_sample(st_transform(ls, 3857), density = units::set_units(1, 1/km)) # one per km\n# five equidistant points including start and end:\nst_line_sample(st_transform(ls, 3857), sample = c(0, 0.25, 0.5, 0.75, 1))",
            "st_m_range": "a = st_sf(a = 1:2, geom = st_sfc(st_point(0:3), st_point(1:4)), crs = 4326)\nst_m_range(a)\nst_m_range(c(mmin = 16.1, mmax = 16.6), crs = st_crs(4326))",
            "st_make_grid": "plot(st_make_grid(what = \"centers\"), axes = TRUE)\nplot(st_make_grid(what = \"corners\"), add = TRUE, col = 'green', pch=3)\nsfc = st_sfc(st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,0)))))\nplot(st_make_grid(sfc, cellsize = .1, square = FALSE))\nplot(sfc, add = TRUE)\n# non-default offset:\nplot(st_make_grid(sfc, cellsize = .1, square = FALSE, offset = c(0, .05 / (sqrt(3)/2))))\nplot(sfc, add = TRUE)\nnc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\ng = st_make_grid(nc)\nplot(g)\nplot(st_geometry(nc), add = TRUE)\n# g[nc] selects cells that intersect with nc:\nplot(g[nc], col = '#ff000088', add = TRUE)",
            "st_nearest_feature": "ls1 = st_linestring(rbind(c(0,0), c(1,0)))\nls2 = st_linestring(rbind(c(0,0.1), c(1,0.1)))\nls3 = st_linestring(rbind(c(0,1), c(1,1)))\n(l = st_sfc(ls1, ls2, ls3))\n\np1 = st_point(c(0.1, -0.1))\np2 = st_point(c(0.1, 0.11))\np3 = st_point(c(0.1, 0.09))\np4 = st_point(c(0.1, 0.9))\n\n(p = st_sfc(p1, p2, p3, p4))\ntry(st_nearest_feature(p, l))\ntry(st_nearest_points(p, l[st_nearest_feature(p,l)], pairwise = TRUE))\n\nr = sqrt(2)/10\nb1 = st_buffer(st_point(c(.1,.1)), r)\nb2 = st_buffer(st_point(c(.9,.9)), r)\nb3 = st_buffer(st_point(c(.9,.1)), r)\ncircles = st_sfc(b1, b2, b3)\nplot(circles, col = NA, border = 2:4)\npts = st_sfc(st_point(c(.3,.1)), st_point(c(.6,.2)), st_point(c(.6,.6)), st_point(c(.4,.8)))\nplot(pts, add = TRUE, col = 1)\n# draw points to nearest circle:\nnearest = try(st_nearest_feature(pts, circles))\nif (inherits(nearest, \"try-error\")) # GEOS 3.6.1 not available\n  nearest = c(1, 3, 2, 2)\nls = st_nearest_points(pts, circles[nearest], pairwise = TRUE)\nplot(ls, col = 5:8, add = TRUE)\n# compute distance between pairs of nearest features:\nst_distance(pts, circles[nearest], by_element = TRUE)",
            "st_nearest_points": "r = sqrt(2)/10\npt1 = st_point(c(.1,.1))\npt2 = st_point(c(.9,.9))\npt3 = st_point(c(.9,.1))\nb1 = st_buffer(pt1, r)\nb2 = st_buffer(pt2, r)\nb3 = st_buffer(pt3, r)\n(ls0 = st_nearest_points(b1, b2)) # sfg\n(ls = st_nearest_points(st_sfc(b1), st_sfc(b2, b3))) # sfc\nplot(b1, xlim = c(-.2,1.2), ylim = c(-.2,1.2), col = NA, border = 'green')\nplot(st_sfc(b2, b3), add = TRUE, col = NA, border = 'blue')\nplot(ls, add = TRUE, col = 'red')\n\nnc = st_read(system.file(\"gpkg/nc.gpkg\", package=\"sf\"))\nplot(st_geometry(nc))\nls = st_nearest_points(nc[1,], nc)\nplot(ls, col = 'red', add = TRUE)\npts = st_cast(ls, \"POINT\") # gives all start & end points\n# starting, \"from\" points, corresponding to x:\nplot(pts[seq(1, 200, 2)], add = TRUE, col = 'blue')\n# ending, \"to\" points, corresponding to y:\nplot(pts[seq(2, 200, 2)], add = TRUE, col = 'green')",
            "st_normalize": "p1 = st_point(c(7,52))\nst_normalize(p1, domain = c(0, 0, 10, 100))\n\np2 = st_point(c(-30,20))\nsfc = st_sfc(p1, p2, crs = 4326)\nsfc\nsfc_norm <- st_normalize(sfc)\nst_bbox(sfc_norm)",
            "st_precision": "x <- st_sfc(st_point(c(pi, pi)))\nst_precision(x)\nst_precision(x) <- 0.01\nst_precision(x)",
            "st_read": "nc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\nsummary(nc) # note that AREA was computed using Euclidian area on lon/lat degrees\n\n## only three fields by select clause\n## only two features by where clause\nnc_sql = st_read(system.file(\"shape/nc.shp\", package=\"sf\"),\n                     query = \"SELECT NAME, SID74, FIPS FROM \\\"nc\\\" WHERE BIR74 > 20000\")\n\\dontrun{\n  library(sp)\n  example(meuse, ask = FALSE, echo = FALSE)\n  try(st_write(st_as_sf(meuse), \"PG:dbname=postgis\", \"meuse\",\n       layer_options = \"OVERWRITE=true\"))\n  try(st_meuse <- st_read(\"PG:dbname=postgis\", \"meuse\"))\n  if (exists(\"st_meuse\"))\n    summary(st_meuse)\n}\n\n\\dontrun{\n## note that we need special escaping of layer  within single quotes (nc.gpkg)\n## and that geom needs to be included in the select, otherwise we don't detect it\nlayer <- st_layers(system.file(\"gpkg/nc.gpkg\", package = \"sf\"))$name[1]\nnc_gpkg_sql = st_read(system.file(\"gpkg/nc.gpkg\", package = \"sf\"),\n   query = sprintf(\"SELECT NAME, SID74, FIPS, geom  FROM \\\"\\%s\\\" WHERE BIR74 > 20000\", layer))\n}\n# spatial filter, as wkt:\nwkt = st_as_text(st_geometry(nc[1,]))\n# filter by (bbox overlaps of) first feature geometry:\nst_read(system.file(\"gpkg/nc.gpkg\", package=\"sf\"), wkt_filter = wkt)\n# read geojson from string:\ngeojson_txt <- paste(\"{\\\"type\\\":\\\"MultiPoint\\\",\\\"coordinates\\\":\",\n   \"[[3.2,4],[3,4.6],[3.8,4.4],[3.5,3.8],[3.4,3.6],[3.9,4.5]]}\")\nx = st_read(geojson_txt)\nx\n\\dontrun{\nlibrary(RPostgreSQL)\ntry(conn <- dbConnect(PostgreSQL(), dbname = \"postgis\"))\nif (exists(\"conn\") && !inherits(conn, \"try-error\")) {\n  x = st_read(conn, \"meuse\", query = \"select * from meuse limit 3;\")\n  x = st_read(conn, table = \"public.meuse\")\n  print(st_crs(x)) # SRID resolved by the database, not by GDAL!\n  dbDisconnect(conn)\n }\n}",
            "st_relate": "p1 = st_point(c(0,0))\np2 = st_point(c(2,2))\npol1 = st_polygon(list(rbind(c(0,0),c(1,0),c(1,1),c(0,1),c(0,0)))) - 0.5\npol2 = pol1 + 1\npol3 = pol1 + 2\nst_relate(st_sfc(p1, p2), st_sfc(pol1, pol2, pol3))\nsfc = st_sfc(st_point(c(0,0)), st_point(c(3,3)))\ngrd = st_make_grid(sfc, n = c(3,3))\nst_intersects(grd)\nst_relate(grd, pattern = \"****1****\") # sides, not corners, internals\nst_relate(grd, pattern = \"****0****\") # only corners touch\nst_rook = function(a, b = a) st_relate(a, b, pattern = \"F***1****\")\nst_rook(grd)\n# queen neighbours, see \\url{https://github.com/r-spatial/sf/issues/234#issuecomment-300511129}\nst_queen <- function(a, b = a) st_relate(a, b, pattern = \"F***T****\")",
            "st_sample": "nc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\np1 = st_sample(nc[1:3, ], 6)\np2 = st_sample(nc[1:3, ], 1:3)\nplot(st_geometry(nc)[1:3])\nplot(p1, add = TRUE)\nplot(p2, add = TRUE, pch = 2)\nx = st_sfc(st_polygon(list(rbind(c(0,0),c(90,0),c(90,90),c(0,90),c(0,0)))), crs = st_crs(4326))\nplot(x, axes = TRUE, graticule = TRUE)\nif (sf_extSoftVersion()[\"proj.4\"] >= \"4.9.0\")\n  plot(p <- st_sample(x, 1000), add = TRUE)\nif (require(lwgeom, quietly = TRUE)) { # for st_segmentize()\n  x2 = st_transform(st_segmentize(x, 1e4), st_crs(\"+proj=ortho +lat_0=30 +lon_0=45\"))\n  g = st_transform(st_graticule(), st_crs(\"+proj=ortho +lat_0=30 +lon_0=45\"))\n  plot(x2, graticule = g)\n  if (sf_extSoftVersion()[\"proj.4\"] >= \"4.9.0\") {\n    p2 = st_transform(p, st_crs(\"+proj=ortho +lat_0=30 +lon_0=45\"))\n    plot(p2, add = TRUE)\n  }\n}\nx = st_sfc(st_polygon(list(rbind(c(0,0),c(90,0),c(90,10),c(0,90),c(0,0))))) # NOT long/lat:\nplot(x)\np_exact = st_sample(x, 1000, exact = TRUE)\np_not_exact = st_sample(x, 1000, exact = FALSE)\nlength(p_exact); length(p_not_exact)\nplot(st_sample(x, 1000), add = TRUE)\nx = st_sfc(st_polygon(list(rbind(c(-180,-90),c(180,-90),c(180,90),c(-180,90),c(-180,-90)))),\n crs=st_crs(4326))\n# FIXME:\n#if (sf_extSoftVersion()[\"proj.4\"] >= \"4.9.0\") {\n#  p = st_sample(x, 1000)\n#  st_sample(p, 3)\n#}\n# hexagonal:\nsfc = st_sfc(st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,0)))))\nplot(sfc)\nh = st_sample(sfc, 100, type = \"hexagonal\")\nh1 = st_sample(sfc, 100, type = \"hexagonal\")\nplot(h, add = TRUE)\nplot(h1, col = 'red', add = TRUE)\nc(length(h), length(h1)) # approximate!\npt = st_multipoint(matrix(1:20,,2))\nls = st_sfc(st_linestring(rbind(c(0,0),c(0,1))),\n st_linestring(rbind(c(0,0),c(.1,0))),\n st_linestring(rbind(c(0,1),c(.1,1))),\n st_linestring(rbind(c(2,2),c(2,2.00001))))\nst_sample(ls, 80)\nplot(st_sample(ls, 80))\n# spatstat example:\nif (require(spatstat.random)) {\n  x <- sf::st_sfc(sf::st_polygon(list(rbind(c(0, 0), c(10, 0), c(10, 10), c(0, 0)))))\n  # for spatstat.random::rThomas(), set type = \"Thomas\":\n  pts <- st_sample(x, kappa = 1, mu = 10, scale = 0.1, type = \"Thomas\") \n}\nbbox = st_bbox(\nc(xmin = 0, xmax = 40, ymax = 70, ymin = 60),\n\tcrs = st_crs('OGC:CRS84')\n)\nset.seed(13531)\ns1 = st_sample(bbox, 400)\nst_bbox(s1) # within bbox\ns2 = st_sample(bbox, 400, great_circles = TRUE)\nst_bbox(s2) # outside bbox",
            "st_shift_longitude": "## sfc\npt1 = st_point(c(-170, 50))\npt2 = st_point(c(170, 50))\n(sfc = st_sfc(pt1, pt2))\nsfc = st_set_crs(sfc, 4326)\nst_shift_longitude(sfc)\n\n## sf\nd = st_as_sf(data.frame(id = 1:2, geometry = sfc))\nst_shift_longitude(d)",
            "st_transform": "p1 = st_point(c(7,52))\np2 = st_point(c(-30,20))\nsfc = st_sfc(p1, p2, crs = 4326)\nsfc\nst_transform(sfc, 3857)\nst_transform(st_sf(a=2:1, geom=sfc), \"EPSG:3857\")\nif (sf_extSoftVersion()[\"GDAL\"] >= \"3.0.0\") {\n  st_transform(sfc, pipeline =\n\t  \"+proj=pipeline +step +proj=axisswap +order=2,1\") # reverse axes\n  st_transform(sfc, pipeline =\n\t  \"+proj=pipeline +step +proj=axisswap +order=2,1\", reverse = TRUE) # also reverse axes\n}\nnc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\nst_area(nc[1,]) # area from long/lat\nst_area(st_transform(nc[1,], 32119)) # NC state plane, m\nst_area(st_transform(nc[1,], 2264)) # NC state plane, US foot\nlibrary(units)\nset_units(st_area(st_transform(nc[1,], 2264)), m^2)\nst_transform(structure(p1, proj4string = \"EPSG:4326\"), \"EPSG:3857\")\nst_wrap_dateline(st_sfc(st_linestring(rbind(c(-179,0),c(179,0))), crs = 4326))\nsf_proj_info(\"datum\")",
            "st_viewport": "library(grid)\nnc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\ngrid.newpage()\npushViewport(viewport(width = 0.8, height = 0.8))\npushViewport(st_viewport(nc))\ninvisible(lapply(st_geometry(nc), function(x) grid.draw(st_as_grob(x, gp = gpar(fill = 'red')))))",
            "st_write": "nc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\nst_write(nc, paste0(tempdir(), \"/\", \"nc.shp\"))\nst_write(nc, paste0(tempdir(), \"/\", \"nc.shp\"), delete_layer = TRUE) # overwrites\nif (require(sp, quietly = TRUE)) {\n data(meuse, package = \"sp\") # loads data.frame from sp\n meuse_sf = st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\n # writes X and Y as columns:\n st_write(meuse_sf, paste0(tempdir(), \"/\", \"meuse.csv\"), layer_options = \"GEOMETRY=AS_XY\")\n st_write(meuse_sf, paste0(tempdir(), \"/\", \"meuse.csv\"), layer_options = \"GEOMETRY=AS_WKT\",\n   delete_dsn=TRUE) # overwrites\n\\dontrun{\n library(sp)\n example(meuse, ask = FALSE, echo = FALSE)\n try(st_write(st_as_sf(meuse), \"PG:dbname=postgis\", \"meuse_sf\",\n     layer_options = c(\"OVERWRITE=yes\", \"LAUNDER=true\")))\n demo(nc, ask = FALSE)\n try(st_write(nc, \"PG:dbname=postgis\", \"sids\", layer_options = \"OVERWRITE=true\"))\n}\n}",
            "st_z_range": "a = st_sf(a = 1:2, geom = st_sfc(st_point(0:2), st_point(1:3)), crs = 4326)\nst_z_range(a)\nst_z_range(c(zmin = 16.1, zmax = 16.6), crs = st_crs(4326))",
            "st_zm": "st_zm(st_linestring(matrix(1:32,8)))\nx = st_sfc(st_linestring(matrix(1:32,8)), st_linestring(matrix(1:8,2)))\nst_zm(x)\na = st_sf(a = 1:2, geom=x)\nst_zm(a)",
            "tidyverse": "if (require(dplyr, quietly = TRUE)) {\n nc = read_sf(system.file(\"shape/nc.shp\", package=\"sf\"))\n nc \\%>\\% filter(AREA > .1) \\%>\\% plot()\n # plot 10 smallest counties in grey:\n st_geometry(nc) \\%>\\% plot()\n nc \\%>\\% select(AREA) \\%>\\% arrange(AREA) \\%>\\% slice(1:10) \\%>\\% plot(add = TRUE, col = 'grey')\n title(\"the ten counties with smallest area\")\n nc2 <- nc \\%>\\% mutate(area10 = AREA/10)\n nc \\%>\\% slice(1:2)\n}\n# plot 10 smallest counties in grey:\nif (require(dplyr, quietly = TRUE)) {\n st_geometry(nc) \\%>\\% plot()\n nc \\%>\\% select(AREA) \\%>\\% arrange(AREA) \\%>\\% slice(1:10) \\%>\\% plot(add = TRUE, col = 'grey')\n title(\"the ten counties with smallest area\")\n}\nif (require(dplyr, quietly = TRUE)) {\n nc$area_cl = cut(nc$AREA, c(0, .1, .12, .15, .25))\n nc \\%>\\% group_by(area_cl) \\%>\\% class()\n}\nif (require(dplyr, quietly = TRUE)) {\n nc2 <- nc \\%>\\% mutate(area10 = AREA/10)\n}\nif (require(dplyr, quietly = TRUE)) {\n nc \\%>\\% transmute(AREA = AREA/10) \\%>\\% class()\n}\nif (require(dplyr, quietly = TRUE)) {\n nc \\%>\\% select(SID74, SID79) \\%>\\% names()\n nc \\%>\\% select(SID74, SID79) \\%>\\% class()\n}\nif (require(dplyr, quietly = TRUE)) {\n nc2 <- nc \\%>\\% rename(area = AREA)\n}\nif (require(dplyr, quietly = TRUE)) {\n nc \\%>\\% slice(1:2)\n}\nif (require(dplyr, quietly = TRUE)) {\n nc$area_cl = cut(nc$AREA, c(0, .1, .12, .15, .25))\n nc.g <- nc \\%>\\% group_by(area_cl)\n nc.g \\%>\\% summarise(mean(AREA))\n nc.g \\%>\\% summarise(mean(AREA)) \\%>\\% plot(col = grey(3:6 / 7))\n nc \\%>\\% as.data.frame \\%>\\% summarise(mean(AREA))\n}\nif (require(dplyr, quietly = TRUE)) {\n nc[c(1:100, 1:10), ] \\%>\\% distinct() \\%>\\% nrow()\n}\nif (require(tidyr, quietly = TRUE) && require(dplyr, quietly = TRUE) && \"geometry\" \\%in\\% names(nc)) {\n nc \\%>\\% select(SID74, SID79) \\%>\\% gather(\"VAR\", \"SID\", -geometry) \\%>\\% summary()\n}\nif (require(tidyr, quietly = TRUE) && require(dplyr, quietly = TRUE) && \"geometry\" \\%in\\% names(nc)) {\n nc$row = 1:100 # needed for spread to work\n nc \\%>\\% select(SID74, SID79, geometry, row) \\%>\\%\n\tgather(\"VAR\", \"SID\", -geometry, -row) \\%>\\%\n\tspread(VAR, SID) \\%>\\% head()\n}\nif (require(tidyr, quietly = TRUE) && require(dplyr, quietly = TRUE)) {\n storms.sf = st_as_sf(storms, coords = c(\"long\", \"lat\"), crs = 4326)\n x <- storms.sf \\%>\\% group_by(name, year) \\%>\\% nest\n trs = lapply(x$data, function(tr) st_cast(st_combine(tr), \"LINESTRING\")[[1]]) \\%>\\%\n    st_sfc(crs = 4326)\n trs.sf = st_sf(x[,1:2], trs)\n plot(trs.sf[\"year\"], axes = TRUE)\n}",
            "transform.sf": "a = data.frame(x1 = 1:3, x2 = 5:7)\nst_geometry(a) = st_sfc(st_point(c(0,0)), st_point(c(1,1)), st_point(c(2,2)))\ntransform(a, x1_sq = x1^2)\ntransform(a, x1_x2 = x1*x2)",
            "valid": "p1 = st_as_sfc(\"POLYGON((0 0, 0 10, 10 0, 10 10, 0 0))\")\nst_is_valid(p1)\nst_is_valid(st_sfc(st_point(0:1), p1[[1]]), reason = TRUE)\nlibrary(sf)\nx = st_sfc(st_polygon(list(rbind(c(0,0),c(0.5,0),c(0.5,0.5),c(0.5,0),c(1,0),c(1,1),c(0,1),c(0,0)))))\nsuppressWarnings(st_is_valid(x))\ny = st_make_valid(x)\nst_is_valid(y)\ny \\%>\\% st_cast()"
        }
    },
    "purrr": {
        "description": "A complete and consistent functional programming toolkit for\n    R.",
        "examples": {
            "accumulate": "# With an associative operation, the final value is always the\n# same, no matter the direction. You'll find it in the first element for a\n# backward (left) accumulation, and in the last element for forward\n# (right) one:\n1:5 |> accumulate(`+`)\n1:5 |> accumulate(`+`, .dir = \"backward\")\n\n# The final value is always equal to the equivalent reduction:\n1:5 |> reduce(`+`)\n\n# It is easier to understand the details of the reduction with\n# `paste()`.\naccumulate(letters[1:5], paste, sep = \".\")\n\n# Note how the intermediary reduced values are passed to the left\n# with a left reduction, and to the right otherwise:\naccumulate(letters[1:5], paste, sep = \".\", .dir = \"backward\")\n\n# By ignoring the input vector (nxt), you can turn output of one step into\n# the input for the next. This code takes 10 steps of a random walk:\naccumulate(1:10, \\(acc, nxt) acc + rnorm(1), .init = 0)\n\n# `accumulate2()` is a version of `accumulate()` that works with\n# 3-argument functions and one additional vector:\npaste2 <- function(acc, nxt, sep = \".\") paste(acc, nxt, sep = sep)\nletters[1:4] |> accumulate(paste2)\nletters[1:4] |> accumulate2(c(\"-\", \".\", \"-\"), paste2)\n\n# You can shortcircuit an accumulation and terminate it early by\n# returning a value wrapped in a done(). In the following example\n# we return early if the result-so-far, which is passed on the LHS,\n# meets a condition:\npaste3 <- function(out, input, sep = \".\") {\n  if (nchar(out) > 4) {\n    return(done(out))\n  }\n  paste(out, input, sep = sep)\n}\nletters |> accumulate(paste3)\n\n# Note how we get twice the same value in the accumulation. That's\n# because we have returned it twice. To prevent this, return an empty\n# done box to signal to accumulate() that it should terminate with the\n# value of the last iteration:\npaste3 <- function(out, input, sep = \".\") {\n  if (nchar(out) > 4) {\n    return(done())\n  }\n  paste(out, input, sep = sep)\n}\nletters |> accumulate(paste3)\n\n# Here the early return branch checks the incoming inputs passed on\n# the RHS:\npaste4 <- function(out, input, sep = \".\") {\n  if (input == \"f\") {\n    return(done())\n  }\n  paste(out, input, sep = sep)\n}\nletters |> accumulate(paste4)\n\n\n# Simulating stochastic processes with drift\n\\dontrun{\nlibrary(dplyr)\nlibrary(ggplot2)\n\nmap(1:5, \\(i) rnorm(100)) |>\n  set_names(paste0(\"sim\", 1:5)) |>\n  map(\\(l) accumulate(l, \\(acc, nxt) .05 + acc + nxt)) |>\n  map(\\(x) tibble(value = x, step = 1:100)) |>\n  list_rbind(names_to = \"simulation\") |>\n  ggplot(aes(x = step, y = value)) +\n    geom_line(aes(color = simulation)) +\n    ggtitle(\"Simulations of a random walk with drift\")\n}",
            "along": "x <- 1:5\nseq_along(x)\nlist_along(x)",
            "array-coercion": "# We create an array with 3 dimensions\nx <- array(1:12, c(2, 2, 3))\n\n# A full margin for such an array would be the vector 1:3. This is\n# the default if you don't specify a margin\n\n# Creating a branch along the full margin is equivalent to\n# as.list(array) and produces a list of size length(x):\narray_branch(x) |> str()\n\n# A branch along the first dimension yields a list of length 2\n# with each element containing a 2x3 array:\narray_branch(x, 1) |> str()\n\n# A branch along the first and third dimensions yields a list of\n# length 2x3 whose elements contain a vector of length 2:\narray_branch(x, c(1, 3)) |> str()\n\n# Creating a tree from the full margin creates a list of lists of\n# lists:\narray_tree(x) |> str()\n\n# The ordering and the depth of the tree are controlled by the\n# margin argument:\narray_tree(x, c(3, 1)) |> str()",
            "as_mapper": "as_mapper(\\(x) x + 1)\nas_mapper(1)\n\nas_mapper(c(\"a\", \"b\", \"c\"))\n# Equivalent to function(x) x[[\"a\"]][[\"b\"]][[\"c\"]]\n\nas_mapper(list(1, \"a\", 2))\n# Equivalent to function(x) x[[1]][[\"a\"]][[2]]\n\nas_mapper(list(1, attr_getter(\"a\")))\n# Equivalent to function(x) attr(x[[1]], \"a\")\n\nas_mapper(c(\"a\", \"b\", \"c\"), .default = NA)",
            "as_vector": "# was\nas.list(letters) |> as_vector(\"character\")\n# now\nas.list(letters) |> list_simplify(ptype = character())\n\n# was:\nlist(1:2, 3:4, 5:6) |> as_vector(integer(2))\n# now:\nlist(1:2, 3:4, 5:6) |> list_c(ptype = integer())",
            "attr_getter": "# attr_getter() takes an attribute name and returns a function to\n# access the attribute:\nget_rownames <- attr_getter(\"row.names\")\nget_rownames(mtcars)\n\n# These getter functions are handy in conjunction with pluck() for\n# extracting deeply into a data structure. Here we'll first\n# extract by position, then by attribute:\nobj1 <- structure(\"obj\", obj_attr = \"foo\")\nobj2 <- structure(\"obj\", obj_attr = \"bar\")\nx <- list(obj1, obj2)\n\npluck(x, 1, attr_getter(\"obj_attr\"))  # From first object\npluck(x, 2, attr_getter(\"obj_attr\"))  # From second object",
            "auto_browse": "# For interactive usage, auto_browse() is useful because it automatically\n# starts a browser() in the right place.\nf <- function(x) {\n  y <- 20\n  if (x > 5) {\n    stop(\"!\")\n  } else {\n    x\n  }\n}\nif (interactive()) {\n  map(1:6, auto_browse(f))\n}",
            "chuck": "x <- list(a = 1, b = 2)\n\n# When indexing an element that doesn't exist `[[` sometimes returns NULL:\nx[[\"y\"]]\n# and sometimes errors:\ntry(x[[3]])\n\n# chuck() consistently errors:\ntry(chuck(x, \"y\"))\ntry(chuck(x, 3))",
            "compose": "not_null <- compose(`!`, is.null)\nnot_null(4)\nnot_null(NULL)\n\nadd1 <- function(x) x + 1\ncompose(add1, add1)(8)\n\nfn <- compose(\\(x) paste(x, \"foo\"), \\(x) paste(x, \"bar\"))\nfn(\"input\")\n\n# Lists of functions can be spliced with !!!\nfns <- list(\n  function(x) paste(x, \"foo\"),\n  \\(x) paste(x, \"bar\")\n)\nfn <- compose(!!!fns)\nfn(\"input\")",
            "cross": "# We build all combinations of names, greetings and separators from our\n# list of data and pass each one to paste()\ndata <- list(\n  id = c(\"John\", \"Jane\"),\n  greeting = c(\"Hello.\", \"Bonjour.\"),\n  sep = c(\"! \", \"... \")\n)\n\ndata |>\n  cross() |>\n  map(lift(paste))\n\n# cross() returns the combinations in long format: many elements,\n# each representing one combination. With cross_df() we'll get a\n# data frame in long format: crossing three objects produces a data\n# frame of three columns with each row being a particular\n# combination. This is the same format that expand.grid() returns.\nargs <- data |> cross_df()\n\n# In case you need a list in long format (and not a data frame)\n# just run as.list() after cross_df()\nargs |> as.list()\n\n# This format is often less practical for functional programming\n# because applying a function to the combinations requires a loop\nout <- vector(\"character\", length = nrow(args))\nfor (i in seq_along(out))\n  out[[i]] <- invoke(\"paste\", map(args, i))\nout\n\n# It's easier to transpose and then use invoke_map()\nargs |> transpose() |> map_chr(\\(x) exec(paste, !!!x))\n\n# Unwanted combinations can be filtered out with a predicate function\nfilter <- function(x, y) x >= y\ncross2(1:5, 1:5, .filter = filter) |> str()\n\n# To give names to the components of the combinations, we map\n# setNames() on the product:\nx <- seq_len(3)\ncross2(x, x, .filter = `==`) |>\n  map(setNames, c(\"x\", \"y\"))\n\n# Alternatively we can encapsulate the arguments in a named list\n# before crossing to get named components:\nlist(x = x, y = x) |>\n  cross(.filter = `==`)",
            "detect": "is_even <- function(x) x \\%\\% 2 == 0\n\n3:10 |> detect(is_even)\n3:10 |> detect_index(is_even)\n\n3:10 |> detect(is_even, .dir = \"backward\")\n3:10 |> detect_index(is_even, .dir = \"backward\")\n\n\n# Since `.f` is passed to as_mapper(), you can supply a\n# lambda-formula or a pluck object:\nx <- list(\n  list(1, foo = FALSE),\n  list(2, foo = TRUE),\n  list(3, foo = TRUE)\n)\n\ndetect(x, \"foo\")\ndetect_index(x, \"foo\")\n\n\n# If you need to find all values, use keep():\nkeep(x, \"foo\")\n\n# If you need to find all positions, use map_lgl():\nwhich(map_lgl(x, \"foo\"))",
            "every": "x <- list(0:10, 5.5)\nx |> every(is.numeric)\nx |> every(is.integer)\nx |> some(is.integer)\nx |> none(is.character)\n\n# Missing values are propagated:\nsome(list(NA, FALSE), identity)\n\n# If you need to use these functions in a context where missing values are\n# unsafe (e.g. in `if ()` conditions), make sure to use safe predicates:\nif (some(list(NA, FALSE), rlang::is_true)) \"foo\" else \"bar\"",
            "flatten": "x <- map(1:3, \\(i) sample(4))\nx\n\n# was\nx |> flatten_int() |> str()\n# now\nx |> list_c() |> str()\n\nx <- list(list(1, 2), list(3, 4))\n# was\nx |> flatten() |> str()\n# now\nx |> list_flatten() |> str()",
            "has_element": "x <- list(1:10, 5, 9.9)\nx |> has_element(1:10)\nx |> has_element(3)",
            "head_while": "pos <- function(x) x >= 0\nhead_while(5:-5, pos)\ntail_while(5:-5, negate(pos))\n\nbig <- function(x) x > 100\nhead_while(0:10, big)\ntail_while(0:10, big)",
            "imap": "imap_chr(sample(10), paste)\n\nimap_chr(sample(10), \\(x, idx) paste0(idx, \": \", x))\n\niwalk(mtcars, \\(x, idx) cat(idx, \": \", median(x), \"\\n\", sep = \"\"))",
            "insistently": "# For the purpose of this example, we first create a custom rate\n# object with a low waiting time between attempts:\nrate <- rate_delay(0.1)\n\n# insistently() makes a function repeatedly try to work\nrisky_runif <- function(lo = 0, hi = 1) {\n  y <- runif(1, lo, hi)\n  if(y < 0.9) {\n    stop(y, \" is too small\")\n  }\n  y\n}\n\n# Let's now create an exponential backoff rate with a low waiting\n# time between attempts:\nrate <- rate_backoff(pause_base = 0.1, pause_min = 0.005, max_times = 4)\n\n# Modify your function to run insistently.\ninsistent_risky_runif <- insistently(risky_runif, rate, quiet = FALSE)\n\nset.seed(6) # Succeeding seed\ninsistent_risky_runif()\n\nset.seed(3) # Failing seed\ntry(insistent_risky_runif())\n\n# You can also use other types of rate settings, like a delay rate\n# that waits for a fixed amount of time. Be aware that a delay rate\n# has an infinite amount of attempts by default:\nrate <- rate_delay(0.2, max_times = 3)\ninsistent_risky_runif <- insistently(risky_runif, rate = rate, quiet = FALSE)\ntry(insistent_risky_runif())\n\n# insistently() and possibly() are a useful combination\nrate <- rate_backoff(pause_base = 0.1, pause_min = 0.005)\npossibly_insistent_risky_runif <- possibly(insistent_risky_runif, otherwise = -99)\n\nset.seed(6)\npossibly_insistent_risky_runif()\n\nset.seed(3)\npossibly_insistent_risky_runif()",
            "invoke": "# was\ninvoke(runif, list(n = 10))\ninvoke(runif, n = 10)\n# now\nexec(runif, n = 10)\n\n# was\nargs <- list(\"01a\", \"01b\")\ninvoke(paste, args, sep = \"-\")\n# now\nexec(paste, !!!args, sep = \"-\")\n\n# was\nfuns <- list(runif, rnorm)\nfuns |> invoke_map(n = 5)\nfuns |> invoke_map(list(list(n = 10), list(n = 5)))\n\n# now\nfuns |> map(exec, n = 5)\nfuns |> map2(list(list(n = 10), list(n = 5)), function(f, args) exec(f, !!!args))\n\n# or use pmap + a tibble\ndf <- tibble::tibble(\n  fun = list(runif, rnorm),\n  args = list(list(n = 10), list(n = 5))\n)\ndf |> pmap(function(fun, args) exec(fun, !!!args))\n\n\n# was\nlist(m1 = mean, m2 = median) |> invoke_map(x = rcauchy(100))\n# now\nlist(m1 = mean, m2 = median) |> map(function(f) f(rcauchy(100)))",
            "keep": "rep(10, 10) |>\n  map(sample, 5) |>\n  keep(function(x) mean(x) > 6)\n\n# Or use a formula\nrep(10, 10) |>\n  map(sample, 5) |>\n  keep(\\(x) mean(x) > 6)\n\n# Using a string instead of a function will select all list elements\n# where that subelement is TRUE\nx <- rerun(5, a = rbernoulli(1), b = sample(10))\nx\nx |> keep(\"a\")\nx |> discard(\"a\")\n\n# compact() discards elements that are NULL or that have length zero\nlist(a = \"a\", b = NULL, c = integer(0), d = NA, e = list()) |>\n  compact()",
            "keep_at": "x <- c(a = 1, b = 2, cat = 10, dog = 15, elephant = 5, e = 10)\nx |> keep_at(letters)\nx |> discard_at(letters)\n\n# Can also use a function\nx |> keep_at(~ nchar(.x) == 3)\nx |> discard_at(~ nchar(.x) == 3)",
            "lift": "### Lifting from ... to list(...) or c(...)\n\nx <- list(x = c(1:100, NA, 1000), na.rm = TRUE, trim = 0.9)\nlift_dl(mean)(x)\n# You can also use the lift() alias for this common operation:\nlift(mean)(x)\n# now:\nexec(mean, !!!x)\n\n# Default arguments can also be specified directly in lift_dl()\nlist(c(1:100, NA, 1000)) |> lift_dl(mean, na.rm = TRUE)()\n# now:\nmean(c(1:100, NA, 1000), na.rm = TRUE)\n\n# lift_dl() and lift_ld() are inverse of each other.\n# Here we transform sum() so that it takes a list\nfun <- sum |> lift_dl()\nfun(list(3, NA, 4, na.rm = TRUE))\n# now:\nfun <- function(x) exec(\"sum\", !!!x)\nexec(sum, 3, NA, 4, na.rm = TRUE)\n### Lifting from c(...) to list(...) or ...\n\n# In other situations we need the vector-valued function to take a\n# variable number of arguments as with pmap(). This is a job for\n# lift_vd():\npmap_dbl(mtcars, lift_vd(mean))\n# now\npmap_dbl(mtcars, \\(...) mean(c(...)))\n### Lifting from list(...) to c(...) or ...\n\n# This kind of lifting is sometimes needed for function\n# composition. An example would be to use pmap() with a function\n# that takes a list. In the following, we use some() on each row of\n# a data frame to check they each contain at least one element\n# satisfying a condition:\nmtcars |> pmap_lgl(lift_ld(some, partial(`<`, 200)))\n# now\nmtcars |> pmap_lgl(\\(...) any(c(...) > 200))",
            "list_assign": "x <- list(x = 1:10, y = 4, z = list(a = 1, b = 2))\nstr(x)\n\n# Update values\nstr(list_assign(x, a = 1))\n\n# Replace values\nstr(list_assign(x, z = 5))\nstr(list_assign(x, z = NULL))\nstr(list_assign(x, z = list(a = 1:5)))\n\n# Replace recursively with list_modify(), leaving the other elements of z alone\nstr(list_modify(x, z = list(a = 1:5)))\n\n# Remove values\nstr(list_assign(x, z = zap()))\n\n# Combine values with list_merge()\nstr(list_merge(x, x = 11, z = list(a = 2:5, c = 3)))\n\n# All these functions support dynamic dots features. Use !!! to splice\n# a list of arguments:\nl <- list(new = 1, y = zap(), z = 5)\nstr(list_assign(x, !!!l))",
            "list_c": "x1 <- list(a = 1, b = 2, c = 3)\nlist_c(x1)\n\nx2 <- list(\n  a = data.frame(x = 1:2),\n  b = data.frame(y = \"a\")\n)\nlist_rbind(x2)\nlist_rbind(x2, names_to = \"id\")\nlist_rbind(unname(x2), names_to = \"id\")\n\nlist_cbind(x2)",
            "list_flatten": "x <- list(1, list(2, 3), list(4, list(5)))\nx |> list_flatten() |> str()\nx |> list_flatten() |> list_flatten() |> str()\n\n# Flat lists are left as is\nlist(1, 2, 3, 4, 5) |> list_flatten() |> str()\n\n# Empty lists will disappear\nlist(1, list(), 2, list(3)) |> list_flatten() |> str()\n\n# Another way to see this is that it reduces the depth of the list\nx <- list(\n  list(),\n  list(list())\n)\nx |> pluck_depth()\nx |> list_flatten() |> pluck_depth()\n\n# Use name_spec to control how inner and outer names are combined\nx <- list(x = list(a = 1, b = 2), y = list(c = 1, d = 2))\nx |> list_flatten() |> names()\nx |> list_flatten(name_spec = \"{outer}\") |> names()\nx |> list_flatten(name_spec = \"{inner}\") |> names()",
            "list_simplify": "list_simplify(list(1, 2, 3))\n\n# Only works when vectors are length one and have compatible types:\ntry(list_simplify(list(1, 2, 1:3)))\ntry(list_simplify(list(1, 2, \"x\")))\n\n# Unless you strict = FALSE, in which case you get the input back:\nlist_simplify(list(1, 2, 1:3), strict = FALSE)\nlist_simplify(list(1, 2, \"x\"), strict = FALSE)",
            "list_transpose": "# list_transpose() is useful in conjunction with safely()\nx <- list(\"a\", 1, 2)\ny <- x |> map(safely(log))\ny |> str()\n# Put all the errors and results together\ny |> list_transpose() |> str()\n# Supply a default result to further simplify\ny |> list_transpose(default = list(result = NA)) |> str()\n\n# list_transpose() will try to simplify by default:\nx <- list(list(a = 1, b = 2), list(a = 3, b = 4), list(a = 5, b = 6))\nx |> list_transpose()\n# this makes list_tranpose() not completely symmetric\nx |> list_transpose() |> list_transpose()\n\n# use simplify = FALSE to always return lists:\nx |> list_transpose(simplify = FALSE) |> str()\nx |>\n  list_transpose(simplify = FALSE) |>\n  list_transpose(simplify = FALSE) |> str()\n\n# Provide an explicit template if you know which elements you want to extract\nll <- list(\n  list(x = 1, y = \"one\"),\n  list(z = \"deux\", x = 2)\n)\nll |> list_transpose()\nll |> list_transpose(template = c(\"x\", \"y\", \"z\"))\nll |> list_transpose(template = 1)\n\n# And specify a default if you want to simplify\nll |> list_transpose(template = c(\"x\", \"y\", \"z\"), default = NA)",
            "lmap": "set.seed(1014)\n\n# Let's write a function that returns a larger list or an empty list\n# depending on some condition. It also uses the input name to name the\n# output\nmaybe_rep <- function(x) {\n  n <- rpois(1, 2)\n  set_names(rep_len(x, n), paste0(names(x), seq_len(n)))\n}\n\n# The output size varies each time we map f()\nx <- list(a = 1:4, b = letters[5:7], c = 8:9, d = letters[10])\nx |> lmap(maybe_rep) |> str()\n\n# We can apply f() on a selected subset of x\nx |> lmap_at(c(\"a\", \"d\"), maybe_rep) |> str()\n\n# Or only where a condition is satisfied\nx |> lmap_if(is.character, maybe_rep) |> str()",
            "map": "# Compute normal distributions from an atomic vector\n1:10 |>\n  map(rnorm, n = 10)\n\n# You can also use an anonymous function\n1:10 |>\n  map(\\(x) rnorm(10, x))\n\n# Simplify output to a vector instead of a list by computing the mean of the distributions\n1:10 |>\n  map(rnorm, n = 10) |>  # output a list\n  map_dbl(mean)           # output an atomic vector\n\n# Using set_names() with character vectors is handy to keep track\n# of the original inputs:\nset_names(c(\"foo\", \"bar\")) |> map_chr(paste0, \":suffix\")\n\n# Working with lists\nfavorite_desserts <- list(Sophia = \"banana bread\", Eliott = \"pancakes\", Karina = \"chocolate cake\")\nfavorite_desserts |> map_chr(\\(food) paste(food, \"rocks!\"))\n\n# Extract by name or position\n# .default specifies value for elements that are missing or NULL\nl1 <- list(list(a = 1L), list(a = NULL, b = 2L), list(b = 3L))\nl1 |> map(\"a\", .default = \"???\")\nl1 |> map_int(\"b\", .default = NA)\nl1 |> map_int(2, .default = NA)\n\n# Supply multiple values to index deeply into a list\nl2 <- list(\n  list(num = 1:3,     letters[1:3]),\n  list(num = 101:103, letters[4:6]),\n  list()\n)\nl2 |> map(c(2, 2))\n\n# Use a list to build an extractor that mixes numeric indices and names,\n# and .default to provide a default value if the element does not exist\nl2 |> map(list(\"num\", 3))\nl2 |> map_int(list(\"num\", 3), .default = NA)\n\n# Working with data frames\n# Use map_lgl(), map_dbl(), etc to return a vector instead of a list:\nmtcars |> map_dbl(sum)\n\n# A more realistic example: split a data frame into pieces, fit a\n# model to each piece, summarise and extract R^2\nmtcars |>\n  split(mtcars$cyl) |>\n  map(\\(df) lm(mpg ~ wt, data = df)) |>\n  map(summary) |>\n  map_dbl(\"r.squared\")\n\n\\dontshow{if (interactive() && requireNamespace(\"mirai\", quietly = TRUE) && requireNamespace(\"carrier\", quietly = TRUE)) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# Run in interactive sessions only as spawns additional processes\n\n# To use parallelized map, set daemons (number of parallel processes) first:\nmirai::daemons(2)\n\nmtcars |> map_dbl(sum, .parallel = TRUE)\n\n1:10 |>\n  map(function(x) stats::rnorm(10, mean = x), .parallel = TRUE) |>\n  map_dbl(mean, .parallel = TRUE)\n\nmirai::daemons(0)\n\\dontshow{\\}) # examplesIf}",
            "map2": "x <- list(1, 1, 1)\ny <- list(10, 20, 30)\n\nmap2(x, y, \\(x, y) x + y)\n# Or just\nmap2(x, y, `+`)\n\n# Split into pieces, fit model to each piece, then predict\nby_cyl <- mtcars |> split(mtcars$cyl)\nmods <- by_cyl |> map(\\(df) lm(mpg ~ wt, data = df))\nmap2(mods, by_cyl, predict)",
            "map_depth": "# map_depth() -------------------------------------------------\n# Use `map_depth()` to recursively traverse nested vectors and map\n# a function at a certain depth:\nx <- list(a = list(foo = 1:2, bar = 3:4), b = list(baz = 5:6))\nx |> str()\nx |> map_depth(2, \\(y) paste(y, collapse = \"/\")) |> str()\n\n# Equivalent to:\nx |> map(\\(y) map(y, \\(z) paste(z, collapse = \"/\"))) |> str()\n\n# When ragged is TRUE, `.f()` will also be passed leaves at depth < `.depth`\nx <- list(1, list(1, list(1, list(1, 1))))\nx |> str()\nx |> map_depth(4, \\(x) length(unlist(x)), .ragged = TRUE) |> str()\nx |> map_depth(3, \\(x) length(unlist(x)), .ragged = TRUE) |> str()\nx |> map_depth(2, \\(x) length(unlist(x)), .ragged = TRUE) |> str()\nx |> map_depth(1, \\(x) length(unlist(x)), .ragged = TRUE) |> str()\nx |> map_depth(0, \\(x) length(unlist(x)), .ragged = TRUE) |> str()\n\n# modify_depth() -------------------------------------------------\nl1 <- list(\n  obj1 = list(\n    prop1 = list(param1 = 1:2, param2 = 3:4),\n    prop2 = list(param1 = 5:6, param2 = 7:8)\n  ),\n  obj2 = list(\n    prop1 = list(param1 = 9:10, param2 = 11:12),\n    prop2 = list(param1 = 12:14, param2 = 15:17)\n  )\n)\n\n# In the above list, \"obj\" is level 1, \"prop\" is level 2 and \"param\"\n# is level 3. To apply sum() on all params, we map it at depth 3:\nl1 |> modify_depth(3, sum) |> str()\n\n# modify() lets us pluck the elements prop1/param2 in obj1 and obj2:\nl1 |> modify(c(\"prop1\", \"param2\")) |> str()\n\n# But what if we want to pluck all param2 elements? Then we need to\n# act at a lower level:\nl1 |> modify_depth(2, \"param2\") |> str()\n\n# modify_depth() can be with other purrr functions to make them operate at\n# a lower level. Here we ask pmap() to map paste() simultaneously over all\n# elements of the objects at the second level. paste() is effectively\n# mapped at level 3.\nl1 |> modify_depth(2, \\(x) pmap(x, paste, sep = \" / \")) |> str()",
            "map_dfr": "# map ---------------------------------------------\n# Was:\nmtcars |>\n  split(mtcars$cyl) |>\n  map(\\(df) lm(mpg ~ wt, data = df)) |>\n  map_dfr(\\(mod) as.data.frame(t(as.matrix(coef(mod)))))\n\n# Now:\nmtcars |>\n  split(mtcars$cyl) |>\n  map(\\(df) lm(mpg ~ wt, data = df)) |>\n  map(\\(mod) as.data.frame(t(as.matrix(coef(mod))))) |>\n  list_rbind()\n\n# for certain pathological inputs `map_dfr()` and `map_dfc()` actually\n# both combine the list by column\ndf <- data.frame(\n  x = c(\" 13\", \"  15 \"),\n  y = c(\"  34\",  \" 67 \")\n)\n\n# Was:\nmap_dfr(df, trimws)\nmap_dfc(df, trimws)\n\n# But list_rbind()/list_cbind() fail because they require data frame inputs\ntry(map(df, trimws) |> list_rbind())\n\n# Instead, use modify() to apply a function to each column of a data frame\nmodify(df, trimws)\n\n# map2 ---------------------------------------------\n\nex_fun <- function(arg1, arg2){\n  col <- arg1 + arg2\n  x <- as.data.frame(col)\n}\narg1 <- 1:4\narg2 <- 10:13\n\n# was\nmap2_dfr(arg1, arg2, ex_fun)\n# now\nmap2(arg1, arg2, ex_fun) |> list_rbind()\n\n# was\nmap2_dfc(arg1, arg2, ex_fun)\n# now\nmap2(arg1, arg2, ex_fun) |> list_cbind()",
            "map_if": "# Use a predicate function to decide whether to map a function:\niris |> map_if(is.factor, as.character) |> str()\n\n# Specify an alternative with the `.else` argument:\niris |> map_if(is.factor, as.character, .else = as.integer) |> str()\n\n# Use numeric vector of positions select elements to change:\niris |> map_at(c(4, 5), is.numeric) |> str()\n\n# Use vector of names to specify which elements to change:\niris |> map_at(\"Species\", toupper) |> str()",
            "modify": "# Convert factors to characters\niris |>\n  modify_if(is.factor, as.character) |>\n  str()\n\n# Specify which columns to map with a numeric vector of positions:\nmtcars |> modify_at(c(1, 4, 5), as.character) |> str()\n\n# Or with a vector of names:\nmtcars |> modify_at(c(\"cyl\", \"am\"), as.character) |> str()\n\nlist(x = sample(c(TRUE, FALSE), 100, replace = TRUE), y = 1:100) |>\n  list_transpose(simplify = FALSE) |>\n  modify_if(\"x\", \\(l) list(x = l$x, y = l$y * 100)) |>\n  list_transpose()\n\n# Use modify2() to map over two vectors and preserve the type of\n# the first one:\nx <- c(foo = 1L, bar = 2L)\ny <- c(TRUE, FALSE)\nmodify2(x, y, \\(x, cond) if (cond) x else 0L)\n\n# Use a predicate function to decide whether to map a function:\nmodify_if(iris, is.factor, as.character)\n\n# Specify an alternative with the `.else` argument:\nmodify_if(iris, is.factor, as.character, .else = as.integer)",
            "modify_in": "# Recall that pluck() returns a component of a data structure that\n# might be arbitrarily deep\nx <- list(list(bar = 1, foo = 2))\npluck(x, 1, \"foo\")\n\n# Use assign_in() to modify the pluck location:\nstr(assign_in(x, list(1, \"foo\"), 100))\n# Or zap to remove it\nstr(assign_in(x, list(1, \"foo\"), zap()))\n\n# Like pluck(), this works even when the element (or its parents) don't exist\npluck(x, 1, \"baz\")\nstr(assign_in(x, list(2, \"baz\"), 100))\n\n# modify_in() applies a function to that location and update the\n# element in place:\nmodify_in(x, list(1, \"foo\"), \\(x) x * 200)\n\n# Additional arguments are passed to the function in the ordinary way:\nmodify_in(x, list(1, \"foo\"), `+`, 100)",
            "modify_tree": "x <- list(list(a = 2:1, c = list(b1 = 2), b = list(c2 = 3, c1 = 4)))\nx |> str()\n\n# Transform each leaf\nx |> modify_tree(leaf = \\(x) x + 100) |>  str()\n\n# Recursively sort the nodes\nsort_named <- function(x) {\n  nms <- names(x)\n  if (!is.null(nms)) {\n    x[order(nms)]\n  } else {\n    x\n   }\n}\nx |> modify_tree(post = sort_named) |> str()",
            "negate": "x <- list(x = 1:10, y = rbernoulli(10), z = letters)\nx |> keep(is.numeric) |> names()\nx |> keep(negate(is.numeric)) |> names()\n# Same as\nx |> discard(is.numeric)",
            "partial": "# Partial is designed to replace the use of anonymous functions for\n# filling in function arguments. Instead of:\ncompact1 <- function(x) discard(x, is.null)\n\n# we can write:\ncompact2 <- partial(discard, .p = is.null)\n\n# partial() works fine with functions that do non-standard\n# evaluation\nmy_long_variable <- 1:10\nplot2 <- partial(plot, my_long_variable)\nplot2()\nplot2(runif(10), type = \"l\")\n\n# Note that you currently can't partialise arguments multiple times:\nmy_mean <- partial(mean, na.rm = TRUE)\nmy_mean <- partial(my_mean, na.rm = FALSE)\ntry(my_mean(1:10))\n\n\n# The evaluation of arguments normally occurs \"lazily\". Concretely,\n# this means that arguments are repeatedly evaluated across invocations:\nf <- partial(runif, n = rpois(1, 5))\nf\nf()\nf()\n\n# You can unquote an argument to fix it to a particular value.\n# Unquoted arguments are evaluated only once when the function is created:\nf <- partial(runif, n = !!rpois(1, 5))\nf\nf()\nf()\n\n\n# By default, partialised arguments are passed before new ones:\nmy_list <- partial(list, 1, 2)\nmy_list(\"foo\")\n\n# Control the position of these arguments by passing an empty\n# `... = ` argument:\nmy_list <- partial(list, 1, ... = , 2)\nmy_list(\"foo\")",
            "pluck": "# Let's create a list of data structures:\nobj1 <- list(\"a\", list(1, elt = \"foo\"))\nobj2 <- list(\"b\", list(2, elt = \"bar\"))\nx <- list(obj1, obj2)\n\n# pluck() provides a way of retrieving objects from such data\n# structures using a combination of numeric positions, vector or\n# list names, and accessor functions.\n\n# Numeric positions index into the list by position, just like `[[`:\npluck(x, 1)\n# same as x[[1]]\n\n# Index from the back\npluck(x, -1)\n# same as x[[2]]\n\npluck(x, 1, 2)\n# same as x[[1]][[2]]\n\n# Supply names to index into named vectors:\npluck(x, 1, 2, \"elt\")\n# same as x[[1]][[2]][[\"elt\"]]\n\n# By default, pluck() consistently returns `NULL` when an element\n# does not exist:\npluck(x, 10)\ntry(x[[10]])\n\n# You can also supply a default value for non-existing elements:\npluck(x, 10, .default = NA)\n\n# The map() functions use pluck() by default to retrieve multiple\n# values from a list:\nmap_chr(x, 1)\nmap_int(x, c(2, 1))\n\n# pluck() also supports accessor functions:\nmy_element <- function(x) x[[2]]$elt\npluck(x, 1, my_element)\npluck(x, 2, my_element)\n\n# Even for this simple data structure, this is more readable than\n# the alternative form because it requires you to read both from\n# right-to-left and from left-to-right in different parts of the\n# expression:\nmy_element(x[[1]])\n\n# If you have a list of accessors, you can splice those in with `!!!`:\nidx <- list(1, my_element)\npluck(x, !!!idx)",
            "pluck_depth": "x <- list(\n  list(),\n  list(list()),\n  list(list(list(1)))\n)\npluck_depth(x)\nx |> map_int(pluck_depth)",
            "pmap": "x <- list(1, 1, 1)\ny <- list(10, 20, 30)\nz <- list(100, 200, 300)\npmap(list(x, y, z), sum)\n\n# Matching arguments by position\npmap(list(x, y, z), function(first, second, third) (first + third) * second)\n\n# Matching arguments by name\nl <- list(a = x, b = y, c = z)\npmap(l, function(c, b, a) (a + c) * b)\n\n# Vectorizing a function over multiple arguments\ndf <- data.frame(\n  x = c(\"apple\", \"banana\", \"cherry\"),\n  pattern = c(\"p\", \"n\", \"h\"),\n  replacement = c(\"P\", \"N\", \"H\"),\n  stringsAsFactors = FALSE\n  )\npmap(df, gsub)\npmap_chr(df, gsub)\n\n# Use `...` to absorb unused components of input list .l\ndf <- data.frame(\n  x = 1:3,\n  y = 10:12,\n  z = letters[1:3]\n)\nplus <- function(x, y) x + y\n\\dontrun{\n# this won't work\npmap(df, plus)\n}\n# but this will\nplus2 <- function(x, y, ...) x + y\npmap_dbl(df, plus2)\n\n# The \"p\" for \"parallel\" in pmap() is the same as in base::pmin()\n# and base::pmax()\ndf <- data.frame(\n  x = c(1, 2, 5),\n  y = c(5, 4, 8)\n)\n# all produce the same result\npmin(df$x, df$y)\nmap2_dbl(df$x, df$y, min)\npmap_dbl(df, min)",
            "possibly": "# To replace errors with a default value, use possibly().\nlist(\"a\", 10, 100) |>\n  map_dbl(possibly(log, NA_real_))\n\n# The default, NULL, will be discarded with `list_c()`\nlist(\"a\", 10, 100) |>\n  map(possibly(log)) |>\n  list_c()",
            "prepend": "x <- as.list(1:3)\n\nx |> append(\"a\")\nx |> prepend(\"a\")\nx |> prepend(list(\"a\", \"b\"), before = 3)\nprepend(list(), x)",
            "quietly": "f <- function() {\n  print(\"Hi!\")\n  message(\"Hello\")\n  warning(\"How are ya?\")\n  \"Gidday\"\n}\nf()\n\nf_quiet <- quietly(f)\nstr(f_quiet())",
            "rate-helpers": "# A delay rate waits the same amount of time:\nrate <- rate_delay(0.02)\nfor (i in 1:3) rate_sleep(rate, quiet = FALSE)\n\n# A backoff rate waits exponentially longer each time, with random\n# jitter by default:\nrate <- rate_backoff(pause_base = 0.2, pause_min = 0.005)\nfor (i in 1:3) rate_sleep(rate, quiet = FALSE)",
            "rbernoulli": "rbernoulli(10)\nrbernoulli(100, 0.1)",
            "rdunif": "table(rdunif(1e3, 10))\ntable(rdunif(1e3, 10, -5))",
            "reduce": "# Reducing `+` computes the sum of a vector while reducing `*`\n# computes the product:\n1:3 |> reduce(`+`)\n1:10 |> reduce(`*`)\n\n# By ignoring the input vector (nxt), you can turn output of one step into\n# the input for the next. This code takes 10 steps of a random walk:\nreduce(1:10, \\(acc, nxt) acc + rnorm(1), .init = 0)\n\n# When the operation is associative, the direction of reduction\n# does not matter:\nreduce(1:4, `+`)\nreduce(1:4, `+`, .dir = \"backward\")\n\n# However with non-associative operations, the reduced value will\n# be different as a function of the direction. For instance,\n# `list()` will create left-leaning lists when reducing from the\n# right, and right-leaning lists otherwise:\nstr(reduce(1:4, list))\nstr(reduce(1:4, list, .dir = \"backward\"))\n\n# reduce2() takes a ternary function and a second vector that is\n# one element smaller than the first vector:\npaste2 <- function(x, y, sep = \".\") paste(x, y, sep = sep)\nletters[1:4] |> reduce(paste2)\nletters[1:4] |> reduce2(c(\"-\", \".\", \"-\"), paste2)\n\nx <- list(c(0, 1), c(2, 3), c(4, 5))\ny <- list(c(6, 7), c(8, 9))\nreduce2(x, y, paste)\n\n\n# You can shortcircuit a reduction and terminate it early by\n# returning a value wrapped in a done(). In the following example\n# we return early if the result-so-far, which is passed on the LHS,\n# meets a condition:\npaste3 <- function(out, input, sep = \".\") {\n  if (nchar(out) > 4) {\n    return(done(out))\n  }\n  paste(out, input, sep = sep)\n}\nletters |> reduce(paste3)\n\n# Here the early return branch checks the incoming inputs passed on\n# the RHS:\npaste4 <- function(out, input, sep = \".\") {\n  if (input == \"j\") {\n    return(done(out))\n  }\n  paste(out, input, sep = sep)\n}\nletters |> reduce(paste4)",
            "rerun": "# old\n5 |> rerun(rnorm(5)) |> str()\n# new\n1:5 |> map(\\(i) rnorm(5)) |> str()\n\n# old\n5 |>\n  rerun(x = rnorm(5), y = rnorm(5)) |>\n  map_dbl(\\(l) cor(l$x, l$y))\n# new\n1:5 |>\n  map(\\(i) list(x = rnorm(5), y = rnorm(5))) |>\n  map_dbl(\\(l) cor(l$x, l$y))",
            "safely": "safe_log <- safely(log)\nsafe_log(10)\nsafe_log(\"a\")\n\nlist(\"a\", 10, 100) |>\n  map(safe_log) |>\n  transpose()\n\n# This is a bit easier to work with if you supply a default value\n# of the same type and use the simplify argument to transpose():\nsafe_log <- safely(log, otherwise = NA_real_)\nlist(\"a\", 10, 100) |>\n  map(safe_log) |>\n  transpose() |>\n  simplify_all()",
            "slowly": "# For these example, we first create a custom rate\n# with a low waiting time between attempts:\nrate <- rate_delay(0.1)\n\n# slowly() causes a function to sleep for a given time between calls:\nslow_runif <- slowly(\\(x) runif(1), rate = rate, quiet = FALSE)\nout <- map(1:5, slow_runif)",
            "splice": "inputs <- list(arg1 = \"a\", arg2 = \"b\")\n\n# splice() concatenates the elements of inputs with arg3\nsplice(inputs, arg3 = c(\"c1\", \"c2\")) |> str()\nlist(inputs, arg3 = c(\"c1\", \"c2\")) |> str()\nc(inputs, arg3 = c(\"c1\", \"c2\")) |> str()",
            "transpose": "x <- map(1:5, \\(i) list(x = runif(1), y = runif(5)))\n# was\nx |> transpose() |> str()\n# now\nx |> list_transpose(simplify = FALSE) |> str()\n\n# transpose() is useful in conjunction with safely() & quietly()\nx <- list(\"a\", 1, 2)\ny <- x |> map(safely(log))\n# was\ny |> transpose() |> str()\n# now:\ny |> list_transpose() |> str()\n\n# Previously, output simplification required a call to another function\nx <- list(list(a = 1, b = 2), list(a = 3, b = 4), list(a = 5, b = 6))\nx |> transpose() |> simplify_all()\n# Now can take advantage of automatic simplification\nx |> list_transpose()\n\n# Provide explicit component names to prevent loss of those that don't\n# appear in first component\nll <- list(\n  list(x = 1, y = \"one\"),\n  list(z = \"deux\", x = 2)\n)\nll |> transpose()\nnms <- ll |> map(names) |> reduce(union)\n# was\nll |> transpose(.names = nms)\n# now\nll |> list_transpose(template = nms)\n# and can supply default value\nll |> list_transpose(template = nms, default = NA)",
            "when": "1:10 |>\n  when(\n    sum(.) <=  50 ~ sum(.),\n    sum(.) <= 100 ~ sum(.)/2,\n    ~ 0\n  )\n\n# now\nx <- 1:10\nif (sum(x) < 10) {\n  sum(x)\n} else if (sum(x) < 100) {\n  sum(x) / 2\n} else {\n  0\n}"
        }
    },
    "httr": {
        "description": "Useful tools for working with HTTP organised by HTTP verbs\n    (GET(), POST(), etc). Configuration functions make it easy to control\n    additional request components (authenticate(), add_headers() and so\n    on).",
        "examples": {
            "BROWSE": "BROWSE(\"http://google.com\")\nBROWSE(\"http://had.co.nz\")",
            "DELETE": "\\dontrun{\nDELETE(\"http://httpbin.org/delete\")\nPOST(\"http://httpbin.org/delete\")\n}",
            "GET": "GET(\"http://google.com/\")\n\\dontrun{\nGET(\"http://google.com/\", path = \"search\")\nGET(\"http://google.com/\", path = \"search\", query = list(q = \"ham\"))\n}\n\n# See what GET is doing with httpbin.org\n\\dontrun{\nurl <- \"http://httpbin.org/get\"\nGET(url)\nGET(url, add_headers(a = 1, b = 2))\nGET(url, set_cookies(a = 1, b = 2))\nGET(url, add_headers(a = 1, b = 2), set_cookies(a = 1, b = 2))\nGET(url, authenticate(\"username\", \"password\"))\nGET(url, verbose())\n}\n\n# You might want to manually specify the handle so you can have multiple\n# independent logins to the same website.\n\\dontrun{\ngoogle <- handle(\"http://google.com\")\nGET(handle = google, path = \"/\")\nGET(handle = google, path = \"search\")\n}",
            "HEAD": "HEAD(\"http://google.com\")\nheaders(HEAD(\"http://google.com\"))",
            "POST": "\\dontrun{\nb2 <- \"http://httpbin.org/post\"\nPOST(b2, body = \"A simple text string\")\nPOST(b2, body = list(x = \"A simple text string\"))\nPOST(b2, body = list(y = upload_file(system.file(\"CITATION\"))))\nPOST(b2, body = list(x = \"A simple text string\"), encode = \"json\")\n\n# body can also be provided as a json string directly to deal\n# with specific case, like an empty element in the json string.\n# passing as string directly\nPOST(b2, body = '{\"a\":1,\"b\":{}}', encode = \"raw\")\n# or building the json string before\njson_body <- jsonlite::toJSON(list(a = 1, b = NULL), auto_unbox = TRUE)\nPOST(b2, body = json_body, encode = \"raw\")\n\n# Various types of empty body:\nPOST(b2, body = NULL, verbose())\nPOST(b2, body = FALSE, verbose())\nPOST(b2, body = \"\", verbose())\n}",
            "PUT": "\\dontrun{\nPOST(\"http://httpbin.org/put\")\nPUT(\"http://httpbin.org/put\")\n\nb2 <- \"http://httpbin.org/put\"\nPUT(b2, body = \"A simple text string\")\nPUT(b2, body = list(x = \"A simple text string\"))\nPUT(b2, body = list(y = upload_file(system.file(\"CITATION\"))))\nPUT(b2, body = list(x = \"A simple text string\"), encode = \"json\")\n}",
            "RETRY": "\\dontrun{\n# Succeeds straight away\nRETRY(\"GET\", \"http://httpbin.org/status/200\")\n# Never succeeds\nRETRY(\"GET\", \"http://httpbin.org/status/500\")\n# Invalid hostname generates curl error condition and is retried but eventually\n# raises an error condition.\nRETRY(\"GET\", \"http://invalidhostname/\")\n}",
            "VERB": "r <- VERB(\n  \"PROPFIND\", \"http://svn.r-project.org/R/tags/\",\n  add_headers(depth = 1), verbose()\n)\nstop_for_status(r)\ncontent(r)\n\n\\dontrun{\nVERB(\"POST\", url = \"http://httpbin.org/post\")\nVERB(\"POST\", url = \"http://httpbin.org/post\", body = \"foobar\")\n}",
            "add_headers": "add_headers(a = 1, b = 2)\nadd_headers(.headers = c(a = \"1\", b = \"2\"))\n\n\\dontrun{\nGET(\"http://httpbin.org/headers\")\n\n# Add arbitrary headers\nGET(\n  \"http://httpbin.org/headers\",\n  add_headers(version = version$version.string)\n)\n\n# Override default headers with empty strings\nGET(\"http://httpbin.org/headers\", add_headers(Accept = \"\"))\n}",
            "authenticate": "\\dontrun{\nGET(\"http://httpbin.org/basic-auth/user/passwd\")\nGET(\n  \"http://httpbin.org/basic-auth/user/passwd\",\n  authenticate(\"user\", \"passwd\")\n)\n}",
            "cache_info": "# Never cached, always causes redownload\nr1 <- GET(\"https://www.google.com\")\ncache_info(r1)\nr1$date\nrerequest(r1)$date\n\n# Expires in a year\nr2 <- GET(\"https://www.google.com/images/srpr/logo11w.png\")\ncache_info(r2)\nr2$date\nrerequest(r2)$date\n\n\\dontrun{\n# Has last-modified and etag, so does revalidation\nr3 <- GET(\"http://httpbin.org/cache\")\ncache_info(r3)\nr3$date\nrerequest(r3)$date\n\n# Expires after 5 seconds\nr4 <- GET(\"http://httpbin.org/cache/5\")\ncache_info(r4)\nr4$date\nrerequest(r4)$date\nSys.sleep(5)\ncache_info(r4)\nrerequest(r4)$date\n}",
            "config": "# There are a number of ways to modify the configuration of a request\n# * you can add directly to a request\nHEAD(\"https://www.google.com\", verbose())\n\n# * you can wrap with with_config()\nwith_config(verbose(), HEAD(\"https://www.google.com\"))\n\n# * you can set global with set_config()\nold <- set_config(verbose())\nHEAD(\"https://www.google.com\")\n# and re-establish the previous settings with\nset_config(old, override = TRUE)\nHEAD(\"https://www.google.com\")\n# or\nreset_config()\nHEAD(\"https://www.google.com\")\n\n# If available, you should use a friendly httr wrapper over RCurl\n# options. But you can pass Curl options (as listed in httr_options())\n# in config\nHEAD(\"https://www.google.com/\", config(verbose = TRUE))",
            "content": "\\dontrun{\nr <- POST(\"http://httpbin.org/post\", body = list(a = 1, b = 2))\ncontent(r) # automatically parses JSON\ncat(content(r, \"text\"), \"\\n\") # text content\ncontent(r, \"raw\") # raw bytes from server\n\nrlogo <- content(GET(\"https://httpbin.org/image/png\"))\nplot(0:1, 0:1, type = \"n\")\nrasterImage(rlogo, 0, 0, 1, 1)\n}",
            "content_type": "\\dontrun{\nGET(\"http://httpbin.org/headers\")\n\nGET(\"http://httpbin.org/headers\", accept_json())\nGET(\"http://httpbin.org/headers\", accept(\"text/csv\"))\nGET(\"http://httpbin.org/headers\", accept(\".doc\"))\n\nGET(\"http://httpbin.org/headers\", content_type_xml())\nGET(\"http://httpbin.org/headers\", content_type(\"text/csv\"))\nGET(\"http://httpbin.org/headers\", content_type(\".xml\"))\n}",
            "cookies": "\\dontrun{\nr <- GET(\"http://httpbin.org/cookies/set\", query = list(a = 1, b = 2))\ncookies(r)\n}",
            "get_callback": "\\dontrun{\n## Log all HTTP requests to the screeen\nreq_logger <- function(req) {\n  cat(\"HTTP request to\", sQuote(req$url), \"\\n\")\n}\n\nold <- set_callback(\"request\", req_logger)\ng1 <- GET(\"https://httpbin.org\")\ng2 <- GET(\"https://httpbin.org/ip\")\nset_callback(\"request\", old)\n\n## Log all HTTP requests and response status codes as well\nreq_logger2 <- function(req) {\n  cat(\"HTTP request to\", sQuote(req$url), \"... \")\n}\nres_logger <- function(req, res) {\n  cat(res$status_code, \"\\n\")\n}\n\nold_req <- set_callback(\"request\", req_logger2)\nold_res <- set_callback(\"response\", res_logger)\ng3 <- GET(\"https://httpbin.org\")\ng4 <- GET(\"https://httpbin.org/ip\")\nset_callback(\"request\", old_req)\nset_callback(\"response\", old_res)\n\n## Return a recorded response, without performing the HTTP request\nreplay <- function(req) {\n  if (req$url == \"https://httpbin.org\") g3\n}\nold_req <- set_callback(\"request\", replay)\ngrec <- GET(\"https://httpbin.org\")\ngrec$date == g3$date\nset_callback(\"request\", old_req)\n}",
            "handle": "handle(\"http://google.com\")\nhandle(\"https://google.com\")\n\nh <- handle(\"http://google.com\")\nGET(handle = h)\n# Should see cookies sent back to server\nGET(handle = h, config = verbose())\n\nh <- handle(\"http://google.com\", cookies = FALSE)\nGET(handle = h)$cookies\n\\dontrun{\n# Using the preferred way of configuring the http methods\n# will not work when using handle():\nGET(handle = h, timeout(10))\n# Passing named arguments will work properly:\nGET(handle = h, config = list(timeout(10), add_headers(Accept = \"\")))\n}",
            "has_content": "\\dontrun{\nhas_content(POST(\"http://httpbin.org/post\", body = FALSE))\nhas_content(HEAD(\"http://httpbin.org/headers\"))\n}",
            "headers": "\\dontrun{\nr <- GET(\"http://httpbin.org/get\")\nheaders(r)\n}",
            "http_condition": "\\dontrun{\n# You can use tryCatch to take different actions based on the type\n# of error. Note that tryCatch will call the first handler that\n# matches any classes of the condition, not the best matching, so\n# always list handlers from most specific to least specific\nf <- function(url) {\n  tryCatch(stop_for_status(GET(url)),\n    http_404 = function(c) \"That url doesn't exist\",\n    http_403 = function(c) \"You need to authenticate!\",\n    http_400 = function(c) \"You made a mistake!\",\n    http_500 = function(c) \"The server screwed up\"\n  )\n}\nf(\"http://httpbin.org/status/404\")\nf(\"http://httpbin.org/status/403\")\nf(\"http://httpbin.org/status/505\")\n}",
            "http_error": "\\dontrun{\n# You can pass a url:\nhttp_error(\"http://www.google.com\")\nhttp_error(\"http://httpbin.org/status/404\")\n\n# Or a request\nr <- GET(\"http://httpbin.org/status/201\")\nhttp_error(r)\n}\n\n# Or an (integer) status code\nhttp_error(200L)\nhttp_error(404L)",
            "http_status": "http_status(100)\nhttp_status(404)\n\n\\dontrun{\nx <- GET(\"http://httpbin.org/status/200\")\nhttp_status(x)\n\nhttp_status(GET(\"http://httpbin.org/status/300\"))\nhttp_status(GET(\"http://httpbin.org/status/301\"))\nhttp_status(GET(\"http://httpbin.org/status/404\"))\n\n# errors out on unknown status\nhttp_status(GET(\"http://httpbin.org/status/320\"))\n}",
            "http_type": "\\dontrun{\nr1 <- GET(\"http://httpbin.org/image/png\")\nhttp_type(r1)\nheaders(r1)[[\"Content-Type\"]]\n\nr2 <- GET(\"http://httpbin.org/ip\")\nhttp_type(r2)\nheaders(r2)[[\"Content-Type\"]]\n}",
            "httr_options": "httr_options()\nhttr_options(\"post\")\n\n# Use curl_docs to read the curl documentation for each option.\n# You can use either the httr or curl option name.\ncurl_docs(\"userpwd\")\ncurl_docs(\"CURLOPT_USERPWD\")",
            "insensitive": "x <- c(\"abc\" = 1, \"def\" = 2)\nx[\"ABC\"]\ny <- insensitive(x)\ny[\"ABC\"]\ny[[\"ABC\"]]",
            "jwt_signature": "\\dontrun{\ncred <- jsonlite::fromJSON(\"~/Desktop/httrtest-45693cbfac92.json\")\njwt_signature(cred, \"https://www.googleapis.com/auth/userinfo.profile\")\n}",
            "oauth_app": "\\dontrun{\ngoogle_app <- oauth_app(\n  \"google\",\n  key = \"123456789.apps.googleusercontent.com\",\n  secret = \"abcdefghijklmnopqrstuvwxyz\"\n)\n}",
            "oauth_endpoint": "linkedin <- oauth_endpoint(\"requestToken\", \"authorize\", \"accessToken\",\n  base_url = \"https://api.linkedin.com/uas/oauth\"\n)\ngithub <- oauth_endpoint(NULL, \"authorize\", \"access_token\",\n  base_url = \"https://github.com/login/oauth\"\n)\nfacebook <- oauth_endpoint(\n  authorize = \"https://www.facebook.com/dialog/oauth\",\n  access = \"https://graph.facebook.com/oauth/access_token\"\n)\n\noauth_endpoints",
            "oauth_endpoints": "oauth_endpoints(\"twitter\")",
            "oauth_service_token": "\\dontrun{\nendpoint <- oauth_endpoints(\"google\")\nsecrets <- jsonlite::fromJSON(\"~/Desktop/httrtest-45693cbfac92.json\")\nscope <- \"https://www.googleapis.com/auth/bigquery.readonly\"\n\ntoken <- oauth_service_token(endpoint, secrets, scope)\n}",
            "parse_http_date": "parse_http_date(\"Sun, 06 Nov 1994 08:49:37 GMT\")\nparse_http_date(\"Sunday, 06-Nov-94 08:49:37 GMT\")\nparse_http_date(\"Sun Nov  6 08:49:37 1994\")\n\nhttp_date(Sys.time())",
            "parse_media": "parse_media(\"text/plain\")\nparse_media(\"text/plain; charset=utf-8\")\nparse_media(\"text/plain; charset=\\\"utf-8\\\"\")\nparse_media(\"text/plain; randomparam=\\\";=;=\\\"\")",
            "parse_url": "parse_url(\"http://google.com/\")\nparse_url(\"http://google.com:80/\")\nparse_url(\"http://google.com:80/?a=1&b=2\")\n\nurl <- parse_url(\"http://google.com/\")\nurl$scheme <- \"https\"\nurl$query <- list(q = \"hello\")\nbuild_url(url)",
            "progress": "cap_speed <- config(max_recv_speed_large = 10000)\n\\dontrun{\n# If file size is known, you get a progress bar:\nx <- GET(\"http://httpbin.org/bytes/102400\", progress(), cap_speed)\n# Otherwise you get the number of bytes downloaded:\nx <- GET(\"http://httpbin.org/stream-bytes/102400\", progress(), cap_speed)\n}",
            "set_config": "GET(\"http://google.com\")\nset_config(verbose())\nGET(\"http://google.com\")\nreset_config()\nGET(\"http://google.com\")",
            "set_cookies": "set_cookies(a = 1, b = 2)\nset_cookies(.cookies = c(a = \"1\", b = \"2\"))\n\n\\dontrun{\nGET(\"http://httpbin.org/cookies\")\nGET(\"http://httpbin.org/cookies\", set_cookies(a = 1, b = 2))\n}",
            "stop_for_status": "\\dontrun{\nx <- GET(\"http://httpbin.org/status/200\")\nstop_for_status(x) # nothing happens\nwarn_for_status(x)\nmessage_for_status(x)\n\nx <- GET(\"http://httpbin.org/status/300\")\nstop_for_status(x)\nwarn_for_status(x)\nmessage_for_status(x)\n\nx <- GET(\"http://httpbin.org/status/404\")\nstop_for_status(x)\nwarn_for_status(x)\nmessage_for_status(x)\n\n# You can provide more information with the task argument\nwarn_for_status(x, \"download spreadsheet\")\nmessage_for_status(x, \"download spreadsheet\")\n}",
            "timeout": "\\dontrun{\nGET(\"http://httpbin.org/delay/3\", timeout(1))\nGET(\"http://httpbin.org/delay/1\", timeout(2))\n}",
            "upload_file": "citation <- upload_file(system.file(\"CITATION\"))\n\\dontrun{\nPOST(\"http://httpbin.org/post\", body = citation)\nPOST(\"http://httpbin.org/post\", body = list(y = citation))\n}",
            "use_proxy": "# See http://www.hidemyass.com/proxy-list for a list of public proxies\n# to test with\n# GET(\"http://had.co.nz\", use_proxy(\"64.251.21.73\", 8080), verbose())",
            "user_agent": "\\dontrun{\nGET(\"http://httpbin.org/user-agent\")\nGET(\"http://httpbin.org/user-agent\", user_agent(\"httr\"))\n}",
            "verbose": "\\dontrun{\nGET(\"http://httpbin.org\", verbose())\nGET(\"http://httpbin.org\", verbose(info = TRUE))\n\nf <- function() {\n  GET(\"http://httpbin.org\")\n}\nwith_verbose(f())\nwith_verbose(f(), info = TRUE)\n\n# verbose() makes it easy to see exactly what POST requests send\nPOST_verbose <- function(body, ...) {\n  POST(\"https://httpbin.org/post\", body = body, verbose(), ...)\n  invisible()\n}\nPOST_verbose(list(x = \"a\", y = \"b\"))\nPOST_verbose(list(x = \"a\", y = \"b\"), encode = \"form\")\nPOST_verbose(FALSE)\nPOST_verbose(NULL)\nPOST_verbose(\"\")\nPOST_verbose(\"xyz\")\n}",
            "with_config": "with_config(verbose(), {\n  GET(\"http://had.co.nz\")\n  GET(\"http://google.com\")\n})\n\n# Or even easier:\nwith_verbose(GET(\"http://google.com\"))",
            "write_disk": "tmp <- tempfile()\nr1 <- GET(\"https://www.google.com\", write_disk(tmp))\nreadLines(tmp)\n\n# The default\nr2 <- GET(\"https://www.google.com\", write_memory())\n\n# Save a very large file\n\\dontrun{\nGET(\n  \"http://www2.census.gov/acs2011_5yr/pums/csv_pus.zip\",\n  write_disk(\"csv_pus.zip\"), progress()\n)\n}",
            "write_stream": "GET(\n  \"https://github.com/jeroen/data/raw/gh-pages/diamonds.json\",\n  write_stream(function(x) {\n    print(length(x))\n    length(x)\n  })\n)"
        }
    },
    "htmltools": {
        "description": "Tools for HTML generation and output.",
        "examples": {
            "HTML": "el <- div(HTML(\"I like <u>turtles</u>\"))\ncat(as.character(el))",
            "bindFillRole": "tagz <- div(\n  id = \"outer\",\n  style = css(\n    height = \"600px\",\n    border = \"3px red solid\"\n  ),\n  div(\n    id = \"inner\",\n    style = css(\n      height = \"400px\",\n      border = \"3px blue solid\"\n    )\n  )\n)\n\n# Inner doesn't fill outer\nif (interactive()) browsable(tagz)\n\ntagz <- bindFillRole(tagz, container = TRUE)\ntagz <- bindFillRole(tagz, item = TRUE, .cssSelector = \"#inner\")\n\n# Inner does fill outer\nif (interactive()) browsable(tagz)",
            "builder": "tags$html(\n  tags$head(\n    tags$title('My first page')\n  ),\n  tags$body(\n    h1('My first heading'),\n    p('My first paragraph, with some ', strong('bold'), ' text.'),\n    div(\n      id = 'myDiv', class = 'simpleDiv',\n      'Here is a div with some attributes.'\n     )\n  )\n)\n\n# html5 <audio> with boolean control attribute\n# https://www.w3.org/TR/html5/infrastructure.html#sec-boolean-attributes\ntags$audio(\n  controls = NA,\n  tags$source(\n    src = \"myfile.wav\",\n    type = \"audio/wav\"\n  )\n)\n\n# suppress the whitespace between tags\ntags$span(\n  tags$strong(\"I'm strong\", .noWS=\"outside\")\n)",
            "capturePlot": "# Default settings\nres <- capturePlot(plot(cars))\n\n# View result\nif (interactive()) browseURL(res)\n\n# Clean up\nunlink(res)\n\n# Custom width/height\npngpath <- tempfile(fileext = \".png\")\ncapturePlot(plot(pressure), pngpath, width = 800, height = 375)\nif (interactive()) browseURL(pngpath)\nunlink(pngpath)\n\n# Use a custom graphics device (e.g., SVG)\nif (capabilities(\"cairo\")) {\n  svgpath <- capturePlot(\n    plot(pressure),\n    tempfile(fileext = \".svg\"),\n    grDevices::svg,\n    width = 8, height = 3.75\n  )\n  if (interactive()) browseURL(svgpath)\n  unlink(svgpath)\n}",
            "css": "padding <- 6\ncss(\n  font.family = \"Helvetica, sans-serif\",\n  margin = paste0(c(10, 20, 10, 20), \"px\"),\n  \"padding!\" = if (!is.null(padding)) padding\n)",
            "htmlDependencies": "# Create a JavaScript dependency\ndep <- htmlDependency(\"jqueryui\", \"1.11.4\", c(href=\"shared/jqueryui\"),\n                      script = \"jquery-ui.min.js\")\n\n# A CSS dependency\nhtmlDependency(\n  \"font-awesome\", \"4.5.0\", c(href=\"shared/font-awesome\"),\n  stylesheet = \"css/font-awesome.min.css\"\n)\n\n# A few different ways to add the dependency to tag objects:\n# Inline as a child of the div()\ndiv(\"Code here\", dep)\n# Inline in a tagList\ntagList(div(\"Code here\"), dep)\n# With attachDependencies\nattachDependencies(div(\"Code here\"), dep)",
            "htmlPreserve": "# htmlPreserve will prevent \"<script>alert(10*2*3);</script>\"\n# from getting an <em> tag inserted in the middle\nmarkup <- paste(sep = \"\\n\",\n  \"This is *emphasized* text in markdown.\",\n  htmlPreserve(\"<script>alert(10*2*3);</script>\"),\n  \"Here is some more *emphasized text*.\"\n)\nextracted <- extractPreserveChunks(markup)\nmarkup <- extracted$value\n# Just think of this next line as Markdown processing\noutput <- gsub(\"\\\\\\\\*(.*?)\\\\\\\\*\", \"<em>\\\\\\\\1</em>\", markup)\noutput <- restorePreserveChunks(output, extracted$chunks)\noutput",
            "parseCssColors": "parseCssColors(c(\n  \"#0d6efd\",\n  \"#DC35457F\",\n  \"rgb(32,201,151)\",\n  \"  rgba( 23 , 162 , 184 , 0.5 )  \",\n  \"hsl(261, 51\\%, 51\\%)\",\n  \"cornflowerblue\"\n))",
            "plotTag": "img <- plotTag({\n  plot(cars)\n}, \"A plot of the 'cars' dataset\", width = 375, height = 275)\n\nif (interactive()) img\n\nif (interactive() && capabilities(\"cairo\")) {\n  plotTag(\n    plot(pressure), \"A plot of the 'pressure' dataset\",\n    device = grDevices::svg, width = 375, height = 275, pixelratio = 1/72,\n    mimeType = \"image/svg+xml\"\n  )\n}",
            "tagAddRenderHook": "# Have a place holder div and return a span instead\nobj <- div(\"example\", .renderHook = function(x) {\n  x$name <- \"span\"\n  x\n})\nobj$name # \"div\"\nprint(obj) # Prints as a `span`\n\n# Add a class to the tag\n# Should print a `span` with class `\"extra\"`\nspanExtra <- tagAddRenderHook(obj, function(x) {\n  tagAppendAttributes(x, class = \"extra\")\n})\nspanExtra\n\n# Replace the previous render method\n# Should print a `div` with class `\"extra\"`\ndivExtra <- tagAddRenderHook(obj, replace = TRUE, function(x) {\n  tagAppendAttributes(x, class = \"extra\")\n})\ndivExtra\n\n# Add more child tags\nspanExtended <- tagAddRenderHook(obj, function(x) {\n  tagAppendChildren(x, \" \", tags$strong(\"bold text\"))\n})\nspanExtended\n\n# Add a new html dependency\nnewDep <- tagAddRenderHook(obj, function(x) {\n  fa <- htmlDependency(\n    \"font-awesome\", \"4.5.0\", c(href=\"shared/font-awesome\"),\n    stylesheet = \"css/font-awesome.min.css\")\n  attachDependencies(x, fa, append = TRUE)\n})\n# Also add a jqueryui html dependency\nhtmlDependencies(newDep) <- htmlDependency(\n  \"jqueryui\", \"1.11.4\", c(href=\"shared/jqueryui\"),\n  script = \"jquery-ui.min.js\")\n# At render time, both dependencies will be found\nrenderTags(newDep)$dependencies\n\n# Ignore the original tag and return something completely new.\nnewObj <- tagAddRenderHook(obj, function(x) {\n  tags$p(\"Something else\")\n})\nnewObj",
            "tagAppendAttributes": "html <- div(a())\ntagAppendAttributes(html, class = \"foo\")\ntagAppendAttributes(html, .cssSelector = \"a\", class = \"bar\")\ntagAppendAttributes(html, contenteditable = NA)\n\ntagHasAttribute(div(foo = \"bar\"), \"foo\")\ntagGetAttribute(div(foo = \"bar\"), \"foo\")",
            "tagAppendChild": "html <- div(a(), h1())\ntagAppendChild(html, span())\ntagAppendChild(html, .cssSelector = \"a\", span())\n\ntagAppendChildren(html, span(), p())\ntagAppendChildren(html, .cssSelector = \"a\", span(), p())\n\ntagSetChildren(html, span(), p())\n\ntagInsertChildren(html, after = 1, span(), p())",
            "tagFunction": "myDivDep <- tagFunction(function() {\n  if (isTRUE(getOption(\"useDep\", TRUE))) {\n    htmlDependency(\n      name = \"lazy-dependency\",\n      version = \"1.0\", src = \"\"\n    )\n  }\n})\nmyDiv <- attachDependencies(div(), myDivDep)\nrenderTags(myDiv)\nwithr::with_options(list(useDep = FALSE), renderTags(myDiv))",
            "tagList": "tagList(\n  h1(\"Title\"),\n  h2(\"Header text\", style = \"color: red;\"),\n  p(\"Text here\")\n)",
            "tagQuery": "tagQ <- tagQuery(div(a()))\ntagQ$find(\"a\")$addClass(\"foo\")\ntagQ\n\n# To learn more, visit https://rstudio.github.io/htmltools/articles/tagQuery.html",
            "validateCssUnit": "validateCssUnit(\"10\\%\")\nvalidateCssUnit(400)  #treated as '400px'",
            "withTags": "# Using tags$ each time\ntags$div(class = \"myclass\",\n  tags$h3(\"header\"),\n  tags$p(\"text\")\n)\n\n# Equivalent to above, but using withTags\nwithTags(\n  div(class = \"myclass\",\n    h3(\"header\"),\n    p(\"text\")\n  )\n)\n\n# Setting .noWS for all tags in withTags()\nwithTags(\n  div(\n    class = \"myclass\",\n    h3(\"header\"),\n    p(\"One\", strong(span(\"two\")), \"three\")\n  ),\n  .noWS = c(\"outside\", \"inside\")\n)"
        }
    },
    "xfun": {
        "description": "Miscellaneous functions commonly used in other packages maintained by 'Yihui Xie'.",
        "examples": {
            "Rscript": "library(xfun)\nRscript(c(\"-e\", \"1+1\"))\nRcmd(c(\"build\", \"--help\"))",
            "Rscript_call": "factorial(10)\n# should return the same value\nxfun::Rscript_call(\"factorial\", list(10))\n\n# the first argument can be either a character string or a function\nxfun::Rscript_call(factorial, list(10))\n\n# Run Rscript starting a vanilla R session\nxfun::Rscript_call(factorial, list(10), options = c(\"--vanilla\"))",
            "alnum_id": "x = c(\"Hello world 123!\", \"a  &b*^##c 456\")\nxfun::alnum_id(x)\nxfun::alnum_id(x, \"[^[:alpha:]]+\")  # only keep alphabetical chars\n# when text contains HTML tags\nxfun::alnum_id(\"<h1>Hello <strong>world</strong>!\")",
            "attr": "z = structure(list(a = 1), foo = 2)\nbase::attr(z, \"f\")  # 2\nxfun::attr(z, \"f\")  # NULL\nxfun::attr(z, \"foo\")  # 2",
            "base64_encode": "xfun::base64_encode(as.raw(1:10))\nlogo = xfun:::R_logo()\nxfun::base64_encode(logo)\nxfun::base64_decode(\"AQIDBAUGBwgJCg==\")",
            "base64_uri": "logo = xfun:::R_logo()\nimg = xfun::html_tag(\"img\", src = xfun::base64_uri(logo), alt = \"R logo\")\nif (interactive()) xfun::html_view(img)",
            "base_pkgs": "xfun::base_pkgs()",
            "bump_version": "xfun::bump_version(c(\"0.1\", \"91.2.14\"))",
            "cache_exec": "# the first run takes about 1 second\ny1 = xfun::cache_exec({\n    x = rnorm(1e+05)\n    Sys.sleep(1)\n    x\n}, path = \":memory:\", id = \"sim-norm\")\n\n# the second run takes almost no time\ny2 = xfun::cache_exec({\n    # comments won't affect caching\n    x = rnorm(1e+05)\n    Sys.sleep(1)\n    x\n}, path = \":memory:\", id = \"sim-norm\")\n\n# y1, y2, and x should be identical\nstopifnot(identical(y1, y2), identical(y1, x))",
            "cache_rds": "f = tempfile()  # the cache file\ncompute = function(...) {\n    res = xfun::cache_rds({\n        Sys.sleep(1)\n        1:10\n    }, file = f, dir = \"\", ...)\n    res\n}\ncompute()  # takes one second\ncompute()  # returns 1:10 immediately\ncompute()  # fast again\ncompute(rerun = TRUE)  # one second to rerun\ncompute()\nunlink(paste0(f, \"_*.rds\"))",
            "csv_options": "xfun::csv_options(\"foo, eval=TRUE, fig.width=5, echo=if (TRUE) FALSE\")",
            "decimal_dot": "opts = options(OutDec = \",\")\nas.character(1.234)  # using ',' as the decimal separator\nprint(1.234)  # same\nxfun::decimal_dot(as.character(1.234))  # using dot\nxfun::decimal_dot(print(1.234))  # using dot\noptions(opts)",
            "divide_chunk": "# parse yaml-like items\nyaml_like = c(\"#| label: mine\", \"#| echo: true\", \"#| fig.width: 8\", \"#| foo: bar\",\n    \"1 + 1\")\nwriteLines(yaml_like)\nxfun::divide_chunk(\"r\", yaml_like)\n\n# parse CSV syntax\ncsv_like = c(\"#| mine, echo = TRUE, fig.width = 8, foo = 'bar'\", \"1 + 1\")\nwriteLines(csv_like)\nxfun::divide_chunk(\"r\", csv_like)",
            "do_once": "do_once(message(\"Today's date is \", Sys.Date()), \"xfun.date.reminder\")\n# if you run it again, it will not emit the message again\ndo_once(message(\"Today's date is \", Sys.Date()), \"xfun.date.reminder\")\n\ndo_once({\n    Sys.sleep(2)\n    1 + 1\n}, \"xfun.task.1plus1\")\ndo_once({\n    Sys.sleep(2)\n    1 + 1\n}, \"xfun.task.1plus1\")",
            "download_cache": "\\dontshow{if (interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# the first time it may take a few seconds\nx1 = xfun::download_cache$get(\"https://www.r-project.org/\")\nhead(x1)\n\n# now you can get the cached content\nx2 = xfun::download_cache$get(\"https://www.r-project.org/\")\nidentical(x1, x2)  # TRUE\n\n# a binary file\nx3 = xfun::download_cache$get(\"https://yihui.org/images/logo.png\", \"raw\")\nlength(x3)\n\n# show a summary\nxfun::download_cache$summary()\n# remove a specific cache file\nxfun::download_cache$remove(\"https://yihui.org/images/logo.png\", \"raw\")\n# remove all cache files\nxfun::download_cache$purge()\n\\dontshow{\\}) # examplesIf}",
            "embed_file": "logo = xfun:::R_logo()\nlink = xfun::embed_file(logo, text = \"Download R logo\")\nlink\nif (interactive()) xfun::html_view(link)",
            "env_option": "xfun::env_option(\"xfun.test.option\")  # NULL\n\nSys.setenv(R_XFUN_TEST_OPTION = \"1234\")\nxfun::env_option(\"xfun.test.option\")  # 1234\n\noptions(xfun.test.option = TRUE)\nxfun::env_option(\"xfun.test.option\")  # TRUE (from options())\noptions(xfun.test.option = NULL)  # reset the option\nxfun::env_option(\"xfun.test.option\")  # 1234 (from env var)\n\nSys.unsetenv(\"R_XFUN_TEST_OPTION\")\nxfun::env_option(\"xfun.test.option\")  # NULL again\n\nxfun::env_option(\"xfun.test.option\", FALSE)  # use default",
            "existing_files": "xfun::existing_files(c(\"foo.txt\", system.file(\"DESCRIPTION\", package = \"xfun\")))",
            "exit_call": "f = function(x) {\n    print(x)\n    xfun::exit_call(function() print(\"The parent function is exiting!\"))\n}\ng = function(y) {\n    f(y)\n    print(\"f() has been called!\")\n}\ng(\"An argument of g()!\")",
            "fenced_block": "# code block with class 'r' and ID 'foo'\nxfun::fenced_block(\"1+1\", c(\".r\", \"#foo\"))\n# fenced Div\nxfun::fenced_block(\"This is a **Div**.\", char = \":\")\n# three backticks by default\nxfun::make_fence(\"1+1\")\n# needs five backticks for the fences because content has four\nxfun::make_fence(c(\"````r\", \"1+1\", \"````\"))",
            "file_ext": "library(xfun)\np = c(\"abc.doc\", \"def123.tex\", \"path/to/foo.Rmd\", \"backup.ppt~\", \"pkg.tar.xz\")\nfile_ext(p)\nsans_ext(p)\nwith_ext(p, \".txt\")\nwith_ext(p, c(\".ppt\", \".sty\", \".Rnw\", \"doc\", \"zip\"))\nwith_ext(p, \"html\")\n\n# allow for more characters in extensions\np = c(\"a.c++\", \"b.c--\", \"c.e##\")\nfile_ext(p)  # -/+/# not recognized by default\nfile_ext(p, extra = \"-+#\")",
            "file_string": "xfun::file_string(system.file(\"DESCRIPTION\", package = \"xfun\"))",
            "find_globals": "x = 2\nxfun::find_globals(\"y = x + 1\")\nxfun::find_globals(\"y = get('x') + 1\")  # x is not recognized\nxfun::find_globals(\"y = zzz + 1\")  # zzz doesn't exist\n\nxfun::find_locals(\"y = x + 1\")\nxfun::find_locals(\"assign('y', x + 1)\")  # it works\nxfun::find_locals(\"assign('y', x + 1, new.env())\")  # still smart\nxfun::find_locals(\"eval(parse(text = 'y = x + 1'))\")  # no way",
            "format_bytes": "xfun::format_bytes(c(1, 1024, 2000, 1e+06, 2e+08))\nxfun::format_bytes(c(1, 1024, 2000, 1e+06, 2e+08), units = \"KB\")",
            "from_root": "\\dontrun{\nxfun::from_root(\"data\", \"mtcars.csv\")\n}",
            "github_releases": "\\dontshow{if (interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nxfun::github_releases(\"yihui/xfun\")\nxfun::github_releases(\"gohugoio/hugo\")\n\\dontshow{\\}) # examplesIf}",
            "grep_sub": "# find elements that matches 'a[b]+c' and capitalize 'b' with perl regex\nxfun::grep_sub(\"a([b]+)c\", \"a\\\\\\\\U\\\\\\\\1c\", c(\"abc\", \"abbbc\", \"addc\", \"123\"), perl = TRUE)",
            "gsub_file": "library(xfun)\nf = tempfile()\nwriteLines(c(\"hello\", \"world\"), f)\ngsub_file(f, \"world\", \"woRld\", fixed = TRUE)\nreadLines(f)",
            "html_tag": "xfun::html_tag(\"a\", \"<R Project>\", href = \"https://www.r-project.org\", target = \"_blank\")\nxfun::html_tag(\"br\")\nxfun::html_tag(\"a\", xfun::html_tag(\"strong\", \"R Project\"), href = \"#\")\nxfun::html_tag(\"a\", list(\"<text>\", xfun::html_tag(\"b\", \"R Project\")), href = \"#\")\nxfun::html_escape(\"\\\" quotes \\\" & brackets < >\")\nxfun::html_escape(\"\\\" & < > \\r \\n\", attr = TRUE)",
            "in_dir": "library(xfun)\nin_dir(tempdir(), {\n    print(getwd())\n    list.files()\n})",
            "is_abs_path": "xfun::is_abs_path(c(\"C:/foo\", \"foo.txt\", \"/Users/john/\", tempdir()))\nxfun::is_rel_path(c(\"C:/foo\", \"foo.txt\", \"/Users/john/\", tempdir()))",
            "is_ascii": "library(xfun)\nis_ascii(letters)  # yes\nis_ascii(intToUtf8(8212))  # no",
            "is_blank": "xfun::is_blank(\"\")\nxfun::is_blank(\"abc\")\nxfun::is_blank(c(\"\", \"  \", \"\\n\\t\"))\nxfun::is_blank(c(\"\", \" \", \"abc\"))",
            "is_sub_path": "xfun::is_sub_path(\"a/b/c.txt\", \"a/b\")  # TRUE\nxfun::is_sub_path(\"a/b/c.txt\", \"d/b\")  # FALSE\nxfun::is_sub_path(\"a/b/c.txt\", \"a\\\\\\\\b\")  # FALSE (even on Windows)",
            "is_web_path": "xfun::is_web_path(\"https://www.r-project.org\")  # TRUE\nxfun::is_web_path(\"www.r-project.org\")  # FALSE",
            "join_words": "join_words(\"a\")\njoin_words(c(\"a\", \"b\"))\njoin_words(c(\"a\", \"b\", \"c\"))\njoin_words(c(\"a\", \"b\", \"c\"), sep = \" / \", and = \"\")\njoin_words(c(\"a\", \"b\", \"c\"), and = \"\")\njoin_words(c(\"a\", \"b\", \"c\"), before = \"\\\"\", after = \"\\\"\")\njoin_words(c(\"a\", \"b\", \"c\"), before = \"\\\"\", after = \"\\\"\", oxford_comma = FALSE)",
            "magic_path": "\\dontrun{\nxfun::magic_path(\"mtcars.csv\")  # find any file that has the base name mtcars.csv\n}",
            "mark_dirs": "mark_dirs(list.files(find.package(\"xfun\"), full.names = TRUE))",
            "md5": "x1 = 1\nx2 = 1:10\nx3 = seq(1, 10)\nx4 = iris\nx5 = paste\n(m = xfun::md5(x1, x2, x3, x4, x5))\nstopifnot(m[2] == m[3])  # x2 and x3 should be identical\n\nxfun::md5(x1 = x1, x2 = x2)  # named arguments",
            "md_table": "xfun::md_table(head(iris))\nxfun::md_table(mtcars, limit = c(10, 6))",
            "mime_type": "\\dontshow{if (tolower(Sys.getenv('CI')) == 'true') (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nf = list.files(R.home(\"doc\"), full.names = TRUE)\nmime_type(f)\nmime_type(f, FALSE)  # don't use mime\nmime_type(f, FALSE, NA)  # run command for files without extension\n\\dontshow{\\}) # examplesIf}",
            "msg_cat": "{\n    # a message without a newline at the end\n    xfun::msg_cat(\"Hello world!\")\n    # add a newline at the end\n    xfun::msg_cat(\" This message appears right after the previous one.\\n\")\n}\nsuppressMessages(xfun::msg_cat(\"Hello world!\"))",
            "native_encode": "library(xfun)\ns = intToUtf8(c(20320, 22909))\nEncoding(s)\n\ns2 = native_encode(s)\nEncoding(s2)",
            "news2md": "\\dontshow{if (interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# news for the current version of R\nxfun::news2md(\"R\", Version == getRversion(), output = NA)\n\\dontshow{\\}) # examplesIf}",
            "normalize_path": "library(xfun)\nnormalize_path(\"~\")",
            "numbers_to_words": "library(xfun)\nn2w(0, cap = TRUE)\nn2w(0:121, and = TRUE)\nn2w(1e+06)\nn2w(1e+11 + 12345678)\nn2w(-987654321)\nn2w(1e+15 - 1)\nn2w(123.456)\nn2w(123.45678901)\nn2w(123.456789098765)",
            "os": "library(xfun)\n# only one of the following statements should be true\nis_windows()\nis_unix() && is_macos()\nis_linux()\n# In newer Macs, CPU can be either Intel or Apple\nis_arm64()  # TRUE on Apple silicone machines",
            "parse_only": "library(xfun)\nparse_only(\"1+1\")\nparse_only(c(\"y~x\", \"1:5 # a comment\"))\nparse_only(character(0))",
            "pkg_attach": "library(xfun)\npkg_attach(\"stats\", \"graphics\")\n# pkg_attach2('servr') # automatically install servr if it is not installed\n\n(pkg_load(\"stats\", \"graphics\"))",
            "pkg_bib": "pkg_bib(c(\"base\", \"MASS\", \"xfun\"))\npkg_bib(\"cluster\", prefix = \"R-pkg-\")  # a different prefix",
            "process_file": "f = tempfile()\nxfun::write_utf8(\"Hello World\", f)\nxfun::process_file(f, function(x) gsub(\"World\", \"woRld\", x))\nxfun::read_utf8(f)  # see if it has been updated\nfile.remove(f)",
            "prose_index": "library(xfun)\nprose_index(c(\"a\", \"```\", \"b\", \"```\", \"c\"))\nprose_index(c(\"a\", \"````\", \"```r\", \"1+1\", \"```\", \"````\", \"c\"))",
            "protect_math": "library(xfun)\nprotect_math(c(\"hi $a+b$\", \"hello $$\\\\\\\\alpha$$\", \"no math here: $x is $10 dollars\"))\nprotect_math(c(\"hi $$\", \"\\\\\\\\begin{equation}\", \"x + y = z\", \"\\\\\\\\end{equation}\"))\nprotect_math(\"$a+b$\", \"===\")",
            "raw_string": "library(xfun)\nraw_string(head(LETTERS))\nraw_string(c(\"a \\\"b\\\"\", \"hello\\tworld!\"))",
            "read_all": "# two files in this package\nfs = system.file(\"scripts\", c(\"call-fun.R\", \"child-pids.sh\"), package = \"xfun\")\nxfun::read_all(fs)\n\n# add file paths before file content and an empty line after content\nxfun::read_all(fs, before = function(f) paste(\"#-----\", f, \"-----\"), after = \"\")\n\n# add constants\nxfun::read_all(fs, before = \"/*\", after = c(\"*/\", \"\"))",
            "read_bin": "f = tempfile()\ncat(\"abc\", file = f)\nxfun::read_bin(f)\nunlink(f)",
            "record": "code = c(\"# a warning test\", \"1:2 + 1:3\", \"par(mar = c(4, 4, 1, .2))\",\n    \"barplot(5:1, col = 2:6, horiz = TRUE)\", \"head(iris)\",\n    \"sunflowerplot(iris[, 3:4], seg.col = 'purple')\",\n    \"if (TRUE) {\\n  message('Hello, xfun::record()!')\\n}\",\n    \"# throw an error\", \"1 + 'a'\")\nres = xfun::record(code, dev.args = list(width = 9, height = 6.75),\n    error = TRUE)\nxfun::tree(res)\nformat(res)\n# find and clean up plot files\nplots = Filter(function(x) inherits(x, \"record_plot\"),\n    res)\nfile.remove(unlist(plots))",
            "relative_path": "xfun::relative_path(\"foo/bar.txt\", \"foo/\")\nxfun::relative_path(\"foo/bar/a.txt\", \"foo/haha/\")\nxfun::relative_path(getwd())",
            "rename_seq": "xfun::rename_seq()\nxfun::rename_seq(\"[.](jpeg|png)$\", format = \"\\%04d\")",
            "rest_api": "\\dontshow{if (interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# a normal GET request\nxfun::rest_api(\"https://httpbin.org\", \"/get\")\nxfun::rest_api_raw(\"https://httpbin.org\", \"/get\")\n\n# send the request with an auth header\nxfun::rest_api(\"https://httpbin.org\", \"/headers\", \"OPEN SESAME!\")\n\n# with query parameters\nxfun::rest_api(\"https://httpbin.org\", \"/response-headers\", params = list(foo = \"bar\"))\n\n# get the rate limit info from GitHub\nxfun::github_api(\"/rate_limit\")\n\\dontshow{\\}) # examplesIf}",
            "retry": "\\dontshow{if (interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# read the GitHub releases info of the repo yihui/xfun\nxfun::retry(xfun::github_releases, \"yihui/xfun\")\n\\dontshow{\\}) # examplesIf}",
            "rstudio_type": "library(xfun)\nif (loadable(\"rstudioapi\") && rstudioapi::isAvailable()) {\n    rstudio_type(\"Hello, RStudio! xfun::rstudio_type() looks pretty cool!\",\n        pause = function() runif(1, 0, 0.5), mistake = 0.1)\n}",
            "same_path": "library(xfun)\nsame_path(\"~/foo\", file.path(Sys.getenv(\"HOME\"), \"foo\"))",
            "session_info": "\\dontshow{if (interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nxfun::session_info()\nif (xfun::loadable(\"MASS\")) xfun::session_info(\"MASS\")\n\\dontshow{\\}) # examplesIf}",
            "set_envvar": "vars = xfun::set_envvar(c(FOO = \"1234\"))\nSys.getenv(\"FOO\")\nxfun::set_envvar(vars)\nSys.getenv(\"FOO\")",
            "shrink_images": "f = xfun:::all_files(\"[.](png|jpe?g)$\", R.home(\"doc\"))\nfile.copy(f, tempdir())\nf = file.path(tempdir(), basename(f))\nmagick::image_info(magick::image_read(f))  # some widths are larger than 300\nxfun::shrink_images(300, files = f)\nmagick::image_info(magick::image_read(f))  # all widths <= 300 now\nfile.remove(f)",
            "split_lines": "xfun::split_lines(c(\"a\", \"b\\nc\"))",
            "split_source": "code = c(\"# comment 1\", \"# comment 2\", \"if (TRUE) {\", \"1 + 1\", \"}\", \"print(1:5)\")\nxfun::split_source(code)\nxfun::split_source(code, merge_comments = TRUE)",
            "str_wrap": "x = sample(c(letters, \" \"), 200, TRUE, c(rep(0.5/26, 26), 0.5))\nx = rep(paste(x, collapse = \"\"), 2)\nstrwrap(x, width = 30)\nxfun::str_wrap(x, width = 30)  # same length as x",
            "strict_list": "library(xfun)\n(z = strict_list(aaa = \"I am aaa\", b = 1:5))\nz$a  # NULL!\nz$aaa  # I am aaa\nz$b\nz$c = \"create a new element\"\n\nz2 = unclass(z)  # a normal list\nz2$a  # partial matching\n\nz3 = as_strict_list(z2)  # a strict list again\nz3$a  # NULL again!",
            "strip_html": "xfun::strip_html(\"<a href=\\\"#\\\">Hello <!-- comment -->world!</a>\")",
            "system3": "\\dontshow{if (interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\na = shQuote(c(\"-e\", \"print(intToUtf8(c(20320, 22909)))\"))\nx2 = system2(\"Rscript\", a, stdout = TRUE)\nEncoding(x2)  # unknown\n\nx3 = xfun::system3(\"Rscript\", a, stdout = TRUE)\n# encoding of x3 should be UTF-8 if the current locale is UTF-8\n!l10n_info()[[\"UTF-8\"]] || Encoding(x3) == \"UTF-8\"  # should be TRUE\n\\dontshow{\\}) # examplesIf}",
            "tabset": "xfun::tabset(iris)\nxfun::tabset(iris, dput)\nxfun::tabset(iris, print)\n\n# a deeply nested list\nplot(1:10)\np = recordPlot()\nxfun::tabset(p)",
            "taml_load": "(res = taml_load(\"a: 1\"))\ntaml_save(res)\n\n(res = taml_load(\"a: 1\\nb: \\\"foo\\\"\\nc: null\"))\ntaml_save(res)\n\n(res = taml_load(\"a:\\n  b: false\\n  c: true\\n  d: 1.234\\ne: bar\"))\ntaml_save(res)\ntaml_save(res, indent = \"\\t\")\n\ntaml_load(\"a: !expr paste(1:10, collapse = \\\", \\\")\")\ntaml_load(\"a: [1, 3, 4, 2]\")\ntaml_load(\"a: [1, \\\"abc\\\", 4, 2]\")\ntaml_load(\"a: [\\\"foo\\\", \\\"bar\\\"]\")\ntaml_load(\"a: [true, false, true]\")\n# the other form of array is not supported\ntaml_load(\"a:\\n  - b\\n  - c\")\n# and you must use the yaml package\nif (loadable(\"yaml\")) yaml_load(\"a:\\n  - b\\n  - c\")",
            "tinify": "\\dontrun{\nf = xfun:::R_logo(\"jpg$\")\nxfun::tinify(f)  # remember to set the API key before trying this\n}",
            "tojson": "library(xfun)\ntojson(NULL)\ntojson(1:10)\ntojson(TRUE)\ntojson(FALSE)\ntojson(list(a = 1, b = list(c = 1:3, d = \"abc\")))\ntojson(list(c(\"a\", \"b\"), 1:5, TRUE, Sys.Date() + 1:3))\ntojson(head(iris))  # each column is in an element\ntojson(unname(head(iris)))  # each row is in an element\ntojson(matrix(1:12, 3))\n\n# literal JS code\nJS = function(x) structure(x, class = \"JS_LITERAL\")\ntojson(list(a = 1:5, b = JS(\"function() {return true;}\")))",
            "tree": "fit = lsfit(1:9, 1:9)\nstr(fit)\nxfun::tree(fit)\n\nfit = lm(dist ~ speed, data = cars)\nstr(fit)\nxfun::tree(fit)\n\n# some trivial examples\nxfun::tree(1:10)\nxfun::tree(iris)",
            "try_error": "xfun::try_error(stop(\"foo\"))  # TRUE\nxfun::try_error(1:10)  # FALSE",
            "try_silent": "library(xfun)\nz = try_silent(stop(\"Wrong!\"))\ninherits(z, \"try-error\")"
        }
    },
    "fansi": {
        "description": "Counterparts to R string manipulation functions that account for\n   the effects of ANSI text formatting control sequences.",
        "examples": {
            "fansi_lines": "NEWS <- readLines(file.path(R.home('doc'), 'NEWS'))\nwriteLines(fansi_lines(NEWS[1:20]))\nwriteLines(fansi_lines(NEWS[1:20], step=8))",
            "has_ctl": "has_ctl(\"hello world\")\nhas_ctl(\"hello\\nworld\")\nhas_ctl(\"hello\\nworld\", \"sgr\")\nhas_ctl(\"hello\\033[31mworld\\033[m\", \"sgr\")",
            "html_code_block": "html_code_block(c(\"hello world\"))\nhtml_code_block(c(\"hello world\"), class=\"pretty\")",
            "html_esc": "html_esc(\"day > night\")\nhtml_esc(\"<SPAN>hello world</SPAN>\")",
            "in_html": "txt <- \"\\033[31;42mHello \\033[7mWorld\\033[m\"\nwriteLines(txt)\nhtml <- to_html(txt)\n\\dontrun{\nin_html(html) # spawns a browser window\n}\nwriteLines(readLines(in_html(html, display=FALSE)))\ncss <- \"SPAN {text-decoration: underline;}\"\nwriteLines(readLines(in_html(html, css=css, display=FALSE)))\n\\dontrun{\nin_html(html, css)\n}",
            "make_styles": "## Generate some class strings; order matters\nclasses <- do.call(paste, c(expand.grid(c(\"fg\", \"bg\"), 0:7), sep=\"-\"))\nwriteLines(classes[1:4])\n\n## Some Default CSS\ncss0 <- \"span {font-size: 60pt; padding: 10px; display: inline-block}\"\n\n## Associated class strings to styles\ncss1 <- make_styles(classes)\nwriteLines(css1[1:4])\n\n## Generate SGR-derived HTML, mapping to classes\nstring <- \"\\033[43mYellow\\033[m\\n\\033[45mMagenta\\033[m\\n\\033[46mCyan\\033[m\"\nhtml <- to_html(string, classes=classes)\nwriteLines(html)\n\n## Combine in a page with styles and display in browser\n\\dontrun{\nin_html(html, css=c(css0, css1))\n}\n\n## Change CSS by remixing colors, and apply to exact same HTML\nmix <- matrix(\n  c(\n    0, 1, 0,  # red output is green input\n    0, 0, 1,  # green output is blue input\n    1, 0, 0   # blue output is red input\n  ),\n  nrow=3, byrow=TRUE\n)\ncss2 <- make_styles(classes, rgb.mix=mix)\n## Display in browser: same HTML but colors changed by CSS\n\\dontrun{\nin_html(html, css=c(css0, css2))\n}",
            "nchar_ctl": "nchar_ctl(\"\\033[31m123\\a\\r\")\n## with some wide characters\ncn.string <-  sprintf(\"\\033[31m\\%s\\a\\r\", \"\\u4E00\\u4E01\\u4E03\")\nnchar_ctl(cn.string)\nnchar_ctl(cn.string, type='width')\n\n## Remember newlines are not counted by default\nnchar_ctl(\"\\t\\n\\r\")\n\n## The 'c0' value for the `ctl` argument does not include\n## newlines.\nnchar_ctl(\"\\t\\n\\r\", ctl=\"c0\")\nnchar_ctl(\"\\t\\n\\r\", ctl=c(\"c0\", \"nl\"))\n\n## The _sgr flavor only treats SGR sequences as zero width\nnchar_sgr(\"\\033[31m123\")\nnchar_sgr(\"\\t\\n\\n123\")\n\n## All of the following are Control Sequences or C0 controls\nnzchar_ctl(\"\\n\\033[42;31m\\033[123P\\a\")",
            "normalize_state": "normalize_state(\"hello\\033[42;33m world\")\nnormalize_state(\"hello\\033[42;33m world\\033[m\")\nnormalize_state(\"\\033[4mhello\\033[42;33m world\\033[m\")\n\n## Superflous codes removed\nnormalize_state(\"\\033[31;32mhello\\033[m\")      # only last color prevails\nnormalize_state(\"\\033[31\\033[32mhello\\033[m\")  # only last color prevails\nnormalize_state(\"\\033[31mhe\\033[49mllo\\033[m\") # unused closing\n\n## Equivalent normalized sequences compare identical\nidentical(\n  normalize_state(\"\\033[31;32mhello\\033[m\"),\n  normalize_state(\"\\033[31mhe\\033[49mllo\\033[m\")\n)\n## External SGR will defeat normalization, unless we `carry` it\nred <- \"\\033[41m\"\nwriteLines(\n  c(\n    paste(red, \"he\\033[0mllo\", \"\\033[0m\"),\n    paste(red, normalize_state(\"he\\033[0mllo\"), \"\\033[0m\"),\n    paste(red, normalize_state(\"he\\033[0mllo\", carry=red), \"\\033[0m\")\n) )",
            "set_knit_hooks": "\\dontrun{\n## The following should be done within an `rmarkdown` document chunk with\n## chunk option `results` set to 'asis' and the chunk option `comment` set\n## to ''.\n\n```{r comment=\"\", results='asis', echo=FALSE}\n## Change the \"output\" hook to handle ANSI CSI SGR\n\nold.hooks <- set_knit_hooks(knitr::knit_hooks)\n\n## Do the same with the warning, error, and message, and add styles for\n## them (alternatively we could have done output as part of this call too)\n\nstyles <- c(\n  getOption('fansi.style', dflt_css()),  # default style\n  \"PRE.fansi CODE {background-color: transparent;}\",\n  \"PRE.fansi-error {background-color: #DD5555;}\",\n  \"PRE.fansi-warning {background-color: #DDDD55;}\",\n  \"PRE.fansi-message {background-color: #EEEEEE;}\"\n)\nold.hooks <- c(\n  old.hooks,\n  fansi::set_knit_hooks(\n    knitr::knit_hooks,\n    which=c('warning', 'error', 'message'),\n    style=styles\n) )\n```\n## You may restore old hooks with the following chunk\n\n## Restore Hooks\n```{r}\ndo.call(knitr::knit_hooks$set, old.hooks)\n```\n}",
            "sgr_256": "writeLines(sgr_256())",
            "state_at_end": "x <- c(\"\\033[44mhello\", \"\\033[33mworld\")\nstate_at_end(x)\nstate_at_end(x, carry=TRUE)\n(close <- close_state(state_at_end(x, carry=TRUE), normalize=TRUE))\nwriteLines(paste0(x, close, \" no style\"))",
            "strip_ctl": "string <- \"hello\\033k\\033[45p world\\n\\033[31mgoodbye\\a moon\"\nstrip_ctl(string)\nstrip_ctl(string, c(\"nl\", \"c0\", \"sgr\", \"csi\", \"esc\")) # equivalently\nstrip_ctl(string, \"sgr\")\nstrip_ctl(string, c(\"c0\", \"esc\"))\n\n## everything but C0 controls, we need to specify \"nl\"\n## in addition to \"c0\" since \"nl\" is not part of \"c0\"\n## as far as the `strip` argument is concerned\nstrip_ctl(string, c(\"all\", \"nl\", \"c0\"))",
            "strip_sgr": "## convenience function, same as `strip_ctl(ctl=c('sgr', 'url'))`\nstring <- \"hello\\033k\\033[45p world\\n\\033[31mgoodbye\\a moon\"\nstrip_sgr(string)",
            "strsplit_ctl": "strsplit_ctl(\"\\033[31mhello\\033[42m world!\", \" \")\n\n## Splitting by newlines does not work as they are _Control\n## Sequences_, but we can use `ctl` to treat them as ordinary\nstrsplit_ctl(\"\\033[31mhello\\033[42m\\nworld!\", \"\\n\")\nstrsplit_ctl(\"\\033[31mhello\\033[42m\\nworld!\", \"\\n\", ctl=c(\"all\", \"nl\"))",
            "strtrim_ctl": "strtrim_ctl(\"\\033[42mHello world\\033[m\", 6)",
            "strwrap_ctl": "hello.1 <- \"hello \\033[41mred\\033[49m world\"\nhello.2 <- \"hello\\t\\033[41mred\\033[49m\\tworld\"\n\nstrwrap_ctl(hello.1, 12)\nstrwrap_ctl(hello.2, 12)\n\n## In default mode strwrap2_ctl is the same as strwrap_ctl\nstrwrap2_ctl(hello.2, 12)\n\n## But you can leave whitespace unchanged, `warn`\n## set to false as otherwise tabs causes warning\nstrwrap2_ctl(hello.2, 12, strip.spaces=FALSE, warn=FALSE)\n\n## And convert tabs to spaces\nstrwrap2_ctl(hello.2, 12, tabs.as.spaces=TRUE)\n\n## If your display has 8 wide tab stops the following two\n## outputs should look the same\nwriteLines(strwrap2_ctl(hello.2, 80, tabs.as.spaces=TRUE))\nwriteLines(hello.2)\n\n## tab stops are NOT auto-detected, but you may provide\n## your own\nstrwrap2_ctl(hello.2, 12, tabs.as.spaces=TRUE, tab.stops=c(6, 12))\n\n## You can also force padding at the end to equal width\nwriteLines(strwrap2_ctl(\"hello how are you today\", 10, pad.end=\".\"))\n\n## And a more involved example where we read the\n## NEWS file, color it line by line, wrap it to\n## 25 width and display some of it in 3 columns\n## (works best on displays that support 256 color\n## SGR sequences)\n\nNEWS <- readLines(file.path(R.home('doc'), 'NEWS'))\nNEWS.C <- fansi_lines(NEWS, step=2)  # color each line\nW <- strwrap2_ctl(NEWS.C, 25, pad.end=\" \", wrap.always=TRUE)\nwriteLines(c(\"\", paste(W[1:20], W[100:120], W[200:220]), \"\"))",
            "substr_ctl": "substr_ctl(\"\\033[42mhello\\033[m world\", 1, 9)\nsubstr_ctl(\"\\033[42mhello\\033[m world\", 3, 9)\n\n## Positions 2 and 4 are in the middle of the full width W (\\uFF37) for\n## the `start` and `stop` positions respectively. Use `round`\n## to control result:\nx <- \"\\uFF37n\\uFF37\"\nx\nsubstr2_ctl(x, 2, 4, type='width', round='start')\nsubstr2_ctl(x, 2, 4, type='width', round='stop')\nsubstr2_ctl(x, 2, 4, type='width', round='neither')\nsubstr2_ctl(x, 2, 4, type='width', round='both')\n\n## We can specify which escapes are considered special:\nsubstr_ctl(\"\\033[31mhello\\tworld\", 1, 6, ctl='sgr', warn=FALSE)\nsubstr_ctl(\"\\033[31mhello\\tworld\", 1, 6, ctl=c('all', 'c0'), warn=FALSE)\n\n## `carry` allows SGR to carry from one element to the next\nsubstr_ctl(c(\"\\033[33mhello\", \"world\"), 1, 3)\nsubstr_ctl(c(\"\\033[33mhello\", \"world\"), 1, 3, carry=TRUE)\nsubstr_ctl(c(\"\\033[33mhello\", \"world\"), 1, 3, carry=\"\\033[44m\")\n\n## We can omit the termination\nbleed <- substr_ctl(c(\"\\033[41mhello\", \"world\"), 1, 3, terminate=FALSE)\nwriteLines(bleed)      # Style will bleed out of string\nend <- \"\\033[0m\\n\"\nwriteLines(end)        # Stanch bleeding\n\n## Trailing sequences omitted unless `stop` past end.\nsubstr_ctl(\"ABC\\033[42m\", 1, 3, terminate=FALSE)\nsubstr_ctl(\"ABC\\033[42m\", 1, 4, terminate=FALSE)\n\n## Replacement functions\nx0<- x1 <- x2 <- x3 <- c(\"\\033[42mABC\", \"\\033[34mDEF\")\nsubstr_ctl(x1, 2, 2) <- \"_\"\nsubstr_ctl(x2, 2, 2) <- \"\\033[m_\"\nsubstr_ctl(x3, 2, 2) <- \"\\033[45m_\"\nwriteLines(c(x0, end, x1, end, x2, end, x3, end))\n\n## With `carry = TRUE` strings look like original\nx0<- x1 <- x2 <- x3 <- c(\"\\033[42mABC\", \"\\033[34mDEF\")\nsubstr_ctl(x0, 2, 2, carry=TRUE) <- \"_\"\nsubstr_ctl(x1, 2, 2, carry=TRUE) <- \"\\033[m_\"\nsubstr_ctl(x2, 2, 2, carry=TRUE) <- \"\\033[45m_\"\nwriteLines(c(x0, end, x1, end, x2, end, x3, end))\n\n## Work-around to specify carry strings in replacement mode\nx <- c(\"ABC\", \"DEF\")\nval <- \"#\"\nx2 <- c(\"\\033[42m\", x)\nval2 <- c(\"\\033[45m\", rep_len(val, length(x)))\nsubstr_ctl(x2, 2, 2, carry=TRUE) <- val2\n(x <- x[-1])",
            "tabs_as_spaces": "string <- '1\\t12\\t123\\t1234\\t12345678'\ntabs_as_spaces(string)\nwriteLines(\n  c(\n    '-------|-------|-------|-------|-------|',\n    tabs_as_spaces(string)\n) )\nwriteLines(\n  c(\n    '-|--|--|--|--|--|--|--|--|--|--|',\n    tabs_as_spaces(string, tab.stops=c(2, 3))\n) )\nwriteLines(\n  c(\n    '-|--|-------|-------|-------|',\n    tabs_as_spaces(string, tab.stops=c(2, 3, 8))\n) )",
            "term_cap_test": "term_cap_test()",
            "to_html": "to_html(\"hello\\033[31;42;1mworld\\033[m\")\nto_html(\"hello\\033[31;42;1mworld\\033[m\", classes=TRUE)\n\n## Input contains HTML special chars\nx <- \"<hello \\033[42m'there' \\033[34m &amp;\\033[m \\\"moon\\\"!\"\nwriteLines(x)\n\\dontrun{\nin_html(\n  c(\n    to_html(html_esc(x)),  # Good\n    to_html(x)             # Bad (warning)!\n) )\n}\n## Generate some class names for basic colors\nclasses <- expand.grid(\n  \"myclass\",\n  c(\"fg\", \"bg\"),\n  c(\"black\", \"red\", \"green\", \"yellow\", \"blue\", \"magenta\", \"cyan\", \"white\")\n)\nclasses  # order is important!\nclasses <- do.call(paste, c(classes, sep=\"-\"))\n## We only provide 16 classes, so Only basic colors are\n## mapped to classes; others styled inline.\nto_html(\n  \"\\033[94mhello\\033[m \\033[31;42;1mworld\\033[m\",\n  classes=classes\n)\n## Create a whole web page with a style sheet for 256 colors and\n## the colors shown in a table.\nclass.256 <- do.call(paste, c(expand.grid(c(\"fg\", \"bg\"), 0:255), sep=\"-\"))\nsgr.256 <- sgr_256()     # A demo of all 256 colors\nwriteLines(sgr.256[1:8]) # SGR formatting\n\n## Convert to HTML using classes instead of inline styles:\nhtml.256 <- to_html(sgr.256, classes=class.256)\nwriteLines(html.256[1])  # No inline colors\n\n## Generate different style sheets.  See `?make_styles` for details.\ndefault <- make_styles(class.256)\nmix <- matrix(c(.6,.2,.2, .2,.6,.2, .2,.2,.6), 3)\ndesaturated <- make_styles(class.256, mix)\nwriteLines(default[1:4])\nwriteLines(desaturated[1:4])\n\n## Embed in HTML page and diplay; only CSS changing\n\\dontrun{\nin_html(html.256)                  # no CSS\nin_html(html.256, css=default)     # default CSS\nin_html(html.256, css=desaturated) # desaturated CSS\n}",
            "trimws_ctl": "trimws_ctl(\" \\033[31m\\thello world\\t\\033[39m  \")",
            "unhandled_ctl": "string <- c(\n  \"\\033[41mhello world\\033[m\", \"foo\\033[22>m\", \"\\033[999mbar\",\n  \"baz \\033[31#3m\", \"a\\033[31k\", \"hello\\033m world\"\n)\nunhandled_ctl(string)"
        }
    },
    "crayon": {
        "description": "The crayon package is now superseded. Please use the 'cli'\n    package for new projects.  Colored terminal output on terminals that\n    support 'ANSI' color and highlight codes. It also works in 'Emacs'\n    'ESS'. 'ANSI' color support is automatically detected. Colors and\n    highlighting can be combined and nested. New styles can also be\n    created easily.  This package was inspired by the 'chalk' 'JavaScript'\n    project.",
        "examples": {
            "col_align": "col_align(red(\"foobar\"), 20, \"left\")\ncol_align(red(\"foobar\"), 20, \"center\")\ncol_align(red(\"foobar\"), 20, \"right\")",
            "col_nchar": "str <- paste(\n  red(\"red\"),\n  \"default\",\n  green(\"green\")\n)\n\ncat(str, \"\\n\")\nnchar(str)\ncol_nchar(str)\nnchar(strip_style(str))",
            "col_strsplit": "str <- red(\"I am red---\") \\%+\\%\n  green(\"and I am green-\") \\%+\\%\n  underline(\"I underlined\")\n\ncat(str, \"\\n\")\n\n# split at dashes, keep color\ncat(col_strsplit(str, \"[-]+\")[[1]], sep = \"\\n\")\nstrsplit(strip_style(str), \"[-]+\")\n\n# split to characters, keep color\ncat(col_strsplit(str, \"\")[[1]], \"\\n\", sep = \" \")\nstrsplit(strip_style(str), \"\")",
            "col_substr": "str <- paste(\n  red(\"red\"),\n  \"default\",\n  green(\"green\")\n)\n\ncat(str, \"\\n\")\ncat(col_substr(str, 1, 5), \"\\n\")\ncat(col_substr(str, 1, 15), \"\\n\")\ncat(col_substr(str, 3, 7), \"\\n\")\n\nsubstr(strip_style(str), 1, 5)\nsubstr(strip_style(str), 1, 15)\nsubstr(strip_style(str), 3, 7)\n\nstr2 <- \"another \" \\%+\\%\n  red(\"multi-\", sep = \"\", underline(\"style\")) \\%+\\%\n  \" text\"\n\ncat(str2, \"\\n\")\ncat(col_substr(c(str, str2), c(3,5), c(7, 18)), sep = \"\\n\")\nsubstr(strip_style(c(str, str2)), c(3,5), c(7, 18))",
            "col_substring": "str <- paste(\n  red(\"red\"),\n  \"default\",\n  green(\"green\")\n)\n\ncat(str, \"\\n\")\ncat(col_substring(str, 1, 5), \"\\n\")\ncat(col_substring(str, 1, 15), \"\\n\")\ncat(col_substring(str, 3, 7), \"\\n\")\n\nsubstring(strip_style(str), 1, 5)\nsubstring(strip_style(str), 1, 15)\nsubstring(strip_style(str), 3, 7)\n\nstr2 <- \"another \" \\%+\\%\n  red(\"multi-\", sep = \"\", underline(\"style\")) \\%+\\%\n  \" text\"\n\ncat(str2, \"\\n\")\ncat(col_substring(str2, c(3,5), c(7, 18)), sep = \"\\n\")\nsubstring(strip_style(str2), c(3,5), c(7, 18))",
            "combine_styles": "## Use style names\nalert <- combine_styles(\"bold\", \"red4\", \"bgCyan\")\ncat(alert(\"Warning!\"), \"\\n\")\n\n## Or style functions\nalert <- combine_styles(bold, red, bgCyan)\ncat(alert(\"Warning!\"), \"\\n\")\n\n## Combine a composite style\nalert <- combine_styles(bold, combine_styles(red, bgCyan))\ncat(alert(\"Warning!\"), \"\\n\")\n\n## Shorter notation\nalert <- bold $ red $ bgCyan\ncat(alert(\"Warning!\"), \"\\n\")",
            "concat": "\"foo\" \\%+\\% \"bar\"\n\nletters[1:10] \\%+\\% chr(1:10)\n\nletters[1:10] \\%+\\% \"-\" \\%+\\% chr(1:10)\n\n## This is empty (unlike for parse)\ncharacter() \\%+\\% \"*\"",
            "crayon": "cat(blue(\"Hello\", \"world!\"))\n\ncat(\"... to highlight the \" \\%+\\% red(\"search term\") \\%+\\%\n    \" in a block of text\")\n\ncat(yellow$bgMagenta$bold('Hello world!'))\n\ncat(green(\n 'I am a green line ' \\%+\\%\n blue$underline$bold('with a blue substring') \\%+\\%\n ' that becomes green again!'\n))\n\nerror <- red $ bold\nwarn <- magenta $ underline\nnote <- cyan\ncat(error(\"Error: subscript out of bounds!\\n\"))\ncat(warn(\"Warning: shorter argument was recycled.\\n\"))\ncat(note(\"Note: no such directory.\\n\"))",
            "drop_style": "make_style(new_style = \"maroon\", bg = TRUE)\ncat(style(\"I am maroon\", \"new_style\"), \"\\n\")\ndrop_style(\"new_style\")\n\"new_style\" \\%in\\% names(styles())",
            "has_color": "has_color()",
            "has_style": "## The second one has style if crayon is enabled\nhas_style(\"foobar\")\nhas_style(red(\"foobar\"))",
            "hyperlink": "cat(\"This is an\", hyperlink(\"R\", \"https://r-project.org\"), \"link.\\n\")\nhas_hyperlink()",
            "make_style": "## Create a style function without creating a style\npink <- make_style(\"pink\")\nbgMaroon <- make_style(rgb(0.93, 0.19, 0.65), bg = TRUE)\ncat(bgMaroon(pink(\"I am pink if your terminal wants it, too.\\n\")))\n\n## Create a new style for pink and maroon background\nmake_style(pink = \"pink\")\nmake_style(bgMaroon = rgb(0.93, 0.19, 0.65), bg = TRUE)\n\"pink\" \\%in\\% names(styles())\n\"bgMaroon\" \\%in\\% names(styles())\ncat(style(\"I am pink, too!\\n\", \"pink\", bg = \"bgMaroon\"))",
            "num_ansi_colors": "num_ansi_colors()",
            "num_colors": "num_colors()",
            "start.crayon": "## The input is red (if color is supported)\nget_name <- function() {\n  cat(\"Enter your name:\", start(red))\n  input <- readline()\n  cat(finish(red))\n  input\n}\nname <- get_name()\nname",
            "strip_style": "strip_style(red(\"foobar\")) == \"foobar\"",
            "style": "## These are equivalent\nstyle(\"foobar\", bold)\nstyle(\"foobar\", \"bold\")\nbold(\"foobar\")",
            "styles": "names(styles())\ncat(styles()[[\"bold\"]]$close)"
        }
    },
    "rmarkdown": {
        "description": "Convert R Markdown documents into a variety of formats.",
        "examples": {
            "available_templates": "# List rmarkdown templates & create a draft\navailable_templates()\n\n# List rticles templates\navailable_templates(\"rticles\")",
            "beamer_presentation": "\\dontrun{\n\nlibrary(rmarkdown)\n\n# simple invocation\nrender(\"pres.Rmd\", beamer_presentation())\n\n# specify an option for incremental rendering\nrender(\"pres.Rmd\", beamer_presentation(incremental = TRUE))\n}",
            "context_document": "\\dontrun{\nlibrary(rmarkdown)\n\n# simple invocation\nrender(\"input.Rmd\", context_document())\n}",
            "convert_ipynb": "# this is not a real ipynb file, but illustrates what convert_ipynb() does\nnb_data <- list(\n  cells = list(\n    list(cell_type = 'markdown', source = 'Hi **Markdown**!'),\n    list(cell_type = 'code', source = 'print(\"Hi R Markdown!\")')\n  ),\n  metadata = list(\n    kernelspec = list(language = 'python')\n  )\n)\nnb_file = tempfile(fileext = '.ipynb')\njsonlite::write_json(nb_data, nb_file, auto_unbox = TRUE, pretty = TRUE)\nxfun::file_string(nb_file)  # show file content\n\n# convert to R Markdown\nnb_rmd = rmarkdown:::convert_ipynb(nb_file)\nxfun::file_string(nb_rmd)",
            "draft": "\\dontrun{\nrmarkdown::draft(\"Q4Report.Rmd\",\n                 template=\"/opt/rmd/templates/quarterly_report\")\n\nrmarkdown::draft(\"Q4Report.Rmd\",\n                 template=\"quarterly_report\", package=\"pubtools\")\n}",
            "find_pandoc": "rmarkdown::find_pandoc()\nrmarkdown::find_pandoc(dir = '~/Downloads/Pandoc')\nrmarkdown::find_pandoc(version = '2.7.3')",
            "html_document": "\\dontrun{\nlibrary(rmarkdown)\n\nrender(\"input.Rmd\", html_document())\n\nrender(\"input.Rmd\", html_document(toc = TRUE))\n}",
            "includes": "\\dontrun{\nlibrary(rmarkdown)\n\nhtml_document(includes = includes(before_body = \"header.htm\"))\n\npdf_document(includes = includes(after_body = \"footer.tex\"))\n}",
            "md_document": "\\dontrun{\nlibrary(rmarkdown)\n\nrender(\"input.Rmd\", md_document())\n\nrender(\"input.Rmd\", md_document(variant = \"markdown_github\"))\n}",
            "metadata": "rmarkdown::metadata",
            "odt_document": "\\dontrun{\nlibrary(rmarkdown)\n\n# simple invocation\nrender(\"input.Rmd\", odt_document())\n\n# specify an option for syntax highlighting\nrender(\"input.Rmd\", odt_document(highlight = \"zenburn\"))\n}",
            "output_format": "\\dontrun{\noutput_format(knitr = knitr_options(opts_chunk = list(dev = 'png')),\n              pandoc = pandoc_options(to = \"html\"))\n}",
            "output_format_dependency": "# Implicitly add lua filters from within a chunk\n# This relies on (implicit) printing of the dependency in a chunk via\n# knitr::knit_print()`\noutput_format_dependency(\n  \"lua_filter1\",\n  pandoc = list(lua_filters = \"example1.lua\")\n)\n\n# Explicitly add lua filters from within a chunk\nknitr::knit_meta_add(list(output_format_dependency(\n  \"lua_filter2\",\n  pandoc = list(lua_filters = \"example2.lua\")\n)))\n\n# List the available dependencies\n# Note that the list may include dependencies with duplicated names. In that\n# case, the first one is merged to the output format and the others are\n# discarded.\nstr(knitr::knit_meta(\"output_format_dependency\", clean = FALSE))",
            "pandoc_args": "\\dontrun{\nlibrary(rmarkdown)\n\npandoc_include_args(before_body = \"header.htm\")\npandoc_include_args(before_body = \"header.tex\")\n\npandoc_highlight_args(\"kate\")\n\npandoc_latex_engine_args(\"pdflatex\")\n\npandoc_toc_args(toc = TRUE, toc_depth = 2)\n}",
            "pandoc_available": "\\dontrun{\nlibrary(rmarkdown)\n\nif (pandoc_available())\n  cat(\"pandoc\", as.character(pandoc_version()), \"is available!\\n\")\n\nif (pandoc_available(\"1.12.3\"))\n  cat(\"required version of pandoc is available!\\n\")\n}",
            "pandoc_convert": "\\dontrun{\nlibrary(rmarkdown)\n\n# convert markdown to various formats\npandoc_convert(\"input.md\", to = \"html\")\npandoc_convert(\"input.md\", to = \"latex\")\n\n# process citations\npandoc_convert(\"input.md\", to = \"html\", citeproc = TRUE)\n\n# add some pandoc options\npandoc_convert(\"input.md\", to = \"latex\", options = c(\"--listings\"))\n}",
            "pdf_document": "\\dontrun{\nlibrary(rmarkdown)\n\n# simple invocation\nrender(\"input.Rmd\", pdf_document())\n\n# specify an option for latex engine\nrender(\"input.Rmd\", pdf_document(latex_engine = \"lualatex\"))\n\n# add a table of contents and pass an option to pandoc\nrender(\"input.Rmd\", pdf_document(toc = TRUE, \"--listings\"))\n}",
            "pkg_file_lua": "# list all Lua filters stored in the rmarkdown package\npkg_file_lua()\n# get a specific filter\npkg_file_lua(c(\"pagebreak.lua\", \"latex_div.lua\"))",
            "publish_site": "\\dontrun{\nlibrary(rmarkdown)\npublish_site()\n}",
            "render": "\\dontrun{\nlibrary(rmarkdown)\n\n# Render the default (first) format defined in the file\nrender(\"input.Rmd\")\n\n# Render all formats defined in the file\nrender(\"input.Rmd\", \"all\")\n\n# Render a single format, using parameters for \\code{html_document} from\n# the YAML header parameters.\nrender(\"input.Rmd\", \"html_document\")\n\n# Render a single format, ignoring parameters for \\code{html_document} in\n# the YAML header. Any parameters not passed as arguments to\n# \\code{html_document()} will be assigned to their default values, regardless\n# of anything in the YAML header\nrender(\"input.Rmd\", html_document(toc = TRUE, toc_depth = 2))\n\n# Render multiple formats\nrender(\"input.Rmd\", c(\"html_document\", \"pdf_document\"))\n}",
            "render_delayed": "\\dontrun{\n# Add the following code to an R Markdown document\n\ndiv(Sys.time())\n\nrender_delayed({\n Sys.sleep(3)      # simulate an expensive computation\n div(Sys.time())\n})\n\ndiv(Sys.time())\n}",
            "rmarkdown_format": "\\dontrun{\nrmarkdown_format(\"-implicit_figures\")\n}",
            "rtf_document": "\\dontrun{\n\nlibrary(rmarkdown)\n\n# simple invocation\nrender(\"input.Rmd\", rtf_document())\n\n# specify table of contents option\nrender(\"input.Rmd\", rtf_document(toc = TRUE))\n}",
            "run": "\\dontrun{\n# Run the Shiny document \"index.Rmd\" in the current directory\nrmarkdown::run()\n\n# Run the Shiny document \"shiny_doc.Rmd\" on port 8241\nrmarkdown::run(\"shiny_doc.Rmd\", shiny_args = list(port = 8241))\n}",
            "slidy_presentation": "\\dontrun{\nlibrary(rmarkdown)\n\n# simple invocation\nrender(\"pres.Rmd\", slidy_presentation())\n\n# specify an option for incremental rendering\nrender(\"pres.Rmd\", slidy_presentation(incremental = TRUE))\n}",
            "word_document": "\\dontrun{\nlibrary(rmarkdown)\n\n# simple invocation\nrender(\"input.Rmd\", word_document())\n\n# specify an option for syntax highlighting\nrender(\"input.Rmd\", word_document(highlight = \"zenburn\"))\n}"
        }
    },
    "r-utf8": {
        "description": "Process and print 'UTF-8' encoded international\n    text (Unicode). Input, validate, normalize, encode, format, and\n    display.",
        "examples": {
            "as_utf8": "# the second element is encoded in latin-1, but declared as UTF-8\nx <- c(\"fa\\u00E7ile\", \"fa\\xE7ile\", \"fa\\xC3\\xA7ile\")\nEncoding(x) <- c(\"UTF-8\", \"UTF-8\", \"bytes\")\n\n# attempt to convert to UTF-8 (fails)\n\\dontrun{as_utf8(x)}\n\ny <- x\nEncoding(y[2]) <- \"latin1\" # mark the correct encoding\nas_utf8(y) # succeeds\n\n# test for valid UTF-8\nutf8_valid(x)",
            "output": "# test whether ANSI style escapes or UTF-8 output are supported\ncat(\"ANSI:\", output_ansi(), \"\\n\")\ncat(\"UTF8:\", output_utf8(), \"\\n\")\n\n# switch to C locale\nSys.setlocale(\"LC_CTYPE\", \"C\")\ncat(\"ANSI:\", output_ansi(), \"\\n\")\ncat(\"UTF8:\", output_utf8(), \"\\n\")\n\n# switch to native locale\nSys.setlocale(\"LC_CTYPE\", \"\")\n\ntmp <- tempfile()\nsink(tmp) # redirect output to a file\ncat(\"ANSI:\", output_ansi(), \"\\n\")\ncat(\"UTF8:\", output_utf8(), \"\\n\")\nsink() # restore stdout\n\n# inspect the output\nreadLines(tmp)",
            "utf8_encode": "# the second element is encoded in latin-1, but declared as UTF-8\nx <- c(\"fa\\u00E7ile\", \"fa\\xE7ile\", \"fa\\xC3\\xA7ile\")\nEncoding(x) <- c(\"UTF-8\", \"UTF-8\", \"bytes\")\n\n# encoding\nutf8_encode(x)\n\n# add style to the escapes\ncat(utf8_encode(\"hello\\nstyled\\\\\\\\world\", escapes = \"1\"), \"\\n\")",
            "utf8_format": "# the second element is encoded in latin-1, but declared as UTF-8\nx <- c(\"fa\\u00E7ile\", \"fa\\xE7ile\", \"fa\\xC3\\xA7ile\")\nEncoding(x) <- c(\"UTF-8\", \"UTF-8\", \"bytes\")\n\n# formatting\nutf8_format(x, chars = 3)\nutf8_format(x, chars = 3, justify = \"centre\", width = 10)\nutf8_format(x, chars = 3, justify = \"right\")",
            "utf8_normalize": "angstrom <- c(\"\\u00c5\", \"\\u0041\\u030a\", \"\\u212b\")\nutf8_normalize(angstrom) == \"\\u00c5\"",
            "utf8_print": "# printing (assumes that output is capable of displaying Unicode 10.0.0)\nprint(intToUtf8(0x1F600 + 0:79)) # with default R print function\nutf8_print(intToUtf8(0x1F600 + 0:79)) # with utf8_print, truncates line\nutf8_print(intToUtf8(0x1F600 + 0:79), chars = 1000) # higher character limit\n\n# in C locale, output ASCII (same results on all platforms)\noldlocale <- Sys.getlocale(\"LC_CTYPE\")\ninvisible(Sys.setlocale(\"LC_CTYPE\", \"C\")) # switch to C locale\nutf8_print(intToUtf8(0x1F600 + 0:79))\ninvisible(Sys.setlocale(\"LC_CTYPE\", oldlocale)) # switch back to old locale\n\n# Mac and Linux only: style the names\n# see https://en.wikipedia.org/wiki/ANSI_escape_code#SGR_(Select_Graphic_Rendition)_parameters\nutf8_print(matrix(as.character(1:20), 4, 5),\n           names = \"1;4\", rownames = \"2;3\")",
            "utf8_width": "# the second element is encoded in latin-1, but declared as UTF-8\nx <- c(\"fa\\u00E7ile\", \"fa\\xE7ile\", \"fa\\xC3\\xA7ile\")\nEncoding(x) <- c(\"UTF-8\", \"UTF-8\", \"bytes\")\n\n# get widths\nutf8_width(x)\nutf8_width(x, encode = FALSE)\nutf8_width('\"')\nutf8_width('\"', quote = TRUE)"
        }
    },
    "data.table": {
        "description": "Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, friendly and fast character-separated-value read/write. Offers a natural and flexible syntax, for faster development.",
        "examples": {
            "IDateTime": "# create IDate:\n(d <- as.IDate(\"2001-01-01\"))\n\n# S4 coercion also works\nidentical(as.IDate(\"2001-01-01\"), methods::as(\"2001-01-01\", \"IDate\"))\n\n# create ITime:\n(t <- as.ITime(\"10:45\"))\n\n# S4 coercion also works\nidentical(as.ITime(\"10:45\"), methods::as(\"10:45\", \"ITime\"))\n\n(t <- as.ITime(\"10:45:04\"))\n\n(t <- as.ITime(\"10:45:04\", format = \"\\%H:\\%M:\\%S\"))\n\nas.POSIXct(\"2001-01-01\") + as.ITime(\"10:45\")\n\ndatetime <- seq(as.POSIXct(\"2001-01-01\"), as.POSIXct(\"2001-01-03\"), by = \"5 hour\")\n(af <- data.table(IDateTime(datetime), a = rep(1:2, 5), key = c(\"a\", \"idate\", \"itime\")))\n\naf[, mean(a), by = \"itime\"]\naf[, mean(a), by = list(hour = hour(itime))]\naf[, mean(a), by = list(wday = factor(weekdays(idate)))]\naf[, mean(a), by = list(wday = wday(idate))]\n\nas.POSIXct(af$idate)\nas.POSIXct(af$idate, time = af$itime)\nas.POSIXct(af$idate, af$itime)\nas.POSIXct(af$idate, time = af$itime, tz = \"GMT\")\n\nas.POSIXct(af$itime, af$idate)\nas.POSIXct(af$itime) # uses today's date\n\n(seqdates <- seq(as.IDate(\"2001-01-01\"), as.IDate(\"2001-08-03\"), by = \"3 weeks\"))\nround(seqdates, \"months\")\n\n(seqtimes <- seq(as.ITime(\"07:00\"), as.ITime(\"08:00\"), by = 20))\nround(seqtimes, \"hours\")\ntrunc(seqtimes, \"hours\")",
            "J": "DT = data.table(A=5:1, B=letters[5:1])\nsetkey(DT, B)   # reorders table and marks it sorted\nDT[J(\"b\")]      # returns the 2nd row\nDT[list(\"b\")]   # same\nDT[.(\"b\")]      # same using the dot alias for list\n\n# CJ usage examples\nCJ(c(5, NA, 1), c(1, 3, 2))                 # sorted and keyed data.table\ndo.call(CJ, list(c(5, NA, 1), c(1, 3, 2)))  # same as above\nCJ(c(5, NA, 1), c(1, 3, 2), sorted=FALSE)   # same order as input, unkeyed\n# use for 'unique=' argument\nx = c(1, 1, 2)\ny = c(4, 6, 4)\nCJ(x, y)              # output columns are automatically named 'x' and 'y'\nCJ(x, y, unique=TRUE) # unique(x) and unique(y) are computed automatically\nCJ(x, y, sorted = FALSE) # retain input order for y",
            "address": "x=1\naddress(x)",
            "all.equal.data.table": "dt1 <- data.table(A = letters[1:10], X = 1:10, key = \"A\")\ndt2 <- data.table(A = letters[5:14], Y = 1:10, key = \"A\")\nisTRUE(all.equal(dt1, dt1))\nis.character(all.equal(dt1, dt2))\n\n# ignore.col.order\nx <- copy(dt1)\ny <- dt1[, .(X, A)]\nall.equal(x, y)\nall.equal(x, y, ignore.col.order = TRUE)\n\n# ignore.row.order\nx <- setkeyv(copy(dt1), NULL)\ny <- dt1[sample(nrow(dt1))]\nall.equal(x, y)\nall.equal(x, y, ignore.row.order = TRUE)\n\n# check.attributes\nx = copy(dt1)\ny = setkeyv(copy(dt1), NULL)\nall.equal(x, y)\nall.equal(x, y, check.attributes = FALSE)\nx = data.table(1L)\ny = 1L\nall.equal(x, y)\nall.equal(x, y, check.attributes = FALSE)\n\n# trim.levels\nx <- data.table(A = factor(letters[1:10])[1:4]) # 10 levels\ny <- data.table(A = factor(letters[1:5])[1:4]) # 5 levels\nall.equal(x, y, trim.levels = FALSE)\nall.equal(x, y, trim.levels = FALSE, check.attributes = FALSE)\nall.equal(x, y)",
            "as.data.table": "nn = c(a=0.1, b=0.2, c=0.3, d=0.4)\nas.data.table(nn)\nas.data.table(nn, keep.rownames=TRUE)\nas.data.table(nn, keep.rownames=\"rownames\")\n\n# char object not converted to factor\ncc = c(X=\"a\", Y=\"b\", Z=\"c\")\nas.data.table(cc)\nas.data.table(cc, keep.rownames=TRUE)\nas.data.table(cc, keep.rownames=\"rownames\")\n\nmm = matrix(1:4, ncol=2, dimnames=list(c(\"r1\", \"r2\"), c(\"c1\", \"c2\")))\nas.data.table(mm)\nas.data.table(mm, keep.rownames=TRUE)\nas.data.table(mm, keep.rownames=\"rownames\")\nas.data.table(mm, key=\"c1\")\n\nll = list(a=1:2, b=3:4)\nas.data.table(ll)\nas.data.table(ll, keep.rownames=TRUE)\nas.data.table(ll, keep.rownames=\"rownames\")\n\nDF = data.frame(x=rep(c(\"x\",\"y\",\"z\"),each=2), y=c(1,3,6), row.names=LETTERS[1:6])\nas.data.table(DF)\nas.data.table(DF, keep.rownames=TRUE)\nas.data.table(DF, keep.rownames=\"rownames\")\n\nDT = data.table(x=rep(c(\"x\",\"y\",\"z\"),each=2), y=c(1:6))\nas.data.table(DT)\nas.data.table(DT, key='x')\n\nar = rnorm(27)\nar[sample(27, 15)] = NA\ndim(ar) = c(3L,3L,3L)\nas.data.table(ar)",
            "as.data.table.xts": "if (requireNamespace(\"xts\", quietly = TRUE)) {\n  data(sample_matrix, package = \"xts\")\n  sample.xts <- xts::as.xts(sample_matrix) # xts might not be attached on search path\n  # print head of xts\n  print(head(sample.xts))\n  # print data.table\n  print(as.data.table(sample.xts))\n}",
            "as.matrix": "DT <- data.table(A = letters[1:10], X = 1:10, Y = 11:20)\nas.matrix(DT) # character matrix\nas.matrix(DT, rownames = \"A\")\nas.matrix(DT, rownames = 1)\nas.matrix(DT, rownames = TRUE)\n\nsetkey(DT, A)\nas.matrix(DT, rownames = TRUE)",
            "as.xts.data.table": "if (requireNamespace(\"xts\", quietly = TRUE)) {\n  sample.dt <- data.table(date = as.Date((Sys.Date()-999):Sys.Date(),origin=\"1970-01-01\"),\n                          quantity = sample(10:50,1000,TRUE),\n                          value = sample(100:1000,1000,TRUE))\n  # print data.table\n  print(sample.dt)\n  # print head of xts\n  print(head(as.xts.data.table(sample.dt))) # xts might not be attached on search path\n}",
            "assign": "DT = data.table(a = LETTERS[c(3L,1:3)], b = 4:7)\nDT[, c := 8]                # add a numeric column, 8 for all rows\nDT[, d := 9L]               # add an integer column, 9L for all rows\nDT[, c := NULL]             # remove column c\nDT[2, d := -8L]             # subassign by reference to d; 2nd row is -8L now\nDT                          # DT changed by reference\nDT[2, d := 10L][]           # shorthand for update and print\n\nDT[b > 4, b := d * 2L]      # subassign to b with d*2L on those rows where b > 4 is TRUE\nDT[b > 4][, b := d * 2L]    # different from above. [, := ] is performed on the subset\n                            # which is an new (ephemeral) data.table. Result needs to be\n                            # assigned to a variable (using `<-`).\n\nDT[, e := mean(d), by = a]  # add new column by group by reference\nDT[\"A\", b := 0L, on = \"a\"]  # ad-hoc update of column b for group \"A\" using\n\t\t\t    # joins-as-subsets with binary search and 'on='\n# same as above but using keys\nsetkey(DT, a)\nDT[\"A\", b := 0L]            # binary search for group \"A\" and set column b using keys\nDT[\"B\", f := mean(d)]       # subassign to new column, NA initialized\n\n# Adding multiple columns\n## by name\nDT[ , c('sin_d', 'log_e', 'cos_d') :=\n   .(sin(d), log(e), cos(d))]\n## by patterned name\nDT[ , paste(c('sin', 'cos'), 'b', sep = '_') :=\n   .(sin(b), cos(b))]\n## using lapply & .SD\nDT[ , paste0('tan_', c('b', 'd', 'e')) :=\n   lapply(.SD, tan), .SDcols = c('b', 'd', 'e')]\n## using forced evaluation to disambiguate a vector of names\n##   and overwrite existing columns with their squares\nsq_cols = c('b', 'd', 'e')\nDT[ , (sq_cols) := lapply(.SD, `^`, 2L), .SDcols = sq_cols]\n## by integer (NB: for robustness, it is not recommended\n##   to use explicit integers to update/define columns)\nDT[ , c(2L, 3L, 4L) := .(sqrt(b), sqrt(d), sqrt(e))]\n## by implicit integer\nDT[ , grep('a$', names(DT)) := tolower(a)]\n## by implicit integer, using forced evaluation\nsq_col_idx = grep('d$', names(DT))\nDT[ , (sq_col_idx) := lapply(.SD, dnorm),\n   .SDcols = sq_col_idx]\n\n# Examples using `set` function\n## Set value for single cell\nset(DT, 1L, \"b\", 10L)\n## Set values for multiple columns in a specific row\nset(DT, 2L, c(\"b\", \"d\"), list(20L, 30L))\n## Set values by column indices\nset(DT, 3L, c(2L, 4L), list(40L, 50L))\n## Set value for an entire column without specifying rows\nset(DT, j = \"b\", value = 100L)\nset(DT, NULL, \"b\", 100L) # equivalent\n## Set values for multiple columns without specifying rows\nset(DT, j = c(\"b\", \"d\"), value = list(200L, 300L))\n## Set values for multiple columns with multiple specified rows.\nset(DT, c(1L, 3L), c(\"b\", \"d\"), value = list(500L, 800L))\n\n\\dontrun{\n# Speed example:\n\nm = matrix(1, nrow = 2e6L, ncol = 100L)\nDF = as.data.frame(m)\nDT = as.data.table(m)\n\nsystem.time(for (i in 1:1000) DF[i, 1] = i)\n# 15.856 seconds\nsystem.time(for (i in 1:1000) DT[i, V1 := i])\n# 0.279 seconds  (57 times faster)\nsystem.time(for (i in 1:1000) set(DT, i, 1L, i))\n# 0.002 seconds  (7930 times faster, overhead of [.data.table is avoided)\n\n# However, normally, we call [.data.table *once* on *large* data, not many times on small data.\n# The above is to demonstrate overhead, not to recommend looping in this way. But the option\n# of set() is there if you need it.\n}",
            "between": "X = data.table(a=1:5, b=6:10, c=c(5:1))\nX[b \\%between\\% c(7,9)]\nX[between(b, 7, 9)] # same as above\n# NEW feature in v1.9.8, vectorised between\nX[c \\%between\\% list(a,b)]\nX[between(c, a, b)] # same as above\nX[between(c, a, b, incbounds=FALSE)] # open interval\n\n# inrange()\nY = data.table(a=c(8,3,10,7,-10), val=runif(5))\nrange = data.table(start = 1:5, end = 6:10)\nY[a \\%inrange\\% range]\nY[inrange(a, range$start, range$end)] # same as above\nY[inrange(a, range$start, range$end, incbounds=FALSE)] # open interval",
            "chmatch": "# Please type 'example(chmatch)' to run this and see timings on your machine\n\nN = 1e5\n# N is set small here (1e5) to reduce runtime because every day CRAN runs and checks\n# all documentation examples in addition to the package's test suite.\n# The comments here apply when N has been changed to 1e8 and were run on 2018-05-13\n# with R 3.5.0 and data.table 1.11.2.\n\nu = as.character(as.hexmode(1:10000))\ny = sample(u,N,replace=TRUE)\nx = sample(u)\n                                           #  With N=1e8 ...\nsystem.time(a <- match(x,y))               #  4.6s\nsystem.time(b <- chmatch(x,y))             #  1.8s\nidentical(a,b)\n\nsystem.time(a <- x \\%in\\% y)               #  4.5s\nsystem.time(b <- x \\%chin\\% y)             #  1.7s\nidentical(a,b)\n\n# Different example with more unique strings ...\nu = as.character(as.hexmode(1:(N/10)))\ny = sample(u,N,replace=TRUE)\nx = sample(u,N,replace=TRUE)\nsystem.time(a <- match(x,y))               # 46s\nsystem.time(b <- chmatch(x,y))             # 16s\nidentical(a,b)",
            "coalesce": "x = c(11L, NA, 13L, NA, 15L, NA)\ny = c(NA, 12L, 5L, NA, NA, NA)\nz = c(11L, NA, 1L, 14L, NA, NA)\nfcoalesce(x, y, z)\nfcoalesce(list(x,y,z))   # same\nfcoalesce(x, list(y,z))  # same",
            "copy": "# Type 'example(copy)' to run these at prompt and browse output\n\nDT = data.table(A=5:1,B=letters[5:1])\nDT2 = copy(DT)        # explicit copy() needed to copy a data.table\nsetkey(DT2,B)         # now just changes DT2\nidentical(DT,DT2)     # FALSE. DT and DT2 are now different tables\n\nDT = data.table(A=5:1, B=letters[5:1])\nnm1 = names(DT)\nnm2 = copy(names(DT))\nDT[, C := 1L]\nidentical(nm1, names(DT)) # TRUE, nm1 is also changed by reference\nidentical(nm2, names(DT)) # FALSE, nm2 is a copy, different from names(DT)",
            "data.table-class": "## Used in inheritance.\nsetClass('SuperDataTable', contains='data.table')\n\n## Used in a slot\nsetClass('Something', representation(x='character', dt='data.table'))\nx <- new(\"Something\", x='check', dt=data.table(a=1:10, b=11:20))",
            "data.table": "\\dontrun{\nexample(data.table)  # to run these examples yourself\n}\nDF = data.frame(x=rep(c(\"b\",\"a\",\"c\"),each=3), y=c(1,3,6), v=1:9)\nDT = data.table(x=rep(c(\"b\",\"a\",\"c\"),each=3), y=c(1,3,6), v=1:9)\nDF\nDT\nidentical(dim(DT), dim(DF))    # TRUE\nidentical(DF$a, DT$a)          # TRUE\nis.list(DF)                    # TRUE\nis.list(DT)                    # TRUE\n\nis.data.frame(DT)              # TRUE\n\ntables()\n\n# basic row subset operations\nDT[2]                          # 2nd row\nDT[3:2]                        # 3rd and 2nd row\nDT[order(x)]                   # no need for order(DT$x)\nDT[order(x), ]                 # same as above. The ',' is optional\nDT[y>2]                        # all rows where DT$y > 2\nDT[y>2 & v>5]                  # compound logical expressions\nDT[!2:4]                       # all rows other than 2:4\nDT[-(2:4)]                     # same\n\n# select|compute columns data.table way\nDT[, v]                        # v column (as vector)\nDT[, list(v)]                  # v column (as data.table)\nDT[, .(v)]                     # same as above, .() is a shorthand alias to list()\nDT[, sum(v)]                   # sum of column v, returned as vector\nDT[, .(sum(v))]                # same, but return data.table (column autonamed V1)\nDT[, .(sv=sum(v))]             # same, but column named \"sv\"\nDT[, .(v, v*2)]                # return two column data.table, v and v*2\n\n# subset rows and select|compute data.table way\nDT[2:3, sum(v)]                # sum(v) over rows 2 and 3, return vector\nDT[2:3, .(sum(v))]             # same, but return data.table with column V1\nDT[2:3, .(sv=sum(v))]          # same, but return data.table with column sv\nDT[2:5, cat(v, \"\\n\")]          # just for j's side effect\n\n# select columns the data.frame way\nDT[, 2]                        # 2nd column, returns a data.table always\ncolNum = 2\nDT[, ..colNum]                 # same, .. prefix conveys one-level-up in calling scope\nDT[[\"v\"]]                      # same as DT[, v] but faster if called in a loop\n\n# grouping operations - j and by\nDT[, sum(v), by=x]             # ad hoc by, order of groups preserved in result\nDT[, sum(v), keyby=x]          # same, but order the result on by cols\nDT[, sum(v), by=x, keyby=TRUE] # same, but using sorting flag\nDT[, sum(v), by=x][order(x)]   # same but by chaining expressions together\n\n# fast ad hoc row subsets (subsets as joins)\nDT[\"a\", on=\"x\"]                # same as x == \"a\" but uses binary search (fast)\nDT[\"a\", on=.(x)]               # same, for convenience, no need to quote every column\nDT[.(\"a\"), on=\"x\"]             # same\nDT[x==\"a\"]                     # same, single \"==\" internally optimised to use binary search (fast)\nDT[x!=\"b\" | y!=3]              # not yet optimized, currently vector scan subset\nDT[.(\"b\", 3), on=c(\"x\", \"y\")]  # join on columns x,y of DT; uses binary search (fast)\nDT[.(\"b\", 3), on=.(x, y)]      # same, but using on=.()\nDT[.(\"b\", 1:2), on=c(\"x\", \"y\")]             # no match returns NA\nDT[.(\"b\", 1:2), on=.(x, y), nomatch=NULL]   # no match row is not returned\nDT[.(\"b\", 1:2), on=c(\"x\", \"y\"), roll=Inf]   # locf, nomatch row gets rolled by previous row\nDT[.(\"b\", 1:2), on=.(x, y), roll=-Inf]      # nocb, nomatch row gets rolled by next row\nDT[\"b\", sum(v*y), on=\"x\"]                   # on rows where DT$x==\"b\", calculate sum(v*y)\n\n# all together now\nDT[x!=\"a\", sum(v), by=x]                    # get sum(v) by \"x\" for each i != \"a\"\nDT[!\"a\", sum(v), by=.EACHI, on=\"x\"]         # same, but using subsets-as-joins\nDT[c(\"b\",\"c\"), sum(v), by=.EACHI, on=\"x\"]   # same\nDT[c(\"b\",\"c\"), sum(v), by=.EACHI, on=.(x)]  # same, using on=.()\n\n# joins as subsets\nX = data.table(x=c(\"c\",\"b\"), v=8:7, foo=c(4,2))\nX\n\nDT[X, on=\"x\"]                         # right join\nX[DT, on=\"x\"]                         # left join\nDT[X, on=\"x\", nomatch=NULL]           # inner join\nDT[!X, on=\"x\"]                        # not join\nDT[X, on=c(y=\"v\")]                    # join using column \"y\" of DT with column \"v\" of X\nDT[X, on=\"y==v\"]                      # same as above (v1.9.8+)\n\nDT[X, on=.(y<=foo)]                   # NEW non-equi join (v1.9.8+)\nDT[X, on=\"y<=foo\"]                    # same as above\nDT[X, on=c(\"y<=foo\")]                 # same as above\nDT[X, on=.(y>=foo)]                   # NEW non-equi join (v1.9.8+)\nDT[X, on=.(x, y<=foo)]                # NEW non-equi join (v1.9.8+)\nDT[X, .(x,y,x.y,v), on=.(x, y>=foo)]  # Select x's join columns as well\n\nDT[X, on=\"x\", mult=\"first\"]           # first row of each group\nDT[X, on=\"x\", mult=\"last\"]            # last row of each group\nDT[X, sum(v), by=.EACHI, on=\"x\"]      # join and eval j for each row in i\nDT[X, sum(v)*foo, by=.EACHI, on=\"x\"]  # join inherited scope\nDT[X, sum(v)*i.v, by=.EACHI, on=\"x\"]  # 'i,v' refers to X's v column\nDT[X, on=.(x, v>=v), sum(y)*foo, by=.EACHI] # NEW non-equi join with by=.EACHI (v1.9.8+)\n\n# setting keys\nkDT = copy(DT)                        # (deep) copy DT to kDT to work with it.\nsetkey(kDT,x)                         # set a 1-column key. No quotes, for convenience.\nsetkeyv(kDT,\"x\")                      # same (v in setkeyv stands for vector)\nv=\"x\"\nsetkeyv(kDT,v)                        # same\nhaskey(kDT)                           # TRUE\nkey(kDT)                              # \"x\"\n\n# fast *keyed* subsets\nkDT[\"a\"]                              # subset-as-join on *key* column 'x'\nkDT[\"a\", on=\"x\"]                      # same, being explicit using 'on=' (preferred)\n\n# all together\nkDT[!\"a\", sum(v), by=.EACHI]          # get sum(v) for each i != \"a\"\n\n# multi-column key\nsetkey(kDT,x,y)                       # 2-column key\nsetkeyv(kDT,c(\"x\",\"y\"))               # same\n\n# fast *keyed* subsets on multi-column key\nkDT[\"a\"]                              # join to 1st column of key\nkDT[\"a\", on=\"x\"]                      # on= is optional, but is preferred\nkDT[.(\"a\")]                           # same, .() is an alias for list()\nkDT[list(\"a\")]                        # same\nkDT[.(\"a\", 3)]                        # join to 2 columns\nkDT[.(\"a\", 3:6)]                      # join 4 rows (2 missing)\nkDT[.(\"a\", 3:6), nomatch=NULL]        # remove missing\nkDT[.(\"a\", 3:6), roll=TRUE]           # locf rolling join\nkDT[.(\"a\", 3:6), roll=Inf]            # same as above\nkDT[.(\"a\", 3:6), roll=-Inf]           # nocb rolling join\nkDT[!.(\"a\")]                          # not join\nkDT[!\"a\"]                             # same\n\n# more on special symbols, see also ?\"special-symbols\"\nDT[.N]                                  # last row\nDT[, .N]                                # total number of rows in DT\nDT[, .N, by=x]                          # number of rows in each group\nDT[, .SD, .SDcols=x:y]                  # select columns 'x' through 'y'\nDT[ , .SD, .SDcols = !x:y]              # drop columns 'x' through 'y'\nDT[ , .SD, .SDcols = patterns('^[xv]')] # select columns matching '^x' or '^v'\nDT[, .SD[1]]                            # first row of all columns\nDT[, .SD[1], by=x]                      # first row of 'y' and 'v' for each group in 'x'\nDT[, c(.N, lapply(.SD, sum)), by=x]     # get rows *and* sum columns 'v' and 'y' by group\nDT[, .I[1], by=x]                       # row number in DT corresponding to each group\nDT[, grp := .GRP, by=x]                 # add a group counter column\nDT[ , dput(.BY), by=.(x,y)]             # .BY is a list of singletons for each group\nX[, DT[.BY, y, on=\"x\"], by=x]           # join within each group\nDT[, {\n  # write each group to a different file\n  fwrite(.SD, file.path(tempdir(), paste0('x=', .BY$x, '.csv')))\n}, by=x]\ndir(tempdir())\n\n# add/update/delete by reference (see ?assign)\nprint(DT[, z:=42L])                   # add new column by reference\nprint(DT[, z:=NULL])                  # remove column by reference\nprint(DT[\"a\", v:=42L, on=\"x\"])        # subassign to existing v column by reference\nprint(DT[\"b\", v2:=84L, on=\"x\"])       # subassign to new column by reference (NA padded)\n\nDT[, m:=mean(v), by=x][]              # add new column by reference by group\n                                      # NB: postfix [] is shortcut to print()\n# advanced usage\nDT = data.table(x=rep(c(\"b\",\"a\",\"c\"),each=3), v=c(1,1,1,2,2,1,1,2,2), y=c(1,3,6), a=1:9, b=9:1)\n\nDT[, sum(v), by=.(y\\%\\%2)]              # expressions in by\nDT[, sum(v), by=.(bool = y\\%\\%2)]       # same, using a named list to change by column name\nDT[, .SD[2], by=x]                    # get 2nd row of each group\nDT[, tail(.SD,2), by=x]               # last 2 rows of each group\nDT[, lapply(.SD, sum), by=x]          # sum of all (other) columns for each group\nDT[, .SD[which.min(v)], by=x]         # nested query by group\n\nDT[, list(MySum=sum(v),\n          MyMin=min(v),\n          MyMax=max(v)),\n    by=.(x, y\\%\\%2)]                    # by 2 expressions\n\nDT[, .(a = .(a), b = .(b)), by=x]     # list columns\nDT[, .(seq = min(a):max(b)), by=x]    # j is not limited to just aggregations\nDT[, sum(v), by=x][V1<20]             # compound query\nDT[, sum(v), by=x][order(-V1)]        # ordering results\nDT[, c(.N, lapply(.SD,sum)), by=x]    # get number of observations and sum per group\nDT[, {tmp <- mean(y);\n      .(a = a-tmp, b = b-tmp)\n      }, by=x]                        # anonymous lambda in 'j', j accepts any valid\n                                      # expression. TO REMEMBER: every element of\n                                      # the list becomes a column in result.\npdf(\"new.pdf\")\nDT[, plot(a,b), by=x]                 # can also plot in 'j'\ndev.off()\n\\dontshow{file.remove(\"new.pdf\")}\n\n# using rleid, get max(y) and min of all cols in .SDcols for each consecutive run of 'v'\nDT[, c(.(y=max(y)), lapply(.SD, min)), by=rleid(v), .SDcols=v:b]\n\n# Support guide and links:\n# https://github.com/Rdatatable/data.table/wiki/Support\n\n\\dontrun{\nif (interactive()) {\n  vignette(package=\"data.table\")  # 9 vignettes\n\n  test.data.table()               # 6,000 tests\n\n  # keep up to date with latest stable version on CRAN\n  update.packages()\n\n  # get the latest devel version that has passed all tests\n  update_dev_pkg()\n  # read more at:\n  # https://github.com/Rdatatable/data.table/wiki/Installation\n}\n}",
            "datatable-optimize": "\\dontrun{\nold = options(datatable.optimize = Inf)\n\n# Generate a big data.table with a relatively many columns\nset.seed(1L)\nDT = lapply(1:20, function(x) sample(c(-100:100), 5e6L, TRUE))\nsetDT(DT)[, id := sample(1e5, 5e6, TRUE)]\nprint(object.size(DT), units=\"Mb\") # 400MB, not huge, but will do\n\n# 'order' optimisation\noptions(datatable.optimize = 1L) # optimisation 'on'\nsystem.time(ans1 <- DT[order(id)])\noptions(datatable.optimize = 0L) # optimisation 'off'\nsystem.time(ans2 <- DT[order(id)])\nidentical(ans1, ans2)\n\n# optimisation of 'lapply(.SD, fun)'\noptions(datatable.optimize = 1L) # optimisation 'on'\nsystem.time(ans1 <- DT[, lapply(.SD, min), by=id])\noptions(datatable.optimize = 0L) # optimisation 'off'\nsystem.time(ans2 <- DT[, lapply(.SD, min), by=id])\nidentical(ans1, ans2)\n\n# optimisation of 'mean'\noptions(datatable.optimize = 1L) # optimisation 'on'\nsystem.time(ans1 <- DT[, lapply(.SD, mean), by=id])\nsystem.time(ans2 <- DT[, lapply(.SD, base::mean), by=id])\nidentical(ans1, ans2)\n\n# optimisation of 'c(.N, lapply(.SD, ))'\noptions(datatable.optimize = 1L) # optimisation 'on'\nsystem.time(ans1 <- DT[, c(.N, lapply(.SD, min)), by=id])\noptions(datatable.optimize = 0L) # optimisation 'off'\nsystem.time(ans2 <- DT[, c(N=.N, lapply(.SD, min)), by=id])\nidentical(ans1, ans2)\n\n# GForce\noptions(datatable.optimize = 2L) # optimisation 'on'\nsystem.time(ans1 <- DT[, lapply(.SD, median), by=id])\nsystem.time(ans2 <- DT[, lapply(.SD, function(x) as.numeric(stats::median(x))), by=id])\nidentical(ans1, ans2)\n\n# optimized subsets\noptions(datatable.optimize = 2L)\nsystem.time(ans1 <- DT[id == 100L]) # vector scan\nsystem.time(ans2 <- DT[id == 100L]) # vector scan\nsystem.time(DT[id \\%in\\% 100:500])    # vector scan\n\noptions(datatable.optimize = 3L)\nsystem.time(ans1 <- DT[id == 100L]) # index + binary search subset\nsystem.time(ans2 <- DT[id == 100L]) # only binary search subset\nsystem.time(DT[id \\%in\\% 100:500])    # only binary search subset again\n\n# sensitivity to collate order\nold_lc_collate = Sys.getlocale(\"LC_COLLATE\")\n\nif (old_lc_collate == \"C\") {\n  Sys.setlocale(\"LC_COLLATE\", \"\")\n}\nDT = data.table(\n  grp = rep(1:2, each = 4L),\n  var = c(\"A\", \"a\", \"0\", \"1\", \"B\", \"b\", \"0\", \"1\")\n)\noptions(datatable.optimize = Inf)\nDT[, .(max(var), min(var)), by=grp]\n# GForce is deactivated because of the ad-hoc column 'tolower(var)',\n#   through which the result for 'max(var)' may also change\nDT[, .(max(var), min(tolower(var))), by=grp]\n\nSys.setlocale(\"LC_COLLATE\", old_lc_collate)\noptions(old)\n}",
            "dcast.data.table": "ChickWeight = as.data.table(ChickWeight)\nsetnames(ChickWeight, tolower(names(ChickWeight)))\nDT <- melt(as.data.table(ChickWeight), id.vars=2:4) # calls melt.data.table\n\n# dcast is an S3 method in data.table from v1.9.6\ndcast(DT, time ~ variable, fun.aggregate=mean)\ndcast(DT, diet ~ variable, fun.aggregate=mean)\ndcast(DT, diet+chick ~ time, drop=FALSE)\ndcast(DT, diet+chick ~ time, drop=FALSE, fill=0)\n\n# using subset\ndcast(DT, chick ~ time, fun.aggregate=mean, subset=.(time < 10 & chick < 20))\n\n# drop argument, #1512\nDT <- data.table(v1 = c(1.1, 1.1, 1.1, 2.2, 2.2, 2.2),\n                 v2 = factor(c(1L, 1L, 1L, 3L, 3L, 3L), levels=1:3),\n                 v3 = factor(c(2L, 3L, 5L, 1L, 2L, 6L), levels=1:6),\n                 v4 = c(3L, 2L, 2L, 5L, 4L, 3L))\n# drop=TRUE\ndcast(DT, v1+v2~v3, value.var='v4')                      # default is drop=TRUE\ndcast(DT, v1+v2~v3, value.var='v4', drop=FALSE)          # all missing combinations of LHS and RHS\ndcast(DT, v1+v2~v3, value.var='v4', drop=c(FALSE, TRUE)) # all missing combinations of LHS only\ndcast(DT, v1+v2~v3, value.var='v4', drop=c(TRUE, FALSE)) # all missing combinations of RHS only\n\n# using . and ...\nDT <- data.table(v1 = rep(1:2, each = 6),\n                 v2 = rep(rep(1:3, 2), each = 2),\n                 v3 = rep(1:2, 6),\n                 v4 = rnorm(6))\ndcast(DT, \\dots ~ v3, value.var=\"v4\") # same as v1+v2 ~ v3, value.var=\"v4\"\ndcast(DT, \\dots ~ v3, value.var=\"v4\", value.var.in.dots=TRUE) # same as v1+v2+v4~v3, value.var=\"v4\"\ndcast(DT, v1+v2+v3 ~ ., value.var=\"v4\")\n\n## for each combination of (v1, v2), add up all values of v4\ndcast(DT, v1+v2 ~ ., value.var=\"v4\", fun.aggregate=sum)\n\n# fill and types\ndcast(DT, v2~v3, value.var='v1', fun.aggregate=length, fill=0L)  #  0L --> 0\ndcast(DT, v2~v3, value.var='v4', fun.aggregate=length, fill=1.1) # 1.1 --> 1L\n\n# multiple value.var and multiple fun.aggregate\nDT = data.table(x=sample(5,20,TRUE), y=sample(2,20,TRUE),\n                z=sample(letters[1:2], 20,TRUE), d1=runif(20), d2=1L)\n# multiple value.var\ndcast(DT, x+y ~ z, fun.aggregate=sum, value.var=c(\"d1\",\"d2\"))\n# multiple fun.aggregate\ndcast(DT, x+y ~ z, fun.aggregate=list(sum, mean), value.var=\"d1\")\n# multiple fun.agg and value.var (all combinations)\ndcast(DT, x+y ~ z, fun.aggregate=list(sum, mean), value.var=c(\"d1\", \"d2\"))\n# multiple fun.agg and value.var (one-to-one)\ndcast(DT, x+y ~ z, fun.aggregate=list(sum, mean), value.var=list(\"d1\", \"d2\"))",
            "duplicated": "DT <- data.table(A = rep(1:3, each=4), B = rep(1:4, each=3),\n                  C = rep(1:2, 6), key = c(\"A\", \"B\"))\nduplicated(DT)\nunique(DT)\n\nduplicated(DT, by=\"B\")\nunique(DT, by=\"B\")\n\nduplicated(DT, by=c(\"A\", \"C\"))\nunique(DT, by=c(\"A\", \"C\"))\n\nDT = data.table(a=c(2L,1L,2L), b=c(1L,2L,1L))   # no key\nunique(DT)                   # rows 1 and 2 (row 3 is a duplicate of row 1)\n\nDT = data.table(a=c(3.142, 4.2, 4.2, 3.142, 1.223, 1.223), b=rep(1,6))\nunique(DT)                   # rows 1,2 and 5\n\nDT = data.table(a=tan(pi*(1/4 + 1:10)), b=rep(1,10))   # example from ?all.equal\nlength(unique(DT$a))         # 10 strictly unique floating point values\nall.equal(DT$a,rep(1,10))    # TRUE, all within tolerance of 1.0\nDT[,which.min(a)]            # row 10, the strictly smallest floating point value\nidentical(unique(DT),DT[1])  # TRUE, stable within tolerance\nidentical(unique(DT),DT[10]) # FALSE\n\n# fromLast = TRUE vs. FALSE\nDT <- data.table(A = c(1, 1, 2, 2, 3), B = c(1, 2, 1, 1, 2), C = c(\"a\", \"b\", \"a\", \"b\", \"a\"))\n\nduplicated(DT, by=\"B\", fromLast=FALSE) # rows 3,4,5 are duplicates\nunique(DT, by=\"B\", fromLast=FALSE) # equivalent: DT[!duplicated(DT, by=\"B\", fromLast=FALSE)]\n\nduplicated(DT, by=\"B\", fromLast=TRUE) # rows 1,2,3 are duplicates\nunique(DT, by=\"B\", fromLast=TRUE) # equivalent: DT[!duplicated(DT, by=\"B\", fromLast=TRUE)]\n\n# anyDuplicated\nanyDuplicated(DT, by=c(\"A\", \"B\"))    # 3L\nany(duplicated(DT, by=c(\"A\", \"B\")))  # TRUE\n\n# uniqueN, unique rows on key columns\nuniqueN(DT, by = key(DT))\n# uniqueN, unique rows on all columns\nuniqueN(DT)\n# uniqueN while grouped by \"A\"\nDT[, .(uN=uniqueN(.SD)), by=A]\n\n# uniqueN's na.rm=TRUE\nx = sample(c(NA, NaN, runif(3)), 10, TRUE)\nuniqueN(x, na.rm = FALSE) # 5, default\nuniqueN(x, na.rm=TRUE) # 3",
            "fcase": "x = 1:10\nfcase(\n\tx < 5L, 1L,\n\tx > 5L, 3L\n)\n\nfcase(\n\tx < 5L, 1L:10L,\n\tx > 5L, 3L:12L\n)\n\n# Lazy evaluation example\nfcase(\n\tx < 5L, 1L,\n\tx >= 5L, 3L,\n\tx == 5L, stop(\"provided value is an unexpected one!\")\n)\n\n# fcase preserves attributes, example with dates\nfcase(\n\tx < 5L, as.Date(\"2019-10-11\"),\n\tx > 5L, as.Date(\"2019-10-14\")\n)\n\n# fcase example with factor; note the matching levels\nfcase(\n\tx < 5L, factor(\"a\", levels=letters[1:3]),\n\tx > 5L, factor(\"b\", levels=letters[1:3])\n)\n\n# Example of using the 'default' argument\nfcase(\n\tx < 5L, 1L,\n\tx > 5L, 3L,\n\tdefault = 5L\n)\n\n# fcase can be used for recursion, unlike fifelse\n# Recursive function to calculate the Greatest Common Divisor\ngcd_dt = function(x,y) {\n  r = x\\%\\%y\n  fcase(!r, y, r, gcd_dt(x, y)) # Recursive call must be in the last when-value pair\n}\ngcd_dt(10L, 1L)",
            "fdroplevels": "# on vectors\nx = factor(letters[1:10])\nfdroplevels(x[1:5])\n# exclude levels from drop\nfdroplevels(x[1:5], exclude = c(\"a\", \"c\"))\n\n# on data.table\nDT = data.table(a = factor(1:10), b = factor(letters[1:10]))\ndroplevels(head(DT))[[\"b\"]]\n# exclude levels\ndroplevels(head(DT), exclude = c(\"b\", \"c\"))[[\"b\"]]\n# except columns from drop\ndroplevels(head(DT), except = 2)[[\"b\"]]\ndroplevels(head(DT), except = 1)[[\"b\"]]",
            "fifelse": "x = c(1:4, 3:2, 1:4)\nfifelse(x > 2L, x, x - 1L)\n\n# unlike ifelse, fifelse preserves attributes, taken from the 'yes' argument\ndates = as.Date(c(\"2011-01-01\",\"2011-01-02\",\"2011-01-03\",\"2011-01-04\",\"2011-01-05\"))\nifelse(dates == \"2011-01-01\", dates - 1, dates)\nfifelse(dates == \"2011-01-01\", dates - 1, dates)\nyes = factor(c(\"a\",\"b\",\"c\"))\nno = yes[1L]\nifelse(c(TRUE,FALSE,TRUE), yes, no)\nfifelse(c(TRUE,FALSE,TRUE), yes, no)\n\n# Example of using the 'na' argument\nfifelse(test = c(-5L:5L < 0L, NA), yes = 1L, no = 0L, na = 2L)\n\n# Example showing both 'yes' and 'no' arguments are evaluated, unlike ifelse\nfifelse(1 == 1, print(\"yes\"), print(\"no\"))\nifelse(1 == 1, print(\"yes\"), print(\"no\"))",
            "foverlaps": "require(data.table)\n## simple example:\nx = data.table(start=c(5,31,22,16), end=c(8,50,25,18), val2 = 7:10)\ny = data.table(start=c(10, 20, 30), end=c(15, 35, 45), val1 = 1:3)\nsetkey(y, start, end)\nfoverlaps(x, y, type=\"any\", which=TRUE) ## return overlap indices\nfoverlaps(x, y, type=\"any\") ## return overlap join\nfoverlaps(x, y, type=\"any\", mult=\"first\") ## returns only first match\nfoverlaps(x, y, type=\"within\") ## matches iff 'x' is within 'y'\n\n## with extra identifiers (ex: in genomics)\nx = data.table(chr=c(\"Chr1\", \"Chr1\", \"Chr2\", \"Chr2\", \"Chr2\"),\n               start=c(5,10, 1, 25, 50), end=c(11,20,4,52,60))\ny = data.table(chr=c(\"Chr1\", \"Chr1\", \"Chr2\"), start=c(1, 15,1),\n               end=c(4, 18, 55), geneid=letters[1:3])\nsetkey(y, chr, start, end)\nfoverlaps(x, y, type=\"any\", which=TRUE)\nfoverlaps(x, y, type=\"any\")\nfoverlaps(x, y, type=\"any\", nomatch=NULL)\nfoverlaps(x, y, type=\"within\", which=TRUE)\nfoverlaps(x, y, type=\"within\")\nfoverlaps(x, y, type=\"start\")\n\n## x and y have different column names - specify by.x\nx = data.table(seq=c(\"Chr1\", \"Chr1\", \"Chr2\", \"Chr2\", \"Chr2\"),\n               start=c(5,10, 1, 25, 50), end=c(11,20,4,52,60))\ny = data.table(chr=c(\"Chr1\", \"Chr1\", \"Chr2\"), start=c(1, 15,1),\n               end=c(4, 18, 55), geneid=letters[1:3])\nsetkey(y, chr, start, end)\nfoverlaps(x, y, by.x=c(\"seq\", \"start\", \"end\"),\n            type=\"any\", which=TRUE)",
            "frank": "# on vectors\nx = c(4, 1, 4, NA, 1, NA, 4)\n# NAs are considered identical (unlike base R)\n# default is average\nfrankv(x) # na.last=TRUE\nfrankv(x, na.last=FALSE)\n\n# ties.method = min\nfrankv(x, ties.method=\"min\")\n# ties.method = dense\nfrankv(x, ties.method=\"dense\")\n\n# on data.table\nDT = data.table(x, y=c(1, 1, 1, 0, NA, 0, 2))\nfrankv(DT, cols=\"x\") # same as frankv(x) from before\nfrankv(DT, cols=\"x\", na.last=\"keep\")\nfrankv(DT, cols=\"x\", ties.method=\"dense\", na.last=NA)\nfrank(DT, x, ties.method=\"dense\", na.last=NA) # equivalent of above using frank\n# on both columns\nfrankv(DT, ties.method=\"first\", na.last=\"keep\")\nfrank(DT, ties.method=\"first\", na.last=\"keep\") # equivalent of above using frank\n\n# order argument\nfrank(DT, x, -y, ties.method=\"first\")\n# equivalent of above using frankv\nfrankv(DT, order=c(1L, -1L), ties.method=\"first\")",
            "fread": "# Reads text input directly :\nfread(\"A,B\\n1,2\\n3,4\")\n\n# Reads pasted input directly :\nfread(\"A,B\n1,2\n3,4\n\")\n\n# Finds the first data line automatically :\nfread(\"\nThis is perhaps a banner line or two or ten.\nA,B\n1,2\n3,4\n\")\n\n# Detects whether column names are present automatically :\nfread(\"\n1,2\n3,4\n\")\n\n# Numerical precision :\n\nDT = fread(\"A\\n1.010203040506070809010203040506\\n\")\n# TODO: add numerals=c(\"allow.loss\", \"warn.loss\", \"no.loss\") from base::read.table, +\"use.Rmpfr\"\ntypeof(DT$A)==\"double\"   # currently \"allow.loss\" with no option\n\nDT = fread(\"A\\n1.46761e-313\\n\")   # read as 'numeric'\nDT[,sprintf(\"\\%.15E\",A)]   # beyond what double precision can store accurately to 15 digits\n# For greater accuracy use colClasses to read as character, then package Rmpfr.\n\n# colClasses\ndata = \"A,B,C,D\\n1,3,5,7\\n2,4,6,8\\n\"\nfread(data, colClasses=c(B=\"character\",C=\"character\",D=\"character\"))  # as read.csv\nfread(data, colClasses=list(character=c(\"B\",\"C\",\"D\")))    # saves typing\nfread(data, colClasses=list(character=2:4))     # same using column numbers\n\n# drop\nfread(data, colClasses=c(\"B\"=\"NULL\",\"C\"=\"NULL\"))   # as read.csv\nfread(data, colClasses=list(NULL=c(\"B\",\"C\")))      #\nfread(data, drop=c(\"B\",\"C\"))      # same but less typing, easier to read\nfread(data, drop=2:3)             # same using column numbers\n\n# select\n# (in read.csv you need to work out which to drop)\nfread(data, select=c(\"A\",\"D\"))    # less typing, easier to read\nfread(data, select=c(1,4))        # same using column numbers\n\n# select and types combined\nfread(data, select=c(A=\"numeric\", D=\"character\"))\nfread(data, select=list(numeric=\"A\", character=\"D\"))\n\n# skip blank lines\nfread(\"a,b\\n1,a\\n2,b\\n\\n\\n3,c\\n\", blank.lines.skip=TRUE)\n# fill\nfread(\"a,b\\n1,a\\n2\\n3,c\\n\", fill=TRUE)\nfread(\"a,b\\n\\n1,a\\n2\\n\\n3,c\\n\\n\", fill=TRUE)\n\n# fill with skip blank lines\nfread(\"a,b\\n\\n1,a\\n2\\n\\n3,c\\n\\n\", fill=TRUE, blank.lines.skip=TRUE)\n\n# check.names usage\nfread(\"a b,a b\\n1,2\\n\")\nfread(\"a b,a b\\n1,2\\n\", check.names=TRUE) # no duplicates + syntactically valid names\n\n\\dontrun{\n# Demo speed-up\nn = 1e6\nDT = data.table( a=sample(1:1000,n,replace=TRUE),\n                 b=sample(1:1000,n,replace=TRUE),\n                 c=rnorm(n),\n                 d=sample(c(\"foo\",\"bar\",\"baz\",\"qux\",\"quux\"),n,replace=TRUE),\n                 e=rnorm(n),\n                 f=sample(1:1000,n,replace=TRUE) )\nDT[2,b:=NA_integer_]\nDT[4,c:=NA_real_]\nDT[3,d:=NA_character_]\nDT[5,d:=\"\"]\nDT[2,e:=+Inf]\nDT[3,e:=-Inf]\n\nwrite.table(DT,\"test.csv\",sep=\",\",row.names=FALSE,quote=FALSE)\ncat(\"File size (MB):\", round(file.info(\"test.csv\")$size/1024^2),\"\\n\")\n# 50 MB (1e6 rows x 6 columns)\n\nsystem.time(DF1 <-read.csv(\"test.csv\",stringsAsFactors=FALSE))\n# 5.4 sec (first time in fresh R session)\n\nsystem.time(DF1 <- read.csv(\"test.csv\",stringsAsFactors=FALSE))\n# 3.9 sec (immediate repeat is faster, varies)\n\nsystem.time(DF2 <- read.table(\"test.csv\",header=TRUE,sep=\",\",quote=\"\",\n    stringsAsFactors=FALSE,comment.char=\"\",nrows=n,\n    colClasses=c(\"integer\",\"integer\",\"numeric\",\n                 \"character\",\"numeric\",\"integer\")))\n# 1.2 sec (consistently). All known tricks and known nrows, see references.\n\nsystem.time(DT <- fread(\"test.csv\"))\n# 0.1 sec (faster and friendlier)\n\nidentical(DF1, DF2)\nall.equal(as.data.table(DF1), DT)\n\n# Scaling up ...\nl = vector(\"list\",10)\nfor (i in 1:10) l[[i]] = DT\nDTbig = rbindlist(l)\ntables()\nwrite.table(DTbig,\"testbig.csv\",sep=\",\",row.names=FALSE,quote=FALSE)\n# 500MB csv (10 million rows x 6 columns)\n\nsystem.time(DF <- read.table(\"testbig.csv\",header=TRUE,sep=\",\",\n    quote=\"\",stringsAsFactors=FALSE,comment.char=\"\",nrows=1e7,\n    colClasses=c(\"integer\",\"integer\",\"numeric\",\n                 \"character\",\"numeric\",\"integer\")))\n# 17.0 sec (varies)\n\nsystem.time(DT <- fread(\"testbig.csv\"))\n#  0.8 sec\n\nall(mapply(all.equal, DF, DT))\n\n# Reads URLs directly :\nfread(\"https://www.stats.ox.ac.uk/pub/datasets/csb/ch11b.dat\")\n\n# Decompresses .gz and .bz2 automatically :\nfread(\"https://github.com/Rdatatable/data.table/raw/1.14.0/inst/tests/ch11b.dat.bz2\")\n\nfread(\"https://github.com/Rdatatable/data.table/raw/1.14.0/inst/tests/issue_785_fread.txt.gz\")\n\n}",
            "froll": "d = as.data.table(list(1:6/2, 3:8/4))\n# rollmean of single vector and single window\nfrollmean(d[, V1], 3)\n# multiple columns at once\nfrollmean(d, 3)\n# multiple windows at once\nfrollmean(d[, .(V1)], c(3, 4))\n# multiple columns and multiple windows at once\nfrollmean(d, c(3, 4))\n## three calls above will use multiple cores when available\n\n# partial window using adaptive rolling function\nan = function(n, len) c(seq.int(n), rep(n, len-n))\nn = an(3, nrow(d))\nfrollmean(d, n, adaptive=TRUE)\n\n# frollsum\nfrollsum(d, 3:4)\n\n# frollapply\nfrollapply(d, 3:4, sum)\nf = function(x, ...) if (sum(x, ...)>5) min(x, ...) else max(x, ...)\nfrollapply(d, 3:4, f, na.rm=TRUE)\n\n# performance vs exactness\nset.seed(108)\nx = sample(c(rnorm(1e3, 1e6, 5e5), 5e9, 5e-9))\nn = 15\nma = function(x, n, na.rm=FALSE) {\n  ans = rep(NA_real_, nx<-length(x))\n  for (i in n:nx) ans[i] = mean(x[(i-n+1):i], na.rm=na.rm)\n  ans\n}\nfastma = function(x, n, na.rm) {\n  if (!missing(na.rm)) stop(\"NAs are unsupported, wrongly propagated by cumsum\")\n  cs = cumsum(x)\n  scs = shift(cs, n)\n  scs[n] = 0\n  as.double((cs-scs)/n)\n}\nsystem.time(ans1<-ma(x, n))\nsystem.time(ans2<-fastma(x, n))\nsystem.time(ans3<-frollmean(x, n))\nsystem.time(ans4<-frollmean(x, n, algo=\"exact\"))\nsystem.time(ans5<-frollapply(x, n, mean))\nanserr = list(\n  fastma = ans2-ans1,\n  froll_fast = ans3-ans1,\n  froll_exact = ans4-ans1,\n  frollapply = ans5-ans1\n)\nerrs = sapply(lapply(anserr, abs), sum, na.rm=TRUE)\nsapply(errs, format, scientific=FALSE) # roundoff\n\n# frollapply corner cases\nf = function(x) head(x, 2)     ## FUN returns non length 1\ntry(frollapply(1:5, 3, f))\nf = function(x) {              ## FUN sometimes returns non length 1\n  n = length(x)\n  # length 1 will be returned only for first iteration where we check length\n  if (n==x[n]) x[1L] else range(x) # range(x)[2L] is silently ignored!\n}\nfrollapply(1:5, 3, f)\noptions(datatable.verbose=TRUE)\nx = c(1,2,1,1,1,2,3,2)\nfrollapply(x, 3, uniqueN)     ## FUN returns integer\nnumUniqueN = function(x) as.numeric(uniqueN(x))\nfrollapply(x, 3, numUniqueN)\nx = c(1,2,1,1,NA,2,NA,2)\nfrollapply(x, 3, anyNA)       ## FUN returns logical\nas.logical(frollapply(x, 3, anyNA))\noptions(datatable.verbose=FALSE)\nf = function(x) {             ## FUN returns character\n  if (sum(x)>5) \"big\" else \"small\"\n}\ntry(frollapply(1:5, 3, f))\nf = function(x) {             ## FUN is not type-stable\n  n = length(x)\n  # double type will be returned only for first iteration where we check type\n  if (n==x[n]) 1 else NA # NA logical turns into garbage without coercion to double\n}\ntry(frollapply(1:5, 3, f))",
            "fsort": "x = runif(1e6)\nsystem.time(ans1 <- sort(x, method=\"quick\"))\nsystem.time(ans2 <- fsort(x))\nidentical(ans1, ans2)",
            "fwrite": "DF = data.frame(A=1:3, B=c(\"foo\",\"A,Name\",\"baz\"))\nfwrite(DF)\nwrite.csv(DF, row.names=FALSE, quote=FALSE)  # same\n\nfwrite(DF, row.names=TRUE, quote=TRUE)\nwrite.csv(DF)                                # same\n\nDF = data.frame(A=c(2.1,-1.234e-307,pi), B=c(\"foo\",\"A,Name\",\"bar\"))\nfwrite(DF, quote='auto')        # Just DF[2,2] is auto quoted\nwrite.csv(DF, row.names=FALSE)  # same numeric formatting\n\nDT = data.table(A=c(2,5.6,-3),B=list(1:3,c(\"foo\",\"A,Name\",\"bar\"),round(pi*1:3,2)))\nfwrite(DT)\nfwrite(DT, sep=\"|\", sep2=c(\"{\",\",\",\"}\"))\n\n\\dontrun{\n\nset.seed(1)\nDT = as.data.table( lapply(1:10, sample,\n         x=as.numeric(1:5e7), size=5e6))                            #     382MB\nsystem.time(fwrite(DT, \"/dev/shm/tmp1.csv\"))                        #      0.8s\nsystem.time(write.csv(DT, \"/dev/shm/tmp2.csv\",                      #     60.6s\n                      quote=FALSE, row.names=FALSE))\nsystem(\"diff /dev/shm/tmp1.csv /dev/shm/tmp2.csv\")                  # identical\n\nset.seed(1)\nN = 1e7\nDT = data.table(\n  str1=sample(sprintf(\"\\%010d\",sample(N,1e5,replace=TRUE)), N, replace=TRUE),\n  str2=sample(sprintf(\"\\%09d\",sample(N,1e5,replace=TRUE)), N, replace=TRUE),\n  str3=sample(sapply(sample(2:30, 100, TRUE), function(n)\n     paste0(sample(LETTERS, n, TRUE), collapse=\"\")), N, TRUE),\n  str4=sprintf(\"\\%05d\",sample(sample(1e5,50),N,TRUE)),\n  num1=sample(round(rnorm(1e6,mean=6.5,sd=15),2), N, replace=TRUE),\n  num2=sample(round(rnorm(1e6,mean=6.5,sd=15),10), N, replace=TRUE),\n  str5=sample(c(\"Y\",\"N\"),N,TRUE),\n  str6=sample(c(\"M\",\"F\"),N,TRUE),\n  int1=sample(ceiling(rexp(1e6)), N, replace=TRUE),\n  int2=sample(N,N,replace=TRUE)-N/2\n)                                                                   #     774MB\nsystem.time(fwrite(DT,\"/dev/shm/tmp1.csv\"))                         #      1.1s\nsystem.time(write.csv(DT,\"/dev/shm/tmp2.csv\",                       #     63.2s\n                      row.names=FALSE, quote=FALSE))\nsystem(\"diff /dev/shm/tmp1.csv /dev/shm/tmp2.csv\")                  # identical\n\nunlink(\"/dev/shm/tmp1.csv\")\nunlink(\"/dev/shm/tmp2.csv\")\n}",
            "groupingsets": "n = 24L\nset.seed(25)\nDT <- data.table(\n    color = sample(c(\"green\",\"yellow\",\"red\"), n, TRUE),\n    year = as.Date(sample(paste0(2011:2015,\"-01-01\"), n, TRUE)),\n    status = as.factor(sample(c(\"removed\",\"active\",\"inactive\",\"archived\"), n, TRUE)),\n    amount = sample(1:5, n, TRUE),\n    value = sample(c(3, 3.5, 2.5, 2), n, TRUE)\n)\n\n# rollup\nby_vars = c(\"color\", \"year\", \"status\")\nrollup(DT, j=sum(value), by=by_vars) # default id=FALSE\nrollup(DT, j=sum(value), by=by_vars, id=TRUE)\nrollup(DT, j=lapply(.SD, sum), by=by_vars, id=TRUE, .SDcols=\"value\")\nrollup(DT, j=c(list(count=.N), lapply(.SD, sum)), by=by_vars, id=TRUE)\nrollup(DT, j=sum(value), by=by_vars,\n       # specify label by variable name\n       label=list(color=\"total\", year=as.Date(\"3000-01-01\"), status=factor(\"total\")))\nrollup(DT, j=sum(value), by=by_vars,\n       # specify label by variable name and first element of class\n       label=list(color=\"total\", Date=as.Date(\"3000-01-01\"), factor=factor(\"total\")))\n# label is character scalar so applies to color only\nrollup(DT, j=sum(value), by=by_vars, label=\"total\")\nrollup(DT, j=.N, by=c(\"color\", \"year\", \"status\", \"value\"),\n       # label can be explicitly specified as NA or NaN\n       label = list(color=NA_character_, year=as.Date(NA), status=factor(NA), value=NaN))\n\n# cube\ncube(DT, j = sum(value), by = c(\"color\",\"year\",\"status\"), id=TRUE)\ncube(DT, j = lapply(.SD, sum), by = c(\"color\",\"year\",\"status\"), id=TRUE, .SDcols=\"value\")\ncube(DT, j = c(list(count=.N), lapply(.SD, sum)), by = c(\"color\",\"year\",\"status\"), id=TRUE)\n\n# groupingsets\ngroupingsets(DT, j = c(list(count=.N), lapply(.SD, sum)), by = c(\"color\",\"year\",\"status\"),\n             sets = list(\"color\", c(\"year\",\"status\"), character()), id=TRUE)",
            "last": "first(1:5) # [1] 1\nx = data.table(x=1:5, y=6:10)\nfirst(x) # same as head(x, 1)\n\nlast(1:5) # [1] 5\nx = data.table(x=1:5, y=6:10)\nlast(x) # same as tail(x, 1)",
            "last.updated": "d = data.table(a=1:4, b=2:5)\nd[2:3, z:=5L]\n.Last.updated\n\n# updated count takes duplicates into account #2837\nDT = data.table(a = 1L)\nDT[c(1L, 1L), a := 2:3]\n.Last.updated",
            "like": "DT = data.table(Name=c(\"Mary\",\"George\",\"Martha\"), Salary=c(2,3,4))\nDT[Name \\%like\\% \"^Mar\"]\nDT[Name \\%ilike\\% \"mar\"]\nDT[Name \\%flike\\% \"Mar\"]\nDT[Name \\%plike\\% \"(?=Ma)(?=.*y)\"]",
            "measure": "(two.iris = data.table(datasets::iris)[c(1,150)])\n# melt into a single value column.\nmelt(two.iris, measure.vars = measure(part, dim, sep=\".\"))\n# do the same, programmatically with measurev\nmy.list = list(part=NULL, dim=NULL)\nmelt(two.iris, measure.vars=measurev(my.list, sep=\".\"))\n# melt into two value columns, one for each part.\nmelt(two.iris, measure.vars = measure(value.name, dim, sep=\".\"))\n# melt into two value columns, one for each dim.\nmelt(two.iris, measure.vars = measure(part, value.name, sep=\".\"))\n# melt using sep, converting child number to integer.\n(two.families = data.table(sex_child1=\"M\", sex_child2=\"F\", age_child1=10, age_child2=20))\nprint(melt(two.families, measure.vars = measure(\n  value.name, child=as.integer,\n  sep=\"_child\"\n)), class=TRUE)\n# same melt using pattern.\nprint(melt(two.families, measure.vars = measure(\n  value.name, child=as.integer,\n  pattern=\"(.*)_child(.)\"\n)), class=TRUE)\n# same melt with pattern and measurev function list.\nprint(melt(two.families, measure.vars = measurev(\n  list(value.name=NULL, child=as.integer),\n  pattern=\"(.*)_child(.)\"\n)), class=TRUE)\n# inspired by data(who, package=\"tidyr\")\n(who <- data.table(id=1, new_sp_m5564=2, newrel_f65=3))\n# melt to three variable columns, all character.\nmelt(who, measure.vars = measure(diagnosis, gender, ages, pattern=\"new_?(.*)_(.)(.*)\"))\n# melt to five variable columns, two numeric (with custom conversion).\nprint(melt(who, measure.vars = measure(\n  diagnosis, gender, ages,\n  ymin=as.numeric,\n  ymax=function(y)ifelse(y==\"\", Inf, as.numeric(y)),\n  pattern=\"new_?(.*)_(.)(([0-9]{2})([0-9]{0,2}))\"\n)), class=TRUE)",
            "melt.data.table": "set.seed(45)\nrequire(data.table)\nDT <- data.table(\n  i_1 = c(1:5, NA),\n  n_1 = c(NA, 6, 7, 8, 9, 10),\n  f_1 = factor(sample(c(letters[1:3], NA), 6L, TRUE)),\n  f_2 = factor(c(\"z\", \"a\", \"x\", \"c\", \"x\", \"x\"), ordered=TRUE),\n  c_1 = sample(c(letters[1:3], NA), 6L, TRUE),\n  c_2 = sample(c(LETTERS[1:2], NA), 6L, TRUE),\n  d_1 = as.Date(c(1:3,NA,4:5), origin=\"2013-09-01\"),\n  d_2 = as.Date(6:1, origin=\"2012-01-01\")\n)\n# add a couple of list cols\nDT[, l_1 := DT[, list(c=list(rep(i_1, sample(5, 1L)))), by = i_1]$c]\nDT[, l_2 := DT[, list(c=list(rep(c_1, sample(5, 1L)))), by = i_1]$c]\n\n# id.vars, measure.vars as character/integer/numeric vectors\nmelt(DT, id.vars=1:2, measure.vars=\"f_1\")\nmelt(DT, id.vars=c(\"i_1\", \"n_1\"), measure.vars=3) # same as above\nmelt(DT, id.vars=1:2, measure.vars=3L, value.factor=TRUE) # same, but 'value' is factor\nmelt(DT, id.vars=1:2, measure.vars=3:4, value.factor=TRUE) # 'value' is *ordered* factor\n\n# preserves attribute when types are identical, ex: Date\nmelt(DT, id.vars=3:4, measure.vars=c(\"d_1\", \"d_2\"))\nmelt(DT, id.vars=3:4, measure.vars=c(\"n_1\", \"d_1\")) # attribute not preserved\n\n# on list\nmelt(DT, id.vars=1, measure.vars=c(\"l_1\", \"l_2\")) # value is a list\nsuppressWarnings(\n  melt(DT, id.vars=1, measure.vars=c(\"c_1\", \"l_1\")) # c1 coerced to list, with warning\n)\n\n# on character\nmelt(DT, id.vars=1, measure.vars=c(\"c_1\", \"f_1\")) # value is char\nsuppressWarnings(\n  melt(DT, id.vars=1, measure.vars=c(\"c_1\", \"n_1\")) # n_1 coerced to char, with warning\n)\n\n# on na.rm=TRUE. NAs are removed efficiently, from within C\nmelt(DT, id.vars=1, measure.vars=c(\"c_1\", \"c_2\"), na.rm=TRUE) # remove NA\n\n# measure.vars can be also a list\n# melt \"f_1,f_2\" and \"d_1,d_2\" simultaneously, retain 'factor' attribute\n# convenient way using internal function patterns()\nmelt(DT, id.vars=1:2, measure.vars=patterns(\"^f_\", \"^d_\"), value.factor=TRUE)\n# same as above, but provide list of columns directly by column names or indices\nmelt(DT, id.vars=1:2, measure.vars=list(3:4, c(\"d_1\", \"d_2\")), value.factor=TRUE)\n# same as above, but provide names directly:\nmelt(DT, id.vars=1:2, measure.vars=patterns(f=\"^f_\", d=\"^d_\"), value.factor=TRUE)\n\n# na.rm=TRUE removes rows with NAs in any 'value' columns\nmelt(DT, id.vars=1:2, measure.vars=patterns(\"f_\", \"d_\"), value.factor=TRUE, na.rm=TRUE)\n\n# 'na.rm=TRUE' also works with list column, but note that is.na only\n# returns TRUE if the list element is a length=1 vector with an NA.\nis.na(list(one.NA=NA, two.NA=c(NA,NA)))\nmelt(DT, id.vars=1:2, measure.vars=patterns(\"l_\", \"d_\"), na.rm=FALSE)\nmelt(DT, id.vars=1:2, measure.vars=patterns(\"l_\", \"d_\"), na.rm=TRUE)\n\n# measure list with missing/short entries results in output with runs of NA\nDT.missing.cols <- DT[, .(d_1, d_2, c_1, f_2)]\nmelt(DT.missing.cols, measure.vars=list(d=1:2, c=\"c_1\", f=c(NA, \"f_2\")))\n\n# specifying columns to melt via separator.\nmelt(DT.missing.cols, measure.vars=measure(value.name, number=as.integer, sep=\"_\"))\n\n# specifying columns to melt via regex.\nmelt(DT.missing.cols, measure.vars=measure(value.name, number=as.integer, pattern=\"(.)_(.)\"))\nmelt(DT.missing.cols, measure.vars=measure(value.name, number=as.integer, pattern=\"([dc])_(.)\"))\n\n# cols arg of measure can be used if you do not want to use regex\nmelt(DT.missing.cols, measure.vars=measure(\n  value.name, number=as.integer, sep=\"_\", cols=c(\"d_1\",\"d_2\",\"c_1\")))",
            "merge": "(dt1 <- data.table(A = letters[1:10], X = 1:10, key = \"A\"))\n(dt2 <- data.table(A = letters[5:14], Y = 1:10, key = \"A\"))\nmerge(dt1, dt2)\nmerge(dt1, dt2, all = TRUE)\n\n(dt1 <- data.table(A = letters[rep(1:3, 2)], X = 1:6, key = \"A\"))\n(dt2 <- data.table(A = letters[rep(2:4, 2)], Y = 6:1, key = \"A\"))\nmerge(dt1, dt2, allow.cartesian=TRUE)\n\n(dt1 <- data.table(A = c(rep(1L, 5), 2L), B = letters[rep(1:3, 2)], X = 1:6, key = c(\"A\", \"B\")))\n(dt2 <- data.table(A = c(rep(1L, 5), 2L), B = letters[rep(2:4, 2)], Y = 6:1, key = c(\"A\", \"B\")))\nmerge(dt1, dt2)\nmerge(dt1, dt2, by=\"B\", allow.cartesian=TRUE)\n\n# test it more:\nd1 <- data.table(a=rep(1:2,each=3), b=1:6, key=c(\"a\", \"b\"))\nd2 <- data.table(a=0:1, bb=10:11, key=\"a\")\nd3 <- data.table(a=0:1, key=\"a\")\nd4 <- data.table(a=0:1, b=0:1, key=c(\"a\", \"b\"))\n\nmerge(d1, d2)\nmerge(d2, d1)\nmerge(d1, d2, all=TRUE)\nmerge(d2, d1, all=TRUE)\n\nmerge(d3, d1)\nmerge(d1, d3)\nmerge(d1, d3, all=TRUE)\nmerge(d3, d1, all=TRUE)\n\nmerge(d1, d4)\nmerge(d1, d4, by=\"a\", suffixes=c(\".d1\", \".d4\"))\nmerge(d4, d1)\nmerge(d1, d4, all=TRUE)\nmerge(d4, d1, all=TRUE)\n\n# setkey is automatic by default\nset.seed(1L)\nd1 <- data.table(a=sample(rep(1:3,each=2)), z=1:6)\nd2 <- data.table(a=2:0, z=10:12)\nmerge(d1, d2, by=\"a\")\nmerge(d1, d2, by=\"a\", all=TRUE)\n\n# using by.x and by.y\nsetnames(d2, \"a\", \"b\")\nmerge(d1, d2, by.x=\"a\", by.y=\"b\")\nmerge(d1, d2, by.x=\"a\", by.y=\"b\", all=TRUE)\nmerge(d2, d1, by.x=\"b\", by.y=\"a\")\n\n# using incomparables values\nd1 <- data.table(a=c(1,2,NA,NA,3,1), z=1:6)\nd2 <- data.table(a=c(1,2,NA), z=10:12)\nmerge(d1, d2, by=\"a\")\nmerge(d1, d2, by=\"a\", incomparables=NA)",
            "na.omit.data.table": "DT = data.table(x=c(1,NaN,NA,3), y=c(NA_integer_, 1:3), z=c(\"a\", NA_character_, \"b\", \"c\"))\n# default behaviour\nna.omit(DT)\n# omit rows where 'x' has a missing value\nna.omit(DT, cols=\"x\")\n# omit rows where either 'x' or 'y' have missing values\nna.omit(DT, cols=c(\"x\", \"y\"))\n\n\\dontrun{\n# Timings on relatively large data\nset.seed(1L)\nDT = data.table(x = sample(c(1:100, NA_integer_), 5e7L, TRUE),\n                y = sample(c(rnorm(100), NA), 5e7L, TRUE))\nsystem.time(ans1 <- na.omit(DT)) ## 2.6 seconds\nsystem.time(ans2 <- stats:::na.omit.data.frame(DT)) ## 29 seconds\n# identical? check each column separately, as ans2 will have additional attribute\nall(sapply(1:2, function(i) identical(ans1[[i]], ans2[[i]]))) ## TRUE\n}",
            "nafill": "x = 1:10\nx[c(1:2, 5:6, 9:10)] = NA\nnafill(x, \"locf\")\n\ndt = data.table(v1=x, v2=shift(x)/2, v3=shift(x, -1L)/2)\nnafill(dt, \"nocb\")\n\nsetnafill(dt, \"locf\", cols=c(\"v2\",\"v3\"))\ndt",
            "notin": "11 \\%notin\\% 1:10 # TRUE\n  \"a\" \\%notin\\% c(\"a\", \"b\") # FALSE\n\n  ## NAs on the LHS\n  NA \\%in\\% 1:2\n  NA \\%notin\\% 1:2\n  ## NAs on the RHS\n  NA \\%in\\% c(1:2,NA)\n  NA \\%notin\\% c(1:2,NA)",
            "openmp-utils": "getDTthreads(verbose=TRUE)",
            "patterns": "DT = data.table(x1 = 1:5, x2 = 6:10, y1 = letters[1:5], y2 = letters[6:10])\n# melt all columns that begin with 'x' & 'y', respectively, into separate columns\nmelt(DT, measure.vars = patterns(\"^x\", \"^y\", cols=names(DT)))\n# when used with melt, 'cols' is implicitly assumed to be names of input\n# data.table, if not provided.\nmelt(DT, measure.vars = patterns(\"^x\", \"^y\"))",
            "print.data.table": "#output compression\n  DT <- data.table(a = 1:1000)\n  print(DT, nrows = 100, topn = 4)\n\n  #`quote` can be used to identify whitespace\n  DT <- data.table(blanks = c(\" 12\", \" 34\"),\n                   noblanks = c(\"12\", \"34\"))\n  print(DT, quote = TRUE)\n\n  #`class` provides handy column type summaries at a glance\n  DT <- data.table(a = vector(\"integer\", 3),\n                   b = vector(\"complex\", 3),\n                   c = as.IDate(paste0(\"2016-02-0\", 1:3)))\n  print(DT, class = TRUE)\n\n  #`row.names` can be eliminated to save space\n  DT <- data.table(a = 1:3)\n  print(DT, row.names = FALSE)\n\n  #`print.keys` can alert which columns are currently keys\n  DT <- data.table(a=1:3, b=4:6, c=7:9, key=c(\"b\", \"a\"))\n  setindexv(DT, c(\"a\", \"b\"))\n  setindexv(DT, \"a\")\n  print(DT, print.keys=TRUE)\n\n  # `trunc.cols` will make it so only columns that fit in console will be printed\n  #    with a message that states the variables not shown\n  old_width = options(\"width\" = 40)\n  DT <- data.table(thing_11 = vector(\"integer\", 3),\n                   thing_21 = vector(\"complex\", 3),\n                   thing_31 = as.IDate(paste0(\"2016-02-0\", 1:3)),\n                   thing_41 = \"aasdfasdfasdfasdfasdfasdfasdfasdfasdfasdf\",\n                   thing_51 = vector(\"integer\", 3),\n                   thing_61 = vector(\"complex\", 3))\n  print(DT, trunc.cols=TRUE)\n  options(old_width)\n\n  # `char.trunc` will truncate the strings,\n  # if their lengths exceed the given limit: `datatable.prettyprint.char`\n  # For example:\n\n  old = options(datatable.prettyprint.char=5L)\n  DT = data.table(x=1:2, y=c(\"abcdefghij\", \"klmnopqrstuv\"))\n  DT\n  options(old)\n\n  # Formatting customization\n  format_col.complex = function(x, ...) sprintf('(\\%.1f, \\%.1fi)', Re(x), Im(x))\n  x = data.table(z = c(1 + 3i, 2 - 1i, pi + 2.718i))\n  print(x)\n\n  old = options(datatable.show.indices=TRUE)\n  NN = 200\n  set.seed(2024)\n  DT = data.table(\n    grp1 = sample(100, NN, TRUE),\n    grp2 = sample(90, NN, TRUE),\n    grp3 = sample(80, NN, TRUE)\n  )\n  setkey(DT, grp1, grp2)\n  setindex(DT, grp1, grp3)\n  print(DT)\n  options(old)\n\n  iris = as.data.table(iris)\n  iris_agg = iris[ , .(reg = list(lm(Sepal.Length ~ Petal.Length))), by = Species]\n  format_list_item.lm = function(x, ...) sprintf('<lm:\\%s>', format(x$call$formula))\n  print(iris_agg)",
            "rbindlist": "# default case\nDT1 = data.table(A=1:3,B=letters[1:3])\nDT2 = data.table(A=4:5,B=letters[4:5])\nl = list(DT1,DT2)\nrbindlist(l)\n\n# bind correctly by names\nDT1 = data.table(A=1:3,B=letters[1:3])\nDT2 = data.table(B=letters[4:5],A=4:5)\nl = list(DT1,DT2)\nrbindlist(l, use.names=TRUE)\n\n# fill missing columns, and match by col names\nDT1 = data.table(A=1:3,B=letters[1:3])\nDT2 = data.table(B=letters[4:5],C=factor(1:2))\nl = list(DT1,DT2)\nrbindlist(l, use.names=TRUE, fill=TRUE)\n\n# generate index column, auto generates indices\nrbindlist(l, use.names=TRUE, fill=TRUE, idcol=TRUE)\n# let's name the list\nsetattr(l, 'names', c(\"a\", \"b\"))\nrbindlist(l, use.names=TRUE, fill=TRUE, idcol=\"ID\")\n\n# bind different classes\nDT1 = data.table(A=1:3,B=letters[1:3])\nDT2 = data.table(A=4:5,B=letters[4:5])\nsetattr(DT1[[\"A\"]], \"class\", c(\"a\", \"integer\"))\nrbind(DT1, DT2, ignore.attr=TRUE)",
            "rleid": "DT = data.table(grp=rep(c(\"A\", \"B\", \"C\", \"A\", \"B\"), c(2,2,3,1,2)), value=1:10)\nrleid(DT$grp) # get run-length ids\nrleidv(DT, \"grp\") # same as above\n\nrleid(DT$grp, prefix=\"grp\") # prefix with 'grp'\n\n# get sum of value over run-length groups\nDT[, sum(value), by=.(grp, rleid(grp))]\nDT[, sum(value), by=.(grp, rleid(grp, prefix=\"grp\"))]",
            "rowid": "DT = data.table(x=c(20,10,10,30,30,20), y=c(\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"), z=1:6)\n\nrowid(DT$x) # 1,1,2,1,2,2\nrowidv(DT, cols=\"x\") # same as above\n\nrowid(DT$x, prefix=\"group\") # prefixed with 'group'\n\nrowid(DT$x, DT$y) # 1,1,2,1,2,1\nrowidv(DT, cols=c(\"x\",\"y\")) # same as above\nDT[, .(N=seq_len(.N)), by=.(x,y)]$N # same as above\n\n# convenient usage with dcast\ndcast(DT, x ~ rowid(x, prefix=\"group\"), value.var=\"z\")\n#     x group1 group2\n# 1: 10      2      3\n# 2: 20      1      6\n# 3: 30      4      5",
            "rowwiseDT": "rowwiseDT(\n  A=,B=, C=,\n  1, \"a\",2:3,\n  2, \"b\",list(5)\n)",
            "setDF": "X = data.table(x=1:5, y=6:10)\n## convert 'X' to data.frame, without any copy.\nsetDF(X)\n\nX = data.table(x=1:5, y=6:10)\n## idem, assigning row names\nsetDF(X, rownames = LETTERS[1:5])\n\nX = list(x=1:5, y=6:10)\n# X is converted to a data.frame without any copy.\nsetDF(X)",
            "setDT": "set.seed(45L)\nX = data.frame(\n  A=sample(3, 10, TRUE),\n  B=sample(letters[1:3], 10, TRUE),\n  C=sample(10))\n\n# Convert X to data.table by reference and\n# get the frequency of each \"A,B\" combination\nsetDT(X)[, .N, by=.(A,B)]\n\n# convert list to data.table\n# autofill names\nX = list(1:4, letters[1:4])\nsetDT(X)\n# don't provide names\nX = list(a=1:4, letters[1:4])\nsetDT(X, FALSE)\n\n# setkey directly\nX = list(a = 4:1, b=runif(4))\nsetDT(X, key=\"a\")[]\n\n# check.names argument\nX = list(a=1:5, a=6:10)\nsetDT(X, check.names=TRUE)[]",
            "setNumericRounding": "DT = data.table(a=seq(0,1,by=0.2),b=1:2, key=\"a\")\nDT\nsetNumericRounding(0)   # By default, rounding is turned off\nDT[.(0.4)]   # works\nDT[.(0.6)]   # no match, can be confusing since 0.6 is clearly there in DT\n             # happens due to floating point representation limitations\n\nsetNumericRounding(2)   # round off last 2 bytes\nDT[.(0.6)]   # works\n\n# using type 'numeric' for integers > 2^31 (typically ids)\nDT = data.table(id = c(1234567890123, 1234567890124, 1234567890125), val=1:3)\nprint(DT, digits=15)\nDT[,.N,by=id]   # 1 row, (last 2 bytes rounded)\nsetNumericRounding(0)\nDT[,.N,by=id]   # 3 rows, (no rounding, default)\n# better to use bit64::integer64 for such ids",
            "setattr": "DT <- data.table(a = 1, b = 2, d = 3)\n\nold <- c(\"a\", \"b\", \"c\", \"d\")\nnew <- c(\"A\", \"B\", \"C\", \"D\")\n\nsetnames(DT, old, new, skip_absent = TRUE) # skips old[3] because \"c\" is not a column name of DT\n\nDF = data.frame(a=1:2,b=3:4)       # base data.frame to demo copies and syntax\nif (capabilities()[\"profmem\"])     # usually memory profiling is available but just in case\n  tracemem(DF)\ncolnames(DF)[1] <- \"A\"             # 4 shallow copies (R >= 3.1, was 4 deep copies before)\nnames(DF)[1] <- \"A\"                # 3 shallow copies\nnames(DF) <- c(\"A\", \"b\")           # 1 shallow copy\n`names<-`(DF,c(\"A\",\"b\"))           # 1 shallow copy\n\nDT = data.table(a=1:2,b=3:4,c=5:6) # compare to data.table\nif (capabilities()[\"profmem\"])\n  tracemem(DT)                     # by reference, no deep or shallow copies\nsetnames(DT,\"b\",\"B\")               # by name, no match() needed (warning if \"b\" is missing)\nsetnames(DT,3,\"C\")                 # by position with warning if 3 > ncol(DT)\nsetnames(DT,2:3,c(\"D\",\"E\"))        # multiple\nsetnames(DT,c(\"a\",\"E\"),c(\"A\",\"F\")) # multiple by name (warning if either \"a\" or \"E\" is missing)\nsetnames(DT,c(\"X\",\"Y\",\"Z\"))        # replace all (length of names must be == ncol(DT))\nsetnames(DT,tolower)               # replace all names with their lower case\nsetnames(DT,2:3,toupper)           # replace the 2nd and 3rd names with their upper case\n\nDT <- data.table(x = 1:3, y = 4:6, z = 7:9)\nsetnames(DT, -2, c(\"a\", \"b\"))      # NEW FR #1443, allows -ve indices in 'old' argument\n\nDT = data.table(a=1:3, b=4:6)\nf = function(\\dots) {\n    # ...\n    setattr(DT,\"myFlag\",TRUE)  # by reference\n    # ...\n    localDT = copy(DT)\n    setattr(localDT,\"myFlag2\",TRUE)\n    # ...\n    invisible()\n}\nf()\nattr(DT,\"myFlag\")   # TRUE\nattr(DT,\"myFlag2\")  # NULL",
            "setcolorder": "set.seed(45L)\nDT = data.table(A=sample(3, 10, TRUE),\n         B=sample(letters[1:3], 10, TRUE), C=sample(10))\n\nsetcolorder(DT, c(\"C\", \"A\", \"B\"))\n\n#incomplete specification\nsetcolorder(DT, \"A\")\n\n# insert new column as first column\nset(DT, j=\"D\", value=sample(10))\nsetcolorder(DT, \"D\", before=1)\n\n# move column to last column place\nsetcolorder(DT, \"A\", after=ncol(DT))",
            "setkey": "# Type 'example(setkey)' to run these at the prompt and browse output\n\nDT = data.table(A=5:1,B=letters[5:1])\nDT # before\nsetkey(DT,B)          # re-orders table and marks it sorted.\nDT # after\ntables()              # KEY column reports the key'd columns\nkey(DT)\nkeycols = c(\"A\",\"B\")\nsetkeyv(DT,keycols)\n\nDT = data.table(A=5:1,B=letters[5:1])\nDT2 = DT              # does not copy\nsetkey(DT2,B)         # does not copy-on-write to DT2\nidentical(DT,DT2)     # TRUE. DT and DT2 are two names for the same keyed table\n\nDT = data.table(A=5:1,B=letters[5:1])\nDT2 = copy(DT)        # explicit copy() needed to copy a data.table\nsetkey(DT2,B)         # now just changes DT2\nidentical(DT,DT2)     # FALSE. DT and DT2 are now different tables\n\nDT = data.table(A=5:1,B=letters[5:1])\nsetindex(DT)          # set indices\nsetindex(DT, A)\nsetindex(DT, B)\nindices(DT)           # get indices single vector\nindices(DT, vectors = TRUE) # get indices list\n\n# Setting multiple indices at once\nDT = data.table(A = 5:1, B = letters[5:1], C = 10:6)\nsetindexv(DT, list(c(\"A\", \"B\"), c(\"B\", \"C\")))\nprint(DT, show.indices=TRUE)\n\n# Use the dot .(subset_value) syntax with integer keys:\nDT = data.table(id = 2:1)\nsetkey(DT, id)\nsubset_value <- 1\nDT[subset_value]  # treats subset_value as an row number\nDT[.(subset_value)]  # matches subset_value against key column (id)",
            "setops": "x = data.table(c(1,2,2,2,3,4,4))\nx2 = data.table(c(1,2,3,4)) # same set of rows as x\ny = data.table(c(2,3,4,4,4,5))\nfintersect(x, y)            # intersect\nfintersect(x, y, all=TRUE)  # intersect all\nfsetdiff(x, y)              # except\nfsetdiff(x, y, all=TRUE)    # except all\nfunion(x, y)                # union\nfunion(x, y, all=TRUE)      # union all\nfsetequal(x, x2, all=FALSE) # setequal\nfsetequal(x, x2)            # setequal all",
            "setorder": "set.seed(45L)\nDT = data.table(A=sample(3, 10, TRUE),\n         B=sample(letters[1:3], 10, TRUE), C=sample(10))\n\n# setorder\nsetorder(DT, A, -B)\n\n# same as above, but using setorderv\nsetorderv(DT, c(\"A\", \"B\"), c(1, -1))",
            "shift": "# on vectors, returns a vector as long as length(n) == 1, #1127\nx = 1:5\n# lag with n=1 and pad with NA (returns vector)\nshift(x, n=1, fill=NA, type=\"lag\")\n# lag with n=1 and 2, and pad with 0 (returns list)\nshift(x, n=1:2, fill=0, type=\"lag\")\n# getting a window by using positive and negative n:\nshift(x, n = -1:1)\nshift(x, n = -1:1, type = \"shift\", give.names = TRUE)\n# cyclic shift where pad uses pushed out values\nshift(x, n = -1:1, type = \"cyclic\")\n\n# on data.tables\nDT = data.table(year=2010:2014, v1=runif(5), v2=1:5, v3=letters[1:5])\n# lag columns 'v1,v2,v3' DT by 1 and fill with 0\ncols = c(\"v1\",\"v2\",\"v3\")\nanscols = paste(\"lead\", cols, sep=\"_\")\nDT[, (anscols) := shift(.SD, 1, 0, \"lead\"), .SDcols=cols]\n\n# return a new data.table instead of updating\n# with names automatically set\nDT = data.table(year=2010:2014, v1=runif(5), v2=1:5, v3=letters[1:5])\nDT[, shift(.SD, 1:2, NA, \"lead\", TRUE), .SDcols=2:4]\n\n# lag/lead in the right order\nDT = data.table(year=2010:2014, v1=runif(5), v2=1:5, v3=letters[1:5])\nDT = DT[sample(nrow(DT))]\n# add lag=1 for columns 'v1,v2,v3' in increasing order of 'year'\ncols = c(\"v1\",\"v2\",\"v3\")\nanscols = paste(\"lag\", cols, sep=\"_\")\nDT[order(year), (cols) := shift(.SD, 1, type=\"lag\"), .SDcols=cols]\nDT[order(year)]\n\n# while grouping\nDT = data.table(year=rep(2010:2011, each=3), v1=1:6)\nDT[, c(\"lag1\", \"lag2\") := shift(.SD, 1:2), by=year]\n\n# on lists\nll = list(1:3, letters[4:1], runif(2))\nshift(ll, 1, type=\"lead\")\nshift(ll, 1, type=\"lead\", give.names=TRUE)\nshift(ll, 1:2, type=\"lead\")\n\n# fill using first or last by group\nDT = data.table(x=1:6, g=rep(1:2, each=3))\nDT[ , shift(x, fill=x[1L]), by=g]\nDT[ , shift(x, fill=x[.N], type=\"lead\"), by=g]",
            "shouldPrint": "# dummy example section to pass release check that all .Rd files have examples",
            "special-symbols": "DT = data.table(x=rep(c(\"b\",\"a\",\"c\"),each=3), v=c(1,1,1,2,2,1,1,2,2), y=c(1,3,6), a=1:9, b=9:1)\nDT\nX = data.table(x=c(\"c\",\"b\"), v=8:7, foo=c(4,2))\nX\n\nDT[.N]                                 # last row, only special symbol allowed in 'i'\nDT[, .N]                               # total number of rows in DT\nDT[, .N, by=x]                         # number of rows in each group\nDT[, .SD, .SDcols=x:y]                 # select columns 'x' through 'y'\nDT[, .SD[1]]                           # first row of all columns\nDT[, .SD[1], by=x]                     # first row of all columns for each group in 'x'\nDT[, c(.N, lapply(.SD, sum)), by=x]    # get rows *and* sum all columns by group\nDT[, .I[1], by=x]                      # row number in DT corresponding to each group\nDT[, .N, by=rleid(v)]                  # get count of consecutive runs of 'v'\nDT[, c(.(y=max(y)), lapply(.SD, min)),\n        by=rleid(v), .SDcols=v:b]      # compute 'j' for each consecutive runs of 'v'\nDT[, grp := .GRP, by=x]                # add a group counter\nDT[, grp_pct := .GRP/.NGRP, by=x]      # add a group \"progress\" counter\nX[, DT[.BY, y, on=\"x\"], by=x]          # join within each group\nDT[X, on=.NATURAL]                     # join X and DT on common column similar to X[on=Y]\n\n# .N can be different in i and j\nDT[{cat(sprintf('in i, .N is \\%d\\n', .N)); a < .N/2},\n   {cat(sprintf('in j, .N is \\%d\\n', .N)); mean(a)}]\n\n# .I can be different in j and by, enabling rowwise operations in by\nDT[, .(.I, min(.SD[,-1]))]\nDT[, .(min(.SD[,-1])), by=.I]\n\n# Do not expect this to correctly append the value of .BY in each group; copy(.BY) will work.\nby_tracker = list()\nDT[, { append(by_tracker, .BY); sum(v) }, by=x]",
            "split": "set.seed(123)\nDT = data.table(x1 = rep(letters[1:2], 6),\n                x2 = rep(letters[3:5], 4),\n                x3 = rep(letters[5:8], 3),\n                y = rnorm(12))\nDT = DT[sample(.N)]\nDF = as.data.frame(DT)\n\n# split consistency with data.frame: `x, f, drop`\nall.equal(\n    split(DT, list(DT$x1, DT$x2)),\n    lapply(split(DF, list(DF$x1, DF$x2)), setDT)\n)\n\n# nested list using `flatten` arguments\nsplit(DT, by=c(\"x1\", \"x2\"))\nsplit(DT, by=c(\"x1\", \"x2\"), flatten=FALSE)\n\n# dealing with factors\nfdt = DT[, c(lapply(.SD, as.factor), list(y=y)), .SDcols=x1:x3]\nfdf = as.data.frame(fdt)\nsdf = split(fdf, list(fdf$x1, fdf$x2))\nall.equal(\n    split(fdt, by=c(\"x1\", \"x2\"), sorted=TRUE),\n    lapply(sdf[sort(names(sdf))], setDT)\n)\n\n# factors having unused levels, drop FALSE, TRUE\nfdt = DT[, .(x1 = as.factor(c(as.character(x1), \"c\"))[-13L],\n             x2 = as.factor(c(\"a\", as.character(x2)))[-1L],\n             x3 = as.factor(c(\"a\", as.character(x3), \"z\"))[c(-1L,-14L)],\n             y = y)]\nfdf = as.data.frame(fdt)\nsdf = split(fdf, list(fdf$x1, fdf$x2))\nall.equal(\n    split(fdt, by=c(\"x1\", \"x2\"), sorted=TRUE),\n    lapply(sdf[sort(names(sdf))], setDT)\n)\nsdf = split(fdf, list(fdf$x1, fdf$x2), drop=TRUE)\nall.equal(\n    split(fdt, by=c(\"x1\", \"x2\"), sorted=TRUE, drop=TRUE),\n    lapply(sdf[sort(names(sdf))], setDT)\n)",
            "subset.data.table": "DT <- data.table(a=sample(c('a', 'b', 'c'), 20, replace=TRUE),\n                 b=sample(c('a', 'b', 'c'), 20, replace=TRUE),\n                 c=sample(20), key=c('a', 'b'))\n\nsub <- subset(DT, a == 'a')\nall.equal(key(sub), key(DT))",
            "substitute2": "## base R substitute vs substitute2\nsubstitute(list(var1 = var2), list(var1 = \"c1\", var2 = 5L))\nsubstitute2(list(var1 = var2), list(var1 = \"c1\", var2 = 5L)) ## works also on names\n\nsubstitute(var1, list(var1 = \"c1\"))\nsubstitute2(var1, list(var1 = I(\"c1\"))) ## enforce character with I\n\nsubstitute(var1, list(var1 = as.name(\"c1\")))\nsubstitute2(var1, list(var1 = \"c1\")) ## turn character into symbol, for convenience\n\n## mix symbols and characters using 'I' function, both lines will yield same result\nsubstitute2(list(var1 = var2), list(var1 = \"c1\", var2 = I(\"some_character\")))\nsubstitute2(list(var1 = var2), I(list(var1 = as.name(\"c1\"), var2 = \"some_character\")))\n\n## list elements are enlist'ed into list calls\n(cl1 = substitute(f(lst), list(lst = list(1L, 2L))))\n(cl2 = substitute2(f(lst), I(list(lst = list(1L, 2L)))))\n(cl3 = substitute2(f(lst), list(lst = I(list(1L, 2L)))))\n(cl4 = substitute2(f(lst), list(lst = quote(list(1L, 2L)))))\n(cl5 = substitute2(f(lst), list(lst = list(1L, 2L))))\ncl1[[2L]] ## base R substitute with list element\ncl2[[2L]] ## same\ncl3[[2L]] ## same\ncl4[[2L]] ## desired\ncl5[[2L]] ## automatically\n\n## character to name and list into list calls works recursively\n(cl1 = substitute2(f(lst), list(lst = list(1L, list(2L)))))\n(cl2 = substitute2(f(lst), I(list(lst = list(1L, list(2L)))))) ## unless I() used\nlast(cl1[[2L]]) ## enlisted recursively\nlast(cl2[[2L]]) ## AsIs\n\n## using substitute2 from another function\nf = function(expr, env) {\n  eval(substitute(\n    substitute2(.expr, env),\n    list(.expr = substitute(expr))\n  ))\n}\nf(list(var1 = var2), list(var1 = \"c1\", var2 = 5L))",
            "tables": "DT = data.table(A=1:10, B=letters[1:10])\nDT2 = data.table(A=1:10000, ColB=10000:1)\nsetkey(DT,B)\ntables()",
            "test": "test = data.table:::test\ntest(1, x = sum(1:5), y = 15L)\ntest(2, log(-1), NaN, warning=\"NaNs\")\ntest(3, sum(\"a\"), error=\"invalid.*character\")\n# test failure example\nstopifnot(\n  test(4, TRUE, FALSE) == FALSE\n)\n# NA_real_ vs NaN\ntest(5.01, NA_real_, NaN)\ntest(5.03, all.equal(NaN, NA_real_))\ntest(5.02, identical(NaN, NA_real_), FALSE)",
            "test.data.table": "\\dontrun{\n  test.data.table()\n  }",
            "timetaken": "started.at=proc.time()\nSys.sleep(1)\ncat(\"Finished in\",timetaken(started.at),\"\\n\")",
            "transform.data.table": "DT <- data.table(a=rep(1:3, each=2), b=1:6)\n\nDT2 <- transform(DT, c = a^2)\nDT[, c:=a^2]\nidentical(DT,DT2)\n\nDT2 <- within(DT, {\n  b <- rev(b)\n  c <- a*2\n  rm(a)\n})\nDT[,`:=`(b = rev(b),\n         c = a*2,\n         a = NULL)]\nidentical(DT,DT2)\n\nDT$d = ave(DT$b, DT$c, FUN=max)               # copies entire DT, even if it is 10GB in RAM\nDT = DT[, transform(.SD, d=max(b)), by=\"c\"]   # same, but even worse as .SD is copied for each group\nDT[, d:=max(b), by=\"c\"]                       # same result, but much faster, shorter and scales\n\n# Multiple update by group. Convenient, fast, scales and easy to read.\nDT[, `:=`(minb = min(b),\n          meanb = mean(b),\n          bplusd = sum(b+d)),  by=c\\%/\\%5]\nDT",
            "transpose": "ll = list(1:5, 6:8)\ntranspose(ll)\nsetDT(transpose(ll, fill=0))[]\n\nDT = data.table(x=1:5, y=6:10)\ntranspose(DT)\n\nDT = data.table(x=1:3, y=c(\"a\",\"b\",\"c\"))\ntranspose(DT, list.cols=TRUE)\n\n# base R equivalent of transpose\nl = list(1:3, c(\"a\", \"b\", \"c\"))\nlapply(seq(length(l[[1]])), function(x) lapply(l, `[[`, x))\ntranspose(l, list.cols=TRUE)\n\nll = list(nm=c('x', 'y'), 1:2, 3:4)\ntranspose(ll, make.names=\"nm\")",
            "truelength": "DT = data.table(a=1:3,b=4:6)\nlength(DT)                 # 2 column pointer slots used\ntruelength(DT)             # 1026 column pointer slots allocated\nsetalloccol(DT, 2048)\nlength(DT)                 # 2 used\ntruelength(DT)             # 2050 allocated, 2048 free\nDT[,c:=7L]                 # add new column by assigning to spare slot\ntruelength(DT)-length(DT)  # 2047 slots spare",
            "tstrsplit": "x = c(\"abcde\", \"ghij\", \"klmnopq\")\nstrsplit(x, \"\", fixed=TRUE)\ntstrsplit(x, \"\", fixed=TRUE)\ntstrsplit(x, \"\", fixed=TRUE, fill=\"<NA>\")\n\n# using keep to return just 1,3,5\ntstrsplit(x, \"\", fixed=TRUE, keep=c(1,3,5))\n\n# names argument\ntstrsplit(x, \"\", fixed=TRUE, keep=c(1,3,5), names=LETTERS[1:3])\n\nDT = data.table(x=c(\"A/B\", \"A\", \"B\"), y=1:3)\nDT[, c(\"c1\") := tstrsplit(x, \"/\", fixed=TRUE, keep=1L)][]\nDT[, c(\"c1\", \"c2\") := tstrsplit(x, \"/\", fixed=TRUE)][]\n\n# type.convert argument\nDT = data.table(\n  w = c(\"Yes/F\", \"No/M\"),\n  x = c(\"Yes 2000-03-01 A/T\", \"No 2000-04-01 E/R\"),\n  y = c(\"1/1/2\", \"2/5/2.5\"),\n  z = c(\"Yes/1/2\", \"No/5/3.5\"),\n  v = c(\"Yes 10 30.5 2000-03-01 A/T\", \"No 20 10.2 2000-04-01 E/R\"))\n\n# convert each element in the transpose list to type factor\nDT[, tstrsplit(w, \"/\", type.convert=as.factor)]\n\n# convert part and leave any others\nDT[, tstrsplit(z, \"/\", type.convert=list(as.numeric=2:3))]\n\n# convert part with one function and any others with another\nDT[, tstrsplit(z, \"/\", type.convert=list(as.factor=1L, as.numeric))]\n\n# convert the remaining using 'type.convert(x, as.is=TRUE)' (i.e. what type.convert=TRUE does)\nDT[, tstrsplit(v, \" \", type.convert=list(as.IDate=4L, function(x) type.convert(x, as.is=TRUE)))]",
            "update_dev_pkg": "\\dontshow{ # using if(FALSE) because \\dontrun could still be run by  --run-dontrun; #5421 }\n  if (FALSE) data.table::update_dev_pkg()"
        }
    },
    "ellipsis": {
        "description": "The ellipsis is a powerful tool for extending functions. Unfortunately \n    this power comes at a cost: misspelled arguments will be silently ignored. \n    The ellipsis package provides a collection of functions to catch problems\n    and alert the user.",
        "examples": {
            "check_dots_empty": "f <- function(x, ..., foofy = 8) {\n  check_dots_empty()\n  x + foofy\n}\n\ntry(f(1, foof = 4))\nf(1, foofy = 4)",
            "check_dots_unnamed": "f <- function(..., foofy = 8) {\n  check_dots_unnamed()\n  c(...)\n}\n\nf(1, 2, 3, foofy = 4)\ntry(f(1, 2, 3, foof = 4))",
            "check_dots_used": "f <- function(...) {\n  check_dots_used()\n  g(...)\n}\n\ng <- function(x, y, ...) {\n  x + y\n}\nf(x = 1, y = 2)\n\ntry(f(x = 1, y = 2, z = 3))\ntry(f(x = 1, y = 2, 3, 4, 5))",
            "safe_median": "x <- c(1:10, NA)\nsafe_median(x, na.rm = TRUE)\nmedian(x, na.rm = TRUE)\n\ntry(median(x, na.mr = TRUE))\ntry(safe_median(x, na.mr = TRUE))\n\ntry(median(1, 2, 3))\ntry(safe_median(1, 2, 3))"
        }
    },
    "fs": {
        "description": "A cross-platform interface to file system operations, built\n    on top of the 'libuv' C library.",
        "examples": {
            "copy": "\\dontshow{.old_wd <- setwd(tempdir())}\nfile_create(\"foo\")\nfile_copy(\"foo\", \"bar\")\ntry(file_copy(\"foo\", \"bar\"))\nfile_copy(\"foo\", \"bar\", overwrite = TRUE)\nfile_delete(c(\"foo\", \"bar\"))\n\ndir_create(\"foo\")\n# Create a directory and put a few files in it\nfiles <- file_create(c(\"foo/bar\", \"foo/baz\"))\nfile_exists(files)\n\n# Copy the directory\ndir_copy(\"foo\", \"foo2\")\nfile_exists(path(\"foo2\", path_file(files)))\n\n# Create a link to the directory\nlink_create(path_abs(\"foo\"), \"loo\")\nlink_path(\"loo\")\nlink_copy(\"loo\", \"loo2\")\nlink_path(\"loo2\")\n\n# Cleanup\ndir_delete(c(\"foo\", \"foo2\"))\nlink_delete(c(\"loo\", \"loo2\"))\n\\dontshow{setwd(.old_wd)}",
            "create": "\\dontshow{.old_wd <- setwd(tempdir())}\nfile_create(\"foo\")\nis_file(\"foo\")\n# dir_create applied to the same path will fail\ntry(dir_create(\"foo\"))\n\ndir_create(\"bar\")\nis_dir(\"bar\")\n# file_create applied to the same path will fail\ntry(file_create(\"bar\"))\n\n# Cleanup\nfile_delete(\"foo\")\ndir_delete(\"bar\")\n\\dontshow{setwd(.old_wd)}",
            "delete": "\\dontshow{.old_wd <- setwd(tempdir())}\n# create a directory, with some files and a link to it\ndir_create(\"dir\")\nfiles <- file_create(path(\"dir\", letters[1:5]))\nlink <- link_create(path_abs(\"dir\"), \"link\")\n\n# All files created\ndir_exists(\"dir\")\nfile_exists(files)\nlink_exists(\"link\")\nfile_exists(link_path(\"link\"))\n\n# Delete a file\nfile_delete(files[1])\nfile_exists(files[1])\n\n# Delete the directory (which deletes the files as well)\ndir_delete(\"dir\")\nfile_exists(files)\ndir_exists(\"dir\")\n\n# The link still exists, but what it points to does not.\nlink_exists(\"link\")\ndir_exists(link_path(\"link\"))\n\n# Delete the link\nlink_delete(\"link\")\nlink_exists(\"link\")\n\\dontshow{setwd(.old_wd)}",
            "dir_ls": "\\dontshow{.old_wd <- setwd(tempdir())}\ndir_ls(R.home(\"share\"), type = \"directory\")\n\n# Create a shorter link\nlink_create(system.file(package = \"base\"), \"base\")\n\ndir_ls(\"base\", recurse = TRUE, glob = \"*.R\")\n\n# If you need the full paths input an absolute path\ndir_ls(path_abs(\"base\"))\n\ndir_map(\"base\", identity)\n\ndir_walk(\"base\", str)\n\ndir_info(\"base\")\n\n# Cleanup\nlink_delete(\"base\")\n\\dontshow{setwd(.old_wd)}",
            "file_access": "file_access(\"/\")\nfile_access(\"/\", \"read\")\nfile_access(\"/\", \"write\")\n\nfile_exists(\"WOMBATS\")",
            "file_chmod": "\\dontshow{.old_wd <- setwd(tempdir())}\nfile_create(\"foo\", mode = \"000\")\nfile_chmod(\"foo\", \"777\")\nfile_info(\"foo\")$permissions\n\nfile_chmod(\"foo\", \"u-x\")\nfile_info(\"foo\")$permissions\n\nfile_chmod(\"foo\", \"a-wrx\")\nfile_info(\"foo\")$permissions\n\nfile_chmod(\"foo\", \"u+wr\")\nfile_info(\"foo\")$permissions\n\n# It is also vectorized\nfiles <- c(\"foo\", file_create(\"bar\", mode = \"000\"))\nfile_chmod(files, \"a+rwx\")\nfile_info(files)$permissions\n\nfile_chmod(files, c(\"644\", \"600\"))\nfile_info(files)$permissions\n\\dontshow{setwd(.old_wd)}",
            "file_info": "\\dontshow{.old_wd <- setwd(tempdir())}\nwrite.csv(mtcars, \"mtcars.csv\")\nfile_info(\"mtcars.csv\")\n\n# Files in the working directory modified more than 20 days ago\nfiles <- file_info(dir_ls())\nfiles$path[difftime(Sys.time(), files$modification_time, units = \"days\") > 20]\n\n# Cleanup\nfile_delete(\"mtcars.csv\")\n\\dontshow{setwd(.old_wd)}",
            "file_move": "\\dontshow{.old_wd <- setwd(tempdir())}\nfile_create(\"foo\")\nfile_move(\"foo\", \"bar\")\nfile_exists(c(\"foo\", \"bar\"))\nfile_delete(\"bar\")\n\\dontshow{setwd(.old_wd)}",
            "file_temp": "\\dontshow{file_temp_push(\"/tmp/filedd461c46df20\")}\n\npath_temp()\npath_temp(\"does-not-exist\")\n\nfile_temp()\nfile_temp(ext = \"png\")\nfile_temp(\"image\", ext = \"png\")\n\n\n# You can make the temp file paths deterministic\nfile_temp_push(letters)\nfile_temp()\nfile_temp()\n\n# Or explicitly remove values\nwhile (!is.null(file_temp_pop())) next\nfile_temp_pop()",
            "file_touch": "\\dontshow{.old_wd <- setwd(tempdir())}\nfile_create(\"foo\")\nfile_touch(\"foo\", \"2018-01-01\")\nfile_info(\"foo\")[c(\"access_time\", \"modification_time\", \"change_time\", \"birth_time\")]\n\\dontshow{setwd(.old_wd)}",
            "fs_bytes": "fs_bytes(\"1\")\nfs_bytes(\"1K\")\nfs_bytes(\"1Kb\")\nfs_bytes(\"1Kib\")\nfs_bytes(\"1MB\")\n\nfs_bytes(\"1KB\") < \"1MB\"\n\nsum(fs_bytes(c(\"1MB\", \"5MB\", \"500KB\")))",
            "fs_perms": "# Integer and numeric\nfs_perms(420L)\nfs_perms(c(511, 420))\n\n# Octal\nfs_perms(\"777\")\nfs_perms(c(\"777\", \"644\"))\n\n# Symbolic\nfs_perms(\"a+rwx\")\nfs_perms(c(\"a+rwx\", \"u+rw,go+r\"))\n\n# Use the `&` and `|`operators to check for certain permissions\n(fs_perms(\"777\") & \"u+r\") == \"u+r\"",
            "id": "# list first 6 groups\nhead(group_ids())\n\n# list first 6 users\nhead(user_ids())",
            "is_absolute_path": "is_absolute_path(\"/foo\")\nis_absolute_path(\"C:\\\\\\\\foo\")\nis_absolute_path(\"\\\\\\\\\\\\\\\\myserver\\\\\\\\foo\\\\\\\\bar\")\n\nis_absolute_path(\"foo/bar\")",
            "is_file": "\\dontshow{.old_wd <- setwd(tempdir())}\ndir_create(\"d\")\n\nfile_create(\"d/file.txt\")\ndir_create(\"d/dir\")\nlink_create(path(path_abs(\"d\"), \"file.txt\"), \"d/link\")\n\npaths <- dir_ls(\"d\")\nis_file(paths)\nis_dir(paths)\nis_link(paths)\n\n# Cleanup\ndir_delete(\"d\")\n\\dontshow{setwd(.old_wd)}",
            "link_path": "\\dontshow{.old_wd <- setwd(tempdir())}\nfile_create(\"foo\")\nlink_create(path_abs(\"foo\"), \"bar\")\nlink_path(\"bar\")\n\n# Cleanup\nfile_delete(c(\"foo\", \"bar\"))\n\\dontshow{setwd(.old_wd)}",
            "path": "path(\"foo\", \"bar\", \"baz\", ext = \"zip\")\n\npath(\"foo\", letters[1:3], ext = \"txt\")",
            "path_expand": "# Expand a path\npath_expand(\"~/bin\")\n\n# You can use `path_home()` without arguments to see what is being used as\n# the home diretory.\npath_home()\npath_home(\"R\")\n\n# This will likely differ from the above on Windows\npath_home_r()",
            "path_file": "path_file(\"dir/file.zip\")\n\npath_dir(\"dir/file.zip\")\n\npath_ext(\"dir/file.zip\")\n\npath_ext(\"file.tar.gz\")\n\npath_ext_remove(\"file.tar.gz\")\n\n# Only one level of extension is removed\npath_ext_set(path_ext_remove(\"file.tar.gz\"), \"zip\")",
            "path_filter": "path_filter(c(\"foo\", \"boo\", \"bar\"), glob = \"*oo\")\npath_filter(c(\"foo\", \"boo\", \"bar\"), glob = \"*oo\", invert = TRUE)\n\npath_filter(c(\"foo\", \"boo\", \"bar\"), regexp = \"b.r\")",
            "path_math": "\\dontshow{.old_wd <- setwd(tempdir())}\ndir_create(\"a\")\nfile_create(\"a/b\")\nlink_create(path_abs(\"a\"), \"c\")\n\n# Realize the path\npath_real(\"c/b\")\n\n# Split a path\nparts <- path_split(\"a/b\")\nparts\n\n# Join it together\npath_join(parts)\n\n# Find the absolute path\npath_abs(\"..\")\n\n# Normalize a path\npath_norm(\"a/../b\\\\\\\\c/.\")\n\n# Compute a relative path\npath_rel(\"/foo/abc\", \"/foo/bar/baz\")\n\n# Find the common path between multiple paths\npath_common(c(\"/foo/bar/baz\", \"/foo/bar/abc\", \"/foo/xyz/123\"))\n\n# Cleanup\ndir_delete(\"a\")\nlink_delete(\"c\")\n\\dontshow{setwd(.old_wd)}",
            "path_package": "path_package(\"base\")\npath_package(\"stats\")\npath_package(\"base\", \"INDEX\")\npath_package(\"splines\", \"help\", \"AnIndex\")",
            "path_sanitize": "# potentially unsafe string\nstr <- \"~/.\\u0001ssh/authorized_keys\"\npath_sanitize(str)\n\npath_sanitize(\"..\")"
        }
    },
    "evaluate": {
        "description": "Parsing and evaluation tools that make it easy to recreate\n    the command line behaviour of R.",
        "examples": {
            "evaluate": "evaluate(c(\n  \"1 + 1\",\n  \"2 + 2\"\n))\n\n# Not that's there's a difference in output between putting multiple\n# expressions on one line vs spreading them across multiple lines\nevaluate(\"1;2;3\")\nevaluate(c(\"1\", \"2\", \"3\"))\n\n# This also affects how errors propagate, matching the behaviour\n# of the R console\nevaluate(\"1;stop(2);3\")\nevaluate(c(\"1\", \"stop(2)\", \"3\"))",
            "inject_funs": "library(evaluate)\n# normally you cannot capture the output of system\nevaluate(\"system('R --version')\")\n\n# replace the system() function\nold <- inject_funs(system = function(...) {\n  cat(base::system(..., intern = TRUE), sep = \"\\n\")\n})\n\nevaluate(\"system('R --version')\")\n\n# restore previously injected functions\ninject_funs(old)",
            "parse_all": "# Each of these inputs are single line, but generate different numbers of\n# expressions\nsource <- c(\n  \"# a comment\",\n  \"x\",\n  \"x;y\",\n  \"x;y;z\"\n)\nparsed <- parse_all(source)\nlengths(parsed$expr)\nstr(parsed$expr)\n\n# Each of these inputs are a single expression, but span different numbers\n# of lines\nsource <- c(\n  \"function() {}\",\n  \"function() {\",\n  \"  # Hello!\",\n  \"}\",\n  \"function() {\",\n  \"  # Hello!\",\n  \"  # Goodbye!\",\n  \"}\"\n)\nparsed <- parse_all(source)\nlengths(parsed$expr)\nparsed$src",
            "replay": "f1 <- function() {\n  cat(\"1\\n\")\n  print(\"2\")\n  warning(\"3\")\n  print(\"4\")\n  message(\"5\")\n  stop(\"6\")\n}\nreplay(evaluate(\"f1()\"))\n\nf2 <- function() {\n  message(\"Hello\")\n  plot(1:10)\n  message(\"Goodbye\")\n}\nreplay(evaluate(\"f2()\"))",
            "set_hooks": "new1 <- list(before.plot.new = function() print(\"Plotted!\"))\nnew2 <- list(before.plot.new = function() print(\"Plotted Again!\"))\nset_hooks(new1)\nset_hooks(new2)\nplot(1)\nremove_hooks(new1)\nplot(1)\nremove_hooks(new2)\nplot(1)",
            "trim_intermediate_plots": "ev <- evaluate(c(\n  \"plot(1:3)\",\n  \"text(1, 1, 'x')\",\n  \"text(1, 1, 'y')\"\n))\n\n# All intermediate plots are captured\nev\n# Only the final plot is shown\ntrim_intermediate_plots(ev)"
        }
    },
    "processx": {
        "description": "Tools to run system processes in the background.  It can\n    check if a background process is running; wait on a background process\n    to finish; get the exit status of finished processes; kill background\n    processes. It can read the standard output and error of the processes,\n    using non-blocking connections. 'processx' can poll a process for\n    standard output or error, with a timeout. It can also poll several\n    processes at once.",
        "examples": {
            "poll": "\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# Different commands to run for windows and unix\ncmd1 <- switch(\n  .Platform$OS.type,\n  \"unix\" = c(\"sh\", \"-c\", \"sleep 1; ls\"),\n  c(\"cmd\", \"/c\", \"ping -n 2 127.0.0.1 && dir /b\")\n)\ncmd2 <- switch(\n  .Platform$OS.type,\n  \"unix\" = c(\"sh\", \"-c\", \"sleep 2; ls 1>&2\"),\n  c(\"cmd\", \"/c\", \"ping -n 2 127.0.0.1 && dir /b 1>&2\")\n)\n\n## Run them. p1 writes to stdout, p2 to stderr, after some sleep\np1 <- process$new(cmd1[1], cmd1[-1], stdout = \"|\")\np2 <- process$new(cmd2[1], cmd2[-1], stderr = \"|\")\n\n## Nothing to read initially\npoll(list(p1 = p1, p2 = p2), 0)\n\n## Wait until p1 finishes. Now p1 has some output\np1$wait()\npoll(list(p1 = p1, p2 = p2), -1)\n\n## Close p1's connection, p2 will have output on stderr, eventually\nclose(p1$get_output_connection())\npoll(list(p1 = p1, p2 = p2), -1)\n\n## Close p2's connection as well, no nothing to poll\nclose(p2$get_error_connection())\npoll(list(p1 = p1, p2 = p2), 0)\n\\dontshow{\\}) # examplesIf}",
            "process": "\\dontshow{if (identical(Sys.getenv(\"IN_PKGDOWN\"), \"true\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- process$new(\"sleep\", \"2\")\np$is_alive()\np\np$kill()\np$is_alive()\n\np <- process$new(\"sleep\", \"1\")\np$is_alive()\nSys.sleep(2)\np$is_alive()\n\\dontshow{\\}) # examplesIf}",
            "processx_connections": "is_valid_fd(0L)      # stdin\nis_valid_fd(1L)      # stdout\nis_valid_fd(2L)      # stderr",
            "processx_fifos": "# Example for a non-blocking FIFO\n\n# Need to open the reading end first, otherwise Unix fails\nreader <- conn_create_fifo()\n\n# Always use poll() before you read, with a timeout if you like.\n# If you read before the other end of the FIFO is connected, then\n# the OS (or processx?) assumes that the FIFO is done, and you cannot\n# read anything.\n# Now poll() tells us that there is no data yet.\npoll(list(reader), 0)\n\nwriter <- conn_connect_fifo(conn_file_name(reader), write = TRUE)\nconn_write(writer, \"hello\\nthere!\\n\")\n\npoll(list(reader), 1000)\nconn_read_lines(reader, 1)\nconn_read_chars(reader)\n\nconn_is_incomplete(reader)\n\nclose(writer)\nconn_read_chars(reader)\nconn_is_incomplete(reader)\n\nclose(reader)",
            "run": "\\dontshow{if (.Platform$OS.type == \"unix\") (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# This works on Unix systems\nrun(\"ls\")\nsystem.time(run(\"sleep\", \"10\", timeout = 1, error_on_status = FALSE))\nsystem.time(\n  run(\n    \"sh\", c(\"-c\", \"for i in 1 2 3 4 5; do echo $i; sleep 1; done\"),\n    timeout = 2, error_on_status = FALSE\n  )\n)\n\\dontshow{\\}) # examplesIf}\n\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# This works on Windows systems, if the ping command is available\nrun(\"ping\", c(\"-n\", \"1\", \"127.0.0.1\"))\nrun(\"ping\", c(\"-n\", \"6\", \"127.0.0.1\"), timeout = 1,\n    error_on_status = FALSE)\n\\dontshow{\\}) # examplesIf}"
        }
    },
    "hms": {
        "description": "Implements an S3 class for storing and formatting time-of-day\n    values, based on the 'difftime' class.",
        "examples": {
            "hms": "hms(56, 34, 12)\nhms()\n\nnew_hms(as.numeric(1:3))\n# Supports numeric only!\ntry(new_hms(1:3))\n\nas_hms(1)\nas_hms(\"12:34:56\")\nas_hms(Sys.time())\nas.POSIXct(hms(1))\ndata.frame(a = hms(1))\nd <- data.frame(hours = 1:3)\nd$hours <- hms(hours = d$hours)\nd",
            "parse_hms": "parse_hms(\"12:34:56\")\nparse_hms(\"12:34:56.789\")\nparse_hm(\"12:34\")",
            "round_hms": "round_hms(as_hms(\"12:34:56\"), 5)\nround_hms(as_hms(\"12:34:56\"), 60)\nround_hms(as_hms(\"12:34:56.78\"), 0.25)\nround_hms(as_hms(\"12:34:56.78\"), digits = 1)\nround_hms(as_hms(\"12:34:56.78\"), digits = -2)\ntrunc_hms(as_hms(\"12:34:56\"), 60)"
        }
    },
    "lubridate": {
        "description": "Functions to work with date-times and time-spans: fast and\n    user friendly parsing of date-time data, extraction and updating of\n    components of a date-time (years, months, days, hours, minutes, and\n    seconds), algebraic manipulation on date-time and time-span objects.\n    The 'lubridate' package has a consistent and memorable syntax that\n    makes working with dates easy and fun.",
        "examples": {
            "DateTimeUpdate": "date <- ymd(\"2009-02-10\")\nupdate(date, year = 2010, month = 1, mday = 1)\n\nupdate(date, year = 2010, month = 13, mday = 1)\n\nupdate(date, minute = 10, second = 3)",
            "am": "x <- ymd(\"2012-03-26\")\nam(x)\npm(x)",
            "as.duration": "span <- interval(ymd(\"2009-01-01\"), ymd(\"2009-08-01\")) # interval\nas.duration(span)\nas.duration(10) # numeric\ndur <- duration(hours = 10, minutes = 6)\nas.numeric(dur, \"hours\")\nas.numeric(dur, \"minutes\")",
            "as.interval": "diff <- make_difftime(days = 31) # difftime\nas.interval(diff, ymd(\"2009-01-01\"))\nas.interval(diff, ymd(\"2009-02-01\"))\n\ndur <- duration(days = 31) # duration\nas.interval(dur, ymd(\"2009-01-01\"))\nas.interval(dur, ymd(\"2009-02-01\"))\n\nper <- period(months = 1) # period\nas.interval(per, ymd(\"2009-01-01\"))\nas.interval(per, ymd(\"2009-02-01\"))\n\nas.interval(3600, ymd(\"2009-01-01\")) # numeric",
            "as.period": "span <- interval(ymd_hms(\"2009-01-01 00:00:00\"), ymd_hms(\"2010-02-02 01:01:01\")) # interval\nas.period(span)\nas.period(span, unit = \"day\")\n\"397d 1H 1M 1S\"\nleap <- interval(ymd(\"2016-01-01\"), ymd(\"2017-01-01\"))\nas.period(leap, unit = \"days\")\nas.period(leap, unit = \"years\")\ndst <- interval(\n  ymd(\"2016-11-06\", tz = \"America/Chicago\"),\n  ymd(\"2016-11-07\", tz = \"America/Chicago\")\n)\n# as.period(dst, unit = \"seconds\")\nas.period(dst, unit = \"hours\")\nper <- period(hours = 10, minutes = 6)\nas.numeric(per, \"hours\")\nas.numeric(per, \"minutes\")",
            "as_date": "dt_utc <- ymd_hms(\"2010-08-03 00:50:50\")\ndt_europe <- ymd_hms(\"2010-08-03 00:50:50\", tz = \"Europe/London\")\nc(as_date(dt_utc), as.Date(dt_utc))\nc(as_date(dt_europe), as.Date(dt_europe))\n## need not supply origin\nas_date(10)\n## Will replace invalid date format with NA\ndt_wrong <- c(\"2009-09-29\", \"2012-11-29\", \"2015-29-12\")\nas_date(dt_wrong)",
            "cyclic_encoding": "times <- ymd_hms(\"2019-01-01 00:00:00\") + hours(0:23)\ncyclic_encoding(times, c(\"day\", \"week\", \"month\"))\nplot(cyclic_encoding(times, \"1d\"))\nplot(cyclic_encoding(times, \"2d\"), xlim = c(-1, 1))\nplot(cyclic_encoding(times, \"4d\"), xlim = c(-1, 1))",
            "date": "x <- ymd_hms(\"2012-03-26 23:12:13\", tz = \"America/New_York\")\ndate(x)\nas.Date(x) # by default as.Date assumes you want to know the date in UTC\nas.Date(x, tz = \"America/New_York\")\ndate(x) <- as.Date(\"2000-01-02\")\nx",
            "date_decimal": "date <- ymd(\"2009-02-10\")\ndecimal <- decimal_date(date) # 2009.11\ndate_decimal(decimal) # \"2009-02-10 UTC\"",
            "date_utils": "is.Date(as.Date(\"2009-08-03\")) # TRUE\nis.Date(difftime(now() + 5, now())) # FALSE",
            "day": "x <- as.Date(\"2009-09-02\")\nwday(x) # 4\nwday(x, label = TRUE) # Wed\n\nwday(x, week_start = 1) # 3\nwday(x, week_start = 7) # 4\n\nwday(x, label = TRUE, week_start = 7) # Wed (Sun is the first level)\nwday(x, label = TRUE, week_start = 1) # Wed (Mon is the first level)\n\nwday(ymd(080101))\nwday(ymd(080101), label = TRUE, abbr = FALSE)\nwday(ymd(080101), label = TRUE, abbr = TRUE)\nwday(ymd(080101) + days(-2:4), label = TRUE, abbr = TRUE)\n\nx <- as.Date(\"2009-09-02\")\nyday(x) # 245\nmday(x) # 2\nyday(x) <- 1 # \"2009-01-01\"\nyday(x) <- 366 # \"2010-01-01\"\nmday(x) > 3",
            "decimal_date": "date <- ymd(\"2009-02-10\")\ndecimal_date(date) # 2009.11",
            "dst": "x <- ymd(\"2012-03-26\")\ndst(x)",
            "duration": "### Separate period and units vectors\n\nduration(90, \"seconds\")\nduration(1.5, \"minutes\")\nduration(-1, \"days\")\n\n### Units as arguments\n\nduration(day = -1)\nduration(second = 90)\nduration(minute = 1.5)\nduration(mins = 1.5)\nduration(second = 3, minute = 1.5, hour = 2, day = 6, week = 1)\nduration(hour = 1, minute = -60)\n\n### Parsing\n\nduration(\"2M 1sec\")\nduration(\"2hours 2minutes 1second\")\nduration(\"2d 2H 2M 2S\")\nduration(\"2days 2hours 2mins 2secs\")\n# Missing numerals default to 1. Repeated units are added up.\nduration(\"day day\")\n\n### ISO 8601 parsing\n\nduration(\"P3Y6M4DT12H30M5S\")\nduration(\"P23DT23H\") # M stands for months\nduration(\"10DT10M\") # M stands for minutes\nduration(\"P23DT60H 20min 100 sec\") # mixing ISO and lubridate style parsing\n\n# Comparison with characters (from v1.6.0)\n\nduration(\"day 2 sec\") > \"day 1sec\"\n\n\n## ELEMENTARY CONSTRUCTORS:\n\ndseconds(1)\ndminutes(3.5)\n\nx <- ymd(\"2009-08-03\", tz = \"America/Chicago\")\nx + ddays(1) + dhours(6) + dminutes(30)\nx + ddays(100) - dhours(8)\n\nclass(as.Date(\"2009-08-09\") + ddays(1)) # retains Date class\nas.Date(\"2009-08-09\") + dhours(12)\nclass(as.Date(\"2009-08-09\") + dhours(12))\n# converts to POSIXt class to accomodate time units\n\ndweeks(1) - ddays(7)\nc(1:3) * dhours(1)\n\n# compare DST handling to durations\nboundary <- ymd_hms(\"2009-03-08 01:59:59\", tz = \"America/Chicago\")\nboundary + days(1) # period\nboundary + ddays(1) # duration\nis.duration(as.Date(\"2009-08-03\")) # FALSE\nis.duration(duration(days = 12.4)) # TRUE",
            "fit_to_timeline": "\\dontrun{\n\ntricky <- structure(list(\n  sec = c(5, 0, 0, -1),\n  min = c(0L, 5L, 5L, 0L),\n  hour = c(2L, 0L, 2L, 2L),\n  mday = c(4L, 4L, 14L, 4L),\n  mon = c(10L, 10L, 2L, 10L),\n  year = c(112L, 112L, 110L, 112L),\n  wday = c(0L, 0L, 0L, 0L),\n  yday = c(308L, 308L, 72L, 308L),\n  isdst = c(1L, 0L, 0L, 1L)\n),\n.Names = c(\n  \"sec\", \"min\", \"hour\", \"mday\", \"mon\",\n  \"year\", \"wday\", \"yday\", \"isdst\"\n),\nclass = c(\"POSIXlt\", \"POSIXt\"),\ntzone = c(\"America/Chicago\", \"CST\", \"CDT\")\n)\n\ntricky\n## [1] \"2012-11-04 02:00:00 CDT\" Doesn't exist because clocks \"fall back\" to 1:00 CST\n## [2] \"2012-11-04 00:05:00 CST\" Times are still CDT, not CST at this instant\n## [3] \"2010-03-14 02:00:00 CDT\" DST gap\n## [4] \"2012-11-04 01:59:59 CDT\" Does exist, but has deceptive internal structure\n\nfit_to_timeline(tricky)\n## Returns:\n## [1] \"2012-11-04 02:00:00 CST\" instant paired with correct tz & DST combination\n## [2] \"2012-11-04 00:05:00 CDT\" instant paired with correct tz & DST combination\n## [3] NA - fake time changed to NA (compare to as.POSIXct(tricky))\n## [4] \"2012-11-04 01:59:59 CDT\" -real instant, left as is\n\nfit_to_timeline(tricky, simple = TRUE)\n## Returns valid time-dates by extrapolating CDT and CST zones:\n## [1] \"2012-11-04 01:00:05 CST\" \"2012-11-04 01:05:00 CDT\"\n## [3] \"2010-03-14 03:05:00 CDT\" \"2012-11-04 01:59:59 CDT\"\n}",
            "force_tz": "x <- ymd_hms(\"2009-08-07 00:00:01\", tz = \"America/New_York\")\nforce_tz(x, \"UTC\")\nforce_tz(x, \"Europe/Amsterdam\")\n\n## DST skip:\ny <- ymd_hms(\"2010-03-14 02:05:05 UTC\")\nforce_tz(y, \"America/New_York\", roll_dst = \"NA\")\nforce_tz(y, \"America/New_York\", roll_dst = \"pre\")\nforce_tz(y, \"America/New_York\", roll_dst = \"boundary\")\nforce_tz(y, \"America/New_York\", roll_dst = \"post\")\n\n## DST repeat\ny <- ymd_hms(\"2014-11-02 01:35:00\", tz = \"UTC\")\nforce_tz(y, \"America/New_York\", roll_dst = \"NA\")\nforce_tz(y, \"America/New_York\", roll_dst = \"pre\")\nforce_tz(y, \"America/New_York\", roll_dst = \"boundary\")\nforce_tz(y, \"America/New_York\", roll_dst = \"post\")\n\n## DST skipped and repeated\ny <- ymd_hms(\"2010-03-14 02:05:05 UTC\", \"2014-11-02 01:35:00\", tz = \"UTC\")\nforce_tz(y, \"America/New_York\", roll_dst = c(\"NA\", \"pre\"))\nforce_tz(y, \"America/New_York\", roll_dst = c(\"boundary\", \"post\"))\n\n## Heterogeneous time-zones:\n\nx <- ymd_hms(c(\"2009-08-07 00:00:01\", \"2009-08-07 01:02:03\"))\nforce_tzs(x, tzones = c(\"America/New_York\", \"Europe/Amsterdam\"))\nforce_tzs(x, tzones = c(\"America/New_York\", \"Europe/Amsterdam\"), tzone_out = \"America/New_York\")\n\nx <- ymd_hms(\"2009-08-07 00:00:01\")\nforce_tzs(x, tzones = c(\"America/New_York\", \"Europe/Amsterdam\"))",
            "format_ISO8601": "format_ISO8601(as.Date(\"02-01-2018\", format = \"\\%m-\\%d-\\%Y\"))\nformat_ISO8601(as.POSIXct(\"2018-02-01 03:04:05\", tz = \"America/New_York\"), usetz = TRUE)\nformat_ISO8601(as.POSIXct(\"2018-02-01 03:04:05\", tz = \"America/New_York\"), precision = \"ymdhm\")",
            "guess_formats": "x <- c('February 20th 1973',\n       \"february  14, 2004\",\n       \"Sunday, May 1, 2000\",\n       \"Sunday, May 1, 2000\",\n       \"february  14, 04\",\n       'Feb 20th 73',\n       \"January 5 1999 at 7pm\",\n       \"jan 3 2010\",\n       \"Jan 1, 1999\",\n       \"jan 3   10\",\n       \"01 3 2010\",\n       \"1 3 10\",\n       '1 13 89',\n       \"5/27/1979\",\n       \"12/31/99\",\n       \"DOB:12/11/00\",\n       \"-----------\",\n       'Thu, 1 July 2004 22:30:00',\n       'Thu, 1st of July 2004 at 22:30:00',\n       'Thu, 1July 2004 at 22:30:00',\n       'Thu, 1July2004 22:30:00',\n       'Thu, 1July04 22:30:00',\n       \"21 Aug 2011, 11:15:34 pm\",\n       \"-----------\",\n       \"1979-05-27 05:00:59\",\n       \"1979-05-27\",\n       \"-----------\",\n       \"3 jan 2000\",\n       \"17 april 85\",\n       \"27/5/1979\",\n       '20 01 89',\n       '00/13/10',\n       \"-------\",\n       \"14 12 00\",\n       \"03:23:22 pm\")\n\nguess_formats(x, \"BdY\")\nguess_formats(x, \"Bdy\")\n## m also matches b and B; y also matches Y\nguess_formats(x, \"mdy\", print_matches = TRUE)\n\n## T also matches IMSp order\nguess_formats(x, \"T\", print_matches = TRUE)\n\n## b and B are equivalent and match, both, abreviated and full names\nguess_formats(x, c(\"mdY\", \"BdY\", \"Bdy\", \"bdY\", \"bdy\"), print_matches = TRUE)\nguess_formats(x, c(\"dmy\", \"dbY\", \"dBy\", \"dBY\"), print_matches = TRUE)\n\n\nguess_formats(x, c(\"dBY HMS\", \"dbY HMS\", \"dmyHMS\", \"BdY H\"), print_matches = TRUE)\n\nguess_formats(x, c(\"ymd HMS\"), print_matches = TRUE)",
            "hms": "ms(c(\"09:10\", \"09:02\", \"1:10\"))\nms(\"7 6\")\nms(\"6,5\")\nhm(c(\"09:10\", \"09:02\", \"1:10\"))\nhm(\"7 6\")\nhm(\"6,5\")\n\nx <- c(\"09:10:01\", \"09:10:02\", \"09:10:03\")\nhms(x)\n\nhms(\"7 6 5\", \"3:23:::2\", \"2 : 23 : 33\", \"Finished in 9 hours, 20 min and 4 seconds\")",
            "hour": "x <- ymd(\"2012-03-26\")\nhour(x)\nhour(x) <- 1\nhour(x) <- 25\nhour(x) > 2",
            "interval": "interval(ymd(20090201), ymd(20090101))\n\ndate1 <- ymd_hms(\"2009-03-08 01:59:59\")\ndate2 <- ymd_hms(\"2000-02-29 12:00:00\")\ninterval(date2, date1)\ninterval(date1, date2)\nspan <- interval(ymd(20090101), ymd(20090201))\n\n### ISO Intervals\n\ninterval(\"2007-03-01T13:00:00Z/2008-05-11T15:30:00Z\")\ninterval(\"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\")\ninterval(\"P1Y2M10DT2H30M/2008-05-11T15:30:00Z\")\ninterval(\"2008-05-11/P2H30M\")\n\n### More permissive parsing (as long as there are no intermittent / characters)\ninterval(\"2008 05 11/P2hours 30minutes\")\ninterval(\"08 05 11/P 2h 30m\")\n\nis.interval(period(months = 1, days = 15)) # FALSE\nis.interval(interval(ymd(20090801), ymd(20090809))) # TRUE\nint <- interval(ymd(\"2001-01-01\"), ymd(\"2002-01-01\"))\nint_start(int)\nint_start(int) <- ymd(\"2001-06-01\")\nint\n\nint <- interval(ymd(\"2001-01-01\"), ymd(\"2002-01-01\"))\nint_end(int)\nint_end(int) <- ymd(\"2002-06-01\")\nint\nint <- interval(ymd(\"2001-01-01\"), ymd(\"2002-01-01\"))\nint_length(int)\nint <- interval(ymd(\"2001-01-01\"), ymd(\"2002-01-01\"))\nint_flip(int)\nint <- interval(ymd(\"2001-01-01\"), ymd(\"2002-01-01\"))\nint_shift(int, duration(days = 11))\nint_shift(int, duration(hours = -1))\nint1 <- interval(ymd(\"2001-01-01\"), ymd(\"2002-01-01\"))\nint2 <- interval(ymd(\"2001-06-01\"), ymd(\"2002-06-01\"))\nint3 <- interval(ymd(\"2003-01-01\"), ymd(\"2004-01-01\"))\n\nint_overlaps(int1, int2) # TRUE\nint_overlaps(int1, int3) # FALSE\nint <- interval(ymd(\"2002-01-01\"), ymd(\"2001-01-01\"))\nint_standardize(int)\nint1 <- interval(ymd(\"2001-01-01\"), ymd(\"2002-01-01\"))\nint2 <- interval(ymd(\"2001-06-01\"), ymd(\"2002-01-01\"))\nint3 <- interval(ymd(\"2003-01-01\"), ymd(\"2004-01-01\"))\n\nint_aligns(int1, int2) # TRUE\nint_aligns(int1, int3) # FALSE\ndates <- now() + days(1:10)\nint_diff(dates)",
            "is.difftime": "is.difftime(as.Date(\"2009-08-03\")) # FALSE\nis.difftime(make_difftime(days = 12.4)) # TRUE",
            "is.instant": "is.instant(as.Date(\"2009-08-03\")) # TRUE\nis.timepoint(5) # FALSE",
            "is.timespan": "is.timespan(as.Date(\"2009-08-03\")) # FALSE\nis.timespan(duration(second = 1)) # TRUE",
            "leap_year": "x <- as.Date(\"2009-08-02\")\nleap_year(x) # FALSE\nleap_year(2009) # FALSE\nleap_year(2008) # TRUE\nleap_year(1900) # FALSE\nleap_year(2000) # TRUE",
            "local_time": "x <- ymd_hms(c(\"2009-08-07 01:02:03\", \"2009-08-07 10:20:30\"))\nlocal_time(x, units = \"secs\")\nlocal_time(x, units = \"hours\")\nlocal_time(x, \"Europe/Amsterdam\")\nlocal_time(x, \"Europe/Amsterdam\") == local_time(with_tz(x, \"Europe/Amsterdam\"))\n\nx <- ymd_hms(\"2009-08-07 01:02:03\")\nlocal_time(x, c(\"America/New_York\", \"Europe/Amsterdam\", \"Asia/Shanghai\"), unit = \"hours\")",
            "make_datetime": "make_datetime(year = 1999, month = 12, day = 22, sec = 10)\nmake_datetime(year = 1999, month = 12, day = 22, sec = c(10, 11))",
            "make_difftime": "make_difftime(1)\nmake_difftime(60)\nmake_difftime(3600)\nmake_difftime(3600, units = \"minute\")\n# Time difference of 60 mins\nmake_difftime(second = 90)\n# Time difference of 1.5 mins\nmake_difftime(minute = 1.5)\n# Time difference of 1.5 mins\nmake_difftime(second = 3, minute = 1.5, hour = 2, day = 6, week = 1)\n# Time difference of 13.08441 days\nmake_difftime(hour = 1, minute = -60)\n# Time difference of 0 secs\nmake_difftime(day = -1)\n# Time difference of -1 days\nmake_difftime(120, day = -1, units = \"minute\")\n# Time differences in mins",
            "minute": "x <- ymd(\"2012-03-26\")\nminute(x)\nminute(x) <- 1\nminute(x) <- 61\nminute(x) > 2",
            "month": "x <- ymd(\"2012-03-26\")\nmonth(x)\nmonth(x) <- 1\nmonth(x) <- 13\nmonth(x) > 3\n\nmonth(ymd(080101))\nmonth(ymd(080101), label = TRUE)\nmonth(ymd(080101), label = TRUE, abbr = FALSE)\nmonth(ymd(080101) + months(0:11), label = TRUE)",
            "mplus": "jan <- ymd_hms(\"2010-01-31 03:04:05\")\njan + months(1:3) # Feb 31 and April 31 returned as NA\n# NA \"2010-03-31 03:04:05 UTC\" NA\njan \\%m+\\% months(1:3) # No rollover\n\nleap <- ymd(\"2012-02-29\")\n\"2012-02-29 UTC\"\nleap \\%m+\\% years(1)\nleap \\%m+\\% years(-1)\nleap \\%m-\\% years(1)\n\nx <- ymd_hms(\"2019-01-29 01:02:03\")\nadd_with_rollback(x, months(1))\nadd_with_rollback(x, months(1), preserve_hms = FALSE)\nadd_with_rollback(x, months(1), roll_to_first = TRUE)\nadd_with_rollback(x, months(1), roll_to_first = TRUE, preserve_hms = FALSE)",
            "now": "now()\nnow(\"GMT\")\nnow(\"\")\nnow() == now() # would be TRUE if computer processed both at the same instant\nnow() < now() # TRUE\nnow() > now() # FALSE\ntoday()\ntoday(\"GMT\")\ntoday() == today(\"GMT\") # not always true\ntoday() < as.Date(\"2999-01-01\") # TRUE  (so far)",
            "origin": "origin",
            "parse_date_time": "## ** orders are much easier to write **\nx <- c(\"09-01-01\", \"09-01-02\", \"09-01-03\")\nparse_date_time(x, \"ymd\")\nparse_date_time(x, \"y m d\")\nparse_date_time(x, \"\\%y\\%m\\%d\")\n#  \"2009-01-01 UTC\" \"2009-01-02 UTC\" \"2009-01-03 UTC\"\n\n## ** heterogeneous date-times **\nx <- c(\"09-01-01\", \"090102\", \"09-01 03\", \"09-01-03 12:02\")\nparse_date_time(x, c(\"ymd\", \"ymd HM\"))\n\n## ** different ymd orders **\nx <- c(\"2009-01-01\", \"02022010\", \"02-02-2010\")\nparse_date_time(x, c(\"dmY\", \"ymd\"))\n##  \"2009-01-01 UTC\" \"2010-02-02 UTC\" \"2010-02-02 UTC\"\n\n## ** truncated time-dates **\nx <- c(\"2011-12-31 12:59:59\", \"2010-01-01 12:11\", \"2010-01-01 12\", \"2010-01-01\")\nparse_date_time(x, \"Ymd HMS\", truncated = 3)\n\n## ** specifying exact formats and avoiding training and guessing **\nparse_date_time(x, c(\"\\%m-\\%d-\\%y\", \"\\%m\\%d\\%y\", \"\\%m-\\%d-\\%y \\%H:\\%M\"), exact = TRUE)\nparse_date_time(c('12/17/1996 04:00:00','4/18/1950 0130'),\n                c('\\%m/\\%d/\\%Y \\%I:\\%M:\\%S','\\%m/\\%d/\\%Y \\%H\\%M'), exact = TRUE)\n\n## ** quarters and partial dates **\nparse_date_time(c(\"2016.2\", \"2016-04\"), orders = \"Yq\")\nparse_date_time(c(\"2016\", \"2016-04\"), orders = c(\"Y\", \"Ym\"))\n\n## ** fast parsing **\n\\dontrun{\n  options(digits.secs = 3)\n  ## random times between 1400 and 3000\n  tt <- as.character(.POSIXct(runif(1000, -17987443200, 32503680000)))\n  tt <- rep.int(tt, 1000)\n\n  system.time(out <- as.POSIXct(tt, tz = \"UTC\"))\n  system.time(out1 <- ymd_hms(tt)) # constant overhead on long vectors\n  system.time(out2 <- parse_date_time2(tt, \"YmdHMOS\"))\n  system.time(out3 <- fast_strptime(tt, \"\\%Y-\\%m-\\%d \\%H:\\%M:\\%OS\"))\n\n  all.equal(out, out1)\n  all.equal(out, out2)\n  all.equal(out, out3)\n}\n\n## ** how to use `select_formats` argument **\n## By default \\%Y has precedence:\nparse_date_time(c(\"27-09-13\", \"27-09-2013\"), \"dmy\")\n\n## to give priority to \\%y format, define your own select_format function:\n\nmy_select <-   function(trained, drop=FALSE, ...){\n   n_fmts <- nchar(gsub(\"[^\\%]\", \"\", names(trained))) + grepl(\"\\%y\", names(trained))*1.5\n   names(trained[ which.max(n_fmts) ])\n}\n\nparse_date_time(c(\"27-09-13\", \"27-09-2013\"), \"dmy\", select_formats = my_select)\n\n## ** invalid times with \"fast\" parsing **\nparse_date_time(\"2010-03-14 02:05:06\",  \"YmdHMS\", tz = \"America/New_York\")\nparse_date_time2(\"2010-03-14 02:05:06\",  \"YmdHMS\", tz = \"America/New_York\")\nparse_date_time2(\"2010-03-14 02:05:06\",  \"YmdHMS\", tz = \"America/New_York\", lt = TRUE)",
            "period": "### Separate period and units vectors\n\nperiod(c(90, 5), c(\"second\", \"minute\"))\n#  \"5M 90S\"\nperiod(-1, \"days\")\nperiod(c(3, 1, 2, 13, 1), c(\"second\", \"minute\", \"hour\", \"day\", \"week\"))\nperiod(c(1, -60), c(\"hour\", \"minute\"))\nperiod(0, \"second\")\n\n### Units as arguments\n\nperiod(second = 90, minute = 5)\nperiod(day = -1)\nperiod(second = 3, minute = 1, hour = 2, day = 13, week = 1)\nperiod(hour = 1, minute = -60)\nperiod(second = 0)\nperiod(c(1, -60), c(\"hour\", \"minute\"), hour = c(1, 2), minute = c(3, 4))\n\n### Lubridate style parsing\n\nperiod(\"2M 1sec\")\nperiod(\"2hours 2minutes 1second\")\nperiod(\"2d 2H 2M 2S\")\nperiod(\"2days 2hours 2mins 2secs\")\nperiod(\"2 days, 2 hours, 2 mins, 2 secs\")\n# Missing numerals default to 1. Repeated units are added up.\nperiod(\"day day\")\n\n### ISO 8601 parsing\n\nperiod(\"P10M23DT23H\") # M stands for months\nperiod(\"10DT10M\") # M stands for minutes\nperiod(\"P3Y6M4DT12H30M5S\") # M for both minutes and months\nperiod(\"P23DT60H 20min 100 sec\") # mixing ISO and lubridate style parsing\n\n### Comparison with characters (from v1.6.0)\n\nperiod(\"day 2 sec\") > \"day 1sec\"\n\n### Elementary Constructors\n\nx <- ymd(\"2009-08-03\")\nx + days(1) + hours(6) + minutes(30)\nx + days(100) - hours(8)\n\nclass(as.Date(\"2009-08-09\") + days(1)) # retains Date class\nas.Date(\"2009-08-09\") + hours(12)\nclass(as.Date(\"2009-08-09\") + hours(12))\n# converts to POSIXt class to accomodate time units\n\nyears(1) - months(7)\nc(1:3) * hours(1)\nhours(1:3)\n\n# sequencing\ny <- ymd(090101) # \"2009-01-01 CST\"\ny + months(0:11)\n\n# compare DST handling to durations\nboundary <- ymd_hms(\"2009-03-08 01:59:59\", tz = \"America/Chicago\")\nboundary + days(1) # period\nboundary + ddays(1) # duration\nis.period(as.Date(\"2009-08-03\")) # FALSE\nis.period(period(months = 1, days = 15)) # TRUE",
            "posix_utils": "is.POSIXt(as.Date(\"2009-08-03\"))\nis.POSIXt(as.POSIXct(\"2009-08-03\"))",
            "pretty_dates": "x <- seq.Date(as.Date(\"2009-08-02\"), by = \"year\", length.out = 2)\npretty_dates(x, 12)",
            "quarter": "x <- ymd(c(\"2012-03-26\", \"2012-05-04\", \"2012-09-23\", \"2012-12-31\"))\nquarter(x)\nquarter(x, type = \"year.quarter\")\nquarter(x, type = \"year.quarter\", fiscal_start = 11)\nquarter(x, type = \"date_first\", fiscal_start = 11)\nquarter(x, type = \"date_last\", fiscal_start = 11)\nsemester(x)\nsemester(x, with_year = TRUE)",
            "rollbackward": "date <- ymd(\"2010-03-03\")\nrollbackward(date)\n\ndates <- date + months(0:2)\nrollbackward(dates)\n\ndate <- ymd_hms(\"2010-03-03 12:44:22\")\nrollbackward(date)\nrollbackward(date, roll_to_first = TRUE)\nrollbackward(date, preserve_hms = FALSE)\nrollbackward(date, roll_to_first = TRUE, preserve_hms = FALSE)",
            "round_date": "## print fractional seconds\noptions(digits.secs = 6)\n\nx <- ymd_hms(\"2009-08-03 12:01:59.23\")\nround_date(x, \".5s\")\nround_date(x, \"sec\")\nround_date(x, \"second\")\nround_date(x, \"minute\")\nround_date(x, \"5 mins\")\nround_date(x, \"hour\")\nround_date(x, \"2 hours\")\nround_date(x, \"day\")\nround_date(x, \"week\")\nround_date(x, \"month\")\nround_date(x, \"bimonth\")\nround_date(x, \"quarter\") == round_date(x, \"3 months\")\nround_date(x, \"halfyear\")\nround_date(x, \"year\")\n\nx <- ymd_hms(\"2009-08-03 12:01:59.23\")\nfloor_date(x, \".1s\")\nfloor_date(x, \"second\")\nfloor_date(x, \"minute\")\nfloor_date(x, \"hour\")\nfloor_date(x, \"day\")\nfloor_date(x, \"week\")\nfloor_date(x, \"month\")\nfloor_date(x, \"bimonth\")\nfloor_date(x, \"quarter\")\nfloor_date(x, \"season\")\nfloor_date(x, \"halfyear\")\nfloor_date(x, \"year\")\n\nx <- ymd_hms(\"2009-08-03 12:01:59.23\")\nceiling_date(x, \".1 sec\") # imprecise representation at 0.1 sec !!!\nceiling_date(x, \"second\")\nceiling_date(x, \"minute\")\nceiling_date(x, \"5 mins\")\nceiling_date(x, \"hour\")\nceiling_date(x, \"day\")\nceiling_date(x, \"week\")\nceiling_date(x, \"month\")\nceiling_date(x, \"bimonth\") == ceiling_date(x, \"2 months\")\nceiling_date(x, \"quarter\")\nceiling_date(x, \"season\")\nceiling_date(x, \"halfyear\")\nceiling_date(x, \"year\")\n\n## Period unit argument\nfloor_date(x, days(2))\nfloor_date(x, years(1))\n\n## As of R 3.4.2 POSIXct printing of fractional numbers is wrong\nas.POSIXct(\"2009-08-03 12:01:59.3\") ## -> \"2009-08-03 12:01:59.2 CEST\"\nceiling_date(x, \".1 sec\") ## -> \"2009-08-03 12:01:59.2 CEST\"\n\n## behaviour of `change_on_boundary`\n## As per default behaviour `NULL`, instants on the boundary remain the\n## same but dates are rounded up\nceiling_date(ymd_hms(\"2000-01-01 00:00:00\"), \"month\")\nceiling_date(ymd(\"2000-01-01\"), \"month\")\n\n## If `TRUE`, both instants and dates on the boundary are rounded up\nceiling_date(ymd_hms(\"2000-01-01 00:00:00\"), \"month\", change_on_boundary = TRUE)\nceiling_date(ymd(\"2000-01-01\"), \"month\")\n\n## If `FALSE`, both instants and dates on the boundary remain the same\nceiling_date(ymd_hms(\"2000-01-01 00:00:00\"), \"month\", change_on_boundary = FALSE)\nceiling_date(ymd(\"2000-01-01\"), \"month\")\n\nx <- ymd_hms(\"2000-01-01 00:00:00\")\nceiling_date(x, \"month\")\nceiling_date(x, \"month\", change_on_boundary = TRUE)\n\n## For Date objects first day of the month is not on the\n## \"boundary\". change_on_boundary applies to instants only.\nx <- ymd(\"2000-01-01\")\nceiling_date(x, \"month\")\nceiling_date(x, \"month\", change_on_boundary = TRUE)",
            "second": "x <- ymd(\"2012-03-26\")\nsecond(x)\nsecond(x) <- 1\nsecond(x) <- 61\nsecond(x) > 2",
            "stamp": "D <- ymd(\"2010-04-05\") - days(1:5)\nstamp(\"March 1, 1999\")(D)\nsf <- stamp(\"Created on Sunday, Jan 1, 1999 3:34 pm\")\nsf(D)\nstamp(\"Jan 01\")(D)\nstamp(\"Sunday, May 1, 2000\", locale = \"C\")(D)\nstamp(\"Sun Aug 5\")(D) #=> \"Sun Aug 04\" \"Sat Aug 04\" \"Fri Aug 04\" \"Thu Aug 04\" \"Wed Aug 03\"\nstamp(\"12/31/99\")(D)              #=> \"06/09/11\"\nstamp(\"Sunday, May 1, 2000 22:10\", locale = \"C\")(D)\nstamp(\"2013-01-01T06:00:00Z\")(D)\nstamp(\"2013-01-01T00:00:00-06\")(D)\nstamp(\"2013-01-01T00:00:00-08:00\")(force_tz(D, \"America/Chicago\"))",
            "time_length": "int <- interval(ymd(\"1980-01-01\"), ymd(\"2014-09-18\"))\ntime_length(int, \"week\")\n\n# Exact age\ntime_length(int, \"year\")\n\n# Age at last anniversary\ntrunc(time_length(int, \"year\"))\n\n# Example of difference between intervals and durations\nint <- interval(ymd(\"1900-01-01\"), ymd(\"1999-12-31\"))\ntime_length(int, \"year\")\ntime_length(as.duration(int), \"year\")",
            "timespan": "duration(3690, \"seconds\")\nperiod(3690, \"seconds\")\nperiod(second = 30, minute = 1, hour = 1)\ninterval(ymd_hms(\"2009-08-09 13:01:30\"), ymd_hms(\"2009-08-09 12:00:00\"))\n\ndate <- ymd_hms(\"2009-03-08 01:59:59\") # DST boundary\ndate + days(1)\ndate + ddays(1)\n\ndate2 <- ymd_hms(\"2000-02-29 12:00:00\")\ndate2 + years(1)\n# self corrects to next real day\n\ndate3 <- ymd_hms(\"2009-01-31 01:00:00\")\ndate3 + c(0:11) * months(1)\n\nspan <- date2 \\%--\\% date # creates interval\n\ndate <- ymd_hms(\"2009-01-01 00:00:00\")\ndate + years(1)\ndate - days(3) + hours(6)\ndate + 3 * seconds(10)\n\nmonths(6) + days(1)",
            "tz": "x <- y <- ymd_hms(\"2012-03-26 10:10:00\", tz = \"UTC\")\ntz(x)\n\n# Note that setting tz() preserved the clock time, which implies\n# that the actual instant in time is changing\ntz(y) <- \"Pacific/Auckland\"\ny\nx - y\n\n# This is the same as force_tz()\nforce_tz(x, \"Pacific/Auckland\")\n\n# Use with_tz() if you want to change the time zone, leave\n# the instant in time the same\nwith_tz(x, \"Pacific/Auckland\")",
            "week": "x <- ymd(\"2012-03-26\")\nweek(x)\nweek(x) <- 1\nweek(x) <- 54\nweek(x) > 3",
            "with_tz": "x <- ymd_hms(\"2009-08-07 00:00:01\", tz = \"America/New_York\")\nwith_tz(x, \"GMT\")",
            "within-interval": "int <- interval(ymd(\"2001-01-01\"), ymd(\"2002-01-01\"))\nint2 <- interval(ymd(\"2001-06-01\"), ymd(\"2002-01-01\"))\n\nymd(\"2001-05-03\") \\%within\\% int # TRUE\nint2 \\%within\\% int # TRUE\nymd(\"1999-01-01\") \\%within\\% int # FALSE\n\n## recycling (carefully note the difference between using a vector of\n## intervals and list of intervals for the second argument)\ndates <- ymd(c(\"2014-12-20\", \"2014-12-30\", \"2015-01-01\", \"2015-01-03\"))\nblackout_vector <- c(\n  interval(ymd(\"2014-12-30\"), ymd(\"2014-12-31\")),\n  interval(ymd(\"2014-12-30\"), ymd(\"2015-01-03\"))\n)\ndates \\%within\\% blackout_vector\n\n## within ANY of the intervals of a list\ndates <- ymd(c(\"2014-12-20\", \"2014-12-30\", \"2015-01-01\", \"2015-01-03\"))\nlst <- list(\n  interval(ymd(\"2014-12-30\"), ymd(\"2014-12-31\")),\n  interval(ymd(\"2014-12-30\"), ymd(\"2015-01-03\"))\n)\ndates \\%within\\% lst\n\n## interval within a list of intervals\nint <- interval(\n  ymd(\"2014-12-20\", \"2014-12-30\"),\n  ymd(\"2015-01-01\", \"2015-01-03\")\n)\nint \\%within\\% lst",
            "year": "x <- ymd(\"2012-03-26\")\nyear(x)\nyear(x) <- 2001\nyear(x) > 1995",
            "ymd": "x <- c(\"09-01-01\", \"09-01-02\", \"09-01-03\")\nymd(x)\nx <- c(\"2009-01-01\", \"2009-01-02\", \"2009-01-03\")\nymd(x)\nymd(090101, 90102)\nnow() > ymd(20090101)\n## TRUE\ndmy(010210)\nmdy(010210)\n\nyq('2014.2')\n\n## heterogeneous formats in a single vector:\nx <- c(20090101, \"2009-01-02\", \"2009 01 03\", \"2009-1-4\",\n       \"2009-1, 5\", \"Created on 2009 1 6\", \"200901 !!! 07\")\nymd(x)\n\n## What lubridate might not handle:\n\n## Extremely weird cases when one of the separators is \"\" and some of the\n## formats are not in double digits might not be parsed correctly:\n\\dontrun{ymd(\"201002-01\", \"201002-1\", \"20102-1\")\ndmy(\"0312-2010\", \"312-2010\")}",
            "ymd_hms": "x <- c(\"2010-04-14-04-35-59\", \"2010-04-01-12-00-00\")\nymd_hms(x)\nx <- c(\"2011-12-31 12:59:59\", \"2010-01-01 12:00:00\")\nymd_hms(x)\n\n\n## ** heterogeneous formats **\nx <- c(20100101120101, \"2009-01-02 12-01-02\", \"2009.01.03 12:01:03\",\n       \"2009-1-4 12-1-4\",\n       \"2009-1, 5 12:1, 5\",\n       \"200901-08 1201-08\",\n       \"2009 arbitrary 1 non-decimal 6 chars 12 in between 1 !!! 6\",\n       \"OR collapsed formats: 20090107 120107 (as long as prefixed with zeros)\",\n       \"Automatic wday, Thu, detection, 10-01-10 10:01:10 and p format: AM\",\n       \"Created on 10-01-11 at 10:01:11 PM\")\nymd_hms(x)\n\n## ** fractional seconds **\nop <- options(digits.secs=3)\ndmy_hms(\"20/2/06 11:16:16.683\")\noptions(op)\n\n## ** different formats for ISO8601 timezone offset **\nymd_hms(c(\"2013-01-24 19:39:07.880-0600\",\n\"2013-01-24 19:39:07.880\", \"2013-01-24 19:39:07.880-06:00\",\n\"2013-01-24 19:39:07.880-06\", \"2013-01-24 19:39:07.880Z\"))\n\n## ** internationalization **\n\\dontrun{\nx_RO <- \"Ma 2012 august 14 11:28:30 \"\n  ymd_hms(x_RO, locale = \"ro_RO.utf8\")\n}\n\n## ** truncated time-dates **\nx <- c(\"2011-12-31 12:59:59\", \"2010-01-01 12:11\", \"2010-01-01 12\", \"2010-01-01\")\nymd_hms(x, truncated = 3)\nx <- c(\"2011-12-31 12:59\", \"2010-01-01 12\", \"2010-01-01\")\nymd_hm(x, truncated = 2)\n## ** What lubridate might not handle **\n## Extremely weird cases when one of the separators is \"\" and some of the\n## formats are not in double digits might not be parsed correctly:\n\\dontrun{\nymd_hm(\"20100201 07-01\", \"20100201 07-1\", \"20100201 7-01\")}"
        }
    },
    "callr": {
        "description": "It is sometimes useful to perform a computation in a separate\n    R process, without affecting the current R process at all.  This\n    packages does exactly that.",
        "examples": {
            "default_repos": "default_repos()",
            "r": "\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# Workspace is empty\nr(function() ls())\n\n# library path is the same by default\nr(function() .libPaths())\n.libPaths()\n\\dontshow{\\}) # examplesIf}",
            "r_bg": "\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nrx <- r_bg(function() 1 + 2)\n\n# wait until it is done\nrx$wait()\nrx$is_alive()\nrx$get_result()\n\\dontshow{\\}) # examplesIf}",
            "r_process": "\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n## List all options and their default values:\nr_process_options()\n\n## Start an R process in the background, wait for it, get result\nopts <- r_process_options(func = function() 1 + 1)\nrp <- r_process$new(opts)\nrp$wait()\nrp$get_result()\n\\dontshow{\\}) # examplesIf}",
            "r_process_options": "## List all options and their default values:\nr_process_options()",
            "r_session": "\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nrs <- r_session$new()\n\nrs$run(function() 1 + 2)\n\nrs$call(function() Sys.sleep(1))\nrs$get_state()\n\nrs$poll_process(-1)\nrs$get_state()\nrs$read()\n\\dontshow{\\}) # examplesIf}",
            "r_session_options": "r_session_options()",
            "r_vanilla": "\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# Compare to r()\nr(function() .libPaths())\nr_vanilla(function() .libPaths())\n\nr(function() getOption(\"repos\"))\nr_vanilla(function() getOption(\"repos\"))\n\\dontshow{\\}) # examplesIf}",
            "rcmd": "\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nrcmd(\"config\", \"CC\")\n\\dontshow{\\}) # examplesIf}",
            "rcmd_process": "\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\noptions <- rcmd_process_options(cmd = \"config\", cmdargs = \"CC\")\nrp <- rcmd_process$new(options)\nrp$wait()\nrp$read_output_lines()\n\\dontshow{\\}) # examplesIf}",
            "rcmd_process_options": "## List all options and their default values:\nrcmd_process_options()",
            "rscript_process": "\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\noptions <- rscript_process_options(script = \"script.R\")\nrp <- rscript_process$new(options)\nrp$wait()\nrp$read_output_lines()\n\\dontshow{\\}) # examplesIf}",
            "rscript_process_options": "## List all options and their default values:\nrscript_process_options()",
            "supported_archs": "supported_archs()"
        }
    },
    "mime": {
        "description": "Guesses the MIME type from a filename extension using the data\n    derived from /etc/mime.types in UNIX-type systems.",
        "examples": {
            "guess_type": "library(mime)\n# well-known file types\nguess_type(c(\"a/b/c.html\", \"d.pdf\", \"e.odt\", \"foo.docx\", \"tex\"))\n# not in the standard table, but in mimeextra\nguess_type(c(\"a.md\", \"b.R\"), mime_extra = NULL)\nguess_type(c(\"a.md\", \"b.R\"))\n\n# override the standard MIME table (tex is text/x-tex by default)\nguess_type(\"tex\", mime_extra = c(tex = \"text/plain\"))\n# unknown extension 'bar'\nguess_type(\"foo.bar\")\n# force unknown types to be plain text\nguess_type(\"foo.bar\", unknown = \"text/plain\")\n\n# empty file extension\nguess_type(\"Makefile\")\n# we know it is a plain text file\nguess_type(\"Makefile\", empty = \"text/plain\")\n\n# subtypes\nguess_type(c(\"abc.html\", \"def.htm\"), subtype = c(\"charset=UTF-8\", \"\"))",
            "mimemap": "str(as.list(mimemap))\nmimemap[\"pdf\"]\nmimemap[c(\"html\", \"js\", \"css\")]\n# additional MIME types (not exported)\nmime:::mimeextra"
        }
    },
    "gtable": {
        "description": "Tools to make it easier to work with \"tables\" of 'grobs'. The\n    'gtable' package defines a 'gtable' grob class that specifies a grid\n    along with a list of grobs and their placement in the grid. Further\n    the package makes it easy to manipulate and combine 'gtable' objects\n    so that complex compositions can be built up sequentially.",
        "examples": {
            "bind": "library(grid)\na <- rectGrob(gp = gpar(fill = \"red\"))\nb <- circleGrob()\nc <- linesGrob()\n\nrow <- matrix(list(a, b), nrow = 1)\ncol <- matrix(list(a, b), ncol = 1)\nmat <- matrix(list(a, b, c, nullGrob()), nrow = 2)\n\nrow_gt <- gtable_matrix(\"demo\", row, unit(c(1, 1), \"null\"), unit(1, \"null\"))\ncol_gt <- gtable_matrix(\"demo\", col, unit(1, \"null\"), unit(c(1, 1), \"null\"))\nmat_gt <- gtable_matrix(\"demo\", mat, unit(c(1, 1), \"null\"), unit(c(1, 1), \"null\"))\n\n# cbind\nc_binded <- cbind(mat_gt, col_gt, size = \"first\")\nplot(c_binded)\n\n# rbind\nr_binded <- rbind(mat_gt, row_gt, size = \"last\")\nplot(r_binded)\n\n# Dimensions must match along bind direction\ntry(cbind(mat_gt, row_gt))",
            "gtable": "library(grid)\na <- gtable(unit(1:3, c(\"cm\")), unit(5, \"cm\"))\na\ngtable_show_layout(a)\n\n# Add a grob:\nrect <- rectGrob(gp = gpar(fill = \"black\"))\na <- gtable_add_grob(a, rect, 1, 1)\na\nplot(a)\n\n# gtables behave like matrices:\ndim(a)\nt(a)\nplot(t(a))\n\n# when subsetting, grobs are retained if their extents lie in the\n# rows/columns that retained.\n\nb <- gtable(unit(c(2, 2, 2), \"cm\"), unit(c(2, 2, 2), \"cm\"))\nb <- gtable_add_grob(b, rect, 2, 2)\nb[1, ]\nb[, 1]\nb[2, 2]\n\n# gtable have row and column names\nrownames(b) <- 1:3\nrownames(b)[2] <- 200\ncolnames(b) <- letters[1:3]\ndimnames(b)",
            "gtable_add_cols": "library(grid)\nrect <- rectGrob(gp = gpar(fill = \"#00000080\"))\ntab <- gtable(unit(rep(1, 3), \"null\"), unit(rep(1, 3), \"null\"))\ntab <- gtable_add_grob(tab, rect, t = 1, l = 1, r = 3)\ntab <- gtable_add_grob(tab, rect, t = 1, b = 3, l = 1)\ntab <- gtable_add_grob(tab, rect, t = 1, b = 3, l = 3)\ndim(tab)\nplot(tab)\n\n# Grobs will continue to span over new rows if added in the middle\ntab2 <- gtable_add_cols(tab, unit(1, \"null\"), 1)\ndim(tab2)\nplot(tab2)\n\n# But not when added to left (0) or right (-1, the default)\ntab3 <- gtable_add_cols(tab, unit(1, \"null\"))\ntab3 <- gtable_add_cols(tab3, unit(1, \"null\"), 0)\ndim(tab3)\nplot(tab3)",
            "gtable_add_grob": "library(grid)\n\ngt <- gtable(widths = unit(c(1, 1), 'null'), heights = unit(c(1, 1), 'null'))\npts <- pointsGrob(x = runif(5), y = runif(5))\n\n# Add a grob to a single cell (top-right cell)\ngt <- gtable_add_grob(gt, pts, t = 1, l = 2)\n\n# Add a grob spanning multiple cells\ngt <- gtable_add_grob(gt, pts, t = 1, l = 1, b = 2)\n\nplot(gt)",
            "gtable_add_padding": "library(grid)\ngt <- gtable(unit(1, \"null\"), unit(1, \"null\"))\ngt <- gtable_add_grob(gt, rectGrob(gp = gpar(fill = \"black\")), 1, 1)\n\nplot(gt)\nplot(cbind(gt, gt))\nplot(rbind(gt, gt))\n\npad <- gtable_add_padding(gt, unit(1, \"cm\"))\nplot(pad)\nplot(cbind(pad, pad))\nplot(rbind(pad, pad))",
            "gtable_add_rows": "library(grid)\nrect <- rectGrob(gp = gpar(fill = \"#00000080\"))\ntab <- gtable(unit(rep(1, 3), \"null\"), unit(rep(1, 3), \"null\"))\ntab <- gtable_add_grob(tab, rect, t = 1, l = 1, r = 3)\ntab <- gtable_add_grob(tab, rect, t = 1, b = 3, l = 1)\ntab <- gtable_add_grob(tab, rect, t = 1, b = 3, l = 3)\ndim(tab)\nplot(tab)\n\n# Grobs will continue to span over new rows if added in the middle\ntab2 <- gtable_add_rows(tab, unit(1, \"null\"), 1)\ndim(tab2)\nplot(tab2)\n\n# But not when added to top (0) or bottom (-1, the default)\ntab3 <- gtable_add_rows(tab, unit(1, \"null\"))\ntab3 <- gtable_add_rows(tab3, unit(1, \"null\"), 0)\ndim(tab3)\nplot(tab3)",
            "gtable_add_space": "library(grid)\n\nrect <- rectGrob()\nrect_mat <- matrix(rep(list(rect), 9), nrow = 3)\n\ngt <- gtable_matrix(\"rects\", rect_mat, widths = unit(rep(1, 3), \"null\"),\n                    heights = unit(rep(1, 3), \"null\"))\n\nplot(gt)\n\n# Add spacing between the grobs\n# same height between all rows\ngt <- gtable_add_row_space(gt, unit(0.5, \"cm\"))\n\n# Different width between the columns\ngt <- gtable_add_col_space(gt, unit(c(0.5, 1), \"cm\"))\n\nplot(gt)",
            "gtable_col": "library(grid)\na <- rectGrob(gp = gpar(fill = \"red\"))\nb <- circleGrob()\nc <- linesGrob()\ngt <- gtable_col(\"demo\", list(a, b, c))\ngt\nplot(gt)\ngtable_show_layout(gt)",
            "gtable_filter": "library(grid)\ngt <- gtable(unit(rep(5, 3), c(\"cm\")), unit(5, \"cm\"))\nrect <- rectGrob(gp = gpar(fill = \"black\"))\ncirc <- circleGrob(gp = gpar(fill = \"red\"))\n\ngt <- gtable_add_grob(gt, rect, 1, 1, name = \"rect\")\ngt <- gtable_add_grob(gt, circ, 1, 3, name = \"circ\")\n\nplot(gtable_filter(gt, \"rect\"))\nplot(gtable_filter(gt, \"rect\", trim = FALSE))\nplot(gtable_filter(gt, \"circ\"))\nplot(gtable_filter(gt, \"circ\", trim = FALSE))",
            "gtable_matrix": "library(grid)\na <- rectGrob(gp = gpar(fill = \"red\"))\nb <- circleGrob()\nc <- linesGrob()\n\nrow <- matrix(list(a, b, c), nrow = 1)\ncol <- matrix(list(a, b, c), ncol = 1)\nmat <- matrix(list(a, b, c, nullGrob()), nrow = 2)\n\ngtable_matrix(\"demo\", row, unit(c(1, 1, 1), \"null\"), unit(1, \"null\"))\ngtable_matrix(\"demo\", col, unit(1, \"null\"), unit(c(1, 1, 1), \"null\"))\ngtable_matrix(\"demo\", mat, unit(c(1, 1), \"null\"), unit(c(1, 1), \"null\"))\n\n# Can specify z ordering\nz <- matrix(c(3, 1, 2, 4), nrow = 2)\ngtable_matrix(\"demo\", mat, unit(c(1, 1), \"null\"), unit(c(1, 1), \"null\"), z = z)",
            "gtable_row": "library(grid)\na <- rectGrob(gp = gpar(fill = \"red\"))\nb <- circleGrob()\nc <- linesGrob()\ngt <- gtable_row(\"demo\", list(a, b, c))\ngt\nplot(gt)\ngtable_show_layout(gt)",
            "gtable_show_layout": "gt <- gtable(widths = grid::unit(c(1, 0.5, 2), c(\"null\", \"cm\", \"null\")),\n             heights = grid::unit(c(0.2, 1, 3), c(\"inch\", \"null\", \"cm\")))\ngtable_show_layout(gt)",
            "gtable_trim": "library(grid)\nrect <- rectGrob(gp = gpar(fill = \"black\"))\nbase <- gtable(unit(c(2, 2, 2), \"cm\"), unit(c(2, 2, 2), \"cm\"))\n\ncenter <- gtable_add_grob(base, rect, 2, 2)\nplot(center)\nplot(gtable_trim(center))\n\ncol <- gtable_add_grob(base, rect, 1, 2, 3, 2)\nplot(col)\nplot(gtable_trim(col))\n\nrow <- gtable_add_grob(base, rect, 2, 1, 2, 3)\nplot(row)\nplot(gtable_trim(row))"
        }
    },
    "highr": {
        "description": "Provides syntax highlighting for R source code. Currently it\n    supports LaTeX and HTML output. Source code of other languages is supported\n    via Andre Simon's highlight package (<https://gitlab.com/saalen/highlight>).",
        "examples": {
            "hi_andre": "\\dontrun{\nhi_andre(\"1+1\", language = \"R\")\nhi_andre(\"void main() {\\nreturn(0)\\n}\", language = \"c\", format = \"latex\")\n}",
            "hilight": "library(highr)\nhilight(\"x=1 # assignment\")\n\ntxt = c(\"a <- 1 # something\", \"c(y=\\\"world\\\", z=\\\"hello\\\")\", \"b=function(x=5) {\",\n    \"for(i in 1:10) {\n  if (i < x) print(i) else break}}\", \"z@child # S4 slot\",\n    \"'special chars <>#$\\%&_{}'\")\ncat(hi_latex(txt), sep = \"\\n\")\ncat(hi_html(txt), sep = \"\\n\")\n\n# the markup data frames\nhighr:::cmd_latex\nhighr:::cmd_html"
        }
    },
    "pkgdown": {
        "description": "Generate an attractive and useful website from a source\n    package.  'pkgdown' converts your documentation, vignettes, 'README',\n    and more to 'HTML' making it easy to share information about your\n    package online.",
        "examples": {
            "autolink_html": "\\dontrun{\nautolink_html(\"path/to/file.html\",\n  local_packages = c(\n    shiny = \"shiny\",\n    shinydashboard = \"shinydashboard\"\n  )\n)\n}",
            "build_site": "\\dontrun{\nbuild_site()\n\nbuild_site(override = list(destination = tempdir()))\n}",
            "in_pkgdown": "in_pkgdown()",
            "rd2html": "rd2html(\"a\\n\\%b\\nc\")\n\nrd2html(\"a & b\")\n\nrd2html(\"\\\\\\\\strong{\\\\\\\\emph{x}}\")",
            "templates": "\\dontrun{\npkgdown::template_navbar()\n}\n\n\\dontrun{\npkgdown::template_reference()\n}\n\n\\dontrun{\npkgdown::template_articles()\n}",
            "test-crayon": "cat(cli::col_red(\"This is red\"), \"\\n\")\ncat(cli::col_blue(\"This is blue\"), \"\\n\")\n\nmessage(cli::col_green(\"This is green\"))\n\nwarning(cli::style_bold(\"This is bold\"))",
            "test-dont": "# \\dontrun{} --------------------------------------------------------\n# always shown; never run\n\nx <- 1\n\\dontrun{x <- 2}\n\\dontrun{\n  x <- 3\n  x <- 4\n}\nx # should be 1\n\n# \\donttest{} -------------------------------------------------------\n# only multiline are shown; always run\n\nx <- 1\n\\donttest{x <- 2}\n\\donttest{\n  x <- 3\n  x <- 4\n}\nx # should be 4\n\n# \\testonly{} -----------------------------------------------------\n# never shown, never run\n\nx <- 1\n\\testonly{x <- 2}\n\\testonly{\n  x <- 3\n  x <- 4\n}\nx # should be 1\n\n# \\dontshow{} -------------------------------------------------------\n# never shown, always run\n\nx <- 1\n\\dontshow{x <- 2}\n\\dontshow{\n  x <- 3\n  x <- 4\n}\nx # should be 4\n\n# @examplesIf ------------------------------------------------------\n# If FALSE, wrapped in if; if TRUE, not seen\n\nx <- 1\n\n\\dontshow{if (FALSE) withAutoprint(\\{ # examplesIf}\nx <- 2\n\\dontshow{\\}) # examplesIf}\n\\dontshow{if (TRUE) withAutoprint(\\{ # examplesIf}\nx <- 3\n\\dontshow{\\}) # examplesIf}\nx # should be 3",
            "test-figures": "x <- seq(0, 2 * pi, length.out = 25)\nplot(x, sin(x))\n\nplot(1:10)\nlines(1:10)\ntext(2, 5, \"Hello\", srt = 30, cex = 2)",
            "test-links": "jsonlite::minify(\"{}\")\n\nlibrary(jsonlite, warn.conflicts = FALSE)\nminify(\"{}\")",
            "test-long-lines": "pkgdown:::ruler()\n\ncat(rep(\"x \", 100), sep = \"\")\ncat(rep(\"xy\", 100), sep = \"\")\ncat(rep(\"x \", 100), sep = \"\")\ncat(rep(\"xy\", 100), sep = \"\")",
            "test-output-styles": "# This example illustrates some important output types\n# The following output should be wrapped over multiple lines\na <- 1:100\na\n\ncat(\"This some text!\\n\")\nmessage(\"This is a message!\")\nwarning(\"This is a warning!\")\n\n# This is a multi-line block\n{\n  1 + 2\n  2 + 2\n}",
            "test-tables": "gt::gt(head(mtcars))"
        }
    },
    "aws.ec2metadata": {
        "description": "Retrieve Amazon EC2 instance metadata from within the running instance.",
        "examples": {
            "ec2metadata": "names(metadata)\n\n\\dontrun{\nif (is_ec2()) {\n  metadata$versions()\n  metadata$items()\n\n  # get instance id\n  metadata$instance_id()\n  # get ami id\n  metadata$ami_id()\n  \n  # get IAM role (NULL if none specified)\n  metadata$iam_info()\n  metadata$iam_role(\"myrole\")\n\n  # get an arbitrary metadata item\n  metadata$item(\"meta-data/placement/availability-zone\")\n  \n  # get region from instance identity document\n  instance_document()$region\n}\n\n# Can also get ECS container metadata\nif (is_ecs()) {\n  # Get ECS role credentials\n  metadata$ecs_task_role()\n  # or\n  ecs_metadata()\n}\n}"
        }
    },
    "readr": {
        "description": "The goal of 'readr' is to provide a fast and friendly way to\n    read rectangular data (like 'csv', 'tsv', and 'fwf').  It is designed\n    to flexibly parse many types of data found in the wild, while still\n    cleanly failing when data unexpectedly changes.",
        "examples": {
            "Tokenizers": "tokenizer_csv()",
            "as.col_spec": "as.col_spec(\"cccnnn\")",
            "callback": "## If given a regular function it is converted to a SideEffectChunkCallback\n\n# view structure of each chunk\nread_lines_chunked(readr_example(\"mtcars.csv\"), str, chunk_size = 5)\n\n# Print starting line of each chunk\nf <- function(x, pos) print(pos)\nread_lines_chunked(readr_example(\"mtcars.csv\"), SideEffectChunkCallback$new(f), chunk_size = 5)\n\n# If combined results are desired you can use the DataFrameCallback\n\n# Cars with 3 gears\nf <- function(x, pos) subset(x, gear == 3)\nread_csv_chunked(readr_example(\"mtcars.csv\"), DataFrameCallback$new(f), chunk_size = 5)\n\n# The ListCallback can be used for more flexible output\nf <- function(x, pos) x$mpg[x$hp > 100]\nread_csv_chunked(readr_example(\"mtcars.csv\"), ListCallback$new(f), chunk_size = 5)\n\n# The AccumulateCallback accumulates results from each chunk\nf <- function(x, pos, acc) sum(x$mpg) + acc\nread_csv_chunked(readr_example(\"mtcars.csv\"), AccumulateCallback$new(f, acc = 0), chunk_size = 5)",
            "cols": "cols(a = col_integer())\ncols_only(a = col_integer())\n\n# You can also use the standard abbreviations\ncols(a = \"i\")\ncols(a = \"i\", b = \"d\", c = \"_\")\n\n# You can also use multiple sets of column definitions by combining\n# them like so:\n\nt1 <- cols(\n  column_one = col_integer(),\n  column_two = col_number()\n)\n\nt2 <- cols(\n  column_three = col_character()\n)\n\nt3 <- t1\nt3$cols <- c(t1$cols, t2$cols)\nt3",
            "count_fields": "count_fields(readr_example(\"mtcars.csv\"), tokenizer_csv())",
            "datasource": "# Literal csv\ndatasource(\"a,b,c\\n1,2,3\")\ndatasource(charToRaw(\"a,b,c\\n1,2,3\"))\n\n# Strings\ndatasource(readr_example(\"mtcars.csv\"))\ndatasource(readr_example(\"mtcars.csv.bz2\"))\ndatasource(readr_example(\"mtcars.csv.zip\"))\n\\dontrun{\ndatasource(\"https://github.com/tidyverse/readr/raw/main/inst/extdata/mtcars.csv\")\n}\n\n# Connection\ncon <- rawConnection(charToRaw(\"abc\\n123\"))\ndatasource(con)\nclose(con)",
            "date_names": "date_names_lang(\"en\")\ndate_names_lang(\"ko\")\ndate_names_lang(\"fr\")",
            "edition_get": "edition_get()",
            "encoding": "guess_encoding(readr_example(\"mtcars.csv\"))\nguess_encoding(read_lines_raw(readr_example(\"mtcars.csv\")))\nguess_encoding(read_file_raw(readr_example(\"mtcars.csv\")))\n\nguess_encoding(\"a\\n\\u00b5\\u00b5\")",
            "format_delim": "# format_()* functions are useful for testing and reprexes\ncat(format_csv(mtcars))\ncat(format_tsv(mtcars))\ncat(format_delim(mtcars, \";\"))\n\n# Specifying missing values\ndf <- data.frame(x = c(1, NA, 3))\nformat_csv(df, na = \"missing\")\n\n# Quotes are automatically added as needed\ndf <- data.frame(x = c(\"a \", '\"', \",\", \"\\n\"))\ncat(format_csv(df))",
            "locale": "locale()\nlocale(\"fr\")\n\n# South American locale\nlocale(\"es\", decimal_mark = \",\")",
            "melt_delim": "# Input sources -------------------------------------------------------------\n# Read from a path\nmelt_csv(readr_example(\"mtcars.csv\"))\nmelt_csv(readr_example(\"mtcars.csv.zip\"))\nmelt_csv(readr_example(\"mtcars.csv.bz2\"))\n\\dontrun{\nmelt_csv(\"https://github.com/tidyverse/readr/raw/main/inst/extdata/mtcars.csv\")\n}\n\n# Or directly from a string (must contain a newline)\nmelt_csv(\"x,y\\n1,2\\n3,4\")\n\n# To import empty cells as 'empty' rather than `NA`\nmelt_csv(\"x,y\\n,NA,\\\"\\\",''\", na = \"NA\")\n\n# File types ----------------------------------------------------------------\nmelt_csv(\"a,b\\n1.0,2.0\")\nmelt_csv2(\"a;b\\n1,0;2,0\")\nmelt_tsv(\"a\\tb\\n1.0\\t2.0\")\nmelt_delim(\"a|b\\n1.0|2.0\", delim = \"|\")",
            "melt_delim_chunked": "# Cars with 3 gears\nf <- function(x, pos) subset(x, data_type == \"integer\")\nmelt_csv_chunked(readr_example(\"mtcars.csv\"), DataFrameCallback$new(f), chunk_size = 5)",
            "melt_fwf": "fwf_sample <- readr_example(\"fwf-sample.txt\")\ncat(read_lines(fwf_sample))\n\n# You can specify column positions in several ways:\n# 1. Guess based on position of empty columns\nmelt_fwf(fwf_sample, fwf_empty(fwf_sample, col_names = c(\"first\", \"last\", \"state\", \"ssn\")))\n# 2. A vector of field widths\nmelt_fwf(fwf_sample, fwf_widths(c(20, 10, 12), c(\"name\", \"state\", \"ssn\")))\n# 3. Paired vectors of start and end positions\nmelt_fwf(fwf_sample, fwf_positions(c(1, 30), c(10, 42), c(\"name\", \"ssn\")))\n# 4. Named arguments with start and end positions\nmelt_fwf(fwf_sample, fwf_cols(name = c(1, 10), ssn = c(30, 42)))\n# 5. Named arguments with column widths\nmelt_fwf(fwf_sample, fwf_cols(name = 20, state = 10, ssn = 12))",
            "melt_table": "fwf <- readr_example(\"fwf-sample.txt\")\nwriteLines(read_lines(fwf))\nmelt_table(fwf)\n\nws <- readr_example(\"whitespace-sample.txt\")\nwriteLines(read_lines(ws))\nmelt_table2(ws)",
            "output_column": "# Most columns are not altered, but POSIXct are converted to ISO8601.\nx <- parse_datetime(\"2016-01-01\")\nstr(output_column(x))",
            "parse_atomic": "parse_integer(c(\"1\", \"2\", \"3\"))\nparse_double(c(\"1\", \"2\", \"3.123\"))\nparse_number(\"$1,123,456.00\")\n\n# Use locale to override default decimal and grouping marks\nes_MX <- locale(\"es\", decimal_mark = \",\")\nparse_number(\"$1.123.456,00\", locale = es_MX)\n\n# Invalid values are replaced with missing values with a warning.\nx <- c(\"1\", \"2\", \"3\", \"-\")\nparse_double(x)\n# Or flag values as missing\nparse_double(x, na = \"-\")",
            "parse_datetime": "# Format strings --------------------------------------------------------\nparse_datetime(\"01/02/2010\", \"\\%d/\\%m/\\%Y\")\nparse_datetime(\"01/02/2010\", \"\\%m/\\%d/\\%Y\")\n# Handle any separator\nparse_datetime(\"01/02/2010\", \"\\%m\\%.\\%d\\%.\\%Y\")\n\n# Dates look the same, but internally they use the number of days since\n# 1970-01-01 instead of the number of seconds. This avoids a whole lot\n# of troubles related to time zones, so use if you can.\nparse_date(\"01/02/2010\", \"\\%d/\\%m/\\%Y\")\nparse_date(\"01/02/2010\", \"\\%m/\\%d/\\%Y\")\n\n# You can parse timezones from strings (as listed in OlsonNames())\nparse_datetime(\"2010/01/01 12:00 US/Central\", \"\\%Y/\\%m/\\%d \\%H:\\%M \\%Z\")\n# Or from offsets\nparse_datetime(\"2010/01/01 12:00 -0600\", \"\\%Y/\\%m/\\%d \\%H:\\%M \\%z\")\n\n# Use the locale parameter to control the default time zone\n# (but note UTC is considerably faster than other options)\nparse_datetime(\"2010/01/01 12:00\", \"\\%Y/\\%m/\\%d \\%H:\\%M\",\n  locale = locale(tz = \"US/Central\")\n)\nparse_datetime(\"2010/01/01 12:00\", \"\\%Y/\\%m/\\%d \\%H:\\%M\",\n  locale = locale(tz = \"US/Eastern\")\n)\n\n# Unlike strptime, the format specification must match the complete\n# string (ignoring leading and trailing whitespace). This avoids common\n# errors:\nstrptime(\"01/02/2010\", \"\\%d/\\%m/\\%y\")\nparse_datetime(\"01/02/2010\", \"\\%d/\\%m/\\%y\")\n\n# Failures -------------------------------------------------------------\nparse_datetime(\"01/01/2010\", \"\\%d/\\%m/\\%Y\")\nparse_datetime(c(\"01/ab/2010\", \"32/01/2010\"), \"\\%d/\\%m/\\%Y\")\n\n# Locales --------------------------------------------------------------\n# By default, readr expects English date/times, but that's easy to change'\nparse_datetime(\"1 janvier 2015\", \"\\%d \\%B \\%Y\", locale = locale(\"fr\"))\nparse_datetime(\"1 enero 2015\", \"\\%d \\%B \\%Y\", locale = locale(\"es\"))\n\n# ISO8601 --------------------------------------------------------------\n# With separators\nparse_datetime(\"1979-10-14\")\nparse_datetime(\"1979-10-14T10\")\nparse_datetime(\"1979-10-14T10:11\")\nparse_datetime(\"1979-10-14T10:11:12\")\nparse_datetime(\"1979-10-14T10:11:12.12345\")\n\n# Without separators\nparse_datetime(\"19791014\")\nparse_datetime(\"19791014T101112\")\n\n# Time zones\nus_central <- locale(tz = \"US/Central\")\nparse_datetime(\"1979-10-14T1010\", locale = us_central)\nparse_datetime(\"1979-10-14T1010-0500\", locale = us_central)\nparse_datetime(\"1979-10-14T1010Z\", locale = us_central)\n# Your current time zone\nparse_datetime(\"1979-10-14T1010\", locale = locale(tz = \"\"))",
            "parse_factor": "# discover the levels from the data\nparse_factor(c(\"a\", \"b\"))\nparse_factor(c(\"a\", \"b\", \"-99\"))\nparse_factor(c(\"a\", \"b\", \"-99\"), na = c(\"\", \"NA\", \"-99\"))\nparse_factor(c(\"a\", \"b\", \"-99\"), na = c(\"\", \"NA\", \"-99\"), include_na = FALSE)\n\n# provide the levels explicitly\nparse_factor(c(\"a\", \"b\"), levels = letters[1:5])\n\nx <- c(\"cat\", \"dog\", \"caw\")\nanimals <- c(\"cat\", \"dog\", \"cow\")\n\n# base::factor() silently converts elements that do not match any levels to\n# NA\nfactor(x, levels = animals)\n\n# parse_factor() generates same factor as base::factor() but throws a warning\n# and reports problems\nparse_factor(x, levels = animals)",
            "parse_guess": "# Logical vectors\nparse_guess(c(\"FALSE\", \"TRUE\", \"F\", \"T\"))\n\n# Integers and doubles\nparse_guess(c(\"1\", \"2\", \"3\"))\nparse_guess(c(\"1.6\", \"2.6\", \"3.4\"))\n\n# Numbers containing grouping mark\nguess_parser(\"1,234,566\")\nparse_guess(\"1,234,566\")\n\n# ISO 8601 date times\nguess_parser(c(\"2010-10-10\"))\nparse_guess(c(\"2010-10-10\"))",
            "parse_number": "## These all return 1000\nparse_number(\"$1,000\") ## leading `$` and grouping character `,` ignored\nparse_number(\"euro1,000\") ## leading non-numeric euro ignored\nparse_number(\"t1000t1000\") ## only parses first number found\n\nparse_number(\"1,234.56\")\n## explicit locale specifying European grouping and decimal marks\nparse_number(\"1.234,56\", locale = locale(decimal_mark = \",\", grouping_mark = \".\"))\n## SI/ISO 31-0 standard spaces for number grouping\nparse_number(\"1 234.56\", locale = locale(decimal_mark = \".\", grouping_mark = \" \"))\n\n## Specifying strings for NAs\nparse_number(c(\"1\", \"2\", \"3\", \"NA\"))\nparse_number(c(\"1\", \"2\", \"3\", \"NA\", \"Nothing\"), na = c(\"NA\", \"Nothing\"))",
            "parse_vector": "x <- c(\"1\", \"2\", \"3\", \"NA\")\nparse_vector(x, col_integer())\nparse_vector(x, col_double())",
            "problems": "x <- parse_integer(c(\"1X\", \"blah\", \"3\"))\nproblems(x)\n\ny <- parse_integer(c(\"1\", \"2\", \"3\"))\nproblems(y)",
            "read_builtin": "read_builtin(\"mtcars\", \"datasets\")",
            "read_delim": "# Input sources -------------------------------------------------------------\n# Read from a path\nread_csv(readr_example(\"mtcars.csv\"))\nread_csv(readr_example(\"mtcars.csv.zip\"))\nread_csv(readr_example(\"mtcars.csv.bz2\"))\n\\dontrun{\n# Including remote paths\nread_csv(\"https://github.com/tidyverse/readr/raw/main/inst/extdata/mtcars.csv\")\n}\n\n# Read from multiple file paths at once\ncontinents <- c(\"africa\", \"americas\", \"asia\", \"europe\", \"oceania\")\nfilepaths <- vapply(\n  paste0(\"mini-gapminder-\", continents, \".csv\"),\n  FUN = readr_example,\n  FUN.VALUE = character(1)\n)\nread_csv(filepaths, id = \"file\")\n\n# Or directly from a string with `I()`\nread_csv(I(\"x,y\\n1,2\\n3,4\"))\n\n# Column selection-----------------------------------------------------------\n# Pass column names or indexes directly to select them\nread_csv(readr_example(\"chickens.csv\"), col_select = c(chicken, eggs_laid))\nread_csv(readr_example(\"chickens.csv\"), col_select = c(1, 3:4))\n\n# Or use the selection helpers\nread_csv(\n  readr_example(\"chickens.csv\"),\n  col_select = c(starts_with(\"c\"), last_col())\n)\n\n# You can also rename specific columns\nread_csv(\n  readr_example(\"chickens.csv\"),\n  col_select = c(egg_yield = eggs_laid, everything())\n)\n\n# Column types --------------------------------------------------------------\n# By default, readr guesses the columns types, looking at `guess_max` rows.\n# You can override with a compact specification:\nread_csv(I(\"x,y\\n1,2\\n3,4\"), col_types = \"dc\")\n\n# Or with a list of column types:\nread_csv(I(\"x,y\\n1,2\\n3,4\"), col_types = list(col_double(), col_character()))\n\n# If there are parsing problems, you get a warning, and can extract\n# more details with problems()\ny <- read_csv(I(\"x\\n1\\n2\\nb\"), col_types = list(col_double()))\ny\nproblems(y)\n\n# Column names --------------------------------------------------------------\n# By default, readr duplicate name repair is noisy\nread_csv(I(\"x,x\\n1,2\\n3,4\"))\n\n# Same default repair strategy, but quiet\nread_csv(I(\"x,x\\n1,2\\n3,4\"), name_repair = \"unique_quiet\")\n\n# There's also a global option that controls verbosity of name repair\nwithr::with_options(\n  list(rlib_name_repair_verbosity = \"quiet\"),\n  read_csv(I(\"x,x\\n1,2\\n3,4\"))\n)\n\n# Or use \"minimal\" to turn off name repair\nread_csv(I(\"x,x\\n1,2\\n3,4\"), name_repair = \"minimal\")\n\n# File types ----------------------------------------------------------------\nread_csv(I(\"a,b\\n1.0,2.0\"))\nread_csv2(I(\"a;b\\n1,0;2,0\"))\nread_tsv(I(\"a\\tb\\n1.0\\t2.0\"))\nread_delim(I(\"a|b\\n1.0|2.0\"), delim = \"|\")",
            "read_delim_chunked": "# Cars with 3 gears\nf <- function(x, pos) subset(x, gear == 3)\nread_csv_chunked(readr_example(\"mtcars.csv\"), DataFrameCallback$new(f), chunk_size = 5)",
            "read_file": "read_file(file.path(R.home(\"doc\"), \"AUTHORS\"))\nread_file_raw(file.path(R.home(\"doc\"), \"AUTHORS\"))\n\ntmp <- tempfile()\n\nx <- format_csv(mtcars[1:6, ])\nwrite_file(x, tmp)\nidentical(x, read_file(tmp))\n\nread_lines(I(x))",
            "read_fwf": "fwf_sample <- readr_example(\"fwf-sample.txt\")\nwriteLines(read_lines(fwf_sample))\n\n# You can specify column positions in several ways:\n# 1. Guess based on position of empty columns\nread_fwf(fwf_sample, fwf_empty(fwf_sample, col_names = c(\"first\", \"last\", \"state\", \"ssn\")))\n# 2. A vector of field widths\nread_fwf(fwf_sample, fwf_widths(c(20, 10, 12), c(\"name\", \"state\", \"ssn\")))\n# 3. Paired vectors of start and end positions\nread_fwf(fwf_sample, fwf_positions(c(1, 30), c(20, 42), c(\"name\", \"ssn\")))\n# 4. Named arguments with start and end positions\nread_fwf(fwf_sample, fwf_cols(name = c(1, 20), ssn = c(30, 42)))\n# 5. Named arguments with column widths\nread_fwf(fwf_sample, fwf_cols(name = 20, state = 10, ssn = 12))",
            "read_lines": "read_lines(file.path(R.home(\"doc\"), \"AUTHORS\"), n_max = 10)\nread_lines_raw(file.path(R.home(\"doc\"), \"AUTHORS\"), n_max = 10)\n\ntmp <- tempfile()\n\nwrite_lines(rownames(mtcars), tmp)\nread_lines(tmp, lazy = FALSE)\nread_file(tmp) # note trailing \\n\n\nwrite_lines(airquality$Ozone, tmp, na = \"-1\")\nread_lines(tmp)",
            "read_log": "read_log(readr_example(\"example.log\"))",
            "read_rds": "temp <- tempfile()\nwrite_rds(mtcars, temp)\nread_rds(temp)\n\\dontrun{\nwrite_rds(mtcars, \"compressed_mtc.rds\", \"xz\", compression = 9L)\n}",
            "read_table": "ws <- readr_example(\"whitespace-sample.txt\")\nwriteLines(read_lines(ws))\nread_table(ws)",
            "readr_example": "readr_example()\nreadr_example(\"challenge.csv\")",
            "spec": "df <- read_csv(readr_example(\"mtcars.csv\"))\ns <- spec(df)\ns\n\ncols_condense(s)",
            "spec_delim": "# Input sources -------------------------------------------------------------\n# Retrieve specs from a path\nspec_csv(system.file(\"extdata/mtcars.csv\", package = \"readr\"))\nspec_csv(system.file(\"extdata/mtcars.csv.zip\", package = \"readr\"))\n\n# Or directly from a string (must contain a newline)\nspec_csv(I(\"x,y\\n1,2\\n3,4\"))\n\n# Column types --------------------------------------------------------------\n# By default, readr guesses the columns types, looking at 1000 rows\n# throughout the file.\n# You can specify the number of rows used with guess_max.\nspec_csv(system.file(\"extdata/mtcars.csv\", package = \"readr\"), guess_max = 20)",
            "tokenize": "tokenize(\"1,2\\n3,4,5\\n\\n6\")\n\n# Only tokenize first two lines\ntokenize(\"1,2\\n3,4,5\\n\\n6\", n = 2)",
            "type_convert": "df <- data.frame(\n  x = as.character(runif(10)),\n  y = as.character(sample(10)),\n  stringsAsFactors = FALSE\n)\nstr(df)\nstr(type_convert(df))\n\ndf <- data.frame(x = c(\"NA\", \"10\"), stringsAsFactors = FALSE)\nstr(type_convert(df))\n\n# Type convert can be used to infer types from an entire dataset\n\n# first read the data as character\ndata <- read_csv(readr_example(\"mtcars.csv\"),\n  col_types = list(.default = col_character())\n)\nstr(data)\n# Then convert it with type_convert\ntype_convert(data)",
            "with_edition": "with_edition(1, edition_get())\nwith_edition(2, edition_get())\n\n# readr 1e and 2e behave differently when input rows have different number\n# number of fields\nwith_edition(1, read_csv(\"1,2\\n3,4,5\", col_names = c(\"X\", \"Y\", \"Z\")))\nwith_edition(2, read_csv(\"1,2\\n3,4,5\", col_names = c(\"X\", \"Y\", \"Z\")))\n\n# local_edition() applies in a specific scope, for example, inside a function\nread_csv_1e <- function(...) {\n  local_edition(1)\n  read_csv(...)\n}\nread_csv(\"1,2\\n3,4,5\", col_names = c(\"X\", \"Y\", \"Z\"))      # 2e behaviour\nread_csv_1e(\"1,2\\n3,4,5\", col_names = c(\"X\", \"Y\", \"Z\"))   # 1e behaviour\nread_csv(\"1,2\\n3,4,5\", col_names = c(\"X\", \"Y\", \"Z\"))      # 2e behaviour",
            "write_delim": "\\dontshow{\n.old_wd <- setwd(tempdir())\n}\n# If only a file name is specified, write_()* will write\n# the file to the current working directory.\nwrite_csv(mtcars, \"mtcars.csv\")\nwrite_tsv(mtcars, \"mtcars.tsv\")\n\n# If you add an extension to the file name, write_()* will\n# automatically compress the output.\nwrite_tsv(mtcars, \"mtcars.tsv.gz\")\nwrite_tsv(mtcars, \"mtcars.tsv.bz2\")\nwrite_tsv(mtcars, \"mtcars.tsv.xz\")\n\\dontshow{\nsetwd(.old_wd)\n}"
        }
    },
    "rgl": {
        "description": "Provides medium to high level functions for 3D interactive graphics, including\n    functions modelled on base graphics (plot3d(), etc.) as well as functions for\n    constructing representations of geometric objects (cube3d(), etc.).  Output\n    may be on screen using OpenGL, or to various standard 3D file formats including\n    WebGL, PLY, OBJ, STL as well as 2D image formats, including PNG, Postscript, SVG, PGF.",
        "examples": {
            "GramSchmidt": "# Proceed through the rows in order\nprint(A <- matrix(rnorm(9), 3, 3))\nGramSchmidt(A[1, ], A[2, ], A[3, ])\n\n# Keep the middle row unchanged\nprint(A <- matrix(c(rnorm(2), 0, 1, 0, 0, rnorm(3)), 3, 3, byrow = TRUE))\nGramSchmidt(A[1, ], A[2, ], A[3, ], order = c(2, 1, 3))",
            "abclines": "plot3d(rnorm(100), rnorm(100), rnorm(100))\nabclines3d(0, 0, 0, a = diag(3), col = \"gray\")",
            "addNormals": "open3d()\ny <- subdivision3d(tetrahedron3d(col = \"red\"), depth = 3)\nshade3d(y) # No normals\ny <- addNormals(y)\nshade3d(translate3d(y, x = 1, y = 0, z = 0)) # With normals",
            "ageControl": "saveopts <- options(rgl.useNULL = TRUE)\n\n  theta <- seq(0, 4*pi, length.out = 100)\n  xyz <- cbind(sin(theta), cos(theta), sin(theta/2))\n  lineid <- plot3d(xyz, type=\"l\", alpha = 0, lwd = 5, col = \"blue\")[\"data\"]\n\n  widget <- rglwidget() \\%>\\%\n  playwidget(ageControl(births = theta,\n                        ages = c(-4*pi, -4*pi, 1-4*pi, 0, 0, 1),\n                        objids = lineid,\n                        alpha = c(0, 1, 0, 0, 1, 0)),\n             start = 0, stop = 4*pi,\n             step = 0.1, rate = 4)\n  if (interactive() || in_pkgdown_example())\n    widget\n  options(saveopts)",
            "arc3d": "normalize <- function(v) v/sqrt(sum(v^2))\n\n# These vectors all have the same length\n\nfrom <- t(apply(matrix(rnorm(9), ncol = 3), 1, normalize))\nto <- normalize(rnorm(3))\ncenter <- c(0, 0, 0)\n\nopen3d()\nspheres3d(center, radius = 1, col = \"white\", alpha = 0.2)\n\narc3d(from, to, center, col = \"red\")\narc3d(from, 2*to, center, col = \"blue\")\n\ntext3d(rbind(from, to, center, 2*to), \n       texts = c(paste0(\"from\", 1:3), \"to\", \"center\", \"2*to\"),\n       depth_mask = FALSE, depth_test = \"always\")",
            "arrow3d": "xyz <- matrix(rnorm(300), ncol = 3)\nplot3d(xyz)\narrow3d(xyz[1,], xyz[2,], type = \"extrusion\", col = \"red\")\narrow3d(xyz[3,], xyz[4,], type = \"flat\",      col = \"blue\")\narrow3d(xyz[5,], xyz[6,], type = \"rotation\",  col = \"green\")\narrow3d(xyz[7,], xyz[8,], type = \"lines\",     col = \"black\")\narrow3d(spriteOrigin = xyz[9:12,],            col = \"purple\")",
            "as.mesh3d.ashape3d": "if (requireNamespace(\"alphashape3d\", quietly = TRUE)) {\n  set.seed(123)\n  n <- 400    # 1000 gives a nicer result, but takes longer\n  xyz <- rbind(cbind(runif(n), runif(n), runif(n)),\n               cbind(runif(n/8, 1, 1.5), \n                     runif(n/8, 0.25, 0.75), \n                     runif(n/8, 0.25, 0.75)))\n  ash <- suppressMessages(alphashape3d::ashape3d(xyz, alpha = 0.2))\n  m <- as.mesh3d(ash, smooth = TRUE)\n  open3d()\n  mfrow3d(1, 2, sharedMouse = TRUE)\n  plot3d(xyz, size = 1)\n  plot3d(m, col = \"red\", alpha = 0.5)\n  points3d(xyz, size = 1)\n}",
            "as.mesh3d.default": "xyz <- matrix(c(-1, -1, -1,\n                -1,  1, -1,\n                 1,  1, -1,\n                 1, -1, -1,\n                -1,  1, -1,\n                -1,  1,  1,\n                 1,  1,  1,\n                 1,  1, -1,\n                 1, -1, -1,\n                 1,  1, -1,\n                 1,  1,  1,\n                 1, -1,  1), byrow = TRUE, ncol = 3)\nmesh <- as.mesh3d(xyz, type = \"quads\", col = \"red\")\nmesh$vb\nmesh$ib\nopen3d()\nshade3d(mesh)\n\n# Stop vertices 2 and 5 from being merged\nnotEQ <- matrix(FALSE, 12, 12)\nnotEQ[2, 5] <- TRUE\nmesh <- as.mesh3d(xyz, type = \"quads\", notEqual = notEQ)\nmesh$vb\nmesh$ib",
            "as.mesh3d.rglId": "# volcano example taken from \"persp\"\n#\ndata(volcano)\n\nz <- 2 * volcano        # Exaggerate the relief\n\nx <- 10 * (1:nrow(z))   # 10 meter spacing (S to N)\ny <- 10 * (1:ncol(z))   # 10 meter spacing (E to W)\n\nzlim <- range(y)\nzlen <- zlim[2] - zlim[1] + 1\n\ncolorlut <- terrain.colors(zlen) # height color lookup table\n\ncol <- colorlut[ z - zlim[1] + 1 ] # assign colors to heights for each point\n\nopen3d(useNULL = TRUE)\nsurface3d(x, y, z, color = col)\nm <- as.mesh3d()\nclose3d()\n\nopen3d()\nshade3d(m)",
            "as.tmesh3d": "x <- cuboctahedron3d()\nx             # has quads and triangles\nas.tmesh3d(x) # has only triangles",
            "as.triangles3d": "open3d()\nx <- surface3d(x = 1:10, y = 1:10, z = rnorm(100), col = \"red\")\ntri <- as.triangles3d(x)\nopen3d()\ntriangles3d(tri, col = \"blue\")",
            "asRow": "if (requireNamespace(\"manipulateWidget\", quietly = TRUE) &&\n    require(\"crosstalk\", quietly = TRUE)) {\n  sd <- SharedData$new(mtcars)\n  ids <- plot3d(sd$origData(), col = mtcars$cyl, type = \"s\")\n  # Copy the key and group from existing shared data\n  rglsd <- rglShared(ids[\"data\"], key = sd$key(), group = sd$groupName())\n  w <- rglwidget(shared = rglsd) \\%>\\%\n       asRow(\"Mouse mode: \", rglMouse(getWidgetId(.)), \n             \"Subset: \", filter_checkbox(\"cylinderselector\", \n\t\t               \"Cylinders\", sd, ~ cyl, inline = TRUE),\n             last = 4, colsize = c(1,2,1,2), height = 60)\n  if (interactive() || in_pkgdown_example())\n    w\n}",
            "aspect3d": "x <- rnorm(100)\n  y <- rnorm(100)*2\n  z <- rnorm(100)*3\n  \n  open3d()\n  plot3d(x, y, z)\n  aspect3d(1, 1, 0.5)\n  highlevel()  # To trigger display\n  open3d()\n  plot3d(x, y, z)\n  aspect3d(\"iso\")\n  highlevel()",
            "attributes": "p <- plot3d(rnorm(100), rnorm(100), rnorm(100), type = \"s\", col = \"red\")\nrgl.attrib(p[\"data\"], \"vertices\", last = 10)",
            "axes3d": "open3d()\n  points3d(rnorm(10), rnorm(10), rnorm(10))\n\n  # First add standard axes\n  axes3d()  \n\n  # and one in the middle (the NA will be ignored, a number would \n  # do as well)\n  axis3d('x', pos = c(NA, 0, 0))\n\n  # add titles\n  title3d('main', 'sub', 'xlab', 'ylab', 'zlab')\n  \n  # Use a log scale for z\n    \n  open3d()\n  \n  x <- rnorm(10)\n  y <- rnorm(10)\n  z <- exp(rnorm(10, mean = 3, sd = 2))\n  \n  logz <- log10(z)\n  zticks <- axisTicks(range(logz), log = TRUE)\n  zat <- log10(zticks)\n  \n  plot3d(x, y, logz, zlab = \"z\")\n  axes3d(zat = zat, zlab = zticks, box = TRUE)",
            "bbox": "open3d()\n  points3d(rnorm(100), rnorm(100), rnorm(100))\n  bbox3d(color = c(\"#333377\", \"black\"), emission = \"#333377\", \n         specular = \"#3333FF\", shininess = 5, alpha = 0.8)",
            "bg": "open3d()\n  \n  # a simple white background\n  \n  bg3d(\"white\")\n\n  # the holo-globe (inspired by star trek):\n\n  bg3d(sphere = TRUE, color = c(\"black\", \"green\"), lit = FALSE, back = \"lines\" )\n\n  # an environmental sphere with a nice texture.\n\n  bg3d(sphere = TRUE, texture = system.file(\"textures/sunsleep.png\", package = \"rgl\"), \n         back = \"filled\" )\n         \n  # The same texture as a fixed background\n  \n  open3d()\n  bg3d(texture = system.file(\"textures/sunsleep.png\", package = \"rgl\"), col = \"white\")",
            "bgplot3d": "x <- rnorm(100)\ny <- rnorm(100)\nz <- rnorm(100)\nopen3d()\n# Needs to be a bigger window than the default\npar3d(windowRect = c(100, 100, 612, 612))\nparent <- currentSubscene3d()\nmfrow3d(2, 2)\nplot3d(x, y, z)\nnext3d(reuse = FALSE)\nbgplot3d(plot(y, z))\nnext3d(reuse = FALSE)\nbgplot3d(plot(x, z))\nnext3d(reuse = FALSE)\nlegend3d(\"center\", c(\"2D Points\", \"3D Points\"), pch = c(1, 16))\nuseSubscene3d(parent)",
            "callbacks": "pan3d <- function(button, dev = cur3d(), subscene = currentSubscene3d(dev)) {\n   start <- list()\n   \n   begin <- function(x, y) {\n     activeSubscene <- par3d(\"activeSubscene\", dev = dev)\n     start$listeners <<- par3d(\"listeners\", dev = dev, subscene = activeSubscene)\n     for (sub in start$listeners) {\n       init <- par3d(c(\"userProjection\",\"viewport\"), dev = dev, subscene = sub)\n       init$pos <- c(x/init$viewport[3], 1 - y/init$viewport[4], 0.5)\n       start[[as.character(sub)]] <<- init\n     }\n   }\n   \n   update <- function(x, y) {\n     for (sub in start$listeners) {\n       init <- start[[as.character(sub)]]\n       xlat <- 2*(c(x/init$viewport[3], 1 - y/init$viewport[4], 0.5) - init$pos)\n       mouseMatrix <- translationMatrix(xlat[1], xlat[2], xlat[3])\n       par3d(userProjection = mouseMatrix \\%*\\% init$userProjection, dev = dev, subscene = sub )\n      }\n   }\n   rgl.setMouseCallbacks(button, begin, update, dev = dev, subscene = subscene)\n   cat(\"Callbacks set on button\", button, \"of RGL device\", dev, \"in subscene\", subscene, \"\\n\")\n }\n open3d()\n shade3d(icosahedron3d(), col = \"yellow\")\n # This only works in the internal display...\n pan3d(1)",
            "check3d": "rgl.dev.list()\n.check3d()\nrgl.dev.list()\n.check3d()\nrgl.dev.list()\nclose3d()",
            "checkDeldir": "checkDeldir()",
            "clipMesh3d": "# Show the problem that minVertices solves:\n\ncube <- cube3d(col = rainbow(6), meshColor = \"faces\")\n\n# This function only has one argument, so it will \n# be passed x, y and z in columns of a matrix\nvecnorm <- function(vals) apply(vals, 1, function(row) sqrt(sum(row^2)))\n\nopen3d()\nmfrow3d(2, 2, sharedMouse = TRUE)\nid1 <- shade3d(cube)\n# All vertices have norm sqrt(3), so this clips nothing:\nclipObj3d(id1, fn = vecnorm, bound = sqrt(2))\nnext3d()\nid2 <- wire3d(cube, lit = FALSE)\nclipObj3d(id2, fn = vecnorm, bound = sqrt(2))\n\n# This subdivides the cube, and does proper clipping:\nnext3d()\nid3 <- shade3d(cube)\nclipObj3d(id3, fn = vecnorm, bound = sqrt(2), minVertices = 200)\nnext3d()\nid4 <- wire3d(cube, lit = FALSE)\nclipObj3d(id4, fn = vecnorm, bound = sqrt(2), minVertices = 200)",
            "clipplaneControl": "open3d()\n  saveopts <- options(rgl.useNULL = TRUE)\n  xyz <- matrix(rnorm(300), ncol = 3)\n  id <- plot3d(xyz, type=\"s\", col = \"blue\", zlim = c(-3,3))[\"clipplanes\"]\n  dvals <- c(3, -3)\n  widget <- rglwidget() \\%>\\%\n    playwidget(clipplaneControl(d = dvals, clipplaneids = id),\n               start = 0, stop = 1, step = 0.01,\n               rate = 0.5)\n  if (interactive() || in_pkgdown_example())\n    widget\n  options(saveopts)",
            "contourLines3d": "# Add contourlines in \"z\" to a persp plot\n\nz <- 2 * volcano        # Exaggerate the relief\nx <- 10 * (1:nrow(z))   # 10 meter spacing (S to N)\ny <- 10 * (1:ncol(z))   # 10 meter spacing (E to W)\n\nopen3d()\nid <- persp3d(x, y, z, aspect = \"iso\",\n      axes = FALSE, box = FALSE, polygon_offset = 1)\ncontourLines3d(id)     # \"z\" is the default function\nfilledContour3d(id, polygon_offset = 1, nlevels = 10, replace = TRUE)\n\n# Draw longitude and latitude lines on a globe\n\nlat <- matrix(seq(90, -90, length.out = 50)*pi/180, 50, 50, byrow = TRUE)\nlong <- matrix(seq(-180, 180, length.out = 50)*pi/180, 50, 50)\n\nr <- 6378.1 # radius of Earth in km\nx <- r*cos(lat)*cos(long)\ny <- r*cos(lat)*sin(long)\nz <- r*sin(lat)\n\nopen3d()\nids <- persp3d(x, y, z, col = \"white\", \n        texture = system.file(\"textures/worldsmall.png\", package = \"rgl\"), \n        specular = \"black\", axes = FALSE, box = FALSE, xlab = \"\", ylab = \"\", zlab = \"\",\n        normal_x = x, normal_y = y, normal_z = z, polygon_offset = 1)\n        \ncontourLines3d(ids, list(latitude = function(x, y, z) asin(z/sqrt(x^2+y^2+z^2))*180/pi,\n                         longitude = function(x, y, z) atan2(y, x)*180/pi))",
            "cube3d": "# render all of the Platonic solids\n  open3d()\n  shade3d( translate3d( tetrahedron3d(col = \"red\"), 0, 0, 0) )\n  shade3d( translate3d( cube3d(col = \"green\"), 3, 0, 0) )\n  shade3d( translate3d( octahedron3d(col = \"blue\"), 6, 0, 0) )\n  shade3d( translate3d( dodecahedron3d(col = \"cyan\"), 9, 0, 0) )\n  shade3d( translate3d( icosahedron3d(col = \"magenta\"), 12, 0, 0) )",
            "cylinder3d": "# A trefoil knot\nopen3d()\ntheta <- seq(0, 2*pi, length.out = 25)\nknot <- cylinder3d(\n      center = cbind(\n        sin(theta) + 2*sin(2*theta), \n        2*sin(3*theta), \n        cos(theta) - 2*cos(2*theta)),\n      e1 = cbind(\n        cos(theta) + 4*cos(2*theta), \n        6*cos(3*theta), \n        sin(theta) + 4*sin(2*theta)),\n      radius = 0.8, \n      closed = TRUE,\n      color = \"green\")\n                     \nshade3d(addNormals(subdivision3d(knot, depth = 2)))",
            "decorate3d": "open3d()\nshade3d(tetrahedron3d(), col = \"red\")\ndecorate3d(main = \"A Tetrahedron\")",
            "drape3d": "#\n# volcano example taken from \"persp\"\n#\n\nz <- 2 * volcano        # Exaggerate the relief\n\nx <- 10 * (1:nrow(z))   # 10 meter spacing (S to N)\ny <- 10 * (1:ncol(z))   # 10 meter spacing (E to W)\n\nzlim <- range(z)\nzlen <- zlim[2] - zlim[1] + 1\n\ncolorlut <- terrain.colors(zlen) # height color lookup table\n\ncol <- colorlut[ z - zlim[1] + 1 ] # assign colors to heights for each point\n\nopen3d()\nid <- surface3d(x, y, z, color = col, polygon_offset = 1)\n\nsegs <- data.frame(x = range(x) + c(100, -100),\n                   y = range(y) + c(150, -100), z = 325)\ndrape3d(id, segs, col = 'yellow', lwd = 3)\nlines3d(segs, col='red', lwd=3)\n\np <- c(350, 205)         # (x,y) of strike & dip reading\noff <- 20*c(-1, +1)      # X-marks-the-spot offset\nsegs <- data.frame(\n    x = c(p[1] + off, NA, p[1] + off),\n    y = c(p[2] + off, NA, p[2] - off),\n    z = rep(350, 5)\n    )\ndrape3d(id, segs, col = \"yellow\", lwd = 3)",
            "ellipse3d": "# Plot a random sample and an ellipsoid of concentration corresponding to a 95\\% \n# probability region for a\n# trivariate normal distribution with mean 0, unit variances and \n# correlation 0.8.\nif (requireNamespace(\"MASS\", quietly = TRUE)) {\n  Sigma <- matrix(c(10, 3, 0, 3, 2, 0, 0, 0, 1), 3, 3)\n  Mean <- 1:3\n  x <- MASS::mvrnorm(1000, Mean, Sigma)\n  \n  open3d()\n  \n  plot3d(x, box = FALSE)\n  \n  plot3d( ellipse3d(Sigma, centre = Mean), col = \"green\", alpha = 0.5, add = TRUE)\n}  \n\n# Plot the estimate and joint 90\\% confidence region for the displacement and cylinder\n# count linear coefficients in the mtcars dataset\n\ndata(mtcars)\nfit <- lm(mpg ~ disp + cyl , mtcars)\n\nopen3d()\nplot3d(ellipse3d(fit, level = 0.90), col = \"blue\", alpha = 0.5, aspect = TRUE)",
            "expect_known_scene": "\\dontrun{\n# These lines can be included in testthat::test_that() code.\nplot3d(1:10, 1:10, 1:10)\nexpect_known_scene(\"plot\")\n}",
            "extrude3d": "x <- c(1:10, 10:1)\ny <- rev(c(rep(c(0, 2), 5), rep(c(1.5, -0.5), 5)))\nplot(x, y, type = \"n\")\npolygon(x, y)\nopen3d()\nshade3d( extrude3d(x, y), col = \"red\" )",
            "facing3d": "open3d()\nd <- rnorm(3)\nd <- d/sqrt(sum(d^2))\nshade3d( facing3d( icosahedron3d(), up = d, strict = FALSE), \n         col = \"yellow\")\nwire3d( facing3d( icosahedron3d(), up = d, front = FALSE), \n         col = \"black\")\n# Show the direction:\narrow3d(-2*d , -d)",
            "figWidth": "# No useful return value outside of R Markdown:\nfigWidth()\nfigHeight()",
            "getBoundary3d": "x <- cube3d(col = \"blue\")\nx$ib <- x$ib[,-(1:2)]\nb <- getBoundary3d(x, sorted = TRUE, col = \"black\")\n\nopen3d()\nshade3d(x, alpha=0.2)\n\nshade3d(b) \n\n# Show edge vertices in sequence:\ntext3d(t(b$vb), texts = 1:ncol(b$vb), adj = 0)\nc(b$is[1,1], b$is[2,])",
            "gltfTypes": "gltfTypes",
            "grid3d": "x <- 1:10\ny <- 1:10\nz <- matrix(outer(x - 5, y - 5) + rnorm(100), 10, 10)\nopen3d()\npersp3d(x, y, z, col = \"red\", alpha = 0.7, aspect = c(1, 1, 0.5))\ngrid3d(c(\"x\", \"y+\", \"z\"))",
            "hover3d": "# Create a labeller to show the coordinates of the selected point.\nlabelLocation <- function(x, y = NULL, z = NULL) {\n  xyz <- xyz.coords(x, y, z)\n  function(sel, ...) {\n    p <- with(xyz, matrix(c(x[sel], y[sel], z[sel]), ncol = 3))\n    c(text3d(p, texts = sprintf(\"x:\\%.2f\", p[1]), \n                  adj = c(-0.2, -0.6), ...),\n      text3d(p, texts = sprintf(\"y:\\%.2f\", p[2]),\n                  adj = c(-0.2, 0.5), ...),\n      text3d(p, texts = sprintf(\"z:\\%.2f\", p[3]),\n                  adj = c(-0.2, 1.6), ...))\n  }\n}\n\nxyz <- matrix(rnorm(30), ncol = 3)\nopen3d()\nids <- plot3d(xyz)\nhover3d(xyz, labeller = labelLocation(xyz), col = \"red\", cex = 0.8)\n# The same thing using the data id:\n# hover3d(ids[\"data\"], \n#         labeller = labelLocation(rgl.attrib(ids[\"data\"], \"vertices\")), \n#         col = \"red\", cex = 0.8)",
            "in_pkgdown_example": "in_pkgdown_example()",
            "light": "#\n# a lightsource moving through the scene\n#\ndata(volcano)\nz <- 2 * volcano # Exaggerate the relief\nx <- 10 * (1:nrow(z)) # 10 meter spacing (S to N)\ny <- 10 * (1:ncol(z)) # 10 meter spacing (E to W)\nzlim <- range(z)\nzlen <- zlim[2] - zlim[1] + 1\ncolorlut <- terrain.colors(zlen) # height color lookup table\ncol <- colorlut[ z - zlim[1] + 1 ] # assign colors to heights for each point\n\nopen3d()\nbg3d(\"gray50\")\nsurface3d(x, y, z, color = col, back = \"lines\")\nr <- max(y) - mean(y)\nlightid <- spheres3d(1, 1, 1, alpha = 0)\nframe <- function(time) {\n    a <- pi*(time - 1)\n    save <- par3d(skipRedraw = TRUE)\n    clear3d(type = \"lights\")\n    pop3d(id = lightid)\n    xyz <- matrix(c(r*sin(a) + mean(x), r*cos(a) + mean(y), max(z)), ncol = 3)\n    light3d(x = xyz, diffuse = \"gray75\", \n            specular = \"gray75\", viewpoint.rel = FALSE) \n    light3d(diffuse = \"gray10\", specular = \"gray25\")\n    lightid <<- spheres3d(xyz, emission = \"white\", radius = 4)\n    par3d(save)\n    Sys.sleep(0.02)\n    NULL\n}\nplay3d(frame, duration = 2)",
            "makeDependency": "\\dontrun{\n# This is a slightly simplified version of the code used to \n# produce one of the dependencies for rglwidget().  \n# It writes to the system library copy of rgl so \n# has been marked not to run in the example code.\n\nmakeDependency(\"rglwidgetClass\", \n               src = \"htmlwidgets/lib/rglClass\",\n               script = c(\"rglClass.src.js\",\n                          \"utils.src.js\",\n                          \"buffer.src.js\",\n                          \"subscenes.src.js\",\n                          \"shaders.src.js\",\n                          \"textures.src.js\",\n                          \"projection.src.js\",\n                          \"mouse.src.js\",\n                          \"init.src.js\",\n                          \"pieces.src.js\",\n                          \"draw.src.js\",\n                          \"controls.src.js\",\n                          \"selection.src.js\",\n                          \"rglTimer.src.js\",\n                          \"pretty.src.js\",\n                          \"axes.src.js\",\n                          \"animation.src.js\"),\n               stylesheet = \"rgl.css\",\n               package = \"rgl\",\n               debugging = isTRUE(as.logical(Sys.getenv(\"RGL_DEBUGGING\", \"FALSE\"))))\n}",
            "material": "save <- material3d(\"color\")\nmaterial3d(color = \"red\")\nmaterial3d(\"color\")\nmaterial3d(color = save)\n\n# this illustrates the effect of depth_test\nx <- c(1:3); xmid <- mean(x)\ny <- c(2, 1, 3); ymid <- mean(y)\nz <- 1\nopen3d()\ntests <- c(\"never\", \"less\", \"equal\", \"lequal\", \"greater\", \n                  \"notequal\", \"gequal\", \"always\")\nfor (i in 1:8) {\n  triangles3d(x, y, z + i, col = heat.colors(8)[i])\n  texts3d(xmid, ymid, z + i, paste(i, tests[i], sep = \". \"), depth_test = tests[i]) \n}\nhighlevel()  # To trigger display\n\n# this illustrates additive blending\nopen3d()\nbg3d(\"darkgray\")\nquad <- cbind(c(-1, 1, 1, -1), 1, c(-1, -1, 1, 1))\nquads3d(rbind(translate3d(quad, -0.5, 0, -0.5),\n              translate3d(quad, 0.5,  0.5, -0.5),\n              translate3d(quad, 0, 1, 0.5)), \n        col = rep(c(\"red\", \"green\", \"blue\"), each = 4),\n        alpha = 0.5, \n        blend = c(\"src_alpha\", \"one\"))",
            "matrices": "# A 90 degree rotation about the x axis:\n\nrotationMatrix(pi/2, 1, 0, 0)\n\n# Find what happens when you rotate (2, 0, 0) by 45 degrees about the y axis:\n\nx <- asHomogeneous(c(2, 0, 0))\ny <- x \\%*\\% rotationMatrix(pi/4, 0, 1, 0)\nasEuclidean(y)\n\n# or more simply...\n\nrotate3d(c(2, 0, 0), pi/4, 0, 1, 0)",
            "merge.mesh3d": "open3d()\n# Notice that the alpha setting for the cube is dropped, because\n# the other shapes don't specify alpha.\nshade3d(merge(cube3d(col=\"red\", alpha = 0.5),\n              translate3d(tetrahedron3d(col=\"green\"), 2, 0, 0),\n              translate3d(octahedron3d(col=\"blue\"), 4, 0, 0)))",
            "mergeVertices": "open3d()\n(mesh1 <- cuboctahedron3d(col = rainbow(14), meshColor = \"face\"))\nid <- shade3d(mesh1)\n(mesh2 <- as.mesh3d(id))\nshade3d(translate3d(mesh2, 3, 0, 0))\n(mesh3 <- mergeVertices(mesh2))\nshade3d(translate3d(mesh3, 6, 0, 0))",
            "mesh3d": "# generate a quad mesh object\n\n  vertices <- c( \n     -1.0, -1.0, 0,\n      1.0, -1.0, 0,\n      1.0,  1.0, 0,\n     -1.0,  1.0, 0\n  )\n  indices <- c( 1, 2, 3, 4 )\n  \n  open3d()  \n  wire3d( mesh3d(vertices = vertices, quads = indices) )",
            "mfrow3d": "shapes <- list(Tetrahedron = tetrahedron3d(), Cube = cube3d(), Octahedron = octahedron3d(),\n               Icosahedron = icosahedron3d(), Dodecahedron = dodecahedron3d(),\n               Cuboctahedron = cuboctahedron3d())\ncol <- rainbow(6)\nopen3d()\nmfrow3d(3, 2)\nfor (i in 1:6) {\n  next3d()   # won't advance the first time, since it is empty\n  shade3d(shapes[[i]], col = col[i])\n}\nhighlevel(integer()) # To trigger display as rglwidget\n\nopen3d()\nmat <- matrix(1:4, 2, 2)\nmat <- rbind(mat, mat + 4, mat + 8)\nlayout3d(mat, height = rep(c(3, 1), 3), sharedMouse = TRUE)\nfor (i in 1:6) {\n  next3d()\n  shade3d(shapes[[i]], col = col[i])\n  next3d()\n  text3d(0, 0, 0, names(shapes)[i])\n}\nhighlevel(integer())",
            "observer3d": "example(surface3d)  # The volcano data\nobserver3d(0, 0, 440) # Viewed from very close up",
            "open3d": "r3dDefaults\n    open3d()\n    shade3d(cube3d(color = rainbow(6), meshColor = \"faces\"))\n    cur3d()",
            "par3d": "open3d()\n    shade3d(cube3d(color = rainbow(6), meshColor = \"faces\"))\n    save <- par3d(userMatrix = rotationMatrix(90*pi/180, 1, 0, 0))\n    highlevel()  # To trigger display\n    save\n    par3d(\"userMatrix\")    \n    par3d(save)\n    highlevel()\n    par3d(\"userMatrix\")",
            "par3dinterp": "f <- par3dinterp( zoom = c(1, 2, 3, 1) )\nf(0)\nf(1)\nf(0.5)\n\\dontrun{\nplay3d(f)\n}",
            "par3dinterpControl": "example(plot3d)\nM <- r3dDefaults$userMatrix\nfn <- par3dinterp(times = (0:2)*0.75, userMatrix = list(M,\n                                      rotate3d(M, pi/2, 1, 0, 0),\n                                      rotate3d(M, pi/2, 0, 1, 0)),\n                                      scale = c(0.5, 1, 2))\ncontrol <- par3dinterpControl(fn, 0, 3, steps = 15)\ncontrol      \nif (interactive() || in_pkgdown_example()) \n  rglwidget(width = 500, height = 250) \\%>\\%\n  playwidget(control,\n       step = 0.01, loop = TRUE, rate = 0.5)",
            "pch3d": "open3d()\ni <- 0:25; x <- i \\%\\% 5; y <- rep(0, 26); z <- i \\%/\\% 5\npch3d(x, y, z, pch = i, bg = \"gray\", color = rainbow(26))\ntext3d(x, y, z + 0.3, i)\npch3d(x + 5, y, z, pch = i+65)\ntext3d(x + 5, y, z + 0.3, i+65)",
            "persp3d": "# (1) The Obligatory Mathematical surface.\n#     Rotated sinc function.\n\nx <- seq(-10, 10, length.out = 20)\ny <- x\nf <- function(x, y) { r <- sqrt(x^2 + y^2); 10 * sin(r)/r }\nz <- outer(x, y, f)\nz[is.na(z)] <- 1\nopen3d()\n\n# Draw the surface twice:  the first draws the solid part, \n# the second draws the grid.  Offset the first so it doesn't\n# obscure the lines.\n\npersp3d(x, y, z, aspect = c(1, 1, 0.5), col = \"lightblue\",\n        xlab = \"X\", ylab = \"Y\", zlab = \"Sinc( r )\", \n        polygon_offset = 1)\npersp3d(x, y, z, front = \"lines\", back = \"lines\", \n        lit = FALSE, add = TRUE)\nhighlevel()   # trigger the plot\n\n# (2) Add to existing persp plot:\n\nxE <- c(-10, 10); xy <- expand.grid(xE, xE)\npoints3d(xy[, 1], xy[, 2], 6, col = \"red\")\nlines3d(x, y = 10, z = 6 + sin(x), col = \"green\")\n\nphi <- seq(0, 2*pi, length.out = 201)\nr1 <- 7.725 # radius of 2nd maximum\nxr <- r1 * cos(phi)\nyr <- r1 * sin(phi)\nlines3d(xr, yr, f(xr, yr), col = \"pink\", lwd = 2)\n\n# (3) Visualizing a simple DEM model\n\nz <- 2 * volcano        # Exaggerate the relief\nx <- 10 * (1:nrow(z))   # 10 meter spacing (S to N)\ny <- 10 * (1:ncol(z))   # 10 meter spacing (E to W)\n\nopen3d()\ninvisible(bg3d(\"slategray\")) # suppress display\nmaterial3d(col = \"black\")\npersp3d(x, y, z, col = \"green3\", aspect = \"iso\",\n      axes = FALSE, box = FALSE)\n\n# (4) A globe\n\nlat <- matrix(seq(90, -90, length.out = 50)*pi/180, 50, 50, byrow = TRUE)\nlong <- matrix(seq(-180, 180, length.out = 50)*pi/180, 50, 50)\n\nr <- 6378.1 # radius of Earth in km\nx <- r*cos(lat)*cos(long)\ny <- r*cos(lat)*sin(long)\nz <- r*sin(lat)\n\nopen3d()\npersp3d(x, y, z, col = \"white\", \n       texture = system.file(\"textures/worldsmall.png\", package = \"rgl\"), \n       specular = \"black\", axes = FALSE, box = FALSE, xlab = \"\", ylab = \"\", zlab = \"\",\n       normal_x = x, normal_y = y, normal_z = z)\n\n\\dontrun{\n# This looks much better, but is slow because the texture is very big\npersp3d(x, y, z, col = \"white\", \n       texture = system.file(\"textures/world.png\", package = \"rgl\"), \n       specular = \"black\", axes = FALSE, box = FALSE, xlab = \"\", ylab = \"\", zlab = \"\",\n       normal_x = x, normal_y = y, normal_z = z)\n}",
            "persp3d.deldir": "x <- rnorm(200, sd = 5)\ny <- rnorm(200, sd = 5)\nr <- sqrt(x^2 + y^2)\nz <- 10 * sin(r)/r\ncol <- cm.colors(20)[1 + round(19*(z - min(z))/diff(range(z)))]\n\nsave <- options(rgl.meshColorWarning = FALSE)\n\n# This code is awkward:  to work with demo(rglExamples),\n# we need auto-printing of the plots.  This means we\n# have to repeat the test for deldir.\n\nhaveDeldir <- checkDeldir()\n              \nif (haveDeldir) {\n  dxyz <- deldir::deldir(x, y, z = z, suppressMsge = TRUE)\n  persp3d(dxyz, col = col)\n}\n\nif (haveDeldir) {\n  open3d()\n  # Do it without smoothing and with a different orientation.\n  persp3d(dxyz, col = col, coords = c(\"z\", \"x\", \"y\"), smooth = FALSE)\n}\n\noptions(save)",
            "persp3d.function": "# (1) The Obligatory Mathematical surface.\n#     Rotated sinc function, with colors\n\nf <- function(x, y) { \n  r <- sqrt(x^2 + y^2)\n  ifelse(r == 0, 10, 10 * sin(r)/r)\n}\nopen3d()\nplot3d(f, col = colorRampPalette(c(\"blue\", \"white\", \"red\")), \n       xlab = \"X\", ylab = \"Y\", zlab = \"Sinc( r )\", \n       xlim = c(-10, 10), ylim = c(-10, 10),\n       aspect = c(1, 1, 0.5))\n       \n# (2) A cylindrical plot\n\nf <- function(s, t) {\n  r <- 1 + exp( -pmin( (s - t)^2, \n                       (s - t - 1)^2, \n                       (s - t + 1)^2 )/0.01 )\n  cbind(r*cos(t*2*pi), r*sin(t*2*pi), s)\n}\n\nopen3d()\nplot3d(f, slim = c(0, 1), tlim = c(0, 1), col = \"red\", alpha = 0.8)\n\n# Add a curve to the plot, fixing s at 0.5.\n\nplot3d(f(0.5, seq.int(0, 1, length.out = 100)), type = \"l\", add = TRUE, \n       lwd = 3, depth_test = \"lequal\")",
            "persp3d.tri": "x <- rnorm(200, sd = 5)\ny <- rnorm(200, sd = 5)\nr <- sqrt(x^2 + y^2)\nz <- 10 * sin(r)/r\ncol <- cm.colors(20)[1 + round(19*(z - min(z))/diff(range(z)))]\nsave <- NULL\nif ((haveinterp <- requireNamespace(\"interp\", quietly = TRUE))) {\n  save <- options(rgl.meshColorWarning = FALSE)\n  dxy <- interp::tri.mesh(x, y)\n  open3d()\n  persp3d(dxy, z, col = col, meshColor = \"vertices\")\n}\nif (haveinterp) {\n  open3d()\n  # Do it without smoothing and with a different orientation.\n  persp3d(dxy, z, col = col, coords = c(\"z\", \"x\", \"y\"), smooth = FALSE)\n}\nif (requireNamespace(\"tripack\", quietly = TRUE)) {\n  if (is.null(save))\n    save <- options(rgl.meshColorWarning = FALSE)\n\n  # Leave a circular hole around (3, 0)\n  theta <- seq(0, 2*pi, length.out = 30)[-1]\n  cx <- 2*cos(theta) + 3\n  cy <- 2*sin(theta)\n  keep <- (x - 3)^2 + y^2 > 4\n  dxy2 <- tripack::tri.mesh(x[keep], y[keep])\n  dxy2 <- tripack::add.constraint(dxy2, cx, cy)\n  z <- dxy2$x^2 - dxy2$y^2\n  col <- terrain.colors(20)[1 + round(19*(z - min(z))/diff(range(z)))]\n  open3d()\n  persp3d(dxy2, z, col = col)\n}\noptions(save)",
            "planes": "# Show regression plane with z as dependent variable\n\nopen3d()\nx <- rnorm(100)\ny <- rnorm(100)\nz <- 0.2*x - 0.3*y + rnorm(100, sd = 0.3)\nfit <- lm(z ~ x + y)\nplot3d(x, y, z, type = \"s\", col = \"red\", size = 1)\n\ncoefs <- coef(fit)\na <- coefs[\"x\"]\nb <- coefs[\"y\"]\nc <- -1\nd <- coefs[\"(Intercept)\"]\nplanes3d(a, b, c, d, alpha = 0.5)\n\nopen3d()\nids <- plot3d(x, y, z, type = \"s\", col = \"red\", size = 1, forceClipregion = TRUE) \noldid <- useSubscene3d(ids[\"clipregion\"])\nclipplanes3d(a, b, c, d)\nuseSubscene3d(oldid)",
            "play3d": "open3d()\nplot3d( cube3d(col = \"green\") )\nM <- par3d(\"userMatrix\")\nif (!rgl.useNULL() && interactive())\n  play3d( par3dinterp(times = (0:2)*0.5, userMatrix = list(M,\n                                     rotate3d(M, pi/2, 1, 0, 0),\n                                     rotate3d(M, pi/2, 0, 1, 0) ) ), \n        duration = 2 )\n\\dontrun{\nmovie3d( spin3d(), duration = 5 )\n}",
            "playwidget": "saveopts <- options(rgl.useNULL = TRUE)\n\nobjid <- plot3d(1:10, 1:10, rnorm(10), col=c(\"red\", \"red\"), type = \"s\")[\"data\"]\n\ncontrol <- ageControl(value=0,\n             births=1:10,\n             ages = c(-5,0,5),\n             colors = c(\"green\", \"yellow\", \"red\"),\n             objids = objid)\n\n\\donttest{\n# This example uses explicit names\nrglwidget(elementId = \"theplot\", controllers = \"theplayer\",\n          height = 300, width = 300)\nplaywidget(\"theplot\", control, start = -5, stop = 5,\n           rate = 3, elementId = \"theplayer\",\n           components = c(\"Play\", \"Slider\"))\n}\n\n# This example uses pipes, and can skip the names\n\nwidget <- rglwidget(height = 300, width = 300) \\%>\\%\nplaywidget(control, start = -5, stop = 5,\n           rate = 3, components = c(\"Play\", \"Slider\"))\nif (interactive() || in_pkgdown_example())\n  widget\n\noptions(saveopts)",
            "plot3d": "open3d()\n  x <- sort(rnorm(1000))\n  y <- rnorm(1000)\n  z <- rnorm(1000) + atan2(x, y)\n  plot3d(x, y, z, col = rainbow(1000))",
            "plot3d.formula": "open3d()\nmfrow3d(1, 2, sharedMouse = TRUE)\nplot3d(mpg ~ wt + qsec, data = mtcars)\nif (checkDeldir())\n  persp3d(mpg ~ wt + qsec, data = mtcars)",
            "plot3d.lm": "open3d()\nids <- plot3d(lm(mpg ~ wt + qsec, data = mtcars), which = 1:3)\nnames(ids)\n\nopen3d()\nplot3d(lm(mpg ~ wt + I(wt^2) + qsec, data = mtcars))\n\nopen3d()\n# Specify vars in the order:  response, pred1, pred2.\nplot3d(lm(mpg ~ poly(wt, 3) + qsec, data = mtcars), \n       vars = mtcars[,c(\"mpg\", \"wt\", \"qsec\")])\n       \nopen3d()\n# Clip parts of the plot with few (wt, qsec) points\nplot3d(lm(mpg ~ poly(wt, 3) + qsec, data = mtcars), \n       vars = mtcars[,c(\"mpg\", \"wt\", \"qsec\")],\n       clip_to_density = 0.1)",
            "plotmath3d": "open3d()\nplotmath3d(1:3, 1:3, 1:3, expression(x[1] == 1, x[2] == 2, x[3] == 3))\n# This lets the text resize with the plot\ntext3d(4, 4, 4, \"resizeable text\", usePlotmath = TRUE, fixedSize = FALSE)",
            "polygon3d": "theta <- seq(0, 4*pi, length.out = 50)\nr <- theta + 1\nr <- c(r[-50], rev(theta*0.8) + 1)\ntheta <- c(theta[-50], rev(theta))\nx <- r*cos(theta)\ny <- r*sin(theta)\nopen3d()\nplot(x, y, type = \"n\")\npolygon(x, y)\npolygon3d(x, y, x + y, col = \"blue\")",
            "postscript": "# Create new files in tempdir\nsavedir <- setwd(tempdir())\n\nx <- y <- seq(-10, 10, length.out = 20)\nz <- outer(x, y, function(x, y) x^2 + y^2)\npersp3d(x, y, z, col = 'lightblue')\n\ntitle3d(\"Using LaTeX text\", col = 'red', line = 3)\nrgl.postscript(\"persp3da.ps\", \"ps\", drawText = FALSE)\nrgl.postscript(\"persp3da.pdf\", \"pdf\", drawText = FALSE)\nrgl.postscript(\"persp3da.tex\", \"tex\")\npop3d()\ntitle3d(\"Using ps/pdf text\", col = 'red', line = 3)\nrgl.postscript(\"persp3db.ps\", \"ps\")\nrgl.postscript(\"persp3db.pdf\", \"pdf\")\nrgl.postscript(\"persp3db.tex\", \"tex\", drawText = FALSE)\n\nsetwd(savedir)\n\n\\dontrun{\n\n#\n# create a series of frames for an animation\n#\n\nopen3d()\nshade3d(oh3d(), color = \"red\")\nview3d(0, 20)\n\nfor (i in 1:45) {\n  view3d(i, 20)\n  filename <- paste(\"pic\", formatC(i, digits = 1, flag = \"0\"), \".eps\", sep = \"\") \n  rgl.postscript(filename, fmt = \"eps\")\n}\n\n}",
            "primitives": "# Show 12 random vertices in various ways. \n\nM <- matrix(rnorm(36), 3, 12, dimnames = list(c('x', 'y', 'z'), \n                                       rep(LETTERS[1:4], 3)))\n\n# Force 4-tuples to be convex in planes so that quads3d works.\n\nfor (i in c(1, 5, 9)) {\n    quad <- as.data.frame(M[, i + 0:3])\n    coeffs <- runif(2, 0, 3)\n    if (mean(coeffs) < 1) coeffs <- coeffs + 1 - mean(coeffs)\n    quad$C <- with(quad, coeffs[1]*(B - A) + coeffs[2]*(D - A) + A)\n    M[, i + 0:3] <- as.matrix(quad)\n}\n\nopen3d()\n\n# Rows of M are x, y, z coords; transpose to plot\n\nM <- t(M)\nshift <- matrix(c(-3, 3, 0), 12, 3, byrow = TRUE)\n\npoints3d(M)\nlines3d(M + shift)\nsegments3d(M + 2*shift)\ntriangles3d(M + 3*shift, col = 'red')\nquads3d(M + 4*shift, col = 'green')  \ntext3d(M + 5*shift, texts = 1:12)\n\n# Add labels\n\nshift <- outer(0:5, shift[1, ])\nshift[, 1] <- shift[, 1] + 3\ntext3d(shift, \n       texts = c('points3d', 'lines3d', 'segments3d',\n         'triangles3d', 'quads3d', 'text3d'),\n       adj = 0)\n rgl.bringtotop()",
            "r3d": "x <- c(0, 1, 0, 0)\n     y <- c(0, 0, 1, 0)\n     z <- c(0, 0, 0, 1)\n     labels <- c(\"Origin\", \"X\", \"Y\", \"Z\")\n     i <- c(1, 2, 1, 3, 1, 4)\n\n     # *3d interface\n     \n     open3d()\n     text3d(x, y, z, labels)\n     text3d(1, 1, 1, \"*3d coordinates\")\n     segments3d(x[i], y[i], z[i])",
            "readSTL": "filename <- tempfile(fileext = \".stl\")\nopen3d()\nshade3d( icosahedron3d(col = \"magenta\") )\nwriteSTL(filename)\nopen3d()\nreadSTL(filename, col = \"red\")",
            "rgl-package": "if (!in_pkgdown_example())\n  file.show(system.file(\"NEWS\", package = \"rgl\"))\nexample(surface3d)\nexample(plot3d)",
            "rgl.attrib.info": "open3d()\nid <- points3d(rnorm(100), rnorm(100), rnorm(100), col = \"green\")\nrgl.attrib.info(id, showAll = TRUE)\nrgl.attrib.count(id, \"vertices\")\n\nmerge(rgl.attrib.info(), ids3d(\"all\"))",
            "rgl.bringtotop": "open3d()\npoints3d(rnorm(1000), rnorm(1000), rnorm(1000), color = heat.colors(1000))\nrgl.bringtotop(stay = TRUE)",
            "rgl.incrementID": "# Get the current ID value\nrgl.incrementID(0)\n\n# Increment it\nrgl.incrementID()",
            "rgl.pixels": "example(surface3d)\ndepth <- rgl.pixels(component = \"depth\")\nif (length(depth) && is.matrix(depth)) # Protect against empty or single pixel windows\n    contour(depth)",
            "rgl.useNULL": "rgl.useNULL()",
            "rgl.user2window": "open3d()\npoints3d(rnorm(100), rnorm(100), rnorm(100))\nif (interactive() || !.Platform$OS == \"unix\") {\n# Calculate a square in the middle of the display and plot it\nsquare <- rgl.window2user(c(0.25, 0.25, 0.75, 0.75, 0.25), \n                          c(0.25, 0.75, 0.75, 0.25, 0.25), 0.5)\npar3d(ignoreExtent = TRUE)\nlines3d(square)\npar3d(ignoreExtent = FALSE)\n}",
            "rglExtrafonts": "if (requireNamespace(\"extrafont\") && !in_pkgdown_example()) {\n  \n  open3d()\n  text3d(1,1,1, \"Default\", family = \"sans\", cex = 2)  \n  \n  # Attempt to register new sans-serif font:\n  newfamily <- rglExtrafonts(newsans = c(\"Comic Sans MS\", \"Impact\", \n                                         \"Verdana\", \"Tahoma\"))\n  \n  text3d(2,2,2, newfamily, family = \"newsans\", cex = 2)\n  \n}",
            "rglFonts": "\\dontrun{\n# These FreeType fonts are available from the Amaya project, and are not shipped\n# with rgl.  You would normally install them to the rgl/fonts directory\n# and use fully qualified pathnames, e.g. \n# system.file(\"fonts/FreeSerif.ttf\", package = \"rgl\")\n\nrglFonts(serif = c(\"FreeSerif.ttf\", \"FreeSerifBold.ttf\", \"FreeSerifItalic.ttf\",\n                 \"FreeSerifBoldItalic.ttf\"),\n         sans  = c(\"FreeSans.ttf\", \"FreeSansBold.ttf\", \"FreeSansOblique.ttf\",\n                 \"FreeSansBoldOblique.ttf\"),\n         mono  = c(\"FreeMono.ttf\", \"FreeMonoBold.ttf\", \"FreeMonoOblique.ttf\",\n                 \"FreeMonoBoldOblique.ttf\"),\n         symbol= c(\"ESSTIX10.TTF\", \"ESSTIX12.TTF\", \"ESSTIX9_.TTF\", \n                 \"ESSTIX11.TTF\"))\n}",
            "rglIds": "x <- matrix(rnorm(30), ncol = 3, dimnames = list(NULL, c(\"x\", \"y\", \"z\")))\np <- plot3d(x, type = \"s\")\nstr(p)\nif (interactive() || in_pkgdown_example())\n  print(p, rglwidget = TRUE)",
            "rglMouse": "if (interactive() || in_pkgdown_example()) {\n  open3d()\n  xyz <- matrix(rnorm(300), ncol = 3)\n  id <- plot3d(xyz, col = \"red\", type = \"s\")[\"data\"]\n  par3d(mouseMode = \"selecting\")\n  share <- rglShared(id)\n\n# This puts the selector below the widget.\n  rglwidget(shared = share, width = 300, height = 300) \\%>\\% rglMouse()\n  \n# This puts the selector above the widget.\n  rglMouse() \\%>\\% rglwidget(shared = share, width = 300, height = 300, controllers = .) \n}",
            "rglShared": "save <- options(rgl.useNULL = TRUE)\n  \n  #  rglShared requires the crosstalk package,\n  #  and the slider and rglMouse require manipulateWidget\n  \n  if (requireNamespace(\"crosstalk\", quietly = TRUE) &&\n      requireNamespace(\"manipulateWidget\", quietly = TRUE)) {\n    open3d()\n    x <- sort(rnorm(100))\n    y <- rnorm(100)\n    z <- rnorm(100) + atan2(x, y)\n    ids <- plot3d(x, y, z, col = rainbow(100))\n\n    # The data will be selected and filtered, not the axes.\n    sharedData <- rglShared(ids[\"data\"])\n  \n    # Also add some labels that are only displayed\n    # when points are selected\n  \n    sharedLabel <- rglShared(text3d(x, y, z, text = 1:100,\n                                    adj = -0.5),\n                             group = sharedData$groupName(),\n                             deselectedFade = 0,\n                             selectedIgnoreNone = FALSE) \n    if (interactive() || in_pkgdown_example()) \n      crosstalk::filter_slider(\"x\", \"x\", sharedData, ~x) \\%>\\%\n      rglwidget(shared = list(sharedData, sharedLabel), controller = .) \\%>\\% \n      rglMouse()\n  }    \n  options(save)",
            "rglToLattice": "persp3d(volcano, col = \"green\")\nif ((hasorientlib <- requireNamespace(\"orientlib\", quietly = TRUE)) && \n    requireNamespace(\"lattice\", quietly = TRUE)) \n    lattice::wireframe(volcano, screen = rglToLattice())\nif (hasorientlib) {\n  angles <- rglToBase()\n  persp(volcano, col = \"green\", border = NA, shade = 0.5,\n        theta = angles$theta, phi = angles$phi)\n}",
            "rglwidget": "save <- options(rgl.useNULL=TRUE)\nexample(\"plot3d\", \"rgl\")\nwidget <- rglwidget()\nif (interactive() || in_pkgdown_example())\n  widget\n  \n\\donttest{\nif (interactive() && !in_pkgdown_example()) {\n  # Save it to a file.  This requires pandoc\n  filename <- tempfile(fileext = \".html\")\n  htmlwidgets::saveWidget(rglwidget(), filename)\n  browseURL(filename)\n}\n}\n\noptions(save)",
            "safe.dev.off": "# Open a graphics device\ndev.new()\nfirst <- dev.cur()\n\n# Open a second graphics device\ndev.new()\nsecond <- dev.cur()\nsecond\n\n# Open another one, and close it using dev.off()\ndev.new()\ndev.off()\ndev.cur() == second # Not the same as second!\n\n# Try again with safe.dev.off()\ndev.set(second)\ndev.new()\nsafe.dev.off()\ndev.cur() == second\n\n# Close the other two devs\nsafe.dev.off()\nsafe.dev.off()"
        }
    },
    "tinytex": {
        "description": "Helper functions to install and maintain the 'LaTeX' distribution\n  named 'TinyTeX' (<https://yihui.org/tinytex/>), a lightweight, cross-platform,\n  portable, and easy-to-maintain version of 'TeX Live'. This package also\n  contains helper functions to compile 'LaTeX' documents, and install missing\n  'LaTeX' packages automatically.",
        "examples": {
            "check_installed": "\\dontshow{if (interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\ntinytex::check_installed('framed')\n\\dontshow{\\}) # examplesIf}",
            "is_tinytex": "tinytex::is_tinytex()",
            "r_texmf": "# running the code below will modify your texmf tree; please do not run\n# unless you know what it means\n\n# r_texmf('remove')\n# r_texmf('add')\n\n# all files under R's texmf tree\nlist.files(file.path(R.home('share'), 'texmf'), recursive = TRUE, full.names = TRUE)",
            "tlmgr": "\\dontshow{if (interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# search for a package that contains titling.sty\ntlmgr_search('titling.sty')\n\n# to match titling.sty exactly, add a slash before the keyword, e.g.\ntlmgr_search('/titling.sty')\n\n# use a regular expression if you want to be more precise, e.g.\ntlmgr_search('/titling\\\\\\\\.sty$')\n\n# list all installed LaTeX packages\ntlmgr(c('info', '--list', '--only-installed', '--data', 'name'))\n\\dontshow{\\}) # examplesIf}"
        }
    },
    "cpp11": {
        "description": "Provides a header only, C++11 interface to R's C\n    interface.  Compared to other approaches 'cpp11' strives to be safe\n    against long jumps from the C API as well as C++ exceptions, conform\n    to normal R function semantics and supports interaction with 'ALTREP'\n    vectors.",
        "examples": {
            "cpp_register": "# create a minimal package\ndir <- tempfile()\ndir.create(dir)\n\nwriteLines(\"Package: testPkg\", file.path(dir, \"DESCRIPTION\"))\nwriteLines(\"useDynLib(testPkg, .registration = TRUE)\", file.path(dir, \"NAMESPACE\"))\n\n# create a C++ file with a decorated function\ndir.create(file.path(dir, \"src\"))\nwriteLines(\"[[cpp11::register]] int one() { return 1; }\", file.path(dir, \"src\", \"one.cpp\"))\n\n# register the functions in the package\ncpp_register(dir)\n\n# Files generated by registration\nfile.exists(file.path(dir, \"R\", \"cpp11.R\"))\nfile.exists(file.path(dir, \"src\", \"cpp11.cpp\"))\n\n# cleanup\nunlink(dir, recursive = TRUE)",
            "cpp_source": "cpp_source(\n  code = '#include \"cpp11/integers.hpp\"\n\n  [[cpp11::register]]\n  int num_odd(cpp11::integers x) {\n    int total = 0;\n    for (int val : x) {\n      if ((val \\% 2) == 1) {\n        ++total;\n      }\n    }\n    return total;\n  }\n  ')\n\nnum_odd(as.integer(c(1:10, 15, 23)))\n\nif (interactive() && require(\"progress\")) {\n\ncpp_source(\n  code = '\n#include <cpp11/R.hpp>\n#include <RProgress.h>\n\n[[cpp11::linking_to(\"progress\")]]\n\n[[cpp11::register]] void\nshow_progress() {\n  RProgress::RProgress pb(\"Processing [:bar] ETA: :eta\");\n\n  pb.tick(0);\n  for (int i = 0; i < 100; i++) {\n    usleep(2.0 / 100 * 1000000);\n    pb.tick();\n  }\n}\n')\n\nshow_progress()\n}",
            "cpp_vendor": "# create a new directory\ndir <- tempfile()\ndir.create(dir)\n\n# vendor the cpp11 headers into the directory\ncpp_vendor(dir)\n\nlist.files(file.path(dir, \"inst\", \"include\", \"cpp11\"))\n\n# cleanup\nunlink(dir, recursive = TRUE)"
        }
    },
    "rsconnect": {
        "description": "Programmatic deployment interface for 'RPubs',\n    'shinyapps.io', and 'Posit Connect'. Supported content types include R\n    Markdown documents, Shiny applications, Plumber APIs, plots, and\n    static web content.",
        "examples": {
            "addLinter": "addLinter(\"no.capitals\", linter(\n\n  ## Identify lines containing capital letters -- either by name or by index\n  apply = function(content, ...) {\n    grep(\"[A-Z]\", content)\n  },\n\n  ## Only use this linter on R files (paths ending with .r or .R)\n  takes = function(paths) {\n    grep(\"[rR]$\", paths)\n  },\n\n  # Use the default message constructor\n  message = function(content, lines, ...) {\n    makeLinterMessage(\"Capital letters found on the following lines\", content, lines)\n  },\n\n  # Give a suggested prescription\n  suggest = \"Do not use capital letters in these documents.\"\n))\naddLinter(\"no.capitals\", linter(\n\n  ## Identify lines containing capital letters -- either by name or by index\n  apply = function(content, ...) {\n    grep(\"[A-Z]\", content)\n  },\n\n  ## Only use this linter on R files (paths ending with .r or .R)\n  takes = function(paths) {\n    grep(\"[rR]$\", paths)\n  },\n\n  # Use the default message constructor\n  message = function(content, lines, ...) {\n    makeLinterMessage(\"Capital letters found on the following lines\", content, lines)\n  },\n\n  # Give a suggested prescription\n  suggest = \"Do not use capital letters in these documents.\"\n))",
            "addServer": "\\dontrun{\n# register a local server\naddServer(\"http://myrsconnect/\", \"myserver\")\n\n# list servers\nservers(local = TRUE)\n\n# connect to an account on the server\nconnectUser(server = \"myserver\")\n}",
            "appDependencies": "\\dontrun{\n\n# dependencies for the app in the current working dir\nappDependencies()\n\n# dependencies for an app in another directory\nappDependencies(\"~/projects/shiny/app1\")\n}",
            "applications": "\\dontrun{\n\n# list all applications for the default account\napplications()\n\n# list all applications for a specific account\napplications(\"myaccount\")\n\n# view the list of applications in the data viewer\nView(applications())\n}",
            "configureApp": "\\dontrun{\n\n# set instance size for an application\nconfigureApp(\"myapp\", size=\"xlarge\")\n}",
            "deployApp": "\\dontrun{\n\n# deploy the application in the current working dir\ndeployApp()\n\n# deploy an application in another directory\ndeployApp(\"~/projects/shiny/app1\")\n\n# deploy using an alternative application name and title\ndeployApp(\"~/projects/shiny/app1\", appName = \"myapp\",\n          appTitle = \"My Application\")\n\n# deploy specifying an explicit account name, then\n# redeploy with no arguments (will automatically use\n# the previously specified account)\ndeployApp(account = \"jsmith\")\ndeployApp()\n\n# deploy but don't launch a browser when completed\ndeployApp(launch.browser = FALSE)\n\n# deploy a Quarto website, using the quarto package to\n# find the Quarto binary\ndeployApp(\"~/projects/quarto/site1\")\n\n# deploy application with environment variables\n# (e.g., `SECRET_PASSWORD=XYZ` is set via an ~/.Renviron file)\nrsconnect::deployApp(envVars = c(\"SECRET_PASSWORD\"))\n}",
            "deployDoc": "\\dontrun{\ndeployDoc(\"my-report.Rmd\")\ndeployDoc(\"static-file.html\")\n}",
            "deployments": "\\dontrun{\n\n# Return all deployments of the ~/r/myapp directory made with the 'abc'\n# account\ndeployments(\"~/r/myapp\", accountFilter=\"abc\")\n}",
            "generateAppName": "\\dontrun{\n# Generate a short name for a sample application\ngenerateAppName(\"My Father's Country\", \"~/fathers-country\", \"myacct\")\n}",
            "linter": "addLinter(\"no.capitals\", linter(\n\n  ## Identify lines containing capital letters -- either by name or by index\n  apply = function(content, ...) {\n    grep(\"[A-Z]\", content)\n  },\n\n  ## Only use this linter on R files (paths ending with .r or .R)\n  takes = function(paths) {\n    grep(\"[rR]$\", paths)\n  },\n\n  # Use the default message constructor\n  message = function(content, lines, ...) {\n    makeLinterMessage(\"Capital letters found on the following lines\", content, lines)\n  },\n\n  # Give a suggested prescription\n  suggest = \"Do not use capital letters in these documents.\"\n))\naddLinter(\"no.capitals\", linter(\n\n  ## Identify lines containing capital letters -- either by name or by index\n  apply = function(content, ...) {\n    grep(\"[A-Z]\", content)\n  },\n\n  ## Only use this linter on R files (paths ending with .r or .R)\n  takes = function(paths) {\n    grep(\"[rR]$\", paths)\n  },\n\n  # Use the default message constructor\n  message = function(content, lines, ...) {\n    makeLinterMessage(\"Capital letters found on the following lines\", content, lines)\n  },\n\n  # Give a suggested prescription\n  suggest = \"Do not use capital letters in these documents.\"\n))",
            "options": "\\dontrun{\n\n# use curl for http connections\noptions(rsconnect.http = \"curl\")\n\n# trace http requests\noptions(rsconnect.http.trace = TRUE)\n\n# print verbose output for http requests\noptions(rsconnect.http.verbose = TRUE)\n\n# print JSON content\noptions(rsconnect.http.trace.json = TRUE)\n\n# don't automatically launch a browser after deployment\noptions(rsconnect.launch.browser = FALSE)\n}",
            "purgeApp": "\\dontrun{\n\n# purge an application\npurgeApp(\"myapp\")\n}",
            "restartApp": "\\dontrun{\n\n# restart an application\nrestartApp(\"myapp\")\n}",
            "rpubsUpload": "\\dontrun{\n# upload a document\nresult <- rpubsUpload(\"My document title\", \"Document.html\")\nif (!is.null(result$continueUrl))\n   browseURL(result$continueUrl)\nelse\n   stop(result$error)\n\n# update the same document with a new title\nupdateResult <- rpubsUpload(\"My updated title\", \"Document.html\",\n                            id = result$id)\n}",
            "servers": "# List all registered servers\nservers()\n\n# Get information about a server\nserverInfo(\"posit.cloud\")",
            "setAccountInfo": "\\dontrun{\n\n# register an account\nsetAccountInfo(\"user\", \"token\", \"secret\")\n\n# remove the same account\nremoveAccount(\"user\")\n}",
            "setProperty": "\\dontrun{\n\n# set instance size for an application\nsetProperty(\"application.instances.count\", 1)\n\n# disable application package cache\nsetProperty(\"application.package.cache\", FALSE)\n\n}",
            "taskLog": "\\dontrun{\n\n# write task log to stdout\ntaskLog(12345)\n\n# write task log to stderr\ntaskLog(12345, output=\"stderr\")\n\n}",
            "tasks": "\\dontrun{\n\n# list tasks for the default account\ntasks()\n\n}",
            "terminateApp": "\\dontrun{\n\n# terminate an application\nterminateApp(\"myapp\")\n}",
            "unsetProperty": "\\dontrun{\n\n# unset application package cache property to revert to default\nunsetProperty(\"application.package.cache\")\n\n}"
        }
    },
    "ps": {
        "description": "List, query and manipulate all system processes, on\n    'Windows', 'Linux' and 'macOS'.",
        "examples": {
            "errno": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nerrno()\n\\dontshow{\\}) # examplesIf}",
            "ps_apps": "\\dontshow{if (ps_is_supported() && ps_os_type()[[\"MACOS\"]] && !ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_apps()\n\\dontshow{\\}) # examplesIf}",
            "ps_children": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_parent(ps_handle())\nps_children(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_cmdline": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_name(p)\nps_exe(p)\nps_cmdline(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_connections": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\nps_connections(p)\nsc <- socketConnection(\"httpbin.org\", port = 80)\nps_connections(p)\nclose(sc)\nps_connections(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_cpu_count": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_cpu_count(logical = TRUE)\nps_cpu_count(logical = FALSE)\n\\dontshow{\\}) # examplesIf}",
            "ps_cpu_times": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_cpu_times(p)\nproc.time()\n\\dontshow{\\}) # examplesIf}",
            "ps_create_time": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_create_time(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_cwd": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_cwd(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_descent": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_descent()\n\\dontshow{\\}) # examplesIf}",
            "ps_disk_io_counters": "\\dontshow{if (ps::ps_is_supported() && ps:::ps_os_name() \\%in\\% c(\"LINUX\", \"WINDOWS\") && !ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_disk_io_counters()\n\\dontshow{\\}) # examplesIf}",
            "ps_disk_partitions": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_disk_partitions(all = TRUE)\nps_disk_partitions()\n\\dontshow{\\}) # examplesIf}",
            "ps_disk_usage": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_disk_usage()\n\\dontshow{\\}) # examplesIf}",
            "ps_environ": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nenv <- ps_environ(p)\nenv[[\"R_HOME\"]]\n\\dontshow{\\}) # examplesIf}",
            "ps_exe": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_name(p)\nps_exe(p)\nps_cmdline(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_fs_info": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_fs_info(c(\"/\", \"~\", \".\"))\n\\dontshow{\\}) # examplesIf}",
            "ps_fs_mount_point": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_fs_mount_point(\".\")\n\\dontshow{\\}) # examplesIf}",
            "ps_fs_stat": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check() && ps_os_type()[[\"POSIX\"]]) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_fs_stat(c(\".\", tempdir()))\n\\dontshow{\\}) # examplesIf}",
            "ps_get_cpu_affinity": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check() && ! ps::ps_os_type()[[\"MACOS\"]]) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# current\norig <- ps_get_cpu_affinity()\norig\n\n# restrict\nps_set_cpu_affinity(affinity = 0:0)\nps_get_cpu_affinity()\n\n# restore\nps_set_cpu_affinity(affinity = orig)\nps_get_cpu_affinity()\n\\dontshow{\\}) # examplesIf}",
            "ps_handle": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\n\\dontshow{\\}) # examplesIf}",
            "ps_is_running": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_is_running(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_kill": "\\dontshow{if (ps::ps_is_supported() && ps::ps_os_type()[\"POSIX\"] && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\npx <- processx::process$new(\"sleep\", \"10\")\np <- ps_handle(px$get_pid())\np\nps_kill(p)\np\nps_is_running(p)\npx$get_exit_status()\n\\dontshow{\\}) # examplesIf}",
            "ps_loadavg": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_loadavg()\n\\dontshow{\\}) # examplesIf}",
            "ps_memory_info": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_memory_info(p)\nps_memory_full_info(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_name": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_name(p)\nps_exe(p)\nps_cmdline(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_num_fds": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\nps_num_fds(p)\nf <- file(tmp <- tempfile(), \"w\")\nps_num_fds(p)\nclose(f)\nunlink(tmp)\nps_num_fds(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_num_threads": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_num_threads(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_open_files": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\nps_open_files(p)\nf <- file(tmp <- tempfile(), \"w\")\nps_open_files(p)\nclose(f)\nunlink(tmp)\nps_open_files(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_os_type": "ps_os_type()\nps_is_supported()",
            "ps_pid": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_pid(p)\nps_pid(p) == Sys.getpid()\n\\dontshow{\\}) # examplesIf}",
            "ps_ppid": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_ppid(p)\nps_parent(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_resume": "\\dontshow{if (ps::ps_is_supported() && ps::ps_os_type()[\"POSIX\"] && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\npx <- processx::process$new(\"sleep\", \"10\")\np <- ps_handle(px$get_pid())\np\nps_suspend(p)\nps_status(p)\nps_resume(p)\nps_status(p)\nps_kill(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_send_signal": "\\dontshow{if (ps::ps_is_supported() && ps::ps_os_type()[\"POSIX\"] && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\npx <- processx::process$new(\"sleep\", \"10\")\np <- ps_handle(px$get_pid())\np\nps_send_signal(p, signals()$SIGINT)\np\nps_is_running(p)\npx$get_exit_status()\n\\dontshow{\\}) # examplesIf}",
            "ps_shared_lib_users": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check() && ps::ps_os_type()[[\"WINDOWS\"]]) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\ndlls <- vapply(getLoadedDLLs(), \"[[\", character(1), \"path\")\npsdll <- dlls[[\"ps\"]][[1]]\nr_procs <- c(\"Rgui.exe\", \"Rterm.exe\", \"rsession.exe\")\nps_shared_lib_users(psdll, filter = r_procs)\n\\dontshow{\\}) # examplesIf}",
            "ps_shared_libs": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check() && ps::ps_os_type()[[\"WINDOWS\"]]) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# The loaded DLLs of the current process\nps_shared_libs()\n\\dontshow{\\}) # examplesIf}",
            "ps_status": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_status(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_suspend": "\\dontshow{if (ps::ps_is_supported() && ps::ps_os_type()[\"POSIX\"] && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\npx <- processx::process$new(\"sleep\", \"10\")\np <- ps_handle(px$get_pid())\np\nps_suspend(p)\nps_status(p)\nps_resume(p)\nps_status(p)\nps_kill(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_system_cpu_times": "\\dontshow{if (ps::ps_is_supported()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_system_cpu_times()\n\\dontshow{\\}) # examplesIf}",
            "ps_system_memory": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_system_memory()\n\\dontshow{\\}) # examplesIf}",
            "ps_system_swap": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nps_system_swap()\n\\dontshow{\\}) # examplesIf}",
            "ps_terminal": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_terminal(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_terminate": "\\dontshow{if (ps::ps_is_supported() && ps::ps_os_type()[\"POSIX\"] && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\npx <- processx::process$new(\"sleep\", \"10\")\np <- ps_handle(px$get_pid())\np\nps_terminate(p)\np\nps_is_running(p)\npx$get_exit_status()\n\\dontshow{\\}) # examplesIf}",
            "ps_tty_size": "# An example that falls back to the 'width' option\ntryCatch(\n  ps_tty_size(),\n  ps_unknown_tty_size = function(err) {\n    c(width = getOption(\"width\"), height = NA_integer_)\n  }\n)",
            "ps_uids": "\\dontshow{if (ps::ps_is_supported() && ps::ps_os_type()[\"POSIX\"] && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_uids(p)\nps_gids(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_username": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\np <- ps_handle()\np\nps_username(p)\n\\dontshow{\\}) # examplesIf}",
            "ps_wait": "\\dontshow{if (ps::ps_is_supported() && ! ps:::is_cran_check() && ps::ps_os_type()[\"POSIX\"]) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# this example calls `sleep`, so it only works on Unix\np1 <- processx::process$new(\"sleep\", \"100\")\np2 <- processx::process$new(\"sleep\", \"100\")\n\n# returns c(FALSE, FALSE) immediately if p1 and p2 are running\nps_wait(list(p1$as_ps_handle(), p2$as_ps_handle()), 0)\n\n# timeouts at one second\nps_wait(list(p1$as_ps_handle(), p2$as_ps_handle()), 1000)\n\np1$kill()\np2$kill()\n# returns c(TRUE, TRUE) immediately\nps_wait(list(p1$as_ps_handle(), p2$as_ps_handle()), 1000)\n\\dontshow{\\}) # examplesIf}"
        }
    },
    "broom": {
        "description": "Summarizes key information about statistical\n    objects in tidy tibbles. This makes it easy to report results, create\n    plots and consistently work with large numbers of models at once.\n    Broom provides three verbs that each provide different types of\n    information about a model. tidy() summarizes information about model\n    components such as coefficients of a regression. glance() reports\n    information about an entire model, such as goodness of fit measures\n    like AIC and BIC. augment() adds information about individual\n    observations to a dataset, such as fitted values or influence\n    measures.",
        "examples": {
            "augment.Mclust": "\\dontshow{if (rlang::is_installed(\"mclust\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load library for models and data\nlibrary(mclust)\n\n# load data manipulation libraries\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(tidyr)\n\nset.seed(27)\n\ncenters <- tibble(\n  cluster = factor(1:3),\n  # number points in each cluster\n  num_points = c(100, 150, 50),\n  # x1 coordinate of cluster center\n  x1 = c(5, 0, -3),\n  # x2 coordinate of cluster center\n  x2 = c(-1, 1, -2)\n)\n\npoints <- centers \\%>\\%\n  mutate(\n    x1 = map2(num_points, x1, rnorm),\n    x2 = map2(num_points, x2, rnorm)\n  ) \\%>\\%\n  select(-num_points, -cluster) \\%>\\%\n  unnest(c(x1, x2))\n\n# fit model\nm <- Mclust(points)\n\n# summarize model fit with tidiers\ntidy(m)\naugment(m, points)\nglance(m)\n\\dontshow{\\}) # examplesIf}",
            "augment.betamfx": "\\dontshow{if (rlang::is_installed(\"mfx\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(mfx)\n\n# Simulate some data\nset.seed(12345)\nn <- 1000\nx <- rnorm(n)\n\n# Beta outcome\ny <- rbeta(n, shape1 = plogis(1 + 0.5 * x), shape2 = (abs(0.2 * x)))\n# Use Smithson and Verkuilen correction\ny <- (y * (n - 1) + 0.5) / n\n\nd <- data.frame(y, x)\nmod_betamfx <- betamfx(y ~ x | x, data = d)\n\ntidy(mod_betamfx, conf.int = TRUE)\n\n# Compare with the naive model coefficients of the equivalent betareg call (not run)\n# tidy(betamfx(y ~ x | x, data = d), conf.int = TRUE)\n\naugment(mod_betamfx)\nglance(mod_betamfx)\n\\dontshow{\\}) # examplesIf}",
            "augment.betareg": "\\dontshow{if (rlang::is_installed(\"betareg\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(betareg)\n\n# load dats\ndata(\"GasolineYield\", package = \"betareg\")\n\n# fit model\nmod <- betareg(yield ~ batch + temp, data = GasolineYield)\n\nmod\n\n# summarize model fit with tidiers\ntidy(mod)\ntidy(mod, conf.int = TRUE)\ntidy(mod, conf.int = TRUE, conf.level = .99)\n\naugment(mod)\n\nglance(mod)\n\\dontshow{\\}) # examplesIf}",
            "augment.clm": "\\dontshow{if (rlang::is_installed(\"ordinal\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(ordinal)\n\n# fit model\nfit <- clm(rating ~ temp * contact, data = wine)\n\n# summarize model fit with tidiers\ntidy(fit)\ntidy(fit, conf.int = TRUE, conf.level = 0.9)\ntidy(fit, conf.int = TRUE, conf.type = \"Wald\", exponentiate = TRUE)\n\nglance(fit)\naugment(fit, type.predict = \"prob\")\naugment(fit, type.predict = \"class\")\n\n# ...and again with another model specification\nfit2 <- clm(rating ~ temp, nominal = ~contact, data = wine)\n\ntidy(fit2)\nglance(fit2)\n\\dontshow{\\}) # examplesIf}",
            "augment.coxph": "\\dontshow{if (rlang::is_installed(c(\"survival\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(survival)\n\n# fit model\ncfit <- coxph(Surv(time, status) ~ age + sex, lung)\n\n# summarize model fit with tidiers\ntidy(cfit)\ntidy(cfit, exponentiate = TRUE)\n\nlp <- augment(cfit, lung)\nrisks <- augment(cfit, lung, type.predict = \"risk\")\nexpected <- augment(cfit, lung, type.predict = \"expected\")\n\nglance(cfit)\n\n# also works on clogit models\nresp <- levels(logan$occupation)\nn <- nrow(logan)\nindx <- rep(1:n, length(resp))\nlogan2 <- data.frame(\n  logan[indx, ],\n  id = indx,\n  tocc = factor(rep(resp, each = n))\n)\n\nlogan2$case <- (logan2$occupation == logan2$tocc)\n\ncl <- clogit(case ~ tocc + tocc:education + strata(id), logan2)\n\ntidy(cl)\nglance(cl)\n\nlibrary(ggplot2)\n\nggplot(lp, aes(age, .fitted, color = sex)) +\n  geom_point()\n\nggplot(risks, aes(age, .fitted, color = sex)) +\n  geom_point()\n\nggplot(expected, aes(time, .fitted, color = sex)) +\n  geom_point()\n\\dontshow{\\}) # examplesIf}",
            "augment.decomposed.ts": "\\dontshow{if (rlang::is_installed(\"ggplot2\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# time series of temperatures in Nottingham, 1920-1939:\nnottem\n\n# perform seasonal decomposition on the data with both decompose\n# and stl:\nd1 <- decompose(nottem)\nd2 <- stl(nottem, s.window = \"periodic\", robust = TRUE)\n\n# compare the original series to its decompositions.\n\ncbind(\n  tidy(nottem), augment(d1),\n  augment(d2)\n)\n\n# visually compare seasonal decompositions in tidy data frames.\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\ndecomps <- tibble(\n  # turn the ts objects into data frames.\n  series = list(as.data.frame(nottem), as.data.frame(nottem)),\n  # add the models in, one for each row.\n  decomp = c(\"decompose\", \"stl\"),\n  model = list(d1, d2)\n) \\%>\\%\n  rowwise() \\%>\\%\n  # pull out the fitted data using broom::augment.\n  mutate(augment = list(broom::augment(model))) \\%>\\%\n  ungroup() \\%>\\%\n  # unnest the data frames into a tidy arrangement of\n  # the series next to its seasonal decomposition, grouped\n  # by the method (stl or decompose).\n  group_by(decomp) \\%>\\%\n  unnest(c(series, augment)) \\%>\\%\n  mutate(index = 1:n()) \\%>\\%\n  ungroup() \\%>\\%\n  select(decomp, index, x, adjusted = .seasadj)\n\nggplot(decomps) +\n  geom_line(aes(x = index, y = x), colour = \"black\") +\n  geom_line(aes(\n    x = index, y = adjusted, colour = decomp,\n    group = decomp\n  ))\n\\dontshow{\\}) # examplesIf}",
            "augment.drc": "\\dontshow{if (rlang::is_installed(\"drc\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(drc)\n\n# fit model\nmod <- drm(dead / total ~ conc, type,\n  weights = total, data = selenium, fct = LL.2(), type = \"binomial\"\n)\n\n# summarize model fit with tidiers\ntidy(mod)\ntidy(mod, conf.int = TRUE)\n\nglance(mod)\n\naugment(mod, selenium)\n\\dontshow{\\}) # examplesIf}",
            "augment.felm": "\\dontshow{if (rlang::is_installed(\"lfe\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(lfe)\n\n# use built-in `airquality` dataset\nhead(airquality)\n\n# no FEs; same as lm()\nest0 <- felm(Ozone ~ Temp + Wind + Solar.R, airquality)\n\n# summarize model fit with tidiers\ntidy(est0)\naugment(est0)\n\n# add month fixed effects\nest1 <- felm(Ozone ~ Temp + Wind + Solar.R | Month, airquality)\n\n# summarize model fit with tidiers\ntidy(est1)\ntidy(est1, fe = TRUE)\naugment(est1)\nglance(est1)\n\n# the \"se.type\" argument can be used to switch out different standard errors\n# types on the fly. In turn, this can be useful exploring the effect of\n# different error structures on model inference.\ntidy(est1, se.type = \"iid\")\ntidy(est1, se.type = \"robust\")\n\n# add clustered SEs (also by month)\nest2 <- felm(Ozone ~ Temp + Wind + Solar.R | Month | 0 | Month, airquality)\n\n# summarize model fit with tidiers\ntidy(est2, conf.int = TRUE)\ntidy(est2, conf.int = TRUE, se.type = \"cluster\")\ntidy(est2, conf.int = TRUE, se.type = \"robust\")\ntidy(est2, conf.int = TRUE, se.type = \"iid\")\n\\dontshow{\\}) # examplesIf}",
            "augment.fixest": "\\dontshow{if (rlang::is_installed(\"fixest\") & !broom:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(fixest)\n\ngravity <-\n  feols(\n    log(Euros) ~ log(dist_km) | Origin + Destination + Product + Year, trade\n  )\n\ntidy(gravity)\nglance(gravity)\naugment(gravity, trade)\n\n# to get robust or clustered SEs, users can either:\n\n# 1) specify the arguments directly in the `tidy()` call\n\ntidy(gravity, conf.int = TRUE, cluster = c(\"Product\", \"Year\"))\n\ntidy(gravity, conf.int = TRUE, se = \"threeway\")\n\n# 2) or, feed tidy() a summary.fixest object that has already accepted\n# these arguments\n\ngravity_summ <- summary(gravity, cluster = c(\"Product\", \"Year\"))\n\ntidy(gravity_summ, conf.int = TRUE)\n\n# approach (1) is preferred.\n\\dontshow{\\}) # examplesIf}",
            "augment.gam": "\\dontshow{if (rlang::is_installed(\"mgcv\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(mgcv)\n\n# fit model\ng <- gam(mpg ~ s(hp) + am + qsec, data = mtcars)\n\n# summarize model fit with tidiers\ntidy(g)\ntidy(g, parametric = TRUE)\nglance(g)\naugment(g)\n\\dontshow{\\}) # examplesIf}",
            "augment.htest": "tt <- t.test(rnorm(10))\n\ntidy(tt)\n\n# the glance output will be the same for each of the below tests\nglance(tt)\n\ntt <- t.test(mpg ~ am, data = mtcars)\n\ntidy(tt)\n\nwt <- wilcox.test(mpg ~ am, data = mtcars, conf.int = TRUE, exact = FALSE)\n\ntidy(wt)\n\nct <- cor.test(mtcars$wt, mtcars$mpg)\n\ntidy(ct)\n\nchit <- chisq.test(xtabs(Freq ~ Sex + Class, data = as.data.frame(Titanic)))\n\ntidy(chit)\naugment(chit)",
            "augment.ivreg": "\\dontshow{if (rlang::is_installed(\"AER\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(AER)\n\n# load data\ndata(\"CigarettesSW\", package = \"AER\")\n\n# fit model\nivr <- ivreg(\n  log(packs) ~ income | population,\n  data = CigarettesSW,\n  subset = year == \"1995\"\n)\n\n# summarize model fit with tidiers\ntidy(ivr)\ntidy(ivr, conf.int = TRUE)\ntidy(ivr, conf.int = TRUE, instruments = TRUE)\n\naugment(ivr)\naugment(ivr, data = CigarettesSW)\naugment(ivr, newdata = CigarettesSW)\n\nglance(ivr)\n\\dontshow{\\}) # examplesIf}",
            "augment.kmeans": "\\dontshow{if (rlang::is_installed(c(\"cluster\", \"modeldata\")) && identical(Sys.getenv(\"NOT_CRAN\"), \"true\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(cluster)\nlibrary(modeldata)\nlibrary(dplyr)\n\ndata(hpc_data)\n\nx <- hpc_data[, 2:5]\n\nfit <- pam(x, k = 4)\n\ntidy(fit)\nglance(fit)\naugment(fit, x)\n\\dontshow{\\}) # examplesIf}",
            "augment.lm": "\\dontshow{if (rlang::is_installed(\"ggplot2\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nmod <- lm(mpg ~ wt + qsec, data = mtcars)\n\ntidy(mod)\nglance(mod)\n\n# coefficient plot\nd <- tidy(mod, conf.int = TRUE)\n\nggplot(d, aes(estimate, term, xmin = conf.low, xmax = conf.high, height = 0)) +\n  geom_point() +\n  geom_vline(xintercept = 0, lty = 4) +\n  geom_errorbarh()\n\n# aside: There are tidy() and glance() methods for lm.summary objects too.\n# this can be useful when you want to conserve memory by converting large lm\n# objects into their leaner summary.lm equivalents.\ns <- summary(mod)\ntidy(s, conf.int = TRUE)\nglance(s)\n\naugment(mod)\naugment(mod, mtcars, interval = \"confidence\")\n\n# predict on new data\nnewdata <- mtcars \\%>\\%\n  head(6) \\%>\\%\n  mutate(wt = wt + 1)\naugment(mod, newdata = newdata)\n\n# ggplot2 example where we also construct 95\\% prediction interval\n\n# simpler bivariate model since we're plotting in 2D\nmod2 <- lm(mpg ~ wt, data = mtcars)\n\nau <- augment(mod2, newdata = newdata, interval = \"prediction\")\n\nggplot(au, aes(wt, mpg)) +\n  geom_point() +\n  geom_line(aes(y = .fitted)) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), col = NA, alpha = 0.3)\n\n# predict on new data without outcome variable. Output does not include .resid\nnewdata <- newdata \\%>\\%\n  select(-mpg)\n\naugment(mod, newdata = newdata)\n\nau <- augment(mod, data = mtcars)\n\nggplot(au, aes(.hat, .std.resid)) +\n  geom_vline(size = 2, colour = \"white\", xintercept = 0) +\n  geom_hline(size = 2, colour = \"white\", yintercept = 0) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\nplot(mod, which = 6)\n\nggplot(au, aes(.hat, .cooksd)) +\n  geom_vline(xintercept = 0, colour = NA) +\n  geom_abline(slope = seq(0, 3, by = 0.5), colour = \"white\") +\n  geom_smooth(se = FALSE) +\n  geom_point()\n\n# column-wise models\na <- matrix(rnorm(20), nrow = 10)\nb <- a + rnorm(length(a))\nresult <- lm(b ~ a)\n\ntidy(result)\n\\dontshow{\\}) # examplesIf}",
            "augment.lmRob": "\\dontshow{if (rlang::is_installed(\"robust\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load modeling library\nlibrary(robust)\n\n# fit model\nm <- lmRob(mpg ~ wt, data = mtcars)\n\n# summarize model fit with tidiers\ntidy(m)\naugment(m)\nglance(m)\n\\dontshow{\\}) # examplesIf}",
            "augment.loess": "lo <- loess(\n  mpg ~ hp + wt,\n  mtcars,\n  control = loess.control(surface = \"direct\")\n)\n\naugment(lo)\n\n# with all columns of original data\naugment(lo, mtcars)\n\n# with a new dataset\naugment(lo, newdata = head(mtcars))",
            "augment.mfx": "\\dontshow{if (rlang::is_installed(\"mfx\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(mfx)\n\n# get the marginal effects from a logit regression\nmod_logmfx <- logitmfx(am ~ cyl + hp + wt, atmean = TRUE, data = mtcars)\n\ntidy(mod_logmfx, conf.int = TRUE)\n\n# compare with the naive model coefficients of the same logit call\ntidy(\n  glm(am ~ cyl + hp + wt, family = binomial, data = mtcars),\n  conf.int = TRUE\n)\n\naugment(mod_logmfx)\nglance(mod_logmfx)\n\n# another example, this time using probit regression\nmod_probmfx <- probitmfx(am ~ cyl + hp + wt, atmean = TRUE, data = mtcars)\n\ntidy(mod_probmfx, conf.int = TRUE)\naugment(mod_probmfx)\nglance(mod_probmfx)\n\\dontshow{\\}) # examplesIf}",
            "augment.mjoint": "\\dontshow{if (rlang::is_installed(\"joineRML\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# broom only skips running these examples because the example models take a\n# while to generate\u2014they should run just fine, though!\n\\dontrun{\n\n\n# load libraries for models and data\nlibrary(joineRML)\n\n# fit a joint model with bivariate longitudinal outcomes\ndata(heart.valve)\n\nhvd <- heart.valve[!is.na(heart.valve$log.grad) &\n  !is.na(heart.valve$log.lvmi) &\n  heart.valve$num <= 50, ]\n\nfit <- mjoint(\n  formLongFixed = list(\n    \"grad\" = log.grad ~ time + sex + hs,\n    \"lvmi\" = log.lvmi ~ time + sex\n  ),\n  formLongRandom = list(\n    \"grad\" = ~ 1 | num,\n    \"lvmi\" = ~ time | num\n  ),\n  formSurv = Surv(fuyrs, status) ~ age,\n  data = hvd,\n  inits = list(\"gamma\" = c(0.11, 1.51, 0.80)),\n  timeVar = \"time\"\n)\n\n# extract the survival fixed effects\ntidy(fit)\n\n# extract the longitudinal fixed effects\ntidy(fit, component = \"longitudinal\")\n\n# extract the survival fixed effects with confidence intervals\ntidy(fit, ci = TRUE)\n\n# extract the survival fixed effects with confidence intervals based\n# on bootstrapped standard errors\nbSE <- bootSE(fit, nboot = 5, safe.boot = TRUE)\ntidy(fit, boot_se = bSE, ci = TRUE)\n\n# augment original data with fitted longitudinal values and residuals\nhvd2 <- augment(fit)\n\n# extract model statistics\nglance(fit)\n}\n\\dontshow{\\}) # examplesIf}",
            "augment.mlogit": "\\dontshow{if (rlang::is_installed(\"mlogit\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(mlogit)\n\ndata(\"Fishing\", package = \"mlogit\")\nFish <- dfidx(Fishing, varying = 2:9, shape = \"wide\", choice = \"mode\")\n\n# fit model\nm <- mlogit(mode ~ price + catch | income, data = Fish)\n\n# summarize model fit with tidiers\ntidy(m)\naugment(m)\nglance(m)\n\\dontshow{\\}) # examplesIf}",
            "augment.nlrq": "\\dontshow{if (rlang::is_installed(\"ggplot2\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# fit model\nn <- nls(mpg ~ k * e^wt, data = mtcars, start = list(k = 1, e = 2))\n\n# summarize model fit with tidiers + visualization\ntidy(n)\naugment(n)\nglance(n)\n\nlibrary(ggplot2)\n\nggplot(augment(n), aes(wt, mpg)) +\n  geom_point() +\n  geom_line(aes(y = .fitted))\n\nnewdata <- head(mtcars)\nnewdata$wt <- newdata$wt + 1\n\naugment(n, newdata = newdata)\n\\dontshow{\\}) # examplesIf}",
            "augment.nls": "\\dontshow{if (rlang::is_installed(\"ggplot2\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# fit model\nn <- nls(mpg ~ k * e^wt, data = mtcars, start = list(k = 1, e = 2))\n\n# summarize model fit with tidiers + visualization\ntidy(n)\naugment(n)\nglance(n)\n\nlibrary(ggplot2)\n\nggplot(augment(n), aes(wt, mpg)) +\n  geom_point() +\n  geom_line(aes(y = .fitted))\n\nnewdata <- head(mtcars)\nnewdata$wt <- newdata$wt + 1\n\naugment(n, newdata = newdata)\n\\dontshow{\\}) # examplesIf}",
            "augment.pam": "\\dontshow{if (rlang::is_installed(c(\"cluster\", \"modeldata\", \"ggplot2\")) && identical(Sys.getenv(\"NOT_CRAN\"), \"true\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cluster)\nlibrary(modeldata)\ndata(hpc_data)\n\nx <- hpc_data[, 2:5]\np <- pam(x, k = 4)\n\n# summarize model fit with tidiers + visualization\ntidy(p)\nglance(p)\naugment(p, x)\n\naugment(p, x) \\%>\\%\n  ggplot(aes(compounds, input_fields)) +\n  geom_point(aes(color = .cluster)) +\n  geom_text(aes(label = cluster), data = tidy(p), size = 10)\n\\dontshow{\\}) # examplesIf}",
            "augment.plm": "\\dontshow{if (rlang::is_installed(\"plm\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(plm)\n\n# load data\ndata(\"Produc\", package = \"plm\")\n\n# fit model\nzz <- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n  data = Produc, index = c(\"state\", \"year\")\n)\n\n# summarize model fit with tidiers\nsummary(zz)\n\ntidy(zz)\ntidy(zz, conf.int = TRUE)\ntidy(zz, conf.int = TRUE, conf.level = 0.9)\n\naugment(zz)\nglance(zz)\n\\dontshow{\\}) # examplesIf}",
            "augment.poLCA": "\\dontshow{if (rlang::is_installed(c(\"poLCA\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(poLCA)\nlibrary(dplyr)\n\n# generate data\ndata(values)\n\nf <- cbind(A, B, C, D) ~ 1\n\n# fit model\nM1 <- poLCA(f, values, nclass = 2, verbose = FALSE)\n\nM1\n\n# summarize model fit with tidiers + visualization\ntidy(M1)\naugment(M1)\nglance(M1)\n\nlibrary(ggplot2)\n\nggplot(tidy(M1), aes(factor(class), estimate, fill = factor(outcome))) +\n  geom_bar(stat = \"identity\", width = 1) +\n  facet_wrap(~variable)\n\n# three-class model with a single covariate.\ndata(election)\n\nf2a <- cbind(\n  MORALG, CARESG, KNOWG, LEADG, DISHONG, INTELG,\n  MORALB, CARESB, KNOWB, LEADB, DISHONB, INTELB\n) ~ PARTY\n\nnes2a <- poLCA(f2a, election, nclass = 3, nrep = 5, verbose = FALSE)\n\ntd <- tidy(nes2a)\ntd\n\nggplot(td, aes(outcome, estimate, color = factor(class), group = class)) +\n  geom_line() +\n  facet_wrap(~variable, nrow = 2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\nau <- augment(nes2a)\n\nau\n\ncount(au, .class)\n\n# if the original data is provided, it leads to NAs in new columns\n# for rows that weren't predicted\nau2 <- augment(nes2a, data = election)\n\nau2\n\ndim(au2)\n\\dontshow{\\}) # examplesIf}",
            "augment.polr": "\\dontshow{if (rlang::is_installed(\"MASS\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(MASS)\n\n# fit model\nfit <- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)\n\n# summarize model fit with tidiers\ntidy(fit, exponentiate = TRUE, conf.int = TRUE)\n\nglance(fit)\naugment(fit, type.predict = \"class\")\n\nfit2 <- polr(factor(gear) ~ am + mpg + qsec, data = mtcars)\n\ntidy(fit, p.values = TRUE)\n\\dontshow{\\}) # examplesIf}",
            "augment.rlm": "\\dontshow{if (rlang::is_installed(\"MASS\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(MASS)\n\n# fit model\nr <- rlm(stack.loss ~ ., stackloss)\n\n# summarize model fit with tidiers\ntidy(r)\naugment(r)\nglance(r)\n\\dontshow{\\}) # examplesIf}",
            "augment.rma": "\\dontshow{if (rlang::is_installed(\"metafor\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load modeling library\nlibrary(metafor)\n\n# generate data and fit\ndf <-\n  escalc(\n    measure = \"RR\",\n    ai = tpos,\n    bi = tneg,\n    ci = cpos,\n    di = cneg,\n    data = dat.bcg\n  )\n\nmeta_analysis <- rma(yi, vi, data = df, method = \"EB\")\n\n# summarize model fit with tidiers\naugment(meta_analysis)\n\\dontshow{\\}) # examplesIf}",
            "augment.robustbase.glmrob": "if (requireNamespace(\"robustbase\", quietly = TRUE)) {\n  # load libraries for models and data\n  library(robustbase)\n\n  data(coleman)\n  set.seed(0)\n\n  m <- lmrob(Y ~ ., data = coleman)\n  tidy(m)\n  augment(m)\n  glance(m)\n\n  data(carrots)\n\n  Rfit <- glmrob(cbind(success, total - success) ~ logdose + block,\n    family = binomial, data = carrots, method = \"Mqle\",\n    control = glmrobMqle.control(tcc = 1.2)\n  )\n\n  tidy(Rfit)\n  augment(Rfit)\n}",
            "augment.robustbase.lmrob": "if (requireNamespace(\"robustbase\", quietly = TRUE)) {\n  # load libraries for models and data\n  library(robustbase)\n\n  data(coleman)\n  set.seed(0)\n\n  m <- lmrob(Y ~ ., data = coleman)\n  tidy(m)\n  augment(m)\n  glance(m)\n\n  data(carrots)\n\n  Rfit <- glmrob(cbind(success, total - success) ~ logdose + block,\n    family = binomial, data = carrots, method = \"Mqle\",\n    control = glmrobMqle.control(tcc = 1.2)\n  )\n\n  tidy(Rfit)\n  augment(Rfit)\n}",
            "augment.rq": "\\dontshow{if (rlang::is_installed(\"quantreg\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load modeling library and data\nlibrary(quantreg)\n\ndata(stackloss)\n\n# median (l1) regression fit for the stackloss data.\nmod1 <- rq(stack.loss ~ stack.x, .5)\n\n# weighted sample median\nmod2 <- rq(rnorm(50) ~ 1, weights = runif(50))\n\n# summarize model fit with tidiers\ntidy(mod1)\nglance(mod1)\naugment(mod1)\n\ntidy(mod2)\nglance(mod2)\naugment(mod2)\n\n# varying tau to generate an rqs object\nmod3 <- rq(stack.loss ~ stack.x, tau = c(.25, .5))\n\ntidy(mod3)\naugment(mod3)\n\n# glance cannot handle rqs objects like `mod3`--use a purrr\n# `map`-based workflow instead\n\\dontshow{\\}) # examplesIf}",
            "augment.rqs": "\\dontshow{if (rlang::is_installed(\"quantreg\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load modeling library and data\nlibrary(quantreg)\n\ndata(stackloss)\n\n# median (l1) regression fit for the stackloss data.\nmod1 <- rq(stack.loss ~ stack.x, .5)\n\n# weighted sample median\nmod2 <- rq(rnorm(50) ~ 1, weights = runif(50))\n\n# summarize model fit with tidiers\ntidy(mod1)\nglance(mod1)\naugment(mod1)\n\ntidy(mod2)\nglance(mod2)\naugment(mod2)\n\n# varying tau to generate an rqs object\nmod3 <- rq(stack.loss ~ stack.x, tau = c(.25, .5))\n\ntidy(mod3)\naugment(mod3)\n\n# glance cannot handle rqs objects like `mod3`--use a purrr\n# `map`-based workflow instead\n\\dontshow{\\}) # examplesIf}",
            "augment.sarlm": "\\dontshow{if (rlang::is_installed(c(\"spdep\", \"spatialreg\")) && identical(Sys.getenv(\"NOT_CRAN\"), \"true\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n\n# load libraries for models and data\nlibrary(spatialreg)\nlibrary(spdep)\n\n# load data\ndata(oldcol, package = \"spdep\")\n\nlistw <- nb2listw(COL.nb, style = \"W\")\n\n# fit model\ncrime_sar <-\n  lagsarlm(CRIME ~ INC + HOVAL,\n    data = COL.OLD,\n    listw = listw,\n    method = \"eigen\"\n  )\n\n# summarize model fit with tidiers\ntidy(crime_sar)\ntidy(crime_sar, conf.int = TRUE)\nglance(crime_sar)\naugment(crime_sar)\n\n# fit another model\ncrime_sem <- errorsarlm(CRIME ~ INC + HOVAL, data = COL.OLD, listw)\n\n# summarize model fit with tidiers\ntidy(crime_sem)\ntidy(crime_sem, conf.int = TRUE)\nglance(crime_sem)\naugment(crime_sem)\n\n# fit another model\ncrime_sac <- sacsarlm(CRIME ~ INC + HOVAL, data = COL.OLD, listw)\n\n# summarize model fit with tidiers\ntidy(crime_sac)\ntidy(crime_sac, conf.int = TRUE)\nglance(crime_sac)\naugment(crime_sac)\n\\dontshow{\\}) # examplesIf}",
            "augment.smooth.spline": "\\dontshow{if (rlang::is_installed(\"ggplot2\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# fit model\nspl <- smooth.spline(mtcars$wt, mtcars$mpg, df = 4)\n\n# summarize model fit with tidiers\naugment(spl, mtcars)\n\n# calls original columns x and y\naugment(spl)\n\nlibrary(ggplot2)\nggplot(augment(spl, mtcars), aes(wt, mpg)) +\n  geom_point() +\n  geom_line(aes(y = .fitted))\n\\dontshow{\\}) # examplesIf}",
            "augment.speedlm": "\\dontshow{if (rlang::is_installed(\"speedglm\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load modeling library\nlibrary(speedglm)\n\n# fit model\nmod <- speedlm(mpg ~ wt + qsec, data = mtcars, fitted = TRUE)\n\n# summarize model fit with tidiers\ntidy(mod)\nglance(mod)\naugment(mod)\n\\dontshow{\\}) # examplesIf}",
            "augment.survreg": "\\dontshow{if (rlang::is_installed(c(\"survival\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(survival)\n\n# fit model\nsr <- survreg(\n  Surv(futime, fustat) ~ ecog.ps + rx,\n  ovarian,\n  dist = \"exponential\"\n)\n\n# summarize model fit with tidiers + visualization\ntidy(sr)\naugment(sr, ovarian)\nglance(sr)\n\n# coefficient plot\ntd <- tidy(sr, conf.int = TRUE)\n\nlibrary(ggplot2)\n\nggplot(td, aes(estimate, term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0) +\n  geom_vline(xintercept = 0)\n\\dontshow{\\}) # examplesIf}",
            "data.frame_tidiers": "\\dontshow{if (rlang::is_installed(\"ggplot2\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\ntd <- tidy(mtcars)\ntd\n\nglance(mtcars)\n\nlibrary(ggplot2)\n# compare mean and standard deviation\nggplot(td, aes(mean, sd)) + geom_point() +\n     geom_text(aes(label = column), hjust = 1, vjust = 1) +\n     scale_x_log10() + scale_y_log10() + geom_abline()\n\\dontshow{\\}) # examplesIf}",
            "durbinWatsonTest_tidiers": "\\dontshow{if (rlang::is_installed(\"car\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load modeling library\nlibrary(car)\n\n# fit model\ndw <- durbinWatsonTest(lm(mpg ~ wt, data = mtcars))\n\n# summarize model fit with tidiers\ntidy(dw)\n\n# same output for all durbinWatsonTests\nglance(dw)\n\\dontshow{\\}) # examplesIf}",
            "glance.Arima": "# fit model\nfit <- arima(lh, order = c(1, 0, 0))\n\n# summarize model fit with tidiers\ntidy(fit)\nglance(fit)",
            "glance.Mclust": "\\dontshow{if (rlang::is_installed(\"mclust\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load library for models and data\nlibrary(mclust)\n\n# load data manipulation libraries\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(tidyr)\n\nset.seed(27)\n\ncenters <- tibble(\n  cluster = factor(1:3),\n  # number points in each cluster\n  num_points = c(100, 150, 50),\n  # x1 coordinate of cluster center\n  x1 = c(5, 0, -3),\n  # x2 coordinate of cluster center\n  x2 = c(-1, 1, -2)\n)\n\npoints <- centers \\%>\\%\n  mutate(\n    x1 = map2(num_points, x1, rnorm),\n    x2 = map2(num_points, x2, rnorm)\n  ) \\%>\\%\n  select(-num_points, -cluster) \\%>\\%\n  unnest(c(x1, x2))\n\n# fit model\nm <- Mclust(points)\n\n# summarize model fit with tidiers\ntidy(m)\naugment(m, points)\nglance(m)\n\\dontshow{\\}) # examplesIf}",
            "glance.aareg": "\\dontshow{if (rlang::is_installed(\"survival\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(survival)\n\n# fit model\nafit <- aareg(\n  Surv(time, status) ~ age + sex + ph.ecog,\n  data = lung,\n  dfbeta = TRUE\n)\n\n# summarize model fit with tidiers\ntidy(afit)\n\\dontshow{\\}) # examplesIf}",
            "glance.anova": "\\dontshow{if (FALSE) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# fit models\na <- lm(mpg ~ wt + qsec + disp, mtcars)\nb <- lm(mpg ~ wt + qsec, mtcars)\n\nmod <- anova(a, b)\n\n# summarize model fit with tidiers\ntidy(mod)\nglance(mod)\n\n# car::linearHypothesis() example\nlibrary(car)\nmod_lht <- linearHypothesis(a, \"wt - disp\")\ntidy(mod_lht)\nglance(mod_lht)\n\\dontshow{\\}) # examplesIf}",
            "glance.aov": "a <- aov(mpg ~ wt + qsec + disp, mtcars)\ntidy(a)",
            "glance.betamfx": "\\dontshow{if (rlang::is_installed(\"mfx\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(mfx)\n\n# Simulate some data\nset.seed(12345)\nn <- 1000\nx <- rnorm(n)\n\n# Beta outcome\ny <- rbeta(n, shape1 = plogis(1 + 0.5 * x), shape2 = (abs(0.2 * x)))\n# Use Smithson and Verkuilen correction\ny <- (y * (n - 1) + 0.5) / n\n\nd <- data.frame(y, x)\nmod_betamfx <- betamfx(y ~ x | x, data = d)\n\ntidy(mod_betamfx, conf.int = TRUE)\n\n# Compare with the naive model coefficients of the equivalent betareg call (not run)\n# tidy(betamfx(y ~ x | x, data = d), conf.int = TRUE)\n\naugment(mod_betamfx)\nglance(mod_betamfx)\n\\dontshow{\\}) # examplesIf}",
            "glance.betareg": "\\dontshow{if (rlang::is_installed(\"betareg\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(betareg)\n\n# load dats\ndata(\"GasolineYield\", package = \"betareg\")\n\n# fit model\nmod <- betareg(yield ~ batch + temp, data = GasolineYield)\n\nmod\n\n# summarize model fit with tidiers\ntidy(mod)\ntidy(mod, conf.int = TRUE)\ntidy(mod, conf.int = TRUE, conf.level = .99)\n\naugment(mod)\n\nglance(mod)\n\\dontshow{\\}) # examplesIf}",
            "glance.biglm": "\\dontshow{if (rlang::is_installed(\"biglm\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load modeling library\nlibrary(biglm)\n\n# fit model -- linear regression\nbfit <- biglm(mpg ~ wt + disp, mtcars)\n\n# summarize model fit with tidiers\ntidy(bfit)\ntidy(bfit, conf.int = TRUE)\ntidy(bfit, conf.int = TRUE, conf.level = .9)\n\nglance(bfit)\n\n# fit model -- logistic regression\nbgfit <- bigglm(am ~ mpg, mtcars, family = binomial())\n\n# summarize model fit with tidiers\ntidy(bgfit)\ntidy(bgfit, exponentiate = TRUE)\ntidy(bgfit, conf.int = TRUE)\ntidy(bgfit, conf.int = TRUE, conf.level = .9)\ntidy(bgfit, conf.int = TRUE, conf.level = .9, exponentiate = TRUE)\n\nglance(bgfit)\n\\dontshow{\\}) # examplesIf}",
            "glance.binDesign": "\\dontshow{if (rlang::is_installed(c(\"binGroup\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(binGroup)\n\ndes <- binDesign(\n  nmax = 300, delta = 0.06,\n  p.hyp = 0.1, power = .8\n)\n\nglance(des)\ntidy(des)\n\nlibrary(ggplot2)\n\nggplot(tidy(des), aes(n, power)) +\n  geom_line()\n\\dontshow{\\}) # examplesIf}",
            "glance.cch": "\\dontshow{if (rlang::is_installed(c(\"survival\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(survival)\n\n# examples come from cch documentation\nsubcoh <- nwtco$in.subcohort\nselccoh <- with(nwtco, rel == 1 | subcoh == 1)\nccoh.data <- nwtco[selccoh, ]\nccoh.data$subcohort <- subcoh[selccoh]\n\n# central-lab histology\nccoh.data$histol <- factor(ccoh.data$histol, labels = c(\"FH\", \"UH\"))\n\n# tumour stage\nccoh.data$stage <- factor(ccoh.data$stage, labels = c(\"I\", \"II\", \"III\", \"IV\"))\nccoh.data$age <- ccoh.data$age / 12 # age in years\n\n# fit model\nfit.ccP <- cch(Surv(edrel, rel) ~ stage + histol + age,\n  data = ccoh.data,\n  subcoh = ~subcohort, id = ~seqno, cohort.size = 4028\n)\n\n# summarize model fit with tidiers + visualization\ntidy(fit.ccP)\n\n# coefficient plot\nlibrary(ggplot2)\n\nggplot(tidy(fit.ccP), aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0) +\n  geom_vline(xintercept = 0)\n\\dontshow{\\}) # examplesIf}",
            "glance.clm": "\\dontshow{if (rlang::is_installed(\"ordinal\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(ordinal)\n\n# fit model\nfit <- clm(rating ~ temp * contact, data = wine)\n\n# summarize model fit with tidiers\ntidy(fit)\ntidy(fit, conf.int = TRUE, conf.level = 0.9)\ntidy(fit, conf.int = TRUE, conf.type = \"Wald\", exponentiate = TRUE)\n\nglance(fit)\naugment(fit, type.predict = \"prob\")\naugment(fit, type.predict = \"class\")\n\n# ...and again with another model specification\nfit2 <- clm(rating ~ temp, nominal = ~contact, data = wine)\n\ntidy(fit2)\nglance(fit2)\n\\dontshow{\\}) # examplesIf}",
            "glance.clmm": "\\dontshow{if (rlang::is_installed(\"ordinal\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(ordinal)\n\n# fit model\nfit <- clmm(rating ~ temp + contact + (1 | judge), data = wine)\n\n# summarize model fit with tidiers\ntidy(fit)\ntidy(fit, conf.int = TRUE, conf.level = 0.9)\ntidy(fit, conf.int = TRUE, exponentiate = TRUE)\n\nglance(fit)\n\n# ...and again with another model specification\nfit2 <- clmm(rating ~ temp + (1 | judge), nominal = ~contact, data = wine)\n\ntidy(fit2)\nglance(fit2)\n\\dontshow{\\}) # examplesIf}",
            "glance.coeftest": "\\dontshow{if (rlang::is_installed(\"lmtest\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(lmtest)\n\nm <- lm(dist ~ speed, data = cars)\n\ncoeftest(m)\ntidy(coeftest(m))\ntidy(coeftest(m, conf.int = TRUE))\n\n# a very common workflow is to combine lmtest::coeftest with alternate\n# variance-covariance matrices via the sandwich package. The lmtest\n# tidiers support this workflow too, enabling you to adjust the standard\n# errors of your tidied models on the fly.\nlibrary(sandwich)\n\n# \"HC3\" (default) robust SEs\ntidy(coeftest(m, vcov = vcovHC))\n\n# \"HC2\" robust SEs\ntidy(coeftest(m, vcov = vcovHC, type = \"HC2\"))\n\n# N-W HAC robust SEs\ntidy(coeftest(m, vcov = NeweyWest))\n\n# the columns of the returned tibble for glance.coeftest() will vary\n# depending on whether the coeftest object retains the underlying model.\n# Users can control this with the \"save = TRUE\" argument of coeftest().\nglance(coeftest(m))\nglance(coeftest(m, save = TRUE))\n\\dontshow{\\}) # examplesIf}",
            "glance.coxph": "\\dontshow{if (rlang::is_installed(c(\"survival\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(survival)\n\n# fit model\ncfit <- coxph(Surv(time, status) ~ age + sex, lung)\n\n# summarize model fit with tidiers\ntidy(cfit)\ntidy(cfit, exponentiate = TRUE)\n\nlp <- augment(cfit, lung)\nrisks <- augment(cfit, lung, type.predict = \"risk\")\nexpected <- augment(cfit, lung, type.predict = \"expected\")\n\nglance(cfit)\n\n# also works on clogit models\nresp <- levels(logan$occupation)\nn <- nrow(logan)\nindx <- rep(1:n, length(resp))\nlogan2 <- data.frame(\n  logan[indx, ],\n  id = indx,\n  tocc = factor(rep(resp, each = n))\n)\n\nlogan2$case <- (logan2$occupation == logan2$tocc)\n\ncl <- clogit(case ~ tocc + tocc:education + strata(id), logan2)\n\ntidy(cl)\nglance(cl)\n\nlibrary(ggplot2)\n\nggplot(lp, aes(age, .fitted, color = sex)) +\n  geom_point()\n\nggplot(risks, aes(age, .fitted, color = sex)) +\n  geom_point()\n\nggplot(expected, aes(time, .fitted, color = sex)) +\n  geom_point()\n\\dontshow{\\}) # examplesIf}",
            "glance.crr": "\\dontshow{if (rlang::is_installed(\"cmprsk\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(cmprsk)\n\n# time to loco-regional failure (lrf)\nlrf_time <- rexp(100)\nlrf_event <- sample(0:2, 100, replace = TRUE)\ntrt <- sample(0:1, 100, replace = TRUE)\nstrt <- sample(1:2, 100, replace = TRUE)\n\n# fit model\nx <- crr(lrf_time, lrf_event, cbind(trt, strt))\n\n# summarize model fit with tidiers\ntidy(x, conf.int = TRUE)\nglance(x)\n\\dontshow{\\}) # examplesIf}",
            "glance.cv.glmnet": "\\dontshow{if (rlang::is_installed(c(\"glmnet\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(glmnet)\n\nset.seed(27)\n\nnobs <- 100\nnvar <- 50\nreal <- 5\n\nx <- matrix(rnorm(nobs * nvar), nobs, nvar)\nbeta <- c(rnorm(real, 0, 1), rep(0, nvar - real))\ny <- c(t(beta) \\%*\\% t(x)) + rnorm(nvar, sd = 3)\n\ncvfit1 <- cv.glmnet(x, y)\n\ntidy(cvfit1)\nglance(cvfit1)\n\nlibrary(ggplot2)\n\ntidied_cv <- tidy(cvfit1)\nglance_cv <- glance(cvfit1)\n\n# plot of MSE as a function of lambda\ng <- ggplot(tidied_cv, aes(lambda, estimate)) +\n  geom_line() +\n  scale_x_log10()\ng\n\n# plot of MSE as a function of lambda with confidence ribbon\ng <- g + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .25)\ng\n\n# plot of MSE as a function of lambda with confidence ribbon and choices\n# of minimum lambda marked\ng <- g +\n  geom_vline(xintercept = glance_cv$lambda.min) +\n  geom_vline(xintercept = glance_cv$lambda.1se, lty = 2)\ng\n\n# plot of number of zeros for each choice of lambda\nggplot(tidied_cv, aes(lambda, nzero)) +\n  geom_line() +\n  scale_x_log10()\n\n# coefficient plot with min lambda shown\ntidied <- tidy(cvfit1$glmnet.fit)\n\nggplot(tidied, aes(lambda, estimate, group = term)) +\n  scale_x_log10() +\n  geom_line() +\n  geom_vline(xintercept = glance_cv$lambda.min) +\n  geom_vline(xintercept = glance_cv$lambda.1se, lty = 2)\n\\dontshow{\\}) # examplesIf}",
            "glance.drc": "\\dontshow{if (rlang::is_installed(\"drc\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(drc)\n\n# fit model\nmod <- drm(dead / total ~ conc, type,\n  weights = total, data = selenium, fct = LL.2(), type = \"binomial\"\n)\n\n# summarize model fit with tidiers\ntidy(mod)\ntidy(mod, conf.int = TRUE)\n\nglance(mod)\n\naugment(mod, selenium)\n\\dontshow{\\}) # examplesIf}",
            "glance.factanal": "set.seed(123)\n\n# generate data\nlibrary(dplyr)\nlibrary(purrr)\n\nm1 <- tibble(\n  v1 = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 4, 5, 6),\n  v2 = c(1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 3, 4, 3, 3, 3, 4, 6, 5),\n  v3 = c(3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 4, 6),\n  v4 = c(3, 3, 4, 3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 5, 6, 4),\n  v5 = c(1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 6, 4, 5),\n  v6 = c(1, 1, 1, 2, 1, 3, 3, 3, 4, 3, 1, 1, 1, 2, 1, 6, 5, 4)\n)\n\n# new data\nm2 <- map_dfr(m1, rev)\n\n# factor analysis objects\nfit1 <- factanal(m1, factors = 3, scores = \"Bartlett\")\nfit2 <- factanal(m1, factors = 3, scores = \"regression\")\n\n# tidying the object\ntidy(fit1)\ntidy(fit2)\n\n# augmented dataframe\naugment(fit1)\naugment(fit2)\n\n# augmented dataframe (with new data)\naugment(fit1, data = m2)\naugment(fit2, data = m2)",
            "glance.felm": "\\dontshow{if (rlang::is_installed(\"lfe\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(lfe)\n\n# use built-in `airquality` dataset\nhead(airquality)\n\n# no FEs; same as lm()\nest0 <- felm(Ozone ~ Temp + Wind + Solar.R, airquality)\n\n# summarize model fit with tidiers\ntidy(est0)\naugment(est0)\n\n# add month fixed effects\nest1 <- felm(Ozone ~ Temp + Wind + Solar.R | Month, airquality)\n\n# summarize model fit with tidiers\ntidy(est1)\ntidy(est1, fe = TRUE)\naugment(est1)\nglance(est1)\n\n# the \"se.type\" argument can be used to switch out different standard errors\n# types on the fly. In turn, this can be useful exploring the effect of\n# different error structures on model inference.\ntidy(est1, se.type = \"iid\")\ntidy(est1, se.type = \"robust\")\n\n# add clustered SEs (also by month)\nest2 <- felm(Ozone ~ Temp + Wind + Solar.R | Month | 0 | Month, airquality)\n\n# summarize model fit with tidiers\ntidy(est2, conf.int = TRUE)\ntidy(est2, conf.int = TRUE, se.type = \"cluster\")\ntidy(est2, conf.int = TRUE, se.type = \"robust\")\ntidy(est2, conf.int = TRUE, se.type = \"iid\")\n\\dontshow{\\}) # examplesIf}",
            "glance.fitdistr": "\\dontshow{if (rlang::is_installed(\"MASS\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(MASS)\n\n# generate data\nset.seed(2015)\nx <- rnorm(100, 5, 2)\n\n#  fit models\nfit <- fitdistr(x, dnorm, list(mean = 3, sd = 1))\n\n# summarize model fit with tidiers\ntidy(fit)\nglance(fit)\n\\dontshow{\\}) # examplesIf}",
            "glance.fixest": "\\dontshow{if (rlang::is_installed(\"fixest\") & !broom:::is_cran_check()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(fixest)\n\ngravity <-\n  feols(\n    log(Euros) ~ log(dist_km) | Origin + Destination + Product + Year, trade\n  )\n\ntidy(gravity)\nglance(gravity)\naugment(gravity, trade)\n\n# to get robust or clustered SEs, users can either:\n\n# 1) specify the arguments directly in the `tidy()` call\n\ntidy(gravity, conf.int = TRUE, cluster = c(\"Product\", \"Year\"))\n\ntidy(gravity, conf.int = TRUE, se = \"threeway\")\n\n# 2) or, feed tidy() a summary.fixest object that has already accepted\n# these arguments\n\ngravity_summ <- summary(gravity, cluster = c(\"Product\", \"Year\"))\n\ntidy(gravity_summ, conf.int = TRUE)\n\n# approach (1) is preferred.\n\\dontshow{\\}) # examplesIf}",
            "glance.gam": "\\dontshow{if (rlang::is_installed(\"mgcv\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(mgcv)\n\n# fit model\ng <- gam(mpg ~ s(hp) + am + qsec, data = mtcars)\n\n# summarize model fit with tidiers\ntidy(g)\ntidy(g, parametric = TRUE)\nglance(g)\naugment(g)\n\\dontshow{\\}) # examplesIf}",
            "glance.geeglm": "\\dontshow{if (rlang::is_installed(\"geepack\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load modeling library\nlibrary(geepack)\n\n# load data\ndata(state)\n\n\nds <- data.frame(state.region, state.x77)\n\n# fit model\ngeefit <- geeglm(Income ~ Frost + Murder,\n  id = state.region,\n  data = ds,\n  corstr = \"exchangeable\"\n)\n\n# summarize model fit with tidiers\ntidy(geefit)\ntidy(geefit, conf.int = TRUE)\n\\dontshow{\\}) # examplesIf}",
            "glance.glm": "g <- glm(am ~ mpg, mtcars, family = \"binomial\")\nglance(g)",
            "glance.glmRob": "\\dontshow{if (rlang::is_installed(\"robust\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(robust)\n\n# fit model\ngm <- glmRob(am ~ wt, data = mtcars, family = \"binomial\")\n\n# summarize model fit with tidiers\ntidy(gm)\nglance(gm)\n\\dontshow{\\}) # examplesIf}",
            "glance.glmnet": "\\dontshow{if (rlang::is_installed(c(\"glmnet\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(glmnet)\n\nset.seed(2014)\nx <- matrix(rnorm(100 * 20), 100, 20)\ny <- rnorm(100)\nfit1 <- glmnet(x, y)\n\n# summarize model fit with tidiers + visualization\ntidy(fit1)\nglance(fit1)\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\ntidied <- tidy(fit1) \\%>\\% filter(term != \"(Intercept)\")\n\nggplot(tidied, aes(step, estimate, group = term)) +\n  geom_line()\n\nggplot(tidied, aes(lambda, estimate, group = term)) +\n  geom_line() +\n  scale_x_log10()\n\nggplot(tidied, aes(lambda, dev.ratio)) +\n  geom_line()\n\n# works for other types of regressions as well, such as logistic\ng2 <- sample(1:2, 100, replace = TRUE)\nfit2 <- glmnet(x, g2, family = \"binomial\")\ntidy(fit2)\n\\dontshow{\\}) # examplesIf}",
            "glance.gmm": "\\dontshow{if (rlang::is_installed(c(\"gmm\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(gmm)\n\n# examples come from the \"gmm\" package\n# CAPM test with GMM\ndata(Finance)\nr <- Finance[1:300, 1:10]\nrm <- Finance[1:300, \"rm\"]\nrf <- Finance[1:300, \"rf\"]\n\nz <- as.matrix(r - rf)\nt <- nrow(z)\nzm <- rm - rf\nh <- matrix(zm, t, 1)\nres <- gmm(z ~ zm, x = h)\n\n# tidy result\ntidy(res)\ntidy(res, conf.int = TRUE)\ntidy(res, conf.int = TRUE, conf.level = .99)\n\n# coefficient plot\nlibrary(ggplot2)\nlibrary(dplyr)\n\ntidy(res, conf.int = TRUE) \\%>\\%\n  mutate(variable = reorder(term, estimate)) \\%>\\%\n  ggplot(aes(estimate, variable)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0, color = \"red\", lty = 2)\n\n# from a function instead of a matrix\ng <- function(theta, x) {\n  e <- x[, 2:11] - theta[1] - (x[, 1] - theta[1]) \\%*\\% matrix(theta[2:11], 1, 10)\n  gmat <- cbind(e, e * c(x[, 1]))\n  return(gmat)\n}\n\nx <- as.matrix(cbind(rm, r))\nres_black <- gmm(g, x = x, t0 = rep(0, 11))\n\ntidy(res_black)\ntidy(res_black, conf.int = TRUE)\n\n# APT test with Fama-French factors and GMM\n\nf1 <- zm\nf2 <- Finance[1:300, \"hml\"] - rf\nf3 <- Finance[1:300, \"smb\"] - rf\nh <- cbind(f1, f2, f3)\nres2 <- gmm(z ~ f1 + f2 + f3, x = h)\n\ntd2 <- tidy(res2, conf.int = TRUE)\ntd2\n\n# coefficient plot\ntd2 \\%>\\%\n  mutate(variable = reorder(term, estimate)) \\%>\\%\n  ggplot(aes(estimate, variable)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0, color = \"red\", lty = 2)\n\\dontshow{\\}) # examplesIf}",
            "glance.ivreg": "\\dontshow{if (rlang::is_installed(\"AER\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(AER)\n\n# load data\ndata(\"CigarettesSW\", package = \"AER\")\n\n# fit model\nivr <- ivreg(\n  log(packs) ~ income | population,\n  data = CigarettesSW,\n  subset = year == \"1995\"\n)\n\n# summarize model fit with tidiers\ntidy(ivr)\ntidy(ivr, conf.int = TRUE)\ntidy(ivr, conf.int = TRUE, instruments = TRUE)\n\naugment(ivr)\naugment(ivr, data = CigarettesSW)\naugment(ivr, newdata = CigarettesSW)\n\nglance(ivr)\n\\dontshow{\\}) # examplesIf}",
            "glance.kmeans": "\\dontshow{if (rlang::is_installed(c(\"cluster\", \"modeldata\")) && identical(Sys.getenv(\"NOT_CRAN\"), \"true\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(cluster)\nlibrary(modeldata)\nlibrary(dplyr)\n\ndata(hpc_data)\n\nx <- hpc_data[, 2:5]\n\nfit <- pam(x, k = 4)\n\ntidy(fit)\nglance(fit)\naugment(fit, x)\n\\dontshow{\\}) # examplesIf}",
            "glance.lavaan": "\\dontshow{if (rlang::is_installed(\"lavaan\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(lavaan)\n\n# fit model\ncfa.fit <- cfa(\n  \"F =~ x1 + x2 + x3 + x4 + x5\",\n  data = HolzingerSwineford1939, group = \"school\"\n)\n\n# summarize model fit with tidiers\nglance(cfa.fit)\n\\dontshow{\\}) # examplesIf}",
            "glance.lm": "\\dontshow{if (rlang::is_installed(\"ggplot2\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nmod <- lm(mpg ~ wt + qsec, data = mtcars)\n\ntidy(mod)\nglance(mod)\n\n# coefficient plot\nd <- tidy(mod, conf.int = TRUE)\n\nggplot(d, aes(estimate, term, xmin = conf.low, xmax = conf.high, height = 0)) +\n  geom_point() +\n  geom_vline(xintercept = 0, lty = 4) +\n  geom_errorbarh()\n\n# aside: There are tidy() and glance() methods for lm.summary objects too.\n# this can be useful when you want to conserve memory by converting large lm\n# objects into their leaner summary.lm equivalents.\ns <- summary(mod)\ntidy(s, conf.int = TRUE)\nglance(s)\n\naugment(mod)\naugment(mod, mtcars, interval = \"confidence\")\n\n# predict on new data\nnewdata <- mtcars \\%>\\%\n  head(6) \\%>\\%\n  mutate(wt = wt + 1)\naugment(mod, newdata = newdata)\n\n# ggplot2 example where we also construct 95\\% prediction interval\n\n# simpler bivariate model since we're plotting in 2D\nmod2 <- lm(mpg ~ wt, data = mtcars)\n\nau <- augment(mod2, newdata = newdata, interval = \"prediction\")\n\nggplot(au, aes(wt, mpg)) +\n  geom_point() +\n  geom_line(aes(y = .fitted)) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), col = NA, alpha = 0.3)\n\n# predict on new data without outcome variable. Output does not include .resid\nnewdata <- newdata \\%>\\%\n  select(-mpg)\n\naugment(mod, newdata = newdata)\n\nau <- augment(mod, data = mtcars)\n\nggplot(au, aes(.hat, .std.resid)) +\n  geom_vline(size = 2, colour = \"white\", xintercept = 0) +\n  geom_hline(size = 2, colour = \"white\", yintercept = 0) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\nplot(mod, which = 6)\n\nggplot(au, aes(.hat, .cooksd)) +\n  geom_vline(xintercept = 0, colour = NA) +\n  geom_abline(slope = seq(0, 3, by = 0.5), colour = \"white\") +\n  geom_smooth(se = FALSE) +\n  geom_point()\n\n# column-wise models\na <- matrix(rnorm(20), nrow = 10)\nb <- a + rnorm(length(a))\nresult <- lm(b ~ a)\n\ntidy(result)\n\\dontshow{\\}) # examplesIf}",
            "glance.lmRob": "\\dontshow{if (rlang::is_installed(\"robust\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load modeling library\nlibrary(robust)\n\n# fit model\nm <- lmRob(mpg ~ wt, data = mtcars)\n\n# summarize model fit with tidiers\ntidy(m)\naugment(m)\nglance(m)\n\\dontshow{\\}) # examplesIf}",
            "glance.lmodel2": "\\dontshow{if (rlang::is_installed(c(\"lmodel2\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(lmodel2)\n\ndata(mod2ex2)\nEx2.res <- lmodel2(Prey ~ Predators, data = mod2ex2, \"relative\", \"relative\", 99)\nEx2.res\n\n# summarize model fit with tidiers + visualization\ntidy(Ex2.res)\nglance(Ex2.res)\n\n# this allows coefficient plots with ggplot2\nlibrary(ggplot2)\n\nggplot(tidy(Ex2.res), aes(estimate, term, color = method)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))\n\\dontshow{\\}) # examplesIf}",
            "glance.margins": "\\dontshow{if (rlang::is_installed(\"margins\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(margins)\n\n# example 1: logit model\nmod_log <- glm(am ~ cyl + hp + wt, data = mtcars, family = binomial)\n\n# get tidied \"naive\" model coefficients\ntidy(mod_log)\n\n# convert to marginal effects with margins()\nmarg_log <- margins(mod_log)\n\n# get tidied marginal effects\ntidy(marg_log)\ntidy(marg_log, conf.int = TRUE)\n\n# requires running the underlying model again. quick for this example\nglance(marg_log)\n\n# augmenting `margins` outputs isn't supported, but\n# you can get the same info by running on the underlying model\naugment(mod_log)\n\n# example 2: threeway interaction terms\nmod_ie <- lm(mpg ~ wt * cyl * disp, data = mtcars)\n\n# get tidied \"naive\" model coefficients\ntidy(mod_ie)\n\n# convert to marginal effects with margins()\nmarg_ie0 <- margins(mod_ie)\n# get tidied marginal effects\ntidy(marg_ie0)\nglance(marg_ie0)\n\n# marginal effects evaluated at specific values of a variable (here: cyl)\nmarg_ie1 <- margins(mod_ie, at = list(cyl = c(4,6,8)))\n\n# summarize model fit with tidiers\ntidy(marg_ie1)\n\n# marginal effects of one interaction variable (here: wt), modulated at\n# specific values of the two other interaction variables (here: cyl and drat)\nmarg_ie2 <- margins(mod_ie,\n                    variables = \"wt\",\n                    at = list(cyl = c(4,6,8), drat = c(3, 3.5, 4)))\n\n# summarize model fit with tidiers\ntidy(marg_ie2)\n\\dontshow{\\}) # examplesIf}",
            "glance.mfx": "\\dontshow{if (rlang::is_installed(\"mfx\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(mfx)\n\n# get the marginal effects from a logit regression\nmod_logmfx <- logitmfx(am ~ cyl + hp + wt, atmean = TRUE, data = mtcars)\n\ntidy(mod_logmfx, conf.int = TRUE)\n\n# compare with the naive model coefficients of the same logit call\ntidy(\n  glm(am ~ cyl + hp + wt, family = binomial, data = mtcars),\n  conf.int = TRUE\n)\n\naugment(mod_logmfx)\nglance(mod_logmfx)\n\n# another example, this time using probit regression\nmod_probmfx <- probitmfx(am ~ cyl + hp + wt, atmean = TRUE, data = mtcars)\n\ntidy(mod_probmfx, conf.int = TRUE)\naugment(mod_probmfx)\nglance(mod_probmfx)\n\\dontshow{\\}) # examplesIf}",
            "glance.mjoint": "\\dontshow{if (rlang::is_installed(\"joineRML\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# broom only skips running these examples because the example models take a\n# while to generate\u2014they should run just fine, though!\n\\dontrun{\n\n\n# load libraries for models and data\nlibrary(joineRML)\n\n# fit a joint model with bivariate longitudinal outcomes\ndata(heart.valve)\n\nhvd <- heart.valve[!is.na(heart.valve$log.grad) &\n  !is.na(heart.valve$log.lvmi) &\n  heart.valve$num <= 50, ]\n\nfit <- mjoint(\n  formLongFixed = list(\n    \"grad\" = log.grad ~ time + sex + hs,\n    \"lvmi\" = log.lvmi ~ time + sex\n  ),\n  formLongRandom = list(\n    \"grad\" = ~ 1 | num,\n    \"lvmi\" = ~ time | num\n  ),\n  formSurv = Surv(fuyrs, status) ~ age,\n  data = hvd,\n  inits = list(\"gamma\" = c(0.11, 1.51, 0.80)),\n  timeVar = \"time\"\n)\n\n# extract the survival fixed effects\ntidy(fit)\n\n# extract the longitudinal fixed effects\ntidy(fit, component = \"longitudinal\")\n\n# extract the survival fixed effects with confidence intervals\ntidy(fit, ci = TRUE)\n\n# extract the survival fixed effects with confidence intervals based\n# on bootstrapped standard errors\nbSE <- bootSE(fit, nboot = 5, safe.boot = TRUE)\ntidy(fit, boot_se = bSE, ci = TRUE)\n\n# augment original data with fitted longitudinal values and residuals\nhvd2 <- augment(fit)\n\n# extract model statistics\nglance(fit)\n}\n\\dontshow{\\}) # examplesIf}",
            "glance.mlogit": "\\dontshow{if (rlang::is_installed(\"mlogit\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(mlogit)\n\ndata(\"Fishing\", package = \"mlogit\")\nFish <- dfidx(Fishing, varying = 2:9, shape = \"wide\", choice = \"mode\")\n\n# fit model\nm <- mlogit(mode ~ price + catch | income, data = Fish)\n\n# summarize model fit with tidiers\ntidy(m)\naugment(m)\nglance(m)\n\\dontshow{\\}) # examplesIf}",
            "glance.muhaz": "\\dontshow{if (rlang::is_installed(c(\"muhaz\", \"survival\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(muhaz)\nlibrary(survival)\n\n# fit model\nx <- muhaz(ovarian$futime, ovarian$fustat)\n\n# summarize model fit with tidiers\ntidy(x)\nglance(x)\n\\dontshow{\\}) # examplesIf}",
            "glance.multinom": "\\dontshow{if (rlang::is_installed(c(\"nnet\", \"MASS\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(nnet)\nlibrary(MASS)\n\nexample(birthwt)\n\nbwt.mu <- multinom(low ~ ., bwt)\n\ntidy(bwt.mu)\nglance(bwt.mu)\n\n# or, for output from a multinomial logistic regression\nfit.gear <- multinom(gear ~ mpg + factor(am), data = mtcars)\ntidy(fit.gear)\nglance(fit.gear)\n\\dontshow{\\}) # examplesIf}",
            "glance.negbin": "\\dontshow{if (rlang::is_installed(\"MASS\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(MASS)\n\n# fit model\nr <- glm.nb(Days ~ Sex / (Age + Eth * Lrn), data = quine)\n\n# summarize model fit with tidiers\ntidy(r)\nglance(r)\n\\dontshow{\\}) # examplesIf}",
            "glance.nlrq": "\\dontshow{if (rlang::is_installed(\"quantreg\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load modeling library\nlibrary(quantreg)\n\n# build artificial data with multiplicative error\nset.seed(1)\ndat <- NULL\ndat$x <- rep(1:25, 20)\ndat$y <- SSlogis(dat$x, 10, 12, 2) * rnorm(500, 1, 0.1)\n\n# fit the median using nlrq\nmod <- nlrq(y ~ SSlogis(x, Asym, mid, scal),\n  data = dat, tau = 0.5, trace = TRUE\n)\n\n# summarize model fit with tidiers\ntidy(mod)\nglance(mod)\naugment(mod)\n\\dontshow{\\}) # examplesIf}",
            "glance.nls": "\\dontshow{if (rlang::is_installed(\"ggplot2\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# fit model\nn <- nls(mpg ~ k * e^wt, data = mtcars, start = list(k = 1, e = 2))\n\n# summarize model fit with tidiers + visualization\ntidy(n)\naugment(n)\nglance(n)\n\nlibrary(ggplot2)\n\nggplot(augment(n), aes(wt, mpg)) +\n  geom_point() +\n  geom_line(aes(y = .fitted))\n\nnewdata <- head(mtcars)\nnewdata$wt <- newdata$wt + 1\n\naugment(n, newdata = newdata)\n\\dontshow{\\}) # examplesIf}",
            "glance.orcutt": "\\dontshow{if (rlang::is_installed(\"orcutt\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(orcutt)\n\n# fit model and summarize results\nreg <- lm(mpg ~ wt + qsec + disp, mtcars)\ntidy(reg)\n\n\nco <- cochrane.orcutt(reg)\ntidy(co)\nglance(co)\n\\dontshow{\\}) # examplesIf}",
            "glance.pam": "\\dontshow{if (rlang::is_installed(c(\"cluster\", \"modeldata\", \"ggplot2\")) && identical(Sys.getenv(\"NOT_CRAN\"), \"true\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cluster)\nlibrary(modeldata)\ndata(hpc_data)\n\nx <- hpc_data[, 2:5]\np <- pam(x, k = 4)\n\n# summarize model fit with tidiers + visualization\ntidy(p)\nglance(p)\naugment(p, x)\n\naugment(p, x) \\%>\\%\n  ggplot(aes(compounds, input_fields)) +\n  geom_point(aes(color = .cluster)) +\n  geom_text(aes(label = cluster), data = tidy(p), size = 10)\n\\dontshow{\\}) # examplesIf}",
            "glance.plm": "\\dontshow{if (rlang::is_installed(\"plm\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(plm)\n\n# load data\ndata(\"Produc\", package = \"plm\")\n\n# fit model\nzz <- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n  data = Produc, index = c(\"state\", \"year\")\n)\n\n# summarize model fit with tidiers\nsummary(zz)\n\ntidy(zz)\ntidy(zz, conf.int = TRUE)\ntidy(zz, conf.int = TRUE, conf.level = 0.9)\n\naugment(zz)\nglance(zz)\n\\dontshow{\\}) # examplesIf}",
            "glance.poLCA": "\\dontshow{if (rlang::is_installed(c(\"poLCA\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(poLCA)\nlibrary(dplyr)\n\n# generate data\ndata(values)\n\nf <- cbind(A, B, C, D) ~ 1\n\n# fit model\nM1 <- poLCA(f, values, nclass = 2, verbose = FALSE)\n\nM1\n\n# summarize model fit with tidiers + visualization\ntidy(M1)\naugment(M1)\nglance(M1)\n\nlibrary(ggplot2)\n\nggplot(tidy(M1), aes(factor(class), estimate, fill = factor(outcome))) +\n  geom_bar(stat = \"identity\", width = 1) +\n  facet_wrap(~variable)\n\n# three-class model with a single covariate.\ndata(election)\n\nf2a <- cbind(\n  MORALG, CARESG, KNOWG, LEADG, DISHONG, INTELG,\n  MORALB, CARESB, KNOWB, LEADB, DISHONB, INTELB\n) ~ PARTY\n\nnes2a <- poLCA(f2a, election, nclass = 3, nrep = 5, verbose = FALSE)\n\ntd <- tidy(nes2a)\ntd\n\nggplot(td, aes(outcome, estimate, color = factor(class), group = class)) +\n  geom_line() +\n  facet_wrap(~variable, nrow = 2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\nau <- augment(nes2a)\n\nau\n\ncount(au, .class)\n\n# if the original data is provided, it leads to NAs in new columns\n# for rows that weren't predicted\nau2 <- augment(nes2a, data = election)\n\nau2\n\ndim(au2)\n\\dontshow{\\}) # examplesIf}",
            "glance.polr": "\\dontshow{if (rlang::is_installed(\"MASS\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(MASS)\n\n# fit model\nfit <- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)\n\n# summarize model fit with tidiers\ntidy(fit, exponentiate = TRUE, conf.int = TRUE)\n\nglance(fit)\naugment(fit, type.predict = \"class\")\n\nfit2 <- polr(factor(gear) ~ am + mpg + qsec, data = mtcars)\n\ntidy(fit, p.values = TRUE)\n\\dontshow{\\}) # examplesIf}",
            "glance.pyears": "\\dontshow{if (rlang::is_installed(\"survival\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(survival)\n\n# generate and format data\ntemp.yr <- tcut(mgus$dxyr, 55:92, labels = as.character(55:91))\ntemp.age <- tcut(mgus$age, 34:101, labels = as.character(34:100))\nptime <- ifelse(is.na(mgus$pctime), mgus$futime, mgus$pctime)\npstat <- ifelse(is.na(mgus$pctime), 0, 1)\npfit <- pyears(Surv(ptime / 365.25, pstat) ~ temp.yr + temp.age + sex, mgus,\n  data.frame = TRUE\n)\n\n# summarize model fit with tidiers\ntidy(pfit)\nglance(pfit)\n\n# if data.frame argument is not given, different information is present in\n# output\npfit2 <- pyears(Surv(ptime / 365.25, pstat) ~ temp.yr + temp.age + sex, mgus)\n\ntidy(pfit2)\nglance(pfit2)\n\\dontshow{\\}) # examplesIf}",
            "glance.ridgelm": "\\dontshow{if (rlang::is_installed(c(\"MASS\", \"ggplot2\"))) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# load libraries for models and data\nlibrary(MASS)\n\nnames(longley)[1] <- \"y\"\n\n# fit model and summarizd results\nfit1 <- lm.ridge(y ~ ., longley)\ntidy(fit1)\n\nfit2 <- lm.ridge(y ~ ., longley, lambda = seq(0.001, .05, .001))\ntd2 <- tidy(fit2)\ng2 <- glance(fit2)\n\n# coefficient plot\nlibrary(ggplot2)\nggplot(td2, aes(lambda, estimate, color = term)) +\n  geom_line()\n\n# GCV plot\nggplot(td2, aes(lambda, GCV)) +\n  geom_line()\n\n# add line for the GCV minimizing estimate\nggplot(td2, aes(lambda, GCV)) +\n  geom_line() +\n  geom_vline(xintercept = g2$lambdaGCV, col = \"red\", lty = 2)\n\\dontshow{\\}) # examplesIf}"
        }
    },
    "readxl": {
        "description": "Import excel files into R. Supports '.xls' via the embedded\n    'libxls' C library <https://github.com/libxls/libxls> and '.xlsx' via\n    the embedded 'RapidXML' C++ library <https://rapidxml.sourceforge.net/>.\n    Works on Windows, Mac and Linux without external dependencies.",
        "examples": {
            "cell-specification": "path <- readxl_example(\"geometry.xls\")\n## Rows 1 and 2 are empty (as are rows 7 and higher)\n## Column 1 aka \"A\" is empty (as are columns 5 of \"E\" and higher)\n\n# By default, the populated data cells are \"shrink-wrapped\" into a\n# minimal data frame\nread_excel(path)\n\n# Specific rectangle that is subset of populated cells, possibly improper\nread_excel(path, range = \"B3:D6\")\nread_excel(path, range = \"C3:D5\")\n\n# Specific rectangle that forces inclusion of unpopulated cells\nread_excel(path, range = \"A3:D5\")\nread_excel(path, range = \"A4:E5\")\nread_excel(path, range = \"C5:E7\")\n\n# Anchor a rectangle of specified size at a particular cell\nread_excel(path, range = anchored(\"C4\", dim = c(3, 2)), col_names = FALSE)\n\n# Specify only the rows\nread_excel(path, range = cell_rows(3:5))\n## is equivalent to\nread_excel(path, range = cell_rows(c(3, 5)))\n\n# Specify only the columns by column number or letter\nread_excel(path, range = cell_cols(\"C:D\"))\nread_excel(path, range = cell_cols(2))\n\n# Specify exactly one row or column bound\nread_excel(path, range = cell_rows(c(5, NA)))\nread_excel(path, range = cell_rows(c(NA, 4)))\nread_excel(path, range = cell_cols(c(\"C\", NA)))\nread_excel(path, range = cell_cols(c(NA, 2)))\n\n# General open rectangles\n# upper left = C4, everything else unspecified\nread_excel(path, range = cell_limits(c(4, 3), c(NA, NA)))\n# upper right = D4, everything else unspecified\nread_excel(path, range = cell_limits(c(4, NA), c(NA, 4)))",
            "excel_format": "files <- c(\n  \"a.xlsx\",\n  \"b.xls\",\n  \"c.png\",\n  file.path(R.home(\"doc\"), \"html\", \"logo.jpg\"),\n  readxl_example(\"clippy.xlsx\"),\n  readxl_example(\"deaths.xls\")\n)\nexcel_format(files)",
            "excel_sheets": "excel_sheets(readxl_example(\"datasets.xlsx\"))\nexcel_sheets(readxl_example(\"datasets.xls\"))\n\n# To load all sheets in a workbook, use lapply()\npath <- readxl_example(\"datasets.xls\")\nlapply(excel_sheets(path), read_excel, path = path)",
            "read_excel": "datasets <- readxl_example(\"datasets.xlsx\")\nread_excel(datasets)\n\n# Specify sheet either by position or by name\nread_excel(datasets, 2)\nread_excel(datasets, \"mtcars\")\n\n# Skip rows and use default column names\nread_excel(datasets, skip = 10, col_names = FALSE)\n\n# Recycle a single column type\nread_excel(datasets, col_types = \"text\")\n\n# Specify some col_types and guess others\nread_excel(\n  readxl_example(\"deaths.xlsx\"),\n  skip = 4, n_max = 10, col_names = TRUE,\n  col_types = c(\"text\", \"text\", \"guess\", \"guess\", \"guess\", \"guess\")\n)\n\n# Accomodate a column with disparate types via col_type = \"list\"\ndf <- read_excel(readxl_example(\"clippy.xlsx\"), col_types = c(\"text\", \"list\"))\ndf\ndf$value\nsapply(df$value, class)\n\n# Limit the number of data rows read\nread_excel(datasets, n_max = 3)\n\n# Read from an Excel range using A1 or R1C1 notation\nread_excel(datasets, range = \"C1:E7\")\nread_excel(datasets, range = \"R1C2:R2C5\")\n\n# Specify the sheet as part of the range\nread_excel(datasets, range = \"mtcars!B1:D5\")\n\n# Read only specific rows or columns\nread_excel(datasets, range = cell_rows(102:151), col_names = FALSE)\nread_excel(datasets, range = cell_cols(\"B:D\"))\n\n# Get a preview of column names\nnames(read_excel(readxl_example(\"datasets.xlsx\"), n_max = 0))\n\n# exploit full .name_repair flexibility from tibble\n\n# \"universal\" names are unique and syntactic\nread_excel(\n  readxl_example(\"deaths.xlsx\"),\n  range = \"arts!A5:F15\",\n  .name_repair = \"universal\"\n)\n\n# specify name repair as a built-in function\nread_excel(readxl_example(\"clippy.xlsx\"), .name_repair = toupper)\n\n# specify name repair as a custom function\nmy_custom_name_repair <- function(nms) tolower(gsub(\"[.]\", \"_\", nms))\nread_excel(\n  readxl_example(\"datasets.xlsx\"),\n  .name_repair = my_custom_name_repair\n)\n\n# specify name repair as an anonymous function\nread_excel(\n  readxl_example(\"datasets.xlsx\"),\n  sheet = \"chickwts\",\n  .name_repair = ~ substr(.x, start = 1, stop = 3)\n)",
            "readxl_example": "readxl_example()\nreadxl_example(\"datasets.xlsx\")"
        }
    },
    "rstudioapi": {
        "description": "Access the RStudio API (if available) and provide informative error\n    messages when it's not.",
        "examples": {
            "askForPassword": "\\dontrun{\nrstudioapi::askForPassword(\"Please enter your password\")\n}",
            "callFun": "if (rstudioapi::isAvailable()) {\n  rstudioapi::callFun(\"versionInfo\")\n}",
            "getDelegatedAzureToken": "\\dontrun{\ngetDelegatedAzureToken(\"https://storage.azure.com\")\n}",
            "hasColorConsole": "\\dontrun{\nif (rstudioapi::hasColorConsole()) {\n  message(\"RStudio console supports ANSI color sequences.\")\n}\n\n}",
            "hasFun": "rstudioapi::hasFun(\"viewer\")",
            "highlightUi": "\\dontrun{rstudioapi::highlightUi(\"#rstudio_workbench_panel_git\")}\n\n# clear current highlights\n\\dontrun{rstudioapi::highlightUi(\"\")}\n\n# highlight within an RMD\n\\dontrun{rstudioapi::highlightUi(\".rstudio_chunk_setup .rstudio_run_chunk\")}\n\n# Optionally provide a callback adjacent to \n# the queries that will be executed when the \n# highlighted element is clicked on.\n\\dontrun{rstudioapi::highlightUi(\n  list(\n    list(\n      query=\"#rstudio_workbench_panel_git\", \n      callback=\"rstudioapi::highlightUi('')\"\n    )\n  )\n)}",
            "isAvailable": "rstudioapi::isAvailable()\n\\dontrun{rstudioapi::verifyAvailable()}",
            "previewRd": "\\dontrun{\nrstudioapi::previewRd(\"~/MyPackage/man/foo.Rd\")\n}",
            "readRStudioPreference": "\\dontrun{\n# Get indentation settings\nspaces <- rstudioapi::readRStudioPreference(\"num_spaces_for_tab\", FALSE)\nmessage(\"Using \", spaces, \" per tab.\")\n}",
            "registerCommandCallback": "\\dontrun{\n# Set up a callback to display an encouraging dialog whenever \n# the user knits a document\nhandle <- rstudioapi::registerCommandCallback(\n  \"knitDocument\", \n  function() {\n    rstudioapi::showDialog(\n      \"Achievement\",\n      \"Congratulations, you have knitted a document. Well done.\"\n    )\n  })\n\n# Knit the document interactively and observe the dialog\n\n# Later: Unregister the callback\nrstudioapi::unregisterCommandCallback(handle)\n}",
            "registerCommandStreamCallback": "\\dontrun{\n# Set up a callback to print the ID of commands executed to the console.\nhandle <- rstudioapi::registerCommandStreamCallback(function(id) {\n  message(\"Command executed: \", id)\n})\n\n# Later: Unregister the callback\nrstudioapi::unregisterCommandCallback(handle)\n}",
            "sendToConsole": "\\dontrun{\nrstudioapi::sendToConsole(\".Platform\", execute = FALSE, animate = TRUE)\n}",
            "showDialog": "if (rstudioapi::isAvailable()) {\n  rstudioapi::showDialog(\"Example Dialog\", \"This is an <b>example</b> dialog.\")\n}",
            "terminalActivate": "\\dontrun{\n# create a hidden terminal and run a lengthy command\ntermId = rstudioapi::terminalCreate(show = FALSE)\nrstudioapi::terminalSend(termId, \"sleep 5\\n\")\n\n# wait until a busy terminal is finished\nwhile (rstudioapi::terminalBusy(termId)) {\n  Sys.sleep(0.1)\n}\nprint(\"Terminal available\")#'\n\nrstudioapi::terminalActivate(termId)\n}",
            "terminalBusy": "\\dontrun{\n# create a hidden terminal and run a lengthy command\ntermId <- rstudioapi::terminalCreate(show = FALSE)\nrstudioapi::terminalSend(termId, \"sleep 5\\n\")\n\n# wait until a busy terminal is finished\nwhile (rstudioapi::terminalBusy(termId)) {\n  Sys.sleep(0.1)\n}\nprint(\"Terminal available\")\n}",
            "terminalClear": "\\dontrun{\ntermId <- rstudioapi::terminalCreate()\nrstudioapi::terminalSend(termId, 'ls -l\\n')\nSys.sleep(3)\nrstudioapi::terminalClear(termId)\n}",
            "terminalContext": "\\dontrun{\ntermId <- rstudioapi::terminalCreate(\"example\", show = FALSE)\nView(rstudioapi::terminalContext(termId))\n\n}",
            "terminalCreate": "\\dontrun{\ntermId <- rstudioapi::terminalCreate('My Terminal')\n}",
            "terminalExecute": "\\dontrun{\ntermId <- rstudioapi::terminalExecute(\n  command = 'echo $HELLO && echo $WORLD',\n  workingDir = '/usr/local',\n  env = c('HELLO=WORLD', 'WORLD=EARTH'),\n  show = FALSE)\n\nwhile (is.null(rstudioapi::terminalExitCode(termId))) {\n  Sys.sleep(0.1)\n}\n\nresult <- terminalBuffer(termId)\nterminalKill(termId)\nprint(result)\n}",
            "terminalRunning": "\\dontrun{\n# termId has a handle to a previously created terminal\n# make sure it is still running before we send it a command\nif (!rstudioapi::terminalRunning(termId)) {\n   rstudioapi::terminalActivate(termId))\n\n   # wait for it to start\n   while (!rstudioapi::terminalRunning(termId)) {\n      Sys.sleep(0.1)\n   }\n\n   terminalSend(termId, \"echo Hello\\n\")\n}\n}",
            "terminalSend": "\\dontrun{\ntermId <- rstudioapi::terminalCreate()\nrstudioapi::terminalSend(termId, 'ls -l\\n')\n}",
            "versionInfo": "\\dontrun{\ninfo <- rstudioapi::versionInfo()\n\n# check what version of RStudio is in use\nif (info$version >= \"1.4\") {\n  # code specific to versions of RStudio 1.4 and newer\n}\n\n# check whether RStudio Desktop or RStudio Server is being used\nif (info$mode == \"desktop\") {\n  # code specific to RStudio Desktop\n}\n\n# Get the citation\ninfo$citation\n\n}",
            "viewer": "\\dontrun{\n\n# run an application inside the IDE\nrstudioapi::viewer(\"http://localhost:8100\")\n\n# run an application and request a height of 500 pixels\nrstudioapi::viewer(\"http://localhost:8100\", height = 500)\n\n# use 'viewer' option if set, or `utils::browseURL()` if unset\nviewer <- getOption(\"viewer\", default = utils::browseURL)\nviewer(\"http://localhost:8100\")\n\n# generate a temporary html file and display it\ndir <- tempfile()\ndir.create(dir)\nhtmlFile <- file.path(dir, \"index.html\")\n# (code to write some content to the file)\nrstudioapi::viewer(htmlFile)\n\n}",
            "writeRStudioPreference": "\\dontrun{\n# Hide RStudio's toolbar.\nrstudioapi::writeRStudioPreference(\"toolbar_visible\", FALSE)\n}"
        }
    },
    "aws.s3": {
        "description": "A simple client package for the Amazon Web Services ('AWS') Simple\n    Storage Service ('S3') 'REST' 'API' <https://aws.amazon.com/s3/>.",
        "examples": {
            "acceleration": "\\dontrun{\nb <- bucketlist()\nget_acceleration(b[[1]])\nput_acceleration(b[[1]], \"Enabled\")\nget_acceleration(b[[1]])\nput_acceleration(b[[1]], \"Suspended\")\n}",
            "encryption": "\\dontrun{\n # example bucket\n put_bucket(\"mybucket\")\n\n # set and check encryption\n put_encryption(\"mybucket\", \"AES256\")\n get_encryption(\"mybucket\")\n\n # delete encryption\n delete_encryption(\"mybucket\")\n}",
            "get_bucket": "\\dontrun{\n  # basic usage\n  b <- bucketlist()\n  get_bucket(b[1,1])\n  get_bucket_df(b[1,1])\n\n  # bucket names with dots\n  ## this (default) should work:\n  get_bucket(\"this.bucket.has.dots\", url_style = \"path\")\n  ## this probably wont:\n  #get_bucket(\"this.bucket.has.dots\", url_style = \"virtual\")\n}",
            "get_object": "\\dontrun{\n  # get an object in memory\n  ## create bucket\n  b <- put_bucket(\"myexamplebucket\")\n  \n  ## save a dataset to the bucket\n  s3save(mtcars, bucket = b, object = \"mtcars\")\n  obj <- get_bucket(b)\n  ## get the object in memory\n  x <- get_object(obj[[1]])\n  load(rawConnection(x))\n  \"mtcars\" \\%in\\% ls()\n\n  # save an object locally\n  y <- save_object(obj[[1]], file = object[[1]][[\"Key\"]])\n  y \\%in\\% dir()\n\n  # return object using 'S3 URI' syntax, with progress bar\n  get_object(\"s3://myexamplebucket/mtcars\", show_progress = TRUE)\n\n  # return parts of an object\n  ## use 'Range' header to specify bytes\n  get_object(object = obj[[1]], headers = list('Range' = 'bytes=1-120'))\n \n  # example of streaming connection\n  ## setup a bucket and object\n  b <- put_bucket(\"myexamplebucket\")\n  s3write_using(mtcars, bucket = b, object = \"mtcars.csv\", FUN = utils::write.csv)\n  \n  ## setup the connection\n  con <- s3connection(\"mtcars.csv\", bucket = b)\n  \n  ## line-by-line read\n  while(length(x <- readLines(con, n = 1L))) {\n    print(x)\n  }\n\n  ## use data.table::fread without saving object to file\n  library(data.table)\n  s3write_using(as.data.table(mtcars), bucket = b, object = \"mtcars2.csv\", FUN = data.table::fwrite)\n  fread(get_object(\"mtcars2.csv\", bucket = b, as = \"text\"))\n\n  ## cleanup\n  close(con)\n  delete_bucket(\"myexamplebucket\")\n}",
            "head_object": "\\dontrun{\n  # get an object in memory\n  ## create bucket\n  b <- put_bucket(\"myexamplebucket\")\n  \n  ## save a dataset to the bucket\n  s3save(mtcars, bucket = b, object = \"mtcars\")\n  \n  # check that object exists\n  object_exists(\"mtcars\", \"myexamplebucket\")\n  object_exists(\"s3://myexamplebucket/mtcars\")\n  \n  # get the object's size\n  object_size(\"s3://myexamplebucket/mtcars\")\n  \n  # get the object\n  get_object(\"s3://myexamplebucket/mtcars\")\n}",
            "put_bucket": "\\dontrun{\n  put_bucket(\"examplebucket\")\n  \n  # set a \"canned\" ACL to, e.g., make bucket publicly readable\n  put_bucket(\"examplebucket\", headers = list(`x-amz-acl` = \"public-read\")\n\n}",
            "put_object": "\\dontrun{\n  library(\"datasets\")\n  \n  # write file to S3\n  tmp <- tempfile()\n  on.exit(unlink(tmp))\n  utils::write.csv(mtcars, file = tmp)\n  # put object with an upload progress bar\n  put_object(file = tmp, object = \"mtcars.csv\", bucket = \"myexamplebucket\", show_progress = TRUE)\n\n  # create a \"folder\" in a bucket (NOT required! Folders are really just 0-length files)\n  put_folder(\"example\", bucket = \"myexamplebucket\")\n  ## write object to the \"folder\"\n  put_object(file = tmp, object = \"example/mtcars.csv\", bucket = \"myexamplebucket\")\n\n  # write serialized, in-memory object to S3\n  x <- rawConnection(raw(), \"w\")\n  utils::write.csv(mtcars, x)\n  put_object(rawConnectionValue(x), object = \"mtcars.csv\", bucket = \"myexamplebucketname\")\n\n  # use `headers` for server-side encryption\n  ## require appropriate bucket policy\n  ## encryption can also be set at the bucket-level using \\code{\\link{put_encryption}}\n  put_object(file = tmp, object = \"mtcars.csv\", bucket = \"myexamplebucket\",\n             headers = c('x-amz-server-side-encryption' = 'AES256'))\n\n  # alternative \"S3 URI\" syntax:\n  put_object(rawConnectionValue(x), object = \"s3://myexamplebucketname/mtcars.csv\")\n  close(x)\n\n  # read the object back from S3\n  read.csv(text = rawToChar(get_object(object = \"s3://myexamplebucketname/mtcars.csv\")))\n\n  # multi-part uploads for objects over 5MB\n  \\donttest{\n  x <- rnorm(3e6)\n  saveRDS(x, tmp)\n  put_object(file = tmp, object = \"rnorm.rds\", bucket = \"myexamplebucket\",\n             show_progress = TRUE, multipart = TRUE, partsize=1e6)\n  identical(x, s3readRDS(\"s3://myexamplebucket/rnorm.rds\"))\n  }\n}",
            "s3read_using": "\\dontrun{\nlibrary(\"datasets\")\n# create bucket\nb <- put_bucket(\"myexamplebucket\")\n\n# save a dataset to the bucket as a csv\nif (require(\"utils\")) {\n  s3write_using(mtcars, FUN = write.csv, object = \"mtcars.csv\", bucket = b)\n}\n\n# load dataset from the bucket as a csv\nif (require(\"utils\")) {\n  s3read_using(FUN = read.csv, object = \"mtcars.csv\", bucket = b)\n}\n\n# cleanup\ndelete_object(object = \"mtcars.csv\", bucket = b)\ndelete_bucket(bucket = b)\n}",
            "s3save": "\\dontrun{\n# create bucket\nb <- put_bucket(\"myexamplebucket\")\n\n# save a dataset to the bucket\ns3save(mtcars, iris, object = \"somedata.Rdata\", bucket = b)\nget_bucket(b)\n\n# load the data from bucket\ne <- new.env()\ns3load(object = \"somedata.Rdata\", bucket = b, envir = e)\nls(e)\n\n# cleanup\nrm(e)\ndelete_object(object = \"somedata.Rdata\", bucket = \"myexamplebucket\")\ndelete_bucket(\"myexamplebucket\")\n}",
            "s3saveRDS": "\\dontrun{\n# create bucket\nb <- put_bucket(\"myexamplebucket\")\n\n# save a single object to s3\ns3saveRDS(x = mtcars, bucket = \"myexamplebucket\", object = \"mtcars.rds\")\n\n# restore it under a different name\nmtcars2 <- s3readRDS(object = \"mtcars.rds\", bucket = \"myexamplebucket\")\nidentical(mtcars, mtcars2)\n\n# cleanup\ndelete_object(object = \"mtcars.rds\", bucket = \"myexamplebucket\")\ndelete_bucket(\"myexamplebucket\")\n}",
            "s3source": "\\dontrun{\n# create bucket\nb <- put_bucket(\"myexamplebucket\")\n\n# save some code to the bucket\ncat(\"x <- 'hello world!'\\nx\", file = \"example.R\")\nput_object(\"example.R\", object = \"example.R\", bucket = b)\nget_bucket(b)\n\n# source the code from the bucket\ns3source(object = \"example.R\", bucket = b, echo = TRUE)\n\n# cleanup\nunlink(\"example.R\")\ndelete_object(object = \"example.R\", bucket = b)\ndelete_bucket(\"myexamplebucket\")\n}",
            "sync": "\\dontrun{\n  put_bucket(\"examplebucket\")\n\n  # sync all files in current directory to bucket (upload-only)\n  s3sync(bucket = \"examplebucket\", direction = \"upload\")\n\n  # two-way sync\n  s3sync(bucket = \"examplebucket\")\n\n  # full sync between a subset of the bucket and a test directory in user's home\n  # corresponding roughly to:\n  #   aws s3 sync ~/test s3://examplebucket/test/\n  #   aws s3 sync s3://examplebucket/test/ ~/test\n  s3sync(\"~/test\", \"examplebucket\", prefix=\"test/\", region=\"us-east-2\")\n}",
            "tagging": "\\dontrun{\n put_tagging(\"mybucket\", tags = list(foo = \"1\", bar = \"2\"))\n get_tagging(\"mybucket\")\n delete_tagging(\"mybucket\")\n}",
            "versions": "\\dontrun{\n put_versioning(\"mybucket\")\n get_versioning(\"mybucket\")\n get_versions(\"mybucket\")\n}"
        }
    },
    "plyr": {
        "description": "A set of tools that solves a common set of problems: you need\n    to break a big problem down into manageable pieces, operate on each\n    piece and then put all the pieces back together.  For example, you\n    might want to fit a model to each spatial location or time point in\n    your study, summarise data by panels or collapse high-dimensional\n    arrays to simpler summary statistics. The development of 'plyr' has\n    been generously supported by 'Becton Dickinson'.",
        "examples": {
            "aaply": "dim(ozone)\naaply(ozone, 1, mean)\naaply(ozone, 1, mean, .drop = FALSE)\naaply(ozone, 3, mean)\naaply(ozone, c(1,2), mean)\n\ndim(aaply(ozone, c(1,2), mean))\ndim(aaply(ozone, c(1,2), mean, .drop = FALSE))\n\naaply(ozone, 1, each(min, max))\naaply(ozone, 3, each(min, max))\n\nstandardise <- function(x) (x - min(x)) / (max(x) - min(x))\naaply(ozone, 3, standardise)\naaply(ozone, 1:2, standardise)\n\naaply(ozone, 1:2, diff)",
            "alply": "alply(ozone, 3, quantile)\nalply(ozone, 3, function(x) table(round(x)))",
            "arrange": "# sort mtcars data by cylinder and displacement\nmtcars[with(mtcars, order(cyl, disp)), ]\n# Same result using arrange: no need to use with(), as the context is implicit\n# NOTE: plyr functions do NOT preserve row.names\narrange(mtcars, cyl, disp)\n# Let's keep the row.names in this example\nmyCars = cbind(vehicle=row.names(mtcars), mtcars)\narrange(myCars, cyl, disp)\n# Sort with displacement in descending order\narrange(myCars, cyl, desc(disp))",
            "as.quoted": "as.quoted(c(\"a\", \"b\", \"log(d)\"))\nas.quoted(a ~ b + log(d))",
            "baseball": "baberuth <- subset(baseball, id == \"ruthba01\")\nbaberuth$cyear <- baberuth$year - min(baberuth$year) + 1\n\ncalculate_cyear <- function(df) {\n  mutate(df,\n    cyear = year - min(year),\n    cpercent = cyear / (max(year) - min(year))\n  )\n}\n\nbaseball <- ddply(baseball, .(id), calculate_cyear)\nbaseball <- subset(baseball, ab >= 25)\n\nmodel <- function(df) {\n  lm(rbi / ab ~ cyear, data=df)\n}\nmodel(baberuth)\nmodels <- dlply(baseball, .(id), model)",
            "colwise": "# Count number of missing values\nnmissing <- function(x) sum(is.na(x))\n\n# Apply to every column in a data frame\ncolwise(nmissing)(baseball)\n# This syntax looks a little different.  It is shorthand for the\n# the following:\nf <- colwise(nmissing)\nf(baseball)\n\n# This is particularly useful in conjunction with d*ply\nddply(baseball, .(year), colwise(nmissing))\n\n# To operate only on specified columns, supply them as the second\n# argument.  Many different forms are accepted.\nddply(baseball, .(year), colwise(nmissing, .(sb, cs, so)))\nddply(baseball, .(year), colwise(nmissing, c(\"sb\", \"cs\", \"so\")))\nddply(baseball, .(year), colwise(nmissing, ~ sb + cs + so))\n\n# Alternatively, you can specify a boolean function that determines\n# whether or not a column should be included\nddply(baseball, .(year), colwise(nmissing, is.character))\nddply(baseball, .(year), colwise(nmissing, is.numeric))\nddply(baseball, .(year), colwise(nmissing, is.discrete))\n\n# These last two cases are particularly common, so some shortcuts are\n# provided:\nddply(baseball, .(year), numcolwise(nmissing))\nddply(baseball, .(year), catcolwise(nmissing))\n\n# You can supply additional arguments to either colwise, or the function\n# it generates:\nnumcolwise(mean)(baseball, na.rm = TRUE)\nnumcolwise(mean, na.rm = TRUE)(baseball)",
            "count": "# Count of each value of \"id\" in the first 100 cases\ncount(baseball[1:100,], vars = \"id\")\n# Count of ids, weighted by their \"g\" loading\ncount(baseball[1:100,], vars = \"id\", wt_var = \"g\")\ncount(baseball, \"id\", \"ab\")\ncount(baseball, \"lg\")\n# How many stints do players do?\ncount(baseball, \"stint\")\n# Count of times each player appeared in each of the years they played\ncount(baseball[1:100,], c(\"id\", \"year\"))\n# Count of counts\ncount(count(baseball[1:100,], c(\"id\", \"year\")), \"id\", \"freq\")\ncount(count(baseball, c(\"id\", \"year\")), \"freq\")",
            "create_progress_bar": "# No progress bar\nl_ply(1:100, identity, .progress = \"none\")\n\\dontrun{\n# Use the Tcl/Tk interface\nl_ply(1:100, identity, .progress = \"tk\")\n}\n# Text-based progress (|======|)\nl_ply(1:100, identity, .progress = \"text\")\n# Choose a progress character, run a length of time you can see\nl_ply(1:10000, identity, .progress = progress_text(char = \".\"))",
            "daply": "daply(baseball, .(year), nrow)\n\n# Several different ways of summarising by variables that should not be\n# included in the summary\n\ndaply(baseball[, c(2, 6:9)], .(year), colwise(mean))\ndaply(baseball[, 6:9], .(baseball$year), colwise(mean))\ndaply(baseball, .(year), function(df) colwise(mean)(df[, 6:9]))",
            "ddply": "# Summarize a dataset by two variables\ndfx <- data.frame(\n  group = c(rep('A', 8), rep('B', 15), rep('C', 6)),\n  sex = sample(c(\"M\", \"F\"), size = 29, replace = TRUE),\n  age = runif(n = 29, min = 18, max = 54)\n)\n\n# Note the use of the '.' function to allow\n# group and sex to be used without quoting\nddply(dfx, .(group, sex), summarize,\n mean = round(mean(age), 2),\n sd = round(sd(age), 2))\n\n# An example using a formula for .variables\nddply(baseball[1:100,], ~ year, nrow)\n# Applying two functions; nrow and ncol\nddply(baseball, .(lg), c(\"nrow\", \"ncol\"))\n\n# Calculate mean runs batted in for each year\nrbi <- ddply(baseball, .(year), summarise,\n  mean_rbi = mean(rbi, na.rm = TRUE))\n# Plot a line chart of the result\nplot(mean_rbi ~ year, type = \"l\", data = rbi)\n\n# make new variable career_year based on the\n# start year for each player (id)\nbase2 <- ddply(baseball, .(id), mutate,\n career_year = year - min(year) + 1\n)",
            "desc": "desc(1:10)\ndesc(factor(letters))\nfirst_day <- seq(as.Date(\"1910/1/1\"), as.Date(\"1920/1/1\"), \"years\")\ndesc(first_day)",
            "dlply": "linmod <- function(df) {\n  lm(rbi ~ year, data = mutate(df, year = year - min(year)))\n}\nmodels <- dlply(baseball, .(id), linmod)\nmodels[[1]]\n\ncoef <- ldply(models, coef)\nwith(coef, plot(`(Intercept)`, year))\nqual <- laply(models, function(mod) summary(mod)$r.squared)\nhist(qual)",
            "each": "# Call min() and max() on the vector 1:10\neach(min, max)(1:10)\n# This syntax looks a little different.  It is shorthand for the\n# the following:\nf<- each(min, max)\nf(1:10)\n# Three equivalent ways to call min() and max() on the vector 1:10\neach(\"min\", \"max\")(1:10)\neach(c(\"min\", \"max\"))(1:10)\neach(c(min, max))(1:10)\n# Call length(), min() and max() on a random normal vector\neach(length, mean, var)(rnorm(100))",
            "failwith": "f <- function(x) if (x == 1) stop(\"Error!\") else 1\n\\dontrun{\nf(1)\nf(2)\n}\n\nsafef <- failwith(NULL, f)\nsafef(1)\nsafef(2)",
            "here": "df <- data.frame(a = rep(c(\"a\",\"b\"), each = 10), b = 1:20)\nf1 <- function(label) {\n   ddply(df, \"a\", mutate, label = paste(label, b))\n}\n\\dontrun{f1(\"name:\")}\n# Doesn't work because mutate can't find label in the current scope\n\nf2 <- function(label) {\n   ddply(df, \"a\", here(mutate), label = paste(label, b))\n}\nf2(\"name:\")\n# Works :)",
            "idata.frame": "system.time(dlply(baseball, \"id\", nrow))\nsystem.time(dlply(idata.frame(baseball), \"id\", nrow))",
            "is.discrete": "is.discrete(1:10)\nis.discrete(c(\"a\", \"b\", \"c\"))\nis.discrete(factor(c(\"a\", \"b\", \"c\")))",
            "join": "first <- ddply(baseball, \"id\", summarise, first = min(year))\nsystem.time(b2 <- merge(baseball, first, by = \"id\", all.x = TRUE))\nsystem.time(b3 <- join(baseball, first, by = \"id\"))\n\nb2 <- arrange(b2, id, year, stint)\nb3 <- arrange(b3, id, year, stint)\nstopifnot(all.equal(b2, b3))",
            "join_all": "dfs <- list(\n  a = data.frame(x = 1:10, a = runif(10)),\n  b = data.frame(x = 1:10, b = runif(10)),\n  c = data.frame(x = 1:10, c = runif(10))\n)\njoin_all(dfs)\njoin_all(dfs, \"x\")",
            "l_ply": "l_ply(llply(mtcars, round), table, .print = TRUE)\nl_ply(baseball, function(x) print(summary(x)))",
            "laply": "laply(baseball, is.factor)\n# cf\nldply(baseball, is.factor)\ncolwise(is.factor)(baseball)\n\nlaply(seq_len(10), identity)\nlaply(seq_len(10), rep, times = 4)\nlaply(seq_len(10), matrix, nrow = 2, ncol = 2)",
            "llply": "llply(llply(mtcars, round), table)\nllply(baseball, summary)\n# Examples from ?lapply\nx <- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE))\n\nllply(x, mean)\nllply(x, quantile, probs = 1:3/4)",
            "maply": "maply(cbind(mean = 1:5, sd = 1:5), rnorm, n = 5)\nmaply(expand.grid(mean = 1:5, sd = 1:5), rnorm, n = 5)\nmaply(cbind(1:5, 1:5), rnorm, n = 5)",
            "mapvalues": "x <- c(\"a\", \"b\", \"c\")\nmapvalues(x, c(\"a\", \"c\"), c(\"A\", \"C\"))\n\n# Works on factors\ny <- factor(c(\"a\", \"b\", \"c\", \"a\"))\nmapvalues(y, c(\"a\", \"c\"), c(\"A\", \"C\"))\n\n# Works on numeric vectors\nz <- c(1, 4, 5, 9)\nmapvalues(z, from = c(1, 5, 9), to = c(10, 50, 90))",
            "match_df": "# count the occurrences of each id in the baseball dataframe, then get the subset with a freq >25\nlongterm <- subset(count(baseball, \"id\"), freq > 25)\n# longterm\n#             id freq\n# 30   ansonca01   27\n# 48   baineha01   27\n# ...\n# Select only rows from these longterm players from the baseball dataframe\n# (match would default to match on shared column names, but here was explicitly set \"id\")\nbb_longterm <- match_df(baseball, longterm, on=\"id\")\nbb_longterm[1:5,]",
            "mdply": "mdply(data.frame(mean = 1:5, sd = 1:5), rnorm, n = 2)\nmdply(expand.grid(mean = 1:5, sd = 1:5), rnorm, n = 2)\nmdply(cbind(mean = 1:5, sd = 1:5), rnorm, n = 5)\nmdply(cbind(mean = 1:5, sd = 1:5), as.data.frame(rnorm), n = 5)",
            "mlply": "mlply(cbind(1:4, 4:1), rep)\nmlply(cbind(1:4, times = 4:1), rep)\n\nmlply(cbind(1:4, 4:1), seq)\nmlply(cbind(1:4, length = 4:1), seq)\nmlply(cbind(1:4, by = 4:1), seq, to = 20)",
            "mutate": "# Examples from transform\nmutate(airquality, Ozone = -Ozone)\nmutate(airquality, new = -Ozone, Temp = (Temp - 32) / 1.8)\n\n# Things transform can't do\nmutate(airquality, Temp = (Temp - 32) / 1.8, OzT = Ozone / Temp)\n\n# mutate is rather faster than transform\nsystem.time(transform(baseball, avg_ab = ab / g))\nsystem.time(mutate(baseball, avg_ab = ab / g))",
            "name_rows": "name_rows(mtcars)\nname_rows(name_rows(mtcars))\n\ndf <- data.frame(a = sample(10))\narrange(df, a)\narrange(name_rows(df), a)\nname_rows(arrange(name_rows(df), a))",
            "ozone": "value <- ozone[1, 1, ]\ntime <- 1:72\nmonth.abbr <- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\",\n \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\nmonth <- factor(rep(month.abbr, length = 72), levels = month.abbr)\nyear <- rep(1:6, each = 12)\ndeseasf <- function(value) lm(value ~ month - 1)\n\nmodels <- alply(ozone, 1:2, deseasf)\ncoefs <- laply(models, coef)\ndimnames(coefs)[[3]] <- month.abbr\nnames(dimnames(coefs))[3] <- \"month\"\n\ndeseas <- laply(models, resid)\ndimnames(deseas)[[3]] <- 1:72\nnames(dimnames(deseas))[3] <- \"time\"\n\ndim(coefs)\ndim(deseas)",
            "progress_none": "l_ply(1:100, identity, .progress = \"none\")",
            "progress_text": "l_ply(1:100, identity, .progress = \"text\")\nl_ply(1:100, identity, .progress = progress_text(char = \"-\"))",
            "progress_time": "l_ply(1:100, function(x) Sys.sleep(.01), .progress = \"time\")",
            "progress_tk": "\\dontrun{\nl_ply(1:100, identity, .progress = \"tk\")\nl_ply(1:100, identity, .progress = progress_tk(width=400))\nl_ply(1:100, identity, .progress = progress_tk(label=\"\"))\n}",
            "progress_win": "\\dontrun{\nl_ply(1:100, identity, .progress = \"win\")\nl_ply(1:100, identity, .progress = progress_win(title=\"Working...\"))\n}",
            "quoted": ".(a, b, c)\n.(first = a, second = b, third = c)\n.(a ^ 2, b - d, log(c))\nas.quoted(~ a + b + c)\nas.quoted(a ~ b + c)\nas.quoted(c(\"a\", \"b\", \"c\"))\n\n# Some examples using ddply - look at the column names\nddply(mtcars, \"cyl\", each(nrow, ncol))\nddply(mtcars, ~ cyl, each(nrow, ncol))\nddply(mtcars, .(cyl), each(nrow, ncol))\nddply(mtcars, .(log(cyl)), each(nrow, ncol))\nddply(mtcars, .(logcyl = log(cyl)), each(nrow, ncol))\nddply(mtcars, .(vs + am), each(nrow, ncol))\nddply(mtcars, .(vsam = vs + am), each(nrow, ncol))",
            "r_ply": "r_ply(10, plot(runif(50)))\nr_ply(25, hist(runif(1000)))",
            "raply": "raply(100, mean(runif(100)))\nraply(100, each(mean, var)(runif(100)))\n\nraply(10, runif(4))\nraply(10, matrix(runif(4), nrow=2))\n\n# See the central limit theorem in action\nhist(raply(1000, mean(rexp(10))))\nhist(raply(1000, mean(rexp(100))))\nhist(raply(1000, mean(rexp(1000))))",
            "rbind.fill": "rbind.fill(mtcars[c(\"mpg\", \"wt\")], mtcars[c(\"wt\", \"cyl\")])",
            "rbind.fill.matrix": "A <- matrix (1:4, 2)\nB <- matrix (6:11, 2)\nA\nB\nrbind.fill.matrix (A, B)\n\ncolnames (A) <- c (3, 1)\nA\nrbind.fill.matrix (A, B)\n\nrbind.fill.matrix (A, 99)",
            "rdply": "rdply(20, mean(runif(100)))\nrdply(20, each(mean, var)(runif(100)))\nrdply(20, data.frame(x = runif(2)))",
            "rename": "x <- c(\"a\" = 1, \"b\" = 2, d = 3, 4)\n# Rename column d to \"c\", updating the variable \"x\" with the result\nx <- rename(x, replace = c(\"d\" = \"c\"))\nx\n# Rename column \"disp\" to \"displacement\"\nrename(mtcars, c(\"disp\" = \"displacement\"))",
            "revalue": "x <- c(\"a\", \"b\", \"c\")\nrevalue(x, c(a = \"A\", c = \"C\"))\nrevalue(x, c(\"a\" = \"A\", \"c\" = \"C\"))\n\ny <- factor(c(\"a\", \"b\", \"c\", \"a\"))\nrevalue(y, c(a = \"A\", c = \"C\"))",
            "rlply": "mods <- rlply(100, lm(y ~ x, data=data.frame(x=rnorm(100), y=rnorm(100))))\nhist(laply(mods, function(x) summary(x)$r.squared))",
            "round_any": "round_any(135, 10)\nround_any(135, 100)\nround_any(135, 25)\nround_any(135, 10, floor)\nround_any(135, 100, floor)\nround_any(135, 25, floor)\nround_any(135, 10, ceiling)\nround_any(135, 100, ceiling)\nround_any(135, 25, ceiling)\n\nround_any(Sys.time() + 1:10, 5)\nround_any(Sys.time() + 1:10, 5, floor)\nround_any(Sys.time(), 3600)",
            "splat": "hp_per_cyl <- function(hp, cyl, ...) hp / cyl\nsplat(hp_per_cyl)(mtcars[1,])\nsplat(hp_per_cyl)(mtcars)\n\nf <- function(mpg, wt, ...) data.frame(mw = mpg / wt)\nddply(mtcars, .(cyl), splat(f))",
            "split_indices": "split_indices(sample(10, 100, rep = TRUE))\nsplit_indices(sample(10, 100, rep = TRUE), 10)",
            "splitter_a": "plyr:::splitter_a(mtcars, 1)\nplyr:::splitter_a(mtcars, 2)\n\nplyr:::splitter_a(ozone, 2)\nplyr:::splitter_a(ozone, 3)\nplyr:::splitter_a(ozone, 1:2)",
            "splitter_d": "plyr:::splitter_d(mtcars, .(cyl))\nplyr:::splitter_d(mtcars, .(vs, am))\nplyr:::splitter_d(mtcars, .(am, vs))\n\nmtcars$cyl2 <- factor(mtcars$cyl, levels = c(2, 4, 6, 8, 10))\nplyr:::splitter_d(mtcars, .(cyl2), drop = TRUE)\nplyr:::splitter_d(mtcars, .(cyl2), drop = FALSE)\n\nmtcars$cyl3 <- ifelse(mtcars$vs == 1, NA, mtcars$cyl)\nplyr:::splitter_d(mtcars, .(cyl3))\nplyr:::splitter_d(mtcars, .(cyl3, vs))\nplyr:::splitter_d(mtcars, .(cyl3, vs), drop = FALSE)",
            "strip_splits": "dlply(mtcars, c(\"vs\", \"am\"))\ndlply(mtcars, c(\"vs\", \"am\"), strip_splits)",
            "summarise": "# Let's extract the number of teams and total period of time\n# covered by the baseball dataframe\nsummarise(baseball,\n duration = max(year) - min(year),\n nteams = length(unique(team)))\n# Combine with ddply to do that for each separate id\nddply(baseball, \"id\", summarise,\n duration = max(year) - min(year),\n nteams = length(unique(team)))",
            "take": "x <- array(seq_len(3 * 4 * 5), c(3, 4, 5))\ntake(x, 3, 1)\ntake(x, 2, 1)\ntake(x, 1, 1)\ntake(x, 3, 1, drop = TRUE)\ntake(x, 2, 1, drop = TRUE)\ntake(x, 1, 1, drop = TRUE)",
            "vaggregate": "# Some examples of use borrowed from ?tapply\nn <- 17; fac <- factor(rep(1:3, length.out = n), levels = 1:5)\ntable(fac)\nvaggregate(1:n, fac, sum)\nvaggregate(1:n, fac, sum, .default = NA_integer_)\nvaggregate(1:n, fac, range)\nvaggregate(1:n, fac, range, .default = c(NA, NA) + 0)\nvaggregate(1:n, fac, quantile)\n# Unlike tapply, vaggregate does not support multi-d output:\ntapply(warpbreaks$breaks, warpbreaks[,-1], sum)\nvaggregate(warpbreaks$breaks, id(warpbreaks[,-1]), sum)\n\n# But it is about 10x faster\nx <- rnorm(1e6)\ny1 <- sample.int(10, 1e6, replace = TRUE)\nsystem.time(tapply(x, y1, mean))\nsystem.time(vaggregate(x, y1, mean))"
        }
    },
    "generics": {
        "description": "In order to reduce potential package dependencies and\n    conflicts, generics provides a number of commonly used S3 generics.",
        "examples": {
            "coercion-factor": "as.factor(letters[1:5])\nas.ordered(letters[1:5])",
            "coercion-time-difference": "as.difftime(1:5, units = \"secs\")\n\nas.difftime(c(\"01:55:22\", \"01:55:25\"))\n\nas.difftime(\"01\", format = \"\\%H\")\nas.difftime(\"01\", format = \"\\%H\", units = \"secs\")",
            "setops": "intersect(1:5, 4:8)\nunion(1:5, 4:8)\n\nsetdiff(1:5, 4:8)\nsetdiff(4:8, 1:5)"
        }
    },
    "backports": {
        "description": "Functions introduced or changed since R v3.0.0 are re-implemented in this\n    package. The backports are conditionally exported in order to let R resolve\n    the function name to either the implemented backport, or the respective base\n    version, if available. Package developers can make use of new functions or\n    arguments by selectively importing specific backports to\n    support older installations.",
        "examples": {
            "R_user_dir": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_R_user_dir = getFromNamespace(\"R_user_dir\", \"backports\")\n\nbp_R_user_dir(\"backports\")",
            "URLencode": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_URLencode = getFromNamespace(\"URLencode\", \"backports\")\n\nURLdecode(z <- \"ab\\%20cd\")\nc(bp_URLencode(z), bp_URLencode(z, repeated = TRUE))",
            "anyNA": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_anyNA = getFromNamespace(\"anyNA\", \"backports\")\n\nbp_anyNA(letters)",
            "asplit": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_asplit = getFromNamespace(\"asplit\", \"backports\")\nx = matrix(1:6, 2, 3)\nbp_asplit(x, 1)",
            "capture.output": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_capture.output <- getFromNamespace(\"capture.output\", \"backports\")\n\ncaptured <- bp_capture.output({ message(\"hi\") }, type = \"message\")\nstr(captured)",
            "dQuote": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_dQuote <- getFromNamespace(\"dQuote\", \"backports\")\nbp_dQuote(\"foo\")\nbp_dQuote(\"foo\", q = TRUE)",
            "deparse1": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_deparse1 = getFromNamespace(\"deparse1\", \"backports\")\n\nbp_deparse1(quote(`foo bar`))",
            "dir.exists": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_dir.exists = getFromNamespace(\"dir.exists\", \"backports\")\n\nbp_dir.exists(tempdir())",
            "dotsElt": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_...elt = getFromNamespace(\"...elt\", \"backports\")\n\nfoo = function(n, ...) bp_...elt(n)\nfoo(n = 2, \"a\", \"b\", \"c\")",
            "dotsLength": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_...length = getFromNamespace(\"...length\", \"backports\")\n\nfoo = function(...) bp_...length()\nfoo(1, 2, 3)",
            "dotsNames": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_...names = getFromNamespace(\"...names\", \"backports\")\n\nfoo = function(...) bp_...names()\nfoo(a = 1, b = 2, 3)",
            "endsWith": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_endsWith = getFromNamespace(\"endsWith\", \"backports\")\n\nbp_endsWith(c(\"aabb\", \"bbcc\"), \"bb\")",
            "file.info": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_file.info = getFromNamespace(\"file.info\", \"backports\")\n\nbp_file.info(file.path(R.home(), \"COPYING\"), extra_cols = FALSE)",
            "file.size": "# get functions from namespace instead of possibly getting\n# implementations shipped with recent R versions:\nbp_file.size = getFromNamespace(\"file.size\", \"backports\")\nbp_file.mode = getFromNamespace(\"file.size\", \"backports\")\nbp_file.mtime = getFromNamespace(\"file.size\", \"backports\")\n\nfn = file.path(R.home(), \"COPYING\")\nbp_file.size(fn)\nbp_file.mode(fn)\nbp_file.size(fn)",
            "get0": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_get0 = getFromNamespace(\"get0\", \"backports\")\n\nbp_get0(\"a\")\nbp_get0(\"a\", ifnotfound = 0)\n\nfoo = 12\nbp_get0(\"foo\")",
            "hasName": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_hasName = getFromNamespace(\"hasName\", \"backports\")\n\nbp_hasName(list(a = 1, b = 2), c(\"a\", \"b\", \"c\"))",
            "import": "\\dontrun{\n# This imports all functions implemented in backports while the package is loaded\n.onLoad <- function(libname, pkgname) {\n  backports::import(pkgname)\n}\n\n# This only imports the function \"trimws\"\n.onLoad <- function(libname, pkgname) {\n  backports::import(pkgname, \"trimws\")\n}\n\n# This imports all backports from base and force-imports \"hasName\" from utils\n.onLoad <- function(libname, pkgname) {\n  backports::import(pkgname)\n  backports::import(pkgname, \"hasName\", force = TRUE)\n}\n}",
            "isFALSE": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_isFALSE = getFromNamespace(\"isFALSE\", \"backports\")\n\nbp_isFALSE(FALSE)\nbp_isFALSE(iris)",
            "isNamespaceLoaded": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_isNamespaceLoaded = getFromNamespace(\"isNamespaceLoaded\", \"backports\")\n\nbp_isNamespaceLoaded(\"backports\")",
            "isTRUE": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_isTRUE = getFromNamespace(\"isTRUE\", \"backports\")\n\nbp_isTRUE(FALSE)\nbp_isTRUE(iris)",
            "lengths": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_lengths = getFromNamespace(\"lengths\", \"backports\")\n\nbp_lengths(list(1:3, 2))",
            "libPaths": "save <- .libPaths()\nsave\n# ignore the site library\n.libPaths(\"test\", include.site = FALSE)\n\n# restore the original\n.libPaths(save)",
            "list2DF": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_list2DF = getFromNamespace(\"list2DF\", \"backports\")\n\nbp_list2DF(list(x = 1:3, y = 2:4))",
            "null_coalesce_operator": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_null_coalesce = getFromNamespace(\"\\%||\\%\", \"backports\")\n\nbp_null_coalesce(NULL, FALSE)",
            "paste": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_paste = getFromNamespace(\"paste\", \"backports\")\nbp_paste(letters[1:3], character(), collapse = NULL, recycle0 = TRUE)",
            "paste0": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_paste0 = getFromNamespace(\"paste0\", \"backports\")\nbp_paste0(letters[1:3], character(), collapse = NULL, recycle0 = TRUE)",
            "removeSource": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_removeSource = getFromNamespace(\"removeSource\", \"backports\")\n\nbp_removeSource(mean)",
            "startsWith": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_startsWith = getFromNamespace(\"startsWith\", \"backports\")\n\nbp_startsWith(c(\"aabb\", \"bbcc\"), \"bb\")",
            "stopifnot": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_stopifnot = getFromNamespace(\"stopifnot\", \"backports\")\n\nm <- matrix(c(1, 3, 12, 1), 2, 2)\n\\dontrun{bp_stopifnot(\"m must be symmetric\" = m == t(m))}",
            "str2expression": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nstr2expression <- getFromNamespace(\"str2expression\", \"backports\")\n\nstr2expression(\"x[3] <- 1+4\")",
            "str2lang": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nstr2lang <- getFromNamespace(\"str2lang\", \"backports\")\n\nstr2lang(\"x[3] <- 1+4\")",
            "strrep": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_strrep = getFromNamespace(\"strrep\", \"backports\")\n\nbp_strrep(\"-\", 10)",
            "suppressWarnings": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_suppressWarnings = getFromNamespace(\"suppressWarnings\", \"backports\")\nbp_suppressWarnings(warningCondition(\"warning\", class = \"testWarning\"), \"testWarning\")",
            "trimws": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_trimws = getFromNamespace(\"trimws\", \"backports\")\nbp_trimws(c(\"  a  \", \"b  \", \"  c\"))\n\nbp_trimws(c(\"  a  \", \"b  \", \"  c\"), which = \"left\")",
            "valid.factor": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_.valid_factor = getFromNamespace(\".valid.factor\", \"backports\")\nbp_.valid_factor(factor(letters[1:3]))",
            "vignetteInfo": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_vignetteInfo = getFromNamespace(\"vignetteInfo\", \"backports\")",
            "warningCondition": "# get function from namespace instead of possibly getting\n# implementation shipped with recent R versions:\nbp_warningCondition = getFromNamespace(\"warningCondition\", \"backports\")\nbp_warningCondition(\"package backports not found\")"
        }
    },
    "testthat": {
        "description": "Software testing is important, but, in part because it is\n    frustrating and boring, many of us avoid it. 'testthat' is a testing\n    framework for R that is easy to learn and use, and integrates with\n    your existing 'workflow'.",
        "examples": {
            "Reporter": "path <- testthat_example(\"success\")\n\ntest_file(path)\n# Override the default by supplying the name of a reporter\ntest_file(path, reporter = \"minimal\")",
            "capture_condition": "f <- function() {\n  message(\"First\")\n  warning(\"Second\")\n  message(\"Third\")\n}\n\ncapture_message(f())\ncapture_messages(f())\n\ncapture_warning(f())\ncapture_warnings(f())\n\n# Condition will capture anything\ncapture_condition(f())",
            "capture_output": "capture_output({\n  cat(\"Hi!\\n\")\n  cat(\"Bye\\n\")\n})\n\ncapture_output_lines({\n  cat(\"Hi!\\n\")\n  cat(\"Bye\\n\")\n})\n\ncapture_output(\"Hi\")\ncapture_output(\"Hi\", print = TRUE)",
            "compare": "# Character -----------------------------------------------------------------\nx <- c(\"abc\", \"def\", \"jih\")\ncompare(x, x)\n\ny <- paste0(x, \"y\")\ncompare(x, y)\n\ncompare(letters, paste0(letters, \"-\"))\n\nx <- \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis cursus\n tincidunt auctor. Vestibulum ac metus bibendum, facilisis nisi non, pulvinar\n dolor. Donec pretium iaculis nulla, ut interdum sapien ultricies a. \"\ny <- \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis cursus\n tincidunt auctor. Vestibulum ac metus1 bibendum, facilisis nisi non, pulvinar\n dolor. Donec pretium iaculis nulla, ut interdum sapien ultricies a. \"\ncompare(x, y)\ncompare(c(x, x), c(y, y))\n\n# Numeric -------------------------------------------------------------------\n\nx <- y <- runif(100)\ny[sample(100, 10)] <- 5\ncompare(x, y)\n\nx <- y <- 1:10\nx[5] <- NA\nx[6] <- 6.5\ncompare(x, y)\n\n# Compare ignores minor numeric differences in the same way\n# as all.equal.\ncompare(x, x + 1e-9)",
            "comparison-expectations": "a <- 9\nexpect_lt(a, 10)\n\n\\dontrun{\nexpect_lt(11, 10)\n}\n\na <- 11\nexpect_gt(a, 10)\n\\dontrun{\nexpect_gt(9, 10)\n}",
            "context": "context(\"String processing\")\ncontext(\"Remote procedure calls\")",
            "describe": "describe(\"matrix()\", {\n  it(\"can be multiplied by a scalar\", {\n    m1 <- matrix(1:4, 2, 2)\n    m2 <- m1 * 2\n    expect_equal(matrix(1:4 * 2, 2, 2), m2)\n  })\n  it(\"can have not yet tested specs\")\n})\n\n# Nested specs:\n## code\naddition <- function(a, b) a + b\ndivision <- function(a, b) a / b\n\n## specs\ndescribe(\"math library\", {\n  describe(\"addition()\", {\n    it(\"can add two numbers\", {\n      expect_equal(1 + 1, addition(1, 1))\n    })\n  })\n  describe(\"division()\", {\n    it(\"can divide two numbers\", {\n      expect_equal(10 / 2, division(10, 2))\n    })\n    it(\"can handle division by 0\") #not yet implemented\n  })\n})",
            "equality-expectations": "a <- 10\nexpect_equal(a, 10)\n\n# Use expect_equal() when testing for numeric equality\n\\dontrun{\nexpect_identical(sqrt(2) ^ 2, 2)\n}\nexpect_equal(sqrt(2) ^ 2, 2)",
            "evaluate_promise": "evaluate_promise({\n  print(\"1\")\n  message(\"2\")\n  warning(\"3\")\n  4\n})",
            "expect_equivalent": "#' # expect_equivalent() ignores attributes\na <- b <- 1:3\nnames(b) <- letters[1:3]\n\\dontrun{\nexpect_equal(a, b)\n}\nexpect_equivalent(a, b)",
            "expect_error": "# Errors ------------------------------------------------------------------\nf <- function() stop(\"My error!\")\nexpect_error(f())\nexpect_error(f(), \"My error!\")\n\n# You can use the arguments of grepl to control the matching\nexpect_error(f(), \"my error!\", ignore.case = TRUE)\n\n# Note that `expect_error()` returns the error object so you can test\n# its components if needed\nerr <- expect_error(rlang::abort(\"a\", n = 10))\nexpect_equal(err$n, 10)\n\n# Warnings ------------------------------------------------------------------\nf <- function(x) {\n  if (x < 0) {\n    warning(\"*x* is already negative\")\n    return(x)\n  }\n  -x\n}\nexpect_warning(f(-1))\nexpect_warning(f(-1), \"already negative\")\nexpect_warning(f(1), NA)\n\n# To test message and output, store results to a variable\nexpect_warning(out <- f(-1), \"already negative\")\nexpect_equal(out, -1)\n\n# Messages ------------------------------------------------------------------\nf <- function(x) {\n  if (x < 0) {\n    message(\"*x* is already negative\")\n    return(x)\n  }\n\n  -x\n}\nexpect_message(f(-1))\nexpect_message(f(-1), \"already negative\")\nexpect_message(f(1), NA)",
            "expect_invisible": "expect_invisible(x <- 10)\nexpect_visible(x)\n\n# Typically you'll assign the result of the expectation so you can\n# also check that the value is as you expect.\ngreet <- function(name) {\n  message(\"Hi \", name)\n  invisible(name)\n}\nout <- expect_invisible(greet(\"Hadley\"))\nexpect_equal(out, \"Hadley\")",
            "expect_known_output": "tmp <- tempfile()\n\n# The first run always succeeds\nexpect_known_output(mtcars[1:10, ], tmp, print = TRUE)\n\n# Subsequent runs will succeed only if the file is unchanged\n# This will succeed:\nexpect_known_output(mtcars[1:10, ], tmp, print = TRUE)\n\n\\dontrun{\n# This will fail\nexpect_known_output(mtcars[1:9, ], tmp, print = TRUE)\n}",
            "expect_length": "expect_length(1, 1)\nexpect_length(1:10, 10)\n\n\\dontrun{\nexpect_length(1:10, 1)\n}",
            "expect_match": "expect_match(\"Testing is fun\", \"fun\")\nexpect_match(\"Testing is fun\", \"f.n\")\nexpect_no_match(\"Testing is fun\", \"horrible\")\n\n\\dontrun{\nexpect_match(\"Testing is fun\", \"horrible\")\n\n# Zero-length inputs always fail\nexpect_match(character(), \".\")\n}",
            "expect_named": "x <- c(a = 1, b = 2, c = 3)\nexpect_named(x)\nexpect_named(x, c(\"a\", \"b\", \"c\"))\n\n# Use options to control sensitivity\nexpect_named(x, c(\"B\", \"C\", \"A\"), ignore.order = TRUE, ignore.case = TRUE)\n\n# Can also check for the absence of names with NULL\nz <- 1:4\nexpect_named(z, NULL)",
            "expect_no_error": "expect_no_warning(1 + 1)\n\nfoo <- function(x) {\n  warning(\"This is a problem!\")\n}\n\n# warning doesn't match so bubbles up:\nexpect_no_warning(foo(), message = \"bananas\")\n\n# warning does match so causes a failure:\ntry(expect_no_warning(foo(), message = \"problem\"))",
            "expect_null": "x <- NULL\ny <- 10\n\nexpect_null(x)\nshow_failure(expect_null(y))",
            "expect_output": "str(mtcars)\nexpect_output(str(mtcars), \"32 obs\")\nexpect_output(str(mtcars), \"11 variables\")\n\n# You can use the arguments of grepl to control the matching\nexpect_output(str(mtcars), \"11 VARIABLES\", ignore.case = TRUE)\nexpect_output(str(mtcars), \"$ mpg\", fixed = TRUE)",
            "expect_setequal": "expect_setequal(letters, rev(letters))\nshow_failure(expect_setequal(letters[-1], rev(letters)))\n\nx <- list(b = 2, a = 1)\nexpect_mapequal(x, list(a = 1, b = 2))\nshow_failure(expect_mapequal(x, list(a = 1)))\nshow_failure(expect_mapequal(x, list(a = 1, b = \"x\")))\nshow_failure(expect_mapequal(x, list(a = 1, b = 2, c = 3)))",
            "expect_silent": "expect_silent(\"123\")\n\nf <- function() {\n  message(\"Hi!\")\n  warning(\"Hey!!\")\n  print(\"OY!!!\")\n}\n\\dontrun{\nexpect_silent(f())\n}",
            "expect_snapshot_file": "# To use expect_snapshot_file() you'll typically need to start by writing\n# a helper function that creates a file from your code, returning a path\nsave_png <- function(code, width = 400, height = 400) {\n  path <- tempfile(fileext = \".png\")\n  png(path, width = width, height = height)\n  on.exit(dev.off())\n  code\n\n  path\n}\npath <- save_png(plot(1:5))\npath\n\n\\dontrun{\nexpect_snapshot_file(save_png(hist(mtcars$mpg)), \"plot.png\")\n}\n\n# You'd then also provide a helper that skips tests where you can't\n# be sure of producing exactly the same output\nexpect_snapshot_plot <- function(name, code) {\n  # Other packages might affect results\n  skip_if_not_installed(\"ggplot2\", \"2.0.0\")\n  # Or maybe the output is different on some operation systems\n  skip_on_os(\"windows\")\n  # You'll need to carefully think about and experiment with these skips\n\n  name <- paste0(name, \".png\")\n\n  # Announce the file before touching `code`. This way, if `code`\n  # unexpectedly fails or skips, testthat will not auto-delete the\n  # corresponding snapshot file.\n  announce_snapshot_file(name = name)\n\n  path <- save_png(code)\n  expect_snapshot_file(path, name)\n}",
            "expect_that": "expect_that(5 * 2, equals(10))\nexpect_that(sqrt(2) ^ 2, equals(2))\n\\dontrun{\nexpect_that(sqrt(2) ^ 2, is_identical_to(2))\n}",
            "expect_vector": "\\dontshow{if (requireNamespace(\"vctrs\")) withAutoprint(\\{ # examplesIf}\nexpect_vector(1:10, ptype = integer(), size = 10)\nshow_failure(expect_vector(1:10, ptype = integer(), size = 5))\nshow_failure(expect_vector(1:10, ptype = character(), size = 5))\n\\dontshow{\\}) # examplesIf}",
            "fail": "\\dontrun{\ntest_that(\"this test fails\", fail())\ntest_that(\"this test succeeds\", succeed())\n}",
            "inheritance-expectations": "x <- data.frame(x = 1:10, y = \"x\", stringsAsFactors = TRUE)\n# A data frame is an S3 object with class data.frame\nexpect_s3_class(x, \"data.frame\")\nshow_failure(expect_s4_class(x, \"data.frame\"))\n# A data frame is built from a list:\nexpect_type(x, \"list\")\n\n# An integer vector is an atomic vector of type \"integer\"\nexpect_type(x$x, \"integer\")\n# It is not an S3 object\nshow_failure(expect_s3_class(x$x, \"integer\"))\n\n# Above, we requested data.frame() converts strings to factors:\nshow_failure(expect_type(x$y, \"character\"))\nexpect_s3_class(x$y, \"factor\")\nexpect_type(x$y, \"integer\")",
            "local_test_context": "local({\n  local_test_context()\n  cat(cli::col_blue(\"Text will not be colored\"))\n  cat(cli::symbol$ellipsis)\n  cat(\"\\n\")\n})\ntest_that(\"test ellipsis\", {\n  local_reproducible_output(unicode = FALSE)\n  expect_equal(cli::symbol$ellipsis, \"...\")\n\n  local_reproducible_output(unicode = TRUE)\n  expect_equal(cli::symbol$ellipsis, \"\\u2026\")\n})",
            "logical-expectations": "expect_true(2 == 2)\n# Failed expectations will throw an error\n\\dontrun{\nexpect_true(2 != 2)\n}\nexpect_true(!(2 != 2))\n# or better:\nexpect_false(2 != 2)\n\na <- 1:3\nexpect_true(length(a) == 3)\n# but better to use more specific expectation, if available\nexpect_equal(length(a), 3)",
            "make_expectation": "x <- 1:10\nmake_expectation(x)\n\nmake_expectation(mtcars$mpg)\n\ndf <- data.frame(x = 2)\nmake_expectation(df)",
            "mock_output_sequence": "# inside local_mocked_bindings()\n\\dontrun{\nlocal_mocked_bindings(readline = mock_output_sequence(\"3\", \"This is a note\", \"n\"))\n}\n# for understanding\nmocked_sequence <- mock_output_sequence(\"3\", \"This is a note\", \"n\")\nmocked_sequence()\nmocked_sequence()\nmocked_sequence()\ntry(mocked_sequence())\nrecycled_mocked_sequence <- mock_output_sequence(\n  \"3\", \"This is a note\", \"n\",\n  recycle = TRUE\n)\nrecycled_mocked_sequence()\nrecycled_mocked_sequence()\nrecycled_mocked_sequence()\nrecycled_mocked_sequence()",
            "quasi_label": "f <- function(i) if (i > 3) i * 9 else i * 10\ni <- 10\n\n# This sort of expression commonly occurs inside a for loop or function\n# And the failure isn't helpful because you can't see the value of i\n# that caused the problem:\nshow_failure(expect_equal(f(i), i * 10))\n\n# To overcome this issue, testthat allows you to unquote expressions using\n# !!. This causes the failure message to show the value rather than the\n# variable name\nshow_failure(expect_equal(f(!!i), !!(i * 10)))",
            "skip": "if (FALSE) skip(\"Some Important Requirement is not available\")\n\ntest_that(\"skip example\", {\n  expect_equal(1, 1L)    # this expectation runs\n  skip('skip')\n  expect_equal(1, 2)     # this one skipped\n  expect_equal(1, 3)     # this one is also skipped\n})",
            "teardown": "\\dontrun{\n# Old approach\ntmp <- tempfile()\nsetup(writeLines(\"some test data\", tmp))\nteardown(unlink(tmp))\n}\n\n# Now recommended:\nlocal_test_data <- function(env = parent.frame()) {\n  tmp <- tempfile()\n  writeLines(\"some test data\", tmp)\n  withr::defer(unlink(tmp), env)\n\n  tmp\n}\n# Then call local_test_data() in your tests",
            "test_file": "path <- testthat_example(\"success\")\ntest_file(path)\ntest_file(path, desc = \"some tests have warnings\")\ntest_file(path, reporter = \"minimal\")",
            "test_path": "\\dontrun{\ntest_path(\"foo.csv\")\ntest_path(\"data\", \"foo.csv\")\n}",
            "test_that": "test_that(\"trigonometric functions match identities\", {\n  expect_equal(sin(pi / 4), 1 / sqrt(2))\n  expect_equal(cos(pi / 4), 1 / sqrt(2))\n  expect_equal(tan(pi / 4), 1)\n})\n\n\\dontrun{\ntest_that(\"trigonometric functions match identities\", {\n  expect_equal(sin(pi / 4), 1)\n})\n}",
            "testthat_examples": "dir(testthat_examples())\ntestthat_example(\"success\")",
            "try_again": "third_try <- local({\n  i <- 3\n  function() {\n    i <<- i - 1\n    if (i > 0) fail(paste0(\"i is \", i))\n  }\n})\ntry_again(3, third_try())"
        }
    },
    "prettyunits": {
        "description": "Pretty, human readable formatting of quantities.\n    Time intervals: '1337000' -> '15d 11h 23m 20s'.\n    Vague time intervals: '2674000' -> 'about a month ago'.\n    Bytes: '1337' -> '1.34 kB'.\n    Rounding: '99' with 3 significant digits -> '99.0'\n    p-values: '0.00001' -> '<0.0001'.\n    Colors: '#FF0000' -> 'red'.\n    Quantities: '1239437' -> '1.24 M'.",
        "examples": {
            "pretty_bytes": "bytes <- c(1337, 133337, 13333337, 1333333337, 133333333337)\npretty_bytes(bytes)\npretty_bytes(bytes, style = \"nopad\")\npretty_bytes(bytes, style = \"6\")",
            "pretty_dt": "pretty_dt(as.difftime(1000, units = \"secs\"))\npretty_dt(as.difftime(0, units = \"secs\"))",
            "pretty_ms": "pretty_ms(c(1337, 13370, 133700, 1337000, 1337000000))\n\npretty_ms(c(1337, 13370, 133700, 1337000, 1337000000),\n          compact = TRUE)",
            "pretty_num": "numbers <- c(1337, 1.3333e-5, 13333337, 1333333337, 133333333337)\npretty_num(numbers)\npretty_num(numbers, style = \"nopad\")\npretty_num(numbers, style = \"6\")",
            "pretty_p_value": "pretty_p_value(c(1, 0, NA, 0.01, 0.0000001))\npretty_p_value(c(1, 0, NA, 0.01, 0.0000001), minval = 0.05)",
            "pretty_sec": "pretty_sec(c(1337, 13370, 133700, 1337000, 13370000))\n\npretty_sec(c(1337, 13370, 133700, 1337000, 13370000),\n           compact = TRUE)",
            "time_ago": "now <- Sys.time()\n\ntime_ago(now)\ntime_ago(now - as.difftime(30, units = \"secs\"))\ntime_ago(now - as.difftime(14, units = \"mins\"))\ntime_ago(now - as.difftime(5, units = \"hours\"))\ntime_ago(now - as.difftime(25, units = \"hours\"))\ntime_ago(now - as.difftime(5, units = \"days\"))\ntime_ago(now - as.difftime(30, units = \"days\"))\ntime_ago(now - as.difftime(365, units = \"days\"))\ntime_ago(now - as.difftime(365 * 10, units = \"days\"))\n\n## Short format\ntime_ago(format = \"short\", now)\ntime_ago(format = \"short\", now - as.difftime(30, units = \"secs\"))\ntime_ago(format = \"short\", now - as.difftime(14, units = \"mins\"))\ntime_ago(format = \"short\", now - as.difftime(5, units = \"hours\"))\ntime_ago(format = \"short\", now - as.difftime(25, units = \"hours\"))\ntime_ago(format = \"short\", now - as.difftime(5, units = \"days\"))\ntime_ago(format = \"short\", now - as.difftime(30, units = \"days\"))\ntime_ago(format = \"short\", now - as.difftime(365, units = \"days\"))\ntime_ago(format = \"short\", now - as.difftime(365 * 10, units = \"days\"))\n\n## Even shorter, terse format, (almost always) exactly 3 characters wide\ntime_ago(format = \"terse\", now)\ntime_ago(format = \"terse\", now - as.difftime(30, units = \"secs\"))\ntime_ago(format = \"terse\", now - as.difftime(14, units = \"mins\"))\ntime_ago(format = \"terse\", now - as.difftime(5, units = \"hours\"))\ntime_ago(format = \"terse\", now - as.difftime(25, units = \"hours\"))\ntime_ago(format = \"terse\", now - as.difftime(5, units = \"days\"))\ntime_ago(format = \"terse\", now - as.difftime(30, units = \"days\"))\ntime_ago(format = \"terse\", now - as.difftime(365, units = \"days\"))\ntime_ago(format = \"terse\", now - as.difftime(365 * 10, units = \"days\"))",
            "vague_dt": "vague_dt(as.difftime(30, units = \"secs\"))\nvague_dt(as.difftime(14, units = \"mins\"))\nvague_dt(as.difftime(5, units = \"hours\"))\nvague_dt(as.difftime(25, units = \"hours\"))\nvague_dt(as.difftime(5, units = \"days\"))\nvague_dt(as.difftime(30, units = \"days\"))\nvague_dt(as.difftime(365, units = \"days\"))\nvague_dt(as.difftime(365 * 10, units = \"days\"))\n\n## Short format\nvague_dt(format = \"short\", as.difftime(30, units = \"secs\"))\nvague_dt(format = \"short\", as.difftime(14, units = \"mins\"))\nvague_dt(format = \"short\", as.difftime(5, units = \"hours\"))\nvague_dt(format = \"short\", as.difftime(25, units = \"hours\"))\nvague_dt(format = \"short\", as.difftime(5, units = \"days\"))\nvague_dt(format = \"short\", as.difftime(30, units = \"days\"))\nvague_dt(format = \"short\", as.difftime(365, units = \"days\"))\nvague_dt(format = \"short\", as.difftime(365 * 10, units = \"days\"))\n\n## Even shorter, terse format, (almost always) exactly 3 characters wide\nvague_dt(format = \"terse\", as.difftime(30, units = \"secs\"))\nvague_dt(format = \"terse\", as.difftime(14, units = \"mins\"))\nvague_dt(format = \"terse\", as.difftime(5, units = \"hours\"))\nvague_dt(format = \"terse\", as.difftime(25, units = \"hours\"))\nvague_dt(format = \"terse\", as.difftime(5, units = \"days\"))\nvague_dt(format = \"terse\", as.difftime(30, units = \"days\"))\nvague_dt(format = \"terse\", as.difftime(365, units = \"days\"))\nvague_dt(format = \"terse\", as.difftime(365 * 10, units = \"days\"))"
        }
    },
    "farver": {
        "description": "The encoding of colour can be handled in many different ways,\n    using different colour spaces. As different colour spaces have\n    different uses, efficient conversion between these representations are\n    important. The 'farver' package provides a set of functions that gives\n    access to very fast colour space conversion and comparisons\n    implemented in C++, and offers speed improvements over the\n    'convertColor' function in the 'grDevices' package.",
        "examples": {
            "as_white_ref": "# Using names\nas_white_ref('D65')\n\n# Using chromaticity values\nas_white_ref(c(0.3, 0.4))",
            "compare_colour": "r <- decode_colour(rainbow(10))\nh <- decode_colour(heat.colors(15))\n\n# Compare two sets of colours\ncompare_colour(r, h, 'rgb', method = 'cie2000')\n\n# Compare a set of colours with itself\ncompare_colour(r, from_space = 'rgb', method = 'cmc')\n\n# Compare colours from different colour spaces\nh_luv <- convert_colour(h, 'rgb', 'luv')\ncompare_colour(r, h_luv, 'rgb', 'luv')",
            "convert_colour": "spectrum <- decode_colour(rainbow(10))\nspec_lab <- convert_colour(spectrum, 'rgb', 'lab')\nspec_lab\n\n# Convert between different white references\nconvert_colour(spec_lab, 'lab', 'lab', white_from = 'D65', white_to = 'F10')",
            "decode_colour": "# basic use\ndecode_colour(c('#43e1f6', 'steelblue', '#67ce9fe4'))\n\n# Return alpha as well (no alpha value is interpreted as 1)\ndecode_colour(c('#43e1f6', 'steelblue', '#67ce9fe4'), alpha = TRUE)\n\n# Decode directly into specific colour space\ndecode_colour(c('#43e1f6', 'steelblue', '#67ce9fe4'), to = 'lch')",
            "encode_colour": "spectrum <- decode_colour(rainbow(10))\n\nencode_colour(spectrum)\n\n# Attach alpha values\nencode_colour(spectrum, alpha = c(0.5, 1))\n\n# Encode from a different colour space\nspectrum_hcl <- convert_colour(spectrum, 'rgb', 'hcl')\nencode_colour(spectrum_hcl, from = 'hcl')",
            "manip_channel": "spectrum <- rainbow(10)\n\n# set a specific channel\nset_channel(spectrum, 'r', c(10, 50))\nset_channel(spectrum, 'l', 50, space = 'lab')\nset_channel(spectrum, 'alpha', c(0.5, 1))\n\n# Add value to channel\nadd_to_channel(spectrum, 'r', c(10, 50))\nadd_to_channel(spectrum, 'l', 50, space = 'lab')\n\n# Multiply a channel\nmultiply_channel(spectrum, 'r', c(10, 50))\nmultiply_channel(spectrum, 'l', 50, space = 'lab')\n\n# set a lower bound on a channel\nraise_channel(spectrum, 'r', c(10, 50))\nraise_channel(spectrum, 'l', 20, space = 'lab')\n\n# set an upper bound on a channel\ncap_channel(spectrum, 'r', c(100, 50))\ncap_channel(spectrum, 'l', 20, space = 'lab')",
            "native_encoding": "# Get native representation of navyblue and #228B22\nnative_col <- encode_native(c('navyblue', '#228B22'))\nnative_col\n\n# Convert back\ndecode_native(native_col)"
        }
    },
    "markdown": {
        "description": "Render Markdown to full and lightweight HTML/LaTeX documents with\n    the 'commonmark' package. This package has been superseded by 'litedown'.",
        "examples": {
            "mark": "library(markdown)\nmark(c(\"Hello _World_!\", \"\", \"Welcome to **markdown**.\"))\n# a few corner cases\nmark(character(0))\nmark(\"\")\n# if input happens to be a file path but should be treated as text, use I()\nmark(I(\"This is *not* a file.md\"))\n# that's equivalent to\nmark(text = \"This is *not* a file.md\")\n\nmark_html(\"Hello _World_!\", template = FALSE)\n# write HTML to an output file\nmark_html(\"_Hello_, **World**!\", output = tempfile())\n\nmark_latex(\"Hello _World_!\", template = FALSE)",
            "markdown_options": "# all available options\nmarkdown::markdown_options()\n\nlibrary(markdown)\n\n# toc example\nmkd <- c(\"# Header 1\", \"p1\", \"## Header 2\", \"p2\")\n\ncat(mark(mkd, options = \"+number_sections\"))\ncat(mark(mkd, options = \"+number_sections+toc\"))\n\n# hard_wrap example\ncat(mark(\"foo\\nbar\\n\"))\ncat(mark(\"foo\\nbar\\n\", options = \"hard_wrap\"))\n\n# latex math example\nmkd <- c(\n  \"`$x$` is inline math $x$!\", \"\", \"Display style:\", \"\", \"$$x + y$$\", \"\",\n  \"\\\\\\\\begin{eqnarray}\na^{2}+b^{2} & = & c^{2}\\\\\\\\\\\\\\\\\n\\\\\\\\sin^{2}(x)+\\\\\\\\cos^{2}(x) & = & 1\n\\\\\\\\end{eqnarray}\"\n)\n\ncat(mark(mkd))\ncat(mark(mkd, options = \"-latex_math\"))\n\n# tables example (need 4 spaces at beginning of line here)\ncat(mark(\"\nFirst Header  | Second Header\n------------- | -------------\nContent Cell  | Content Cell\nContent Cell  | Content Cell\n\"))\n\n# but not here\ncat(mark(\"\nFirst Header  | Second Header\n------------- | -------------\nContent Cell  | Content Cell\nContent Cell  | Content Cell\n\", options = '-table'))\n\n# autolink example\ncat(mark(\"https://www.r-project.org/\"))\ncat(mark(\"https://www.r-project.org/\", options = \"-autolink\"))\n\n# strikethrough example\ncat(mark(\"~~awesome~~\"))\ncat(mark(\"~~awesome~~\", options = \"-strikethrough\"))\n\n# superscript and subscript examples\ncat(mark(\"2^10^\"))\ncat(mark(\"2^10^\", options = \"-superscript\"))\ncat(mark(\"H~2~O\"))\ncat(mark(\"H~2~O\", options = \"-subscript\"))\n\n# code blocks\ncat(mark('```r\\n1 + 1;\\n```'))\ncat(mark('```{.r}\\n1 + 1;\\n```'))\ncat(mark('```{.r .js}\\n1 + 1;\\n```'))\ncat(mark('```{.r .js #foo}\\n1 + 1;\\n```'))\ncat(mark('```{.r .js #foo style=\"color:red;\"}\\n1 + 1;\\n```'))\ncat(mark('````\\n```{r, echo=TRUE}\\n1 + 1;\\n```\\n````'))\n\n# raw blocks\ncat(mark('```{=html}\\n<p>raw HTML</p>\\n```'))\ncat(mark('```{=latex}\\n<p>raw HTML</p>\\n```'))\n\n# skip_html tags\nmkd = '<style>a {}</style><script type=\"text/javascript\">console.log(\"No!\");</script>\\n[Hello](#)'\ncat(mark(mkd))\n# TODO: wait for https://github.com/r-lib/commonmark/issues/15 to be fixed\n# cat(mark(mkd, options = \"tagfilter\"))",
            "rpubsUpload": "\\dontrun{\n# upload a document\nresult <- rpubsUpload(\"My document title\", \"Document.html\")\nif (!is.null(result$continueUrl))\n    browseURL(result$continueUrl) else stop(result$error)\n\n# update the same document with a new title\nupdateResult <- rpubsUpload(\"My updated title\", \"Document.html\", result$id)\n}",
            "smartypants": "cat(smartypants(\"1/2 (c)\\n\"))"
        }
    },
    "DBI": {
        "description": "A database interface definition for communication between R\n    and relational database management systems.  All classes in this\n    package are virtual and need to be extended by the various R/DBMS\n    implementations.",
        "examples": {
            "ANSI": "ANSI()",
            "DBI-package": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\nRSQLite::SQLite()\n\\dontshow{\\}) # examplesIf}",
            "DBIConnection-class": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\ncon\ndbDisconnect(con)\n\\dontrun{\ncon <- dbConnect(RPostgreSQL::PostgreSQL(), \"username\", \"password\")\ncon\ndbDisconnect(con)\n}\n\\dontshow{\\}) # examplesIf}",
            "DBIConnector-class": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\n# Create a connector:\ncnr <- new(\"DBIConnector\",\n  .drv = RSQLite::SQLite(),\n  .conn_args = list(dbname = \":memory:\")\n)\ncnr\n\n# Establish a connection through this connector:\ncon <- dbConnect(cnr)\ncon\n\n# Access the database through this connection:\ndbGetQuery(con, \"SELECT 1 AS a\")\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "DBIObject-class": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ndrv <- RSQLite::SQLite()\ncon <- dbConnect(drv)\n\nrs <- dbSendQuery(con, \"SELECT 1\")\nis(drv, \"DBIObject\")   ## True\nis(con, \"DBIObject\")   ## True\nis(rs, \"DBIObject\")\n\ndbClearResult(rs)\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "Id": "# Identifies a table in a specific schema:\nId(\"dbo\", \"Customer\")\n# You can name the components if you want, but it's not needed\nId(table = \"Customer\", schema = \"dbo\")\n\n# Create a SQL expression for an identifier:\ndbQuoteIdentifier(ANSI(), Id(\"nycflights13\", \"flights\"))\n\n# Write a table in a specific schema:\n\\dontrun{\ndbWriteTable(con, Id(\"myschema\", \"mytable\"), data.frame(a = 1))\n}",
            "SQL": "dbQuoteIdentifier(ANSI(), \"SELECT\")\ndbQuoteString(ANSI(), \"SELECT\")\n\n# SQL vectors are always passed through as is\nvar_name <- SQL(\"SELECT\")\nvar_name\n\ndbQuoteIdentifier(ANSI(), var_name)\ndbQuoteString(ANSI(), var_name)\n\n# This mechanism is used to prevent double escaping\ndbQuoteString(ANSI(), dbQuoteString(ANSI(), \"SELECT\"))",
            "dbAppendTable": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\ndbCreateTable(con, \"iris\", iris)\ndbAppendTable(con, \"iris\", iris)\ndbReadTable(con, \"iris\")\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbAppendTableArrow": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE) && requireNamespace(\"nanoarrow\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\ndbCreateTableArrow(con, \"iris\", iris[0, ])\ndbAppendTableArrow(con, \"iris\", iris[1:5, ])\ndbReadTable(con, \"iris\")\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbBind": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\n# Data frame flow:\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"iris\", iris)\n\n# Using the same query for different values\niris_result <- dbSendQuery(con, \"SELECT * FROM iris WHERE [Petal.Width] > ?\")\ndbBind(iris_result, list(2.3))\ndbFetch(iris_result)\ndbBind(iris_result, list(3))\ndbFetch(iris_result)\ndbClearResult(iris_result)\n\n# Executing the same statement with different values at once\niris_result <- dbSendStatement(con, \"DELETE FROM iris WHERE [Species] = $species\")\ndbBind(iris_result, list(species = c(\"setosa\", \"versicolor\", \"unknown\")))\ndbGetRowsAffected(iris_result)\ndbClearResult(iris_result)\n\nnrow(dbReadTable(con, \"iris\"))\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}\n\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE) && requireNamespace(\"nanoarrow\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\n\n# Arrow flow:\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"iris\", iris)\n\n# Using the same query for different values\niris_result <- dbSendQueryArrow(con, \"SELECT * FROM iris WHERE [Petal.Width] > ?\")\ndbBindArrow(\n  iris_result,\n  nanoarrow::as_nanoarrow_array_stream(data.frame(2.3, fix.empty.names = FALSE))\n)\nas.data.frame(dbFetchArrow(iris_result))\ndbBindArrow(\n  iris_result,\n  nanoarrow::as_nanoarrow_array_stream(data.frame(3, fix.empty.names = FALSE))\n)\nas.data.frame(dbFetchArrow(iris_result))\ndbClearResult(iris_result)\n\n# Executing the same statement with different values at once\niris_result <- dbSendStatement(con, \"DELETE FROM iris WHERE [Species] = $species\")\ndbBindArrow(iris_result, nanoarrow::as_nanoarrow_array_stream(data.frame(\n  species = c(\"setosa\", \"versicolor\", \"unknown\")\n)))\ndbGetRowsAffected(iris_result)\ndbClearResult(iris_result)\n\nnrow(dbReadTable(con, \"iris\"))\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbCanConnect": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\n# SQLite only needs a path to the database. (Here, \":memory:\" is a special\n# path that creates an in-memory database.) Other database drivers\n# will require more details (like user, password, host, port, etc.)\ndbCanConnect(RSQLite::SQLite(), \":memory:\")\n\\dontshow{\\}) # examplesIf}",
            "dbClearResult": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\nrs <- dbSendQuery(con, \"SELECT 1\")\nprint(dbFetch(rs))\n\ndbClearResult(rs)\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbColumnInfo": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\nrs <- dbSendQuery(con, \"SELECT 1 AS a, 2 AS b\")\ndbColumnInfo(rs)\ndbFetch(rs)\n\ndbClearResult(rs)\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbConnect": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\n# SQLite only needs a path to the database. (Here, \":memory:\" is a special\n# path that creates an in-memory database.) Other database drivers\n# will require more details (like user, password, host, port, etc.)\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\ncon\n\ndbListTables(con)\n\ndbDisconnect(con)\n\n# Bad, for subtle reasons:\n# This code fails when RSQLite isn't loaded yet,\n# because dbConnect() doesn't know yet about RSQLite.\ndbListTables(con <- dbConnect(RSQLite::SQLite(), \":memory:\"))\n\\dontshow{\\}) # examplesIf}",
            "dbCreateTable": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\ndbCreateTable(con, \"iris\", iris)\ndbReadTable(con, \"iris\")\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbCreateTableArrow": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE) && requireNamespace(\"nanoarrow\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\nptype <- data.frame(a = numeric())\ndbCreateTableArrow(con, \"df\", nanoarrow::infer_nanoarrow_schema(ptype))\ndbReadTable(con, \"df\")\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbDataType": "dbDataType(ANSI(), 1:5)\ndbDataType(ANSI(), 1)\ndbDataType(ANSI(), TRUE)\ndbDataType(ANSI(), Sys.Date())\ndbDataType(ANSI(), Sys.time())\ndbDataType(ANSI(), Sys.time() - as.POSIXct(Sys.Date()))\ndbDataType(ANSI(), c(\"x\", \"abc\"))\ndbDataType(ANSI(), list(raw(10), raw(20)))\ndbDataType(ANSI(), I(3))\n\ndbDataType(ANSI(), iris)\n\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\n\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbDataType(con, 1:5)\ndbDataType(con, 1)\ndbDataType(con, TRUE)\ndbDataType(con, Sys.Date())\ndbDataType(con, Sys.time())\ndbDataType(con, Sys.time() - as.POSIXct(Sys.Date()))\ndbDataType(con, c(\"x\", \"abc\"))\ndbDataType(con, list(raw(10), raw(20)))\ndbDataType(con, I(3))\n\ndbDataType(con, iris)\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbDisconnect": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbDriver": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\n# Create a RSQLite driver with a string\nd <- dbDriver(\"SQLite\")\nd\n\n# But better, access the object directly\nRSQLite::SQLite()\n\\dontshow{\\}) # examplesIf}",
            "dbExecute": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"cars\", head(cars, 3))\ndbReadTable(con, \"cars\")   # there are 3 rows\ndbExecute(\n  con,\n  \"INSERT INTO cars (speed, dist) VALUES (1, 1), (2, 2), (3, 3)\"\n)\ndbReadTable(con, \"cars\")   # there are now 6 rows\n\n# Pass values using the param argument:\ndbExecute(\n  con,\n  \"INSERT INTO cars (speed, dist) VALUES (?, ?)\",\n  params = list(4:7, 5:8)\n)\ndbReadTable(con, \"cars\")   # there are now 10 rows\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbExistsTable": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbExistsTable(con, \"iris\")\ndbWriteTable(con, \"iris\", iris)\ndbExistsTable(con, \"iris\")\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbFetch": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\n\n# Fetch all results\nrs <- dbSendQuery(con, \"SELECT * FROM mtcars WHERE cyl = 4\")\ndbFetch(rs)\ndbClearResult(rs)\n\n# Fetch in chunks\nrs <- dbSendQuery(con, \"SELECT * FROM mtcars\")\nwhile (!dbHasCompleted(rs)) {\n  chunk <- dbFetch(rs, 10)\n  print(nrow(chunk))\n}\n\ndbClearResult(rs)\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbFetchArrow": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE) && requireNamespace(\"nanoarrow\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\n\n# Fetch all results\nrs <- dbSendQueryArrow(con, \"SELECT * FROM mtcars WHERE cyl = 4\")\nas.data.frame(dbFetchArrow(rs))\ndbClearResult(rs)\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbFetchArrowChunk": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE) && requireNamespace(\"nanoarrow\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\n\n# Fetch all results\nrs <- dbSendQueryArrow(con, \"SELECT * FROM mtcars WHERE cyl = 4\")\ndbHasCompleted(rs)\nas.data.frame(dbFetchArrowChunk(rs))\ndbHasCompleted(rs)\nas.data.frame(dbFetchArrowChunk(rs))\ndbClearResult(rs)\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbGetConnectArgs": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncnr <- new(\"DBIConnector\",\n  .drv = RSQLite::SQLite(),\n  .conn_args = list(dbname = \":memory:\", password = function() \"supersecret\")\n)\ndbGetConnectArgs(cnr)\ndbGetConnectArgs(cnr, eval = FALSE)\n\\dontshow{\\}) # examplesIf}",
            "dbGetInfo": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ndbGetInfo(RSQLite::SQLite())\n\\dontshow{\\}) # examplesIf}",
            "dbGetQuery": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\ndbGetQuery(con, \"SELECT * FROM mtcars\")\ndbGetQuery(con, \"SELECT * FROM mtcars\", n = 6)\n\n# Pass values using the param argument:\n# (This query runs eight times, once for each different\n# parameter. The resulting rows are combined into a single\n# data frame.)\ndbGetQuery(\n  con,\n  \"SELECT COUNT(*) FROM mtcars WHERE cyl = ?\",\n  params = list(1:8)\n)\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbGetQueryArrow": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE) && requireNamespace(\"nanoarrow\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\n# Retrieve data as arrow table\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\ndbGetQueryArrow(con, \"SELECT * FROM mtcars\")\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbGetRowCount": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\nrs <- dbSendQuery(con, \"SELECT * FROM mtcars\")\n\ndbGetRowCount(rs)\nret1 <- dbFetch(rs, 10)\ndbGetRowCount(rs)\nret2 <- dbFetch(rs)\ndbGetRowCount(rs)\nnrow(ret1) + nrow(ret2)\n\ndbClearResult(rs)\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbGetRowsAffected": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\nrs <- dbSendStatement(con, \"DELETE FROM mtcars\")\ndbGetRowsAffected(rs)\nnrow(mtcars)\n\ndbClearResult(rs)\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbGetStatement": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\nrs <- dbSendQuery(con, \"SELECT * FROM mtcars\")\ndbGetStatement(rs)\n\ndbClearResult(rs)\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbHasCompleted": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\nrs <- dbSendQuery(con, \"SELECT * FROM mtcars\")\n\ndbHasCompleted(rs)\nret1 <- dbFetch(rs, 10)\ndbHasCompleted(rs)\nret2 <- dbFetch(rs)\ndbHasCompleted(rs)\n\ndbClearResult(rs)\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbIsReadOnly": "dbIsReadOnly(ANSI())",
            "dbIsValid": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ndbIsValid(RSQLite::SQLite())\n\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\ndbIsValid(con)\n\nrs <- dbSendQuery(con, \"SELECT 1\")\ndbIsValid(rs)\n\ndbClearResult(rs)\ndbIsValid(rs)\n\ndbDisconnect(con)\ndbIsValid(con)\n\\dontshow{\\}) # examplesIf}",
            "dbListFields": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\ndbListFields(con, \"mtcars\")\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbListObjects": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbListObjects(con)\ndbWriteTable(con, \"mtcars\", mtcars)\ndbListObjects(con)\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbListTables": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbListTables(con)\ndbWriteTable(con, \"mtcars\", mtcars)\ndbListTables(con)\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbQuoteIdentifier": "# Quoting ensures that arbitrary input is safe for use in a query\nname <- \"Robert'); DROP TABLE Students;--\"\ndbQuoteIdentifier(ANSI(), name)\n\n# Use Id() to specify other components such as the schema\nid_name <- Id(schema = \"schema_name\", table = \"table_name\")\nid_name\ndbQuoteIdentifier(ANSI(), id_name)\n\n# SQL vectors are always passed through as is\nvar_name <- SQL(\"select\")\nvar_name\ndbQuoteIdentifier(ANSI(), var_name)\n\n# This mechanism is used to prevent double escaping\ndbQuoteIdentifier(ANSI(), dbQuoteIdentifier(ANSI(), name))",
            "dbQuoteLiteral": "# Quoting ensures that arbitrary input is safe for use in a query\nname <- \"Robert'); DROP TABLE Students;--\"\ndbQuoteLiteral(ANSI(), name)\n\n# NAs become NULL\ndbQuoteLiteral(ANSI(), c(1:3, NA))\n\n# Logicals become integers by default\ndbQuoteLiteral(ANSI(), c(TRUE, FALSE, NA))\n\n# Raw vectors become hex strings by default\ndbQuoteLiteral(ANSI(), list(as.raw(1:3), NULL))\n\n# SQL vectors are always passed through as is\nvar_name <- SQL(\"select\")\nvar_name\ndbQuoteLiteral(ANSI(), var_name)\n\n# This mechanism is used to prevent double escaping\ndbQuoteLiteral(ANSI(), dbQuoteLiteral(ANSI(), name))",
            "dbQuoteString": "# Quoting ensures that arbitrary input is safe for use in a query\nname <- \"Robert'); DROP TABLE Students;--\"\ndbQuoteString(ANSI(), name)\n\n# NAs become NULL\ndbQuoteString(ANSI(), c(\"x\", NA))\n\n# SQL vectors are always passed through as is\nvar_name <- SQL(\"select\")\nvar_name\ndbQuoteString(ANSI(), var_name)\n\n# This mechanism is used to prevent double escaping\ndbQuoteString(ANSI(), dbQuoteString(ANSI(), name))",
            "dbReadTable": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars[1:10, ])\ndbReadTable(con, \"mtcars\")\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbReadTableArrow": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE) && requireNamespace(\"nanoarrow\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\n# Read data as Arrow table\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars[1:10, ])\ndbReadTableArrow(con, \"mtcars\")\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbRemoveTable": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbExistsTable(con, \"iris\")\ndbWriteTable(con, \"iris\", iris)\ndbExistsTable(con, \"iris\")\ndbRemoveTable(con, \"iris\")\ndbExistsTable(con, \"iris\")\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbSendQuery": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\nrs <- dbSendQuery(con, \"SELECT * FROM mtcars WHERE cyl = 4\")\ndbFetch(rs)\ndbClearResult(rs)\n\n# Pass one set of values with the param argument:\nrs <- dbSendQuery(\n  con,\n  \"SELECT * FROM mtcars WHERE cyl = ?\",\n  params = list(4L)\n)\ndbFetch(rs)\ndbClearResult(rs)\n\n# Pass multiple sets of values with dbBind():\nrs <- dbSendQuery(con, \"SELECT * FROM mtcars WHERE cyl = ?\")\ndbBind(rs, list(6L))\ndbFetch(rs)\ndbBind(rs, list(8L))\ndbFetch(rs)\ndbClearResult(rs)\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbSendQueryArrow": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE) && requireNamespace(\"nanoarrow\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\n# Retrieve data as arrow table\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars)\nrs <- dbSendQueryArrow(con, \"SELECT * FROM mtcars WHERE cyl = 4\")\ndbFetchArrow(rs)\ndbClearResult(rs)\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbSendStatement": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"cars\", head(cars, 3))\n\nrs <- dbSendStatement(\n  con,\n  \"INSERT INTO cars (speed, dist) VALUES (1, 1), (2, 2), (3, 3)\"\n)\ndbHasCompleted(rs)\ndbGetRowsAffected(rs)\ndbClearResult(rs)\ndbReadTable(con, \"cars\")   # there are now 6 rows\n\n# Pass one set of values directly using the param argument:\nrs <- dbSendStatement(\n  con,\n  \"INSERT INTO cars (speed, dist) VALUES (?, ?)\",\n  params = list(4L, 5L)\n)\ndbClearResult(rs)\n\n# Pass multiple sets of values using dbBind():\nrs <- dbSendStatement(\n  con,\n  \"INSERT INTO cars (speed, dist) VALUES (?, ?)\"\n)\ndbBind(rs, list(5:6, 6:7))\ndbBind(rs, list(7L, 8L))\ndbClearResult(rs)\ndbReadTable(con, \"cars\")   # there are now 10 rows\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbUnquoteIdentifier": "# Unquoting allows to understand the structure of a\n# possibly complex quoted identifier\ndbUnquoteIdentifier(\n  ANSI(),\n  SQL(c('\"Catalog\".\"Schema\".\"Table\"', '\"Schema\".\"Table\"', '\"UnqualifiedTable\"'))\n)\n\n# The returned object is always a list,\n# also for Id objects\ndbUnquoteIdentifier(ANSI(), Id(\"Catalog\", \"Schema\", \"Table\"))\n\n# Quoting and unquoting are inverses\ndbQuoteIdentifier(\n  ANSI(),\n  dbUnquoteIdentifier(ANSI(), SQL(\"UnqualifiedTable\"))[[1]]\n)\n\ndbQuoteIdentifier(\n  ANSI(),\n  dbUnquoteIdentifier(ANSI(), Id(\"Schema\", \"Table\"))[[1]]\n)",
            "dbWithTransaction": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"cash\", data.frame(amount = 100))\ndbWriteTable(con, \"account\", data.frame(amount = 2000))\n\n# All operations are carried out as logical unit:\ndbWithTransaction(\n  con,\n  {\n    withdrawal <- 300\n    dbExecute(con, \"UPDATE cash SET amount = amount + ?\", list(withdrawal))\n    dbExecute(con, \"UPDATE account SET amount = amount - ?\", list(withdrawal))\n  }\n)\n\n# The code is executed as if in the current environment:\nwithdrawal\n\n# The changes are committed to the database after successful execution:\ndbReadTable(con, \"cash\")\ndbReadTable(con, \"account\")\n\n# Rolling back with dbBreak():\ndbWithTransaction(\n  con,\n  {\n    withdrawal <- 5000\n    dbExecute(con, \"UPDATE cash SET amount = amount + ?\", list(withdrawal))\n    dbExecute(con, \"UPDATE account SET amount = amount - ?\", list(withdrawal))\n    if (dbReadTable(con, \"account\")$amount < 0) {\n      dbBreak()\n    }\n  }\n)\n\n# These changes were not committed to the database:\ndbReadTable(con, \"cash\")\ndbReadTable(con, \"account\")\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dbWriteTable": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"mtcars\", mtcars[1:5, ])\ndbReadTable(con, \"mtcars\")\n\ndbWriteTable(con, \"mtcars\", mtcars[6:10, ], append = TRUE)\ndbReadTable(con, \"mtcars\")\n\ndbWriteTable(con, \"mtcars\", mtcars[1:10, ], overwrite = TRUE)\ndbReadTable(con, \"mtcars\")\n\n# No row names\ndbWriteTable(con, \"mtcars\", mtcars[1:10, ], overwrite = TRUE, row.names = FALSE)\ndbReadTable(con, \"mtcars\")\n\\dontshow{\\}) # examplesIf}",
            "dbWriteTableArrow": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE) && requireNamespace(\"nanoarrow\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTableArrow(con, \"mtcars\", nanoarrow::as_nanoarrow_array_stream(mtcars[1:5, ]))\ndbReadTable(con, \"mtcars\")\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "dot-SQL92Keywords": "\"SELECT\" \\%in\\% .SQL92Keywords",
            "rownames": "# If have row names\nsqlRownamesToColumn(head(mtcars))\nsqlRownamesToColumn(head(mtcars), FALSE)\nsqlRownamesToColumn(head(mtcars), \"ROWNAMES\")\n\n# If don't have\nsqlRownamesToColumn(head(iris))\nsqlRownamesToColumn(head(iris), TRUE)\nsqlRownamesToColumn(head(iris), \"ROWNAMES\")",
            "sqlAppendTable": "sqlAppendTable(ANSI(), \"iris\", head(iris))\n\nsqlAppendTable(ANSI(), \"mtcars\", head(mtcars))\nsqlAppendTable(ANSI(), \"mtcars\", head(mtcars), row.names = FALSE)\nsqlAppendTableTemplate(ANSI(), \"iris\", iris)\n\nsqlAppendTableTemplate(ANSI(), \"mtcars\", mtcars)\nsqlAppendTableTemplate(ANSI(), \"mtcars\", mtcars, row.names = FALSE)",
            "sqlCreateTable": "sqlCreateTable(ANSI(), \"my-table\", c(a = \"integer\", b = \"text\"))\nsqlCreateTable(ANSI(), \"my-table\", iris)\n\n# By default, character row names are converted to a row_names colum\nsqlCreateTable(ANSI(), \"mtcars\", mtcars[, 1:5])\nsqlCreateTable(ANSI(), \"mtcars\", mtcars[, 1:5], row.names = FALSE)",
            "sqlData": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\nsqlData(con, head(iris))\nsqlData(con, head(mtcars))\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}",
            "sqlInterpolate": "sql <- \"SELECT * FROM X WHERE name = ?name\"\nsqlInterpolate(ANSI(), sql, name = \"Hadley\")\n\n# This is safe because the single quote has been double escaped\nsqlInterpolate(ANSI(), sql, name = \"H'); DROP TABLE--;\")\n\n# Using paste0() could lead to dangerous SQL with carefully crafted inputs\n# (SQL injection)\nname <- \"H'); DROP TABLE--;\"\npaste0(\"SELECT * FROM X WHERE name = '\", name, \"'\")\n\n# Use SQL() or dbQuoteIdentifier() to avoid escaping\nsql2 <- \"SELECT * FROM ?table WHERE name in ?names\"\nsqlInterpolate(ANSI(), sql2,\n  table = dbQuoteIdentifier(ANSI(), \"X\"),\n  names = SQL(\"('a', 'b')\")\n)\n\n# Don't use SQL() to escape identifiers to avoid SQL injection\nsqlInterpolate(ANSI(), sql2,\n  table = SQL(\"X; DELETE FROM X; SELECT * FROM X\"),\n  names = SQL(\"('a', 'b')\")\n)\n\n# Use dbGetQuery() or dbExecute() to process these queries:\nif (requireNamespace(\"RSQLite\", quietly = TRUE)) {\n  con <- dbConnect(RSQLite::SQLite())\n  sql <- \"SELECT ?value AS value\"\n  query <- sqlInterpolate(con, sql, value = 3)\n  print(dbGetQuery(con, query))\n  dbDisconnect(con)\n}",
            "sqlParseVariables": "# Use [] for quoting and no comments\nsqlParseVariablesImpl(\"[?a]\",\n  list(sqlQuoteSpec(\"[\", \"]\", \"\\\\\\\\\", FALSE)),\n  list()\n)\n\n# Standard quotes, use # for commenting\nsqlParseVariablesImpl(\"# ?a\\n?b\",\n  list(sqlQuoteSpec(\"'\", \"'\"), sqlQuoteSpec('\"', '\"')),\n  list(sqlCommentSpec(\"#\", \"\\n\", FALSE))\n)",
            "transactions": "\\dontshow{if (requireNamespace(\"RSQLite\", quietly = TRUE)) withAutoprint(\\{ # examplesIf}\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbWriteTable(con, \"cash\", data.frame(amount = 100))\ndbWriteTable(con, \"account\", data.frame(amount = 2000))\n\n# All operations are carried out as logical unit:\ndbBegin(con)\nwithdrawal <- 300\ndbExecute(con, \"UPDATE cash SET amount = amount + ?\", list(withdrawal))\ndbExecute(con, \"UPDATE account SET amount = amount - ?\", list(withdrawal))\ndbCommit(con)\n\ndbReadTable(con, \"cash\")\ndbReadTable(con, \"account\")\n\n# Rolling back after detecting negative value on account:\ndbBegin(con)\nwithdrawal <- 5000\ndbExecute(con, \"UPDATE cash SET amount = amount + ?\", list(withdrawal))\ndbExecute(con, \"UPDATE account SET amount = amount - ?\", list(withdrawal))\nif (dbReadTable(con, \"account\")$amount >= 0) {\n  dbCommit(con)\n} else {\n  dbRollback(con)\n}\n\ndbReadTable(con, \"cash\")\ndbReadTable(con, \"account\")\n\ndbDisconnect(con)\n\\dontshow{\\}) # examplesIf}"
        }
    },
    "haven": {
        "description": "Import foreign statistical formats into R via the embedded\n    'ReadStat' C library, <https://github.com/WizardMac/ReadStat>.",
        "examples": {
            "as_factor": "x <- labelled(sample(5, 10, replace = TRUE), c(Bad = 1, Good = 5))\n\n# Default method uses values where available\nas_factor(x)\n# You can also extract just the labels\nas_factor(x, levels = \"labels\")\n# Or just the values\nas_factor(x, levels = \"values\")\n# Or combine value and label\nas_factor(x, levels = \"both\")\n\n# as_factor() will preserve SPSS missing values from values and ranges\ny <- labelled_spss(1:10, na_values = c(2, 4), na_range = c(8, 10))\nas_factor(y)\n# use zap_missing() first to convert to NAs\nzap_missing(y)\nas_factor(zap_missing(y))",
            "labelled": "s1 <- labelled(c(\"M\", \"M\", \"F\"), c(Male = \"M\", Female = \"F\"))\ns2 <- labelled(c(1, 1, 2), c(Male = 1, Female = 2))\ns3 <- labelled(\n  c(1, 1, 2),\n  c(Male = 1, Female = 2),\n  label = \"Assigned sex at birth\"\n)\n\n# Unfortunately it's not possible to make as.factor work for labelled objects\n# so instead use as_factor. This works for all types of labelled vectors.\nas_factor(s1)\nas_factor(s1, levels = \"values\")\nas_factor(s2)\n\n# Other statistical software supports multiple types of missing values\ns3 <- labelled(\n  c(\"M\", \"M\", \"F\", \"X\", \"N/A\"),\n  c(Male = \"M\", Female = \"F\", Refused = \"X\", \"Not applicable\" = \"N/A\")\n)\ns3\nas_factor(s3)\n\n# Often when you have a partially labelled numeric vector, labelled values\n# are special types of missing. Use zap_labels to replace labels with missing\n# values\nx <- labelled(c(1, 2, 1, 2, 10, 9), c(Unknown = 9, Refused = 10))\nzap_labels(x)",
            "labelled_spss": "x1 <- labelled_spss(1:10, c(Good = 1, Bad = 8), na_values = c(9, 10))\nis.na(x1)\n\nx2 <- labelled_spss(\n  1:10,\n  c(Good = 1, Bad = 8),\n  na_range = c(9, Inf),\n  label = \"Quality rating\"\n)\nis.na(x2)\n\n# Print data and metadata\nx2",
            "print_labels": "s1 <- labelled(c(\"M\", \"M\", \"F\"), c(Male = \"M\", Female = \"F\"))\ns2 <- labelled(c(1, 1, 2), c(Male = 1, Female = 2))\nlabelled_df <- tibble::tibble(s1, s2)\n\nfor (var in names(labelled_df)) {\n  print_labels(labelled_df[[var]], var)\n}",
            "read_dta": "path <- system.file(\"examples\", \"iris.dta\", package = \"haven\")\nread_dta(path)\n\ntmp <- tempfile(fileext = \".dta\")\nwrite_dta(mtcars, tmp)\nread_dta(tmp)\nread_stata(tmp)",
            "read_sas": "path <- system.file(\"examples\", \"iris.sas7bdat\", package = \"haven\")\nread_sas(path)",
            "read_spss": "path <- system.file(\"examples\", \"iris.sav\", package = \"haven\")\nread_sav(path)\n\ntmp <- tempfile(fileext = \".sav\")\nwrite_sav(mtcars, tmp)\nread_sav(tmp)",
            "read_xpt": "tmp <- tempfile(fileext = \".xpt\")\nwrite_xpt(mtcars, tmp)\nread_xpt(tmp)",
            "tagged_na": "x <- c(1:5, tagged_na(\"a\"), tagged_na(\"z\"), NA)\n\n# Tagged NA's work identically to regular NAs\nx\nis.na(x)\n\n# To see that they're special, you need to use na_tag(),\n# is_tagged_na(), or print_tagged_na():\nis_tagged_na(x)\nna_tag(x)\nprint_tagged_na(x)\n\n# You can test for specific tagged NAs with the second argument\nis_tagged_na(x, \"a\")\n\n# Because the support for tagged's NAs is somewhat tagged on to R,\n# the left-most NA will tend to be preserved in arithmetic operations.\nna_tag(tagged_na(\"a\") + tagged_na(\"z\"))",
            "zap_empty": "x <- c(\"a\", \"\", \"c\")\nzap_empty(x)",
            "zap_label": "x1 <- labelled(1:5, c(good = 1, bad = 5), label = \"rating\")\nx1\nzap_label(x1)\n\nx2 <- labelled_spss(c(1:4, 9), label = \"score\", na_values = 9)\nx2\nzap_label(x2)\n\n# zap_label also works with data frames\ndf <- tibble::tibble(x1, x2)\nstr(df)\nstr(zap_label(df))",
            "zap_labels": "x1 <- labelled(1:5, c(good = 1, bad = 5))\nx1\nzap_labels(x1)\n\nx2 <- labelled_spss(c(1:4, 9), c(good = 1, bad = 5), na_values = 9)\nx2\nzap_labels(x2)\n\n# Keep the user defined missing values\nzap_labels(x2, user_na = TRUE)\n\n# zap_labels also works with data frames\ndf <- tibble::tibble(x1, x2)\ndf\nzap_labels(df)",
            "zap_missing": "x1 <- labelled(\n  c(1, 5, tagged_na(\"a\", \"b\")),\n  c(Unknown = tagged_na(\"a\"), Refused = tagged_na(\"b\"))\n)\nx1\nzap_missing(x1)\n\nx2 <- labelled_spss(\n  c(1, 2, 1, 99),\n  c(missing = 99),\n  na_value = 99\n)\nx2\nzap_missing(x2)\n\n# You can also apply to data frames\ndf <- tibble::tibble(x1, x2, y = 4:1)\ndf\nzap_missing(df)"
        }
    },
    "rvest": {
        "description": "Wrappers around the 'xml2' and 'httr' packages to make it\n    easy to download, then manipulate, HTML and XML.",
        "examples": {
            "LiveHTML": "\\dontrun{\n# To retrieve data for this paginated site, we need to repeatedly push\n# the \"Load More\" button\nsess <- read_html_live(\"https://www.bodybuilding.com/exercises/finder\")\nsess$view()\n\nsess \\%>\\% html_elements(\".ExResult-row\") \\%>\\% length()\nsess$click(\".ExLoadMore-btn\")\nsess \\%>\\% html_elements(\".ExResult-row\") \\%>\\% length()\nsess$click(\".ExLoadMore-btn\")\nsess \\%>\\% html_elements(\".ExResult-row\") \\%>\\% length()\n}",
            "html_attr": "html <- minimal_html('<ul>\n  <li><a href=\"https://a.com\" class=\"important\">a</a></li>\n  <li class=\"active\"><a href=\"https://c.com\">b</a></li>\n  <li><a href=\"https://c.com\">b</a></li>\n  </ul>')\n\nhtml \\%>\\% html_elements(\"a\") \\%>\\% html_attrs()\n\nhtml \\%>\\% html_elements(\"a\") \\%>\\% html_attr(\"href\")\nhtml \\%>\\% html_elements(\"li\") \\%>\\% html_attr(\"class\")\nhtml \\%>\\% html_elements(\"li\") \\%>\\% html_attr(\"class\", default = \"inactive\")",
            "html_children": "html <- minimal_html(\"<ul><li>1<li>2<li>3</ul>\")\nul <- html_elements(html, \"ul\")\nhtml_children(ul)\n\nhtml <- minimal_html(\"<p>Hello <b>Hadley</b><i>!</i>\")\np <- html_elements(html, \"p\")\nhtml_children(p)",
            "html_element": "html <- minimal_html(\"\n  <h1>This is a heading</h1>\n  <p id='first'>This is a paragraph</p>\n  <p class='important'>This is an important paragraph</p>\n\")\n\nhtml \\%>\\% html_element(\"h1\")\nhtml \\%>\\% html_elements(\"p\")\nhtml \\%>\\% html_elements(\".important\")\nhtml \\%>\\% html_elements(\"#first\")\n\n# html_element() vs html_elements() --------------------------------------\nhtml <- minimal_html(\"\n  <ul>\n    <li><b>C-3PO</b> is a <i>droid</i> that weighs <span class='weight'>167 kg</span></li>\n    <li><b>R2-D2</b> is a <i>droid</i> that weighs <span class='weight'>96 kg</span></li>\n    <li><b>Yoda</b> weighs <span class='weight'>66 kg</span></li>\n    <li><b>R4-P17</b> is a <i>droid</i></li>\n  </ul>\n\")\nli <- html \\%>\\% html_elements(\"li\")\n\n# When applied to a node set, html_elements() returns all matching elements\n# beneath any of the inputs, flattening results into a new node set.\nli \\%>\\% html_elements(\"i\")\n\n# When applied to a node set, html_element() always returns a vector the\n# same length as the input, using a \"missing\" element where needed.\nli \\%>\\% html_element(\"i\")\n# and html_text() and html_attr() will return NA\nli \\%>\\% html_element(\"i\") \\%>\\% html_text2()\nli \\%>\\% html_element(\"span\") \\%>\\% html_attr(\"class\")",
            "html_encoding_guess": "# A file with bad encoding included in the package\npath <- system.file(\"html-ex\", \"bad-encoding.html\", package = \"rvest\")\nx <- read_html(path)\nx \\%>\\% html_elements(\"p\") \\%>\\% html_text()\n\nhtml_encoding_guess(x)\n# Two valid encodings, only one of which is correct\nread_html(path, encoding = \"ISO-8859-1\") \\%>\\% html_elements(\"p\") \\%>\\% html_text()\nread_html(path, encoding = \"ISO-8859-2\") \\%>\\% html_elements(\"p\") \\%>\\% html_text()",
            "html_form": "html <- read_html(\"http://www.google.com\")\nsearch <- html_form(html)[[1]]\n\nsearch <- search \\%>\\% html_form_set(q = \"My little pony\", hl = \"fr\")\n\n# Or if you have a list of values, use !!!\nvals <- list(q = \"web scraping\", hl = \"en\")\nsearch <- search \\%>\\% html_form_set(!!!vals)\n\n# To submit and get result:\n\\dontrun{\nresp <- html_form_submit(search)\nread_html(resp)\n}",
            "html_name": "url <- \"https://rvest.tidyverse.org/articles/starwars.html\"\nhtml <- read_html(url)\n\nhtml \\%>\\%\n  html_element(\"div\") \\%>\\%\n  html_children() \\%>\\%\n  html_name()",
            "html_table": "sample1 <- minimal_html(\"<table>\n  <tr><th>Col A</th><th>Col B</th></tr>\n  <tr><td>1</td><td>x</td></tr>\n  <tr><td>4</td><td>y</td></tr>\n  <tr><td>10</td><td>z</td></tr>\n</table>\")\nsample1 \\%>\\%\n  html_element(\"table\") \\%>\\%\n  html_table()\n\n# Values in merged cells will be duplicated\nsample2 <- minimal_html(\"<table>\n  <tr><th>A</th><th>B</th><th>C</th></tr>\n  <tr><td>1</td><td>2</td><td>3</td></tr>\n  <tr><td colspan='2'>4</td><td>5</td></tr>\n  <tr><td>6</td><td colspan='2'>7</td></tr>\n</table>\")\nsample2 \\%>\\%\n  html_element(\"table\") \\%>\\%\n  html_table()\n\n# If a row is missing cells, they'll be filled with NAs\nsample3 <- minimal_html(\"<table>\n  <tr><th>A</th><th>B</th><th>C</th></tr>\n  <tr><td colspan='2'>1</td><td>2</td></tr>\n  <tr><td colspan='2'>3</td></tr>\n  <tr><td>4</td></tr>\n</table>\")\nsample3 \\%>\\%\n  html_element(\"table\") \\%>\\%\n  html_table()",
            "html_text": "# To understand the difference between html_text() and html_text2()\n# take the following html:\n\nhtml <- minimal_html(\n  \"<p>This is a paragraph.\n    This another sentence.<br>This should start on a new line\"\n)\n\n# html_text() returns the raw underlying text, which includes whitespace\n# that would be ignored by a browser, and ignores the <br>\nhtml \\%>\\% html_element(\"p\") \\%>\\% html_text() \\%>\\% writeLines()\n\n# html_text2() simulates what a browser would display. Non-significant\n# whitespace is collapsed, and <br> is turned into a line break\nhtml \\%>\\% html_element(\"p\") \\%>\\% html_text2() \\%>\\% writeLines()\n\n# By default, html_text2() also converts non-breaking spaces to regular\n# spaces:\nhtml <- minimal_html(\"<p>x&nbsp;y</p>\")\nx1 <- html \\%>\\% html_element(\"p\") \\%>\\% html_text()\nx2 <- html \\%>\\% html_element(\"p\") \\%>\\% html_text2()\n\n# When printed, non-breaking spaces look exactly like regular spaces\nx1\nx2\n# But aren't actually the same:\nx1 == x2\n# Which you can confirm by looking at their underlying binary\n# representaion:\ncharToRaw(x1)\ncharToRaw(x2)",
            "minimal_html": "minimal_html(\"<p>test</p>\")",
            "read_html": "# Start by reading a HTML page with read_html():\nstarwars <- read_html(\"https://rvest.tidyverse.org/articles/starwars.html\")\n\n# Then find elements that match a css selector or XPath expression\n# using html_elements(). In this example, each <section> corresponds\n# to a different film\nfilms <- starwars \\%>\\% html_elements(\"section\")\nfilms\n\n# Then use html_element() to extract one element per film. Here\n# we the title is given by the text inside <h2>\ntitle <- films \\%>\\%\n  html_element(\"h2\") \\%>\\%\n  html_text2()\ntitle\n\n# Or use html_attr() to get data out of attributes. html_attr() always\n# returns a string so we convert it to an integer using a readr function\nepisode <- films \\%>\\%\n  html_element(\"h2\") \\%>\\%\n  html_attr(\"data-id\") \\%>\\%\n  readr::parse_integer()\nepisode",
            "read_html_live": "\\dontrun{\n# When we retrieve the raw HTML for this site, it doesn't contain the\n# data we're interested in:\nstatic <- read_html(\"https://www.forbes.com/top-colleges/\")\nstatic \\%>\\% html_elements(\".TopColleges2023_tableRow__BYOSU\")\n\n# Instead, we need to run the site in a real web browser, causing it to\n# download a JSON file and then dynamically generate the html:\n\nsess <- read_html_live(\"https://www.forbes.com/top-colleges/\")\nsess$view()\nrows <- sess \\%>\\% html_elements(\".TopColleges2023_tableRow__BYOSU\")\nrows \\%>\\% html_element(\".TopColleges2023_organizationName__J1lEV\") \\%>\\% html_text()\nrows \\%>\\% html_element(\".grant-aid\") \\%>\\% html_text()\n}",
            "session": "s <- session(\"http://hadley.nz\")\ns \\%>\\%\n  session_jump_to(\"hadley.jpg\") \\%>\\%\n  session_jump_to(\"/\") \\%>\\%\n  session_history()\n\ns \\%>\\%\n  session_jump_to(\"hadley.jpg\") \\%>\\%\n  session_back() \\%>\\%\n  session_history()\n\n\\donttest{\ns \\%>\\%\n  session_follow_link(css = \"p a\") \\%>\\%\n  html_elements(\"p\")\n}"
        }
    },
    "rprojroot": {
        "description": "Robust, reliable and flexible paths to files below\n    a project root. The 'root' of a project is defined as a directory that\n    matches a certain criterion, e.g., it contains a certain regular file.",
        "examples": {
            "find_root": "\\dontrun{\nfind_root(has_file_pattern(\n  pattern = glob2rx(\"DESCRIPTION\"),\n  contents = \"^Package: \"\n))\n}",
            "find_root_file": "\\dontrun{\nfind_package_root_file(\"tests\", \"testthat.R\")\nhas_file(\"DESCRIPTION\", \"^Package: \")$find_file\nhas_file(\"DESCRIPTION\", \"^Package: \")$make_fix_file(\".\")\n}",
            "root_criterion": "root_criterion(function(path) file.exists(file.path(path, \"somefile\")), \"has somefile\")\nhas_file(\"DESCRIPTION\")\nis_r_package\n\\dontrun{\nis_r_package$find_file\nis_r_package$make_fix_file(\".\")\n}",
            "rprojroot-package": "criteria\n\\dontrun{\nis_r_package$find_file(\"NAMESPACE\")\nroot_fun <- is_r_package$make_fix_file()\nroot_fun(\"NAMESPACE\")\n}",
            "thisfile": "\\dontrun{\nthisfile()\n}"
        }
    },
    "reshape": {
        "description": "Flexibly restructure and aggregate data using just two\n    functions: melt and 'dcast' (or 'acast').",
        "examples": {
            "cast": "#Air quality example\nnames(airquality) <- tolower(names(airquality))\naqm <- melt(airquality, id=c(\"month\", \"day\"), na.rm=TRUE)\n\nacast(aqm, day ~ month ~ variable)\nacast(aqm, month ~ variable, mean)\nacast(aqm, month ~ variable, mean, margins = TRUE)\ndcast(aqm, month ~ variable, mean, margins = c(\"month\", \"variable\"))\n\nlibrary(plyr) # needed to access . function\nacast(aqm, variable ~ month, mean, subset = .(variable == \"ozone\"))\nacast(aqm, variable ~ month, mean, subset = .(month == 5))\n\n#Chick weight example\nnames(ChickWeight) <- tolower(names(ChickWeight))\nchick_m <- melt(ChickWeight, id=2:4, na.rm=TRUE)\n\ndcast(chick_m, time ~ variable, mean) # average effect of time\ndcast(chick_m, diet ~ variable, mean) # average effect of diet\nacast(chick_m, diet ~ time, mean) # average effect of diet & time\n\n# How many chicks at each time? - checking for balance\nacast(chick_m, time ~ diet, length)\nacast(chick_m, chick ~ time, mean)\nacast(chick_m, chick ~ time, mean, subset = .(time < 10 & chick < 20))\n\nacast(chick_m, time ~ diet, length)\n\ndcast(chick_m, diet + chick ~ time)\nacast(chick_m, diet + chick ~ time)\nacast(chick_m, chick ~ time ~ diet)\nacast(chick_m, diet + chick ~ time, length, margins=\"diet\")\nacast(chick_m, diet + chick ~ time, length, drop = FALSE)\n\n#Tips example\ndcast(melt(tips), sex ~ smoker, mean, subset = .(variable == \"total_bill\"))\n\nff_d <- melt(french_fries, id=1:4, na.rm=TRUE)\nacast(ff_d, subject ~ time, length)\nacast(ff_d, subject ~ time, length, fill=0)\ndcast(ff_d, treatment ~ variable, mean, margins = TRUE)\ndcast(ff_d, treatment + subject ~ variable, mean, margins=\"treatment\")\nif (require(\"lattice\")) {\n lattice::xyplot(`1` ~ `2` | variable, dcast(ff_d, ... ~ rep), aspect=\"iso\")\n}",
            "colsplit": "x <- c(\"a_1\", \"a_2\", \"b_2\", \"c_3\")\nvars <- colsplit(x, \"_\", c(\"trt\", \"time\"))\nvars\nstr(vars)",
            "melt.array": "a <- array(c(1:23, NA), c(2,3,4))\nmelt(a)\nmelt(a, na.rm = TRUE)\nmelt(a, varnames=c(\"X\",\"Y\",\"Z\"))\ndimnames(a) <- lapply(dim(a), function(x) LETTERS[1:x])\nmelt(a)\nmelt(a, varnames=c(\"X\",\"Y\",\"Z\"))\ndimnames(a)[1] <- list(NULL)\nmelt(a)",
            "melt.data.frame": "names(airquality) <- tolower(names(airquality))\nmelt(airquality, id=c(\"month\", \"day\"))\nnames(ChickWeight) <- tolower(names(ChickWeight))\nmelt(ChickWeight, id=2:4)",
            "melt.list": "a <- as.list(c(1:4, NA))\nmelt(a)\nnames(a) <- letters[1:4]\nmelt(a)\na <- list(matrix(1:4, ncol=2), matrix(1:6, ncol=2))\nmelt(a)\na <- list(matrix(1:4, ncol=2), array(1:27, c(3,3,3)))\nmelt(a)\nmelt(list(1:5, matrix(1:4, ncol=2)))\nmelt(list(list(1:3), 1, list(as.list(3:4), as.list(1:2))))",
            "parse_formula": "reshape2:::parse_formula(\"a + ...\", letters[1:6])\nreshape2:::parse_formula(\"a ~ b + d\")\nreshape2:::parse_formula(\"a + b ~ c ~ .\")",
            "recast": "recast(french_fries, time ~ variable, id.var = 1:4)"
        }
    },
    "fastmap": {
        "description": "Fast implementation of data structures, including a key-value\n    store, stack, and queue. Environments are commonly used as key-value stores\n    in R, but every time a new key is used, it is added to R's global symbol\n    table, causing a small amount of memory leakage. This can be problematic in\n    cases where many different keys are used. Fastmap avoids this memory leak\n    issue by implementing the map using data structures in C++.",
        "examples": {
            "fastmap": "# Create the fastmap object\nm <- fastmap()\n\n# Set some key-value pairs\nm$set(\"x\", 100)\nm$set(\"letters\", c(\"a\", \"b\", \"c\"))\nm$mset(numbers = c(10, 20, 30), nothing = NULL)\n\n# Get values using keys\nm$get(\"x\")\nm$get(\"numbers\")\nm$mget(c(\"letters\", \"numbers\"))\n\n# Missing keys return NULL by default, but this can be customized\nm$get(\"xyz\")\n\n# Check for existence of keys\nm$has(\"x\")\nm$has(\"nothing\")\nm$has(\"xyz\")\n\n# Remove one or more items\nm$remove(c(\"letters\", \"x\"))\n\n# Return number of items\nm$size()\n\n# Get all keys\nm$keys()\n\n# Return named list that represents all key-value pairs\nstr(m$as_list())\n\n# Clear the map\nm$reset()\n\n\n# Specify missing value when get() is called\nm <- fastmap()\nm$get(\"x\", missing = key_missing())\n#> <Key Missing>\n\n# Specify the default missing value\nm <- fastmap(missing_default = key_missing())\nm$get(\"x\")\n#> <Key Missing>"
        }
    },
    "shiny": {
        "description": "Makes it incredibly easy to build interactive web\n    applications with R. Automatic \"reactive\" binding between inputs and\n    outputs and extensive prebuilt widgets make it possible to build\n    beautiful, responsive, and powerful applications with minimal effort.",
        "examples": {
            "ExtendedTask": "\\dontshow{if (rlang::is_interactive() && rlang::is_installed(\"future\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(future)\nplan(multisession)\n\nui <- page_fluid(\n  titlePanel(\"Extended Task Demo\"),\n  p(\n    'Click the button below to perform a \"calculation\"',\n    \"that takes a while to perform.\"\n  ),\n  input_task_button(\"recalculate\", \"Recalculate\"),\n  p(textOutput(\"result\"))\n)\n\nserver <- function(input, output) {\n  rand_task <- ExtendedTask$new(function() {\n    future(\n      {\n        # Slow operation goes here\n        Sys.sleep(2)\n        sample(1:100, 1)\n      },\n      seed = TRUE\n    )\n  })\n\n  # Make button state reflect task.\n  # If using R >=4.1, you can do this instead:\n  # rand_task <- ExtendedTask$new(...) |> bind_task_button(\"recalculate\")\n  bind_task_button(rand_task, \"recalculate\")\n\n  observeEvent(input$recalculate, {\n    # Invoke the extended in an observer\n    rand_task$invoke()\n  })\n\n  output$result <- renderText({\n    # React to updated results when the task completes\n    number <- rand_task$result()\n    paste0(\"Your number is \", number, \".\")\n  })\n}\n\nshinyApp(ui, server)\n\\dontshow{\\}) # examplesIf}",
            "MockShinySession": "## ------------------------------------------------\n## Method `MockShinySession$setInputs`\n## ------------------------------------------------\n\n\\dontrun{\nsession$setInputs(x=1, y=2)\n}",
            "Progress": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  plotOutput(\"plot\")\n)\n\nserver <- function(input, output, session) {\n  output$plot <- renderPlot({\n    progress <- Progress$new(session, min=1, max=15)\n    on.exit(progress$close())\n\n    progress$set(message = 'Calculation in progress',\n                 detail = 'This may take a while...')\n\n    for (i in 1:15) {\n      progress$set(value = i)\n      Sys.sleep(0.5)\n    }\n    plot(cars)\n  })\n}\n\nshinyApp(ui, server)\n}",
            "actionButton": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  sliderInput(\"obs\", \"Number of observations\", 0, 1000, 500),\n  actionButton(\"goButton\", \"Go!\", class = \"btn-success\"),\n  plotOutput(\"distPlot\")\n)\n\nserver <- function(input, output) {\n  output$distPlot <- renderPlot({\n    # Take a dependency on input$goButton. This will run once initially,\n    # because the value changes from NULL to 0.\n    input$goButton\n\n    # Use isolate() to avoid dependency on input$obs\n    dist <- isolate(rnorm(input$obs))\n    hist(dist)\n  })\n}\n\nshinyApp(ui, server)\n\n}\n\n## Example of adding extra class values\nactionButton(\"largeButton\", \"Large Primary Button\", class = \"btn-primary btn-lg\")\nactionLink(\"infoLink\", \"Information Link\", class = \"btn-info\")",
            "bindCache": "\\dontrun{\nrc <- bindCache(\n  x = reactive({\n    Sys.sleep(2)   # Pretend this is expensive\n    input$x * 100\n  }),\n  input$x\n)\n\n# Can make it prettier with the \\%>\\% operator\nlibrary(magrittr)\n\nrc <- reactive({\n  Sys.sleep(2)\n  input$x * 100\n}) \\%>\\%\n  bindCache(input$x)\n\n}\n\n## Only run app examples in interactive R sessions\nif (interactive()) {\n\n# Basic example\nshinyApp(\n  ui = fluidPage(\n    sliderInput(\"x\", \"x\", 1, 10, 5),\n    sliderInput(\"y\", \"y\", 1, 10, 5),\n    div(\"x * y: \"),\n    verbatimTextOutput(\"txt\")\n  ),\n  server = function(input, output) {\n    r <- reactive({\n      # The value expression is an _expensive_ computation\n      message(\"Doing expensive computation...\")\n      Sys.sleep(2)\n      input$x * input$y\n    }) \\%>\\%\n      bindCache(input$x, input$y)\n\n    output$txt <- renderText(r())\n  }\n)\n\n\n# Caching renderText\nshinyApp(\n  ui = fluidPage(\n    sliderInput(\"x\", \"x\", 1, 10, 5),\n    sliderInput(\"y\", \"y\", 1, 10, 5),\n    div(\"x * y: \"),\n    verbatimTextOutput(\"txt\")\n  ),\n  server = function(input, output) {\n    output$txt <- renderText({\n      message(\"Doing expensive computation...\")\n      Sys.sleep(2)\n      input$x * input$y\n    }) \\%>\\%\n      bindCache(input$x, input$y)\n  }\n)\n\n\n# Demo of using events and caching with an actionButton\nshinyApp(\n  ui = fluidPage(\n    sliderInput(\"x\", \"x\", 1, 10, 5),\n    sliderInput(\"y\", \"y\", 1, 10, 5),\n    actionButton(\"go\", \"Go\"),\n    div(\"x * y: \"),\n    verbatimTextOutput(\"txt\")\n  ),\n  server = function(input, output) {\n    r <- reactive({\n      message(\"Doing expensive computation...\")\n      Sys.sleep(2)\n      input$x * input$y\n    }) \\%>\\%\n      bindCache(input$x, input$y) \\%>\\%\n      bindEvent(input$go)\n      # The cached, eventified reactive takes a reactive dependency on\n      # input$go, but doesn't use it for the cache key. It uses input$x and\n      # input$y for the cache key, but doesn't take a reactive dependency on\n      # them, because the reactive dependency is superseded by addEvent().\n\n    output$txt <- renderText(r())\n  }\n)\n\n}",
            "bookmarkButton": "## Only run these examples in interactive sessions\nif (interactive()) {\n\n# This example shows how to use multiple bookmark buttons. If you only need\n# a single bookmark button, see examples in ?enableBookmarking.\nui <- function(request) {\n  fluidPage(\n    tabsetPanel(id = \"tabs\",\n      tabPanel(\"One\",\n        checkboxInput(\"chk1\", \"Checkbox 1\"),\n        bookmarkButton(id = \"bookmark1\")\n      ),\n      tabPanel(\"Two\",\n        checkboxInput(\"chk2\", \"Checkbox 2\"),\n        bookmarkButton(id = \"bookmark2\")\n      )\n    )\n  )\n}\nserver <- function(input, output, session) {\n  # Need to exclude the buttons from themselves being bookmarked\n  setBookmarkExclude(c(\"bookmark1\", \"bookmark2\"))\n\n  # Trigger bookmarking with either button\n  observeEvent(input$bookmark1, {\n    session$doBookmark()\n  })\n  observeEvent(input$bookmark2, {\n    session$doBookmark()\n  })\n}\nenableBookmarking(store = \"url\")\nshinyApp(ui, server)\n}",
            "brushedPoints": "\\dontrun{\n# Note that in practice, these examples would need to go in reactives\n# or observers.\n\n# This would select all points within 5 pixels of the click\nnearPoints(mtcars, input$plot_click)\n\n# Select just the nearest point within 10 pixels of the click\nnearPoints(mtcars, input$plot_click, threshold = 10, maxpoints = 1)\n\n}",
            "busyIndicatorOptions": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(bslib)\n\ncard_ui <- function(id, spinner_type = id) {\n  card(\n    busyIndicatorOptions(spinner_type = spinner_type),\n    card_header(paste(\"Spinner:\", spinner_type)),\n    plotOutput(shiny::NS(id, \"plot\"))\n  )\n}\n\ncard_server <- function(id, simulate = reactive()) {\n  moduleServer(\n    id = id,\n    function(input, output, session) {\n      output$plot <- renderPlot({\n        Sys.sleep(1)\n        simulate()\n        plot(x = rnorm(100), y = rnorm(100))\n      })\n    }\n  )\n}\n\nui <- page_fillable(\n  useBusyIndicators(),\n  input_task_button(\"simulate\", \"Simulate\", icon = icon(\"refresh\")),\n  layout_columns(\n    card_ui(\"ring\"),\n    card_ui(\"bars\"),\n    card_ui(\"dots\"),\n    card_ui(\"pulse\"),\n    col_widths = 6\n  )\n)\n\nserver <- function(input, output, session) {\n  simulate <- reactive(input$simulate)\n  card_server(\"ring\", simulate)\n  card_server(\"bars\", simulate)\n  card_server(\"dots\", simulate)\n  card_server(\"pulse\", simulate)\n}\n\nshinyApp(ui, server)\n\\dontshow{\\}) # examplesIf}",
            "checkboxGroupInput": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  checkboxGroupInput(\"variable\", \"Variables to show:\",\n                     c(\"Cylinders\" = \"cyl\",\n                       \"Transmission\" = \"am\",\n                       \"Gears\" = \"gear\")),\n  tableOutput(\"data\")\n)\n\nserver <- function(input, output, session) {\n  output$data <- renderTable({\n    mtcars[, c(\"mpg\", input$variable), drop = FALSE]\n  }, rownames = TRUE)\n}\n\nshinyApp(ui, server)\n\nui <- fluidPage(\n  checkboxGroupInput(\"icons\", \"Choose icons:\",\n    choiceNames =\n      list(icon(\"calendar\"), icon(\"bed\"),\n           icon(\"cog\"), icon(\"bug\")),\n    choiceValues =\n      list(\"calendar\", \"bed\", \"cog\", \"bug\")\n  ),\n  textOutput(\"txt\")\n)\n\nserver <- function(input, output, session) {\n  output$txt <- renderText({\n    icons <- paste(input$icons, collapse = \", \")\n    paste(\"You chose\", icons)\n  })\n}\n\nshinyApp(ui, server)\n}",
            "checkboxInput": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  checkboxInput(\"somevalue\", \"Some value\", FALSE),\n  verbatimTextOutput(\"value\")\n)\nserver <- function(input, output) {\n  output$value <- renderText({ input$somevalue })\n}\nshinyApp(ui, server)\n}",
            "column": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  fluidRow(\n    column(4,\n      sliderInput(\"obs\", \"Number of observations:\",\n                  min = 1, max = 1000, value = 500)\n    ),\n    column(8,\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  output$distPlot <- renderPlot({\n    hist(rnorm(input$obs))\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nui <- fluidPage(\n  fluidRow(\n    column(width = 4,\n      \"4\"\n    ),\n    column(width = 3, offset = 2,\n      \"3 offset 2\"\n    )\n  )\n)\nshinyApp(ui, server = function(input, output) { })\n}",
            "conditionalPanel": "## Only run this example in interactive R sessions\nif (interactive()) {\n  ui <- fluidPage(\n    sidebarPanel(\n      selectInput(\"plotType\", \"Plot Type\",\n        c(Scatter = \"scatter\", Histogram = \"hist\")\n      ),\n      # Only show this panel if the plot type is a histogram\n      conditionalPanel(\n        condition = \"input.plotType == 'hist'\",\n        selectInput(\n          \"breaks\", \"Breaks\",\n          c(\"Sturges\", \"Scott\", \"Freedman-Diaconis\", \"[Custom]\" = \"custom\")\n        ),\n        # Only show this panel if Custom is selected\n        conditionalPanel(\n          condition = \"input.breaks == 'custom'\",\n          sliderInput(\"breakCount\", \"Break Count\", min = 1, max = 50, value = 10)\n        )\n      )\n    ),\n    mainPanel(\n      plotOutput(\"plot\")\n    )\n  )\n\n  server <- function(input, output) {\n    x <- rnorm(100)\n    y <- rnorm(100)\n\n    output$plot <- renderPlot({\n      if (input$plotType == \"scatter\") {\n        plot(x, y)\n      } else {\n        breaks <- input$breaks\n        if (breaks == \"custom\") {\n          breaks <- input$breakCount\n        }\n\n        hist(x, breaks = breaks)\n      }\n    })\n  }\n\n  shinyApp(ui, server)\n}",
            "createRenderFunction": "# A custom render function that repeats the supplied value 3 times\nrenderTriple <- function(expr) {\n  # Wrap user-supplied reactive expression into a function\n  func <- quoToFunction(rlang::enquo0(expr))\n\n  createRenderFunction(\n    func,\n    transform = function(value, session, name, ...) {\n      paste(rep(value, 3), collapse=\", \")\n    },\n    outputFunc = textOutput\n  )\n}\n\n# For better legacy support, consider using installExprFunction() over quoToFunction()\nrenderTripleLegacy <- function(expr, env = parent.frame(), quoted = FALSE) {\n  func <- installExprFunction(expr, \"func\", env, quoted)\n\n  createRenderFunction(\n    func,\n    transform = function(value, session, name, ...) {\n      paste(rep(value, 3), collapse=\", \")\n    },\n    outputFunc = textOutput\n  )\n}\n\n# Test render function from the console\nreactiveConsole(TRUE)\n\nv <- reactiveVal(\"basic\")\nr <- renderTriple({ v() })\nr()\n#> [1] \"basic, basic, basic\"\n\n# User can supply quoted code via rlang::quo(). Note that evaluation of the\n# expression happens when r2() is invoked, not when r2 is created.\nq <- rlang::quo({ v() })\nr2 <- rlang::inject(renderTriple(!!q))\nv(\"rlang\")\nr2()\n#> [1] \"rlang, rlang, rlang\"\n\n# Supplying quoted code without rlang::quo() requires installExprFunction()\nexpr <- quote({ v() })\nr3 <- renderTripleLegacy(expr, quoted = TRUE)\nv(\"legacy\")\nr3()\n#> [1] \"legacy, legacy, legacy\"\n\n# The legacy approach also supports with quosures (env is ignored in this case)\nq <- rlang::quo({ v() })\nr4 <- renderTripleLegacy(q, quoted = TRUE)\nv(\"legacy-rlang\")\nr4()\n#> [1] \"legacy-rlang, legacy-rlang, legacy-rlang\"\n\n# Turn off reactivity in the console\nreactiveConsole(FALSE)",
            "dateInput": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  dateInput(\"date1\", \"Date:\", value = \"2012-02-29\"),\n\n  # Default value is the date in client's time zone\n  dateInput(\"date2\", \"Date:\"),\n\n  # value is always yyyy-mm-dd, even if the display format is different\n  dateInput(\"date3\", \"Date:\", value = \"2012-02-29\", format = \"mm/dd/yy\"),\n\n  # Pass in a Date object\n  dateInput(\"date4\", \"Date:\", value = Sys.Date()-10),\n\n  # Use different language and different first day of week\n  dateInput(\"date5\", \"Date:\",\n          language = \"ru\",\n          weekstart = 1),\n\n  # Start with decade view instead of default month view\n  dateInput(\"date6\", \"Date:\",\n            startview = \"decade\"),\n\n  # Disable Mondays and Tuesdays.\n  dateInput(\"date7\", \"Date:\", daysofweekdisabled = c(1,2)),\n\n  # Disable specific dates.\n  dateInput(\"date8\", \"Date:\", value = \"2012-02-29\",\n            datesdisabled = c(\"2012-03-01\", \"2012-03-02\"))\n)\n\nshinyApp(ui, server = function(input, output) { })\n}",
            "dateRangeInput": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  dateRangeInput(\"daterange1\", \"Date range:\",\n                 start = \"2001-01-01\",\n                 end   = \"2010-12-31\"),\n\n  # Default start and end is the current date in the client's time zone\n  dateRangeInput(\"daterange2\", \"Date range:\"),\n\n  # start and end are always specified in yyyy-mm-dd, even if the display\n  # format is different\n  dateRangeInput(\"daterange3\", \"Date range:\",\n                 start  = \"2001-01-01\",\n                 end    = \"2010-12-31\",\n                 min    = \"2001-01-01\",\n                 max    = \"2012-12-21\",\n                 format = \"mm/dd/yy\",\n                 separator = \" - \"),\n\n  # Pass in Date objects\n  dateRangeInput(\"daterange4\", \"Date range:\",\n                 start = Sys.Date()-10,\n                 end = Sys.Date()+10),\n\n  # Use different language and different first day of week\n  dateRangeInput(\"daterange5\", \"Date range:\",\n                 language = \"de\",\n                 weekstart = 1),\n\n  # Start with decade view instead of default month view\n  dateRangeInput(\"daterange6\", \"Date range:\",\n                 startview = \"decade\")\n)\n\nshinyApp(ui, server = function(input, output) { })\n}",
            "debounce": "## Only run examples in interactive R sessions\nif (interactive()) {\noptions(device.ask.default = FALSE)\n\nlibrary(shiny)\nlibrary(magrittr)\n\nui <- fluidPage(\n  plotOutput(\"plot\", click = clickOpts(\"hover\")),\n  helpText(\"Quickly click on the plot above, while watching the result table below:\"),\n  tableOutput(\"result\")\n)\n\nserver <- function(input, output, session) {\n  hover <- reactive({\n    if (is.null(input$hover))\n      list(x = NA, y = NA)\n    else\n      input$hover\n  })\n  hover_d <- hover \\%>\\% debounce(1000)\n  hover_t <- hover \\%>\\% throttle(1000)\n\n  output$plot <- renderPlot({\n    plot(cars)\n  })\n\n  output$result <- renderTable({\n    data.frame(\n      mode = c(\"raw\", \"throttle\", \"debounce\"),\n      x = c(hover()$x, hover_t()$x, hover_d()$x),\n      y = c(hover()$y, hover_t()$y, hover_d()$y)\n    )\n  })\n}\n\nshinyApp(ui, server)\n}",
            "devmode": "# Enable Shiny Developer mode\ndevmode()\n\nin_devmode() # TRUE/FALSE?\n\n# Execute code in a temporary shiny dev mode\nwith_devmode(TRUE, in_devmode()) # TRUE\n\n# Ex: Within shiny, we register the option \"shiny.minified\"\n#   to default to `FALSE` when in Dev Mode\n\\dontrun{register_devmode_option(\n  \"shiny.minified\",\n  devmode_message = paste0(\n    \"Using full shiny javascript file. \",\n    \"To use the minified version, call `options(shiny.minified = TRUE)`\"\n  ),\n  devmode_default = FALSE\n)}\n\n# Used within `shiny::runApp(launch.browser)`\nget_devmode_option(\"shiny.minified\", TRUE) # TRUE if Dev mode is off\nis_minified <- with_devmode(TRUE, {\n  get_devmode_option(\"shiny.minified\", TRUE)\n})\nis_minified # FALSE",
            "downloadButton": "\\dontrun{\nui <- fluidPage(\n  p(\"Choose a dataset to download.\"),\n  selectInput(\"dataset\", \"Dataset\", choices = c(\"mtcars\", \"airquality\")),\n  downloadButton(\"downloadData\", \"Download\")\n)\n\nserver <- function(input, output) {\n  # The requested dataset\n  data <- reactive({\n    get(input$dataset)\n  })\n\n  output$downloadData <- downloadHandler(\n    filename = function() {\n      # Use the selected dataset as the suggested file name\n      paste0(input$dataset, \".csv\")\n    },\n    content = function(file) {\n      # Write the dataset to the `file` that will be downloaded\n      write.csv(data(), file)\n    }\n  )\n}\n\nshinyApp(ui, server)\n}",
            "downloadHandler": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  downloadButton(\"downloadData\", \"Download\")\n)\n\nserver <- function(input, output) {\n  # Our dataset\n  data <- mtcars\n\n  output$downloadData <- downloadHandler(\n    filename = function() {\n      paste(\"data-\", Sys.Date(), \".csv\", sep=\"\")\n    },\n    content = function(file) {\n      write.csv(data, file)\n    }\n  )\n}\n\nshinyApp(ui, server)\n}",
            "enableBookmarking": "## Only run these examples in interactive R sessions\nif (interactive()) {\n\n# Basic example with state encoded in URL\nui <- function(request) {\n  fluidPage(\n    textInput(\"txt\", \"Text\"),\n    checkboxInput(\"chk\", \"Checkbox\"),\n    bookmarkButton()\n  )\n}\nserver <- function(input, output, session) { }\nenableBookmarking(\"url\")\nshinyApp(ui, server)\n\n\n# An alternative to calling enableBookmarking(): use shinyApp's\n# enableBookmarking argument\nshinyApp(ui, server, enableBookmarking = \"url\")\n\n\n# Same basic example with state saved to disk\nenableBookmarking(\"server\")\nshinyApp(ui, server)\n\n\n# Save/restore arbitrary values\nui <- function(req) {\n  fluidPage(\n    textInput(\"txt\", \"Text\"),\n    checkboxInput(\"chk\", \"Checkbox\"),\n    bookmarkButton(),\n    br(),\n    textOutput(\"lastSaved\")\n  )\n}\nserver <- function(input, output, session) {\n  vals <- reactiveValues(savedTime = NULL)\n  output$lastSaved <- renderText({\n    if (!is.null(vals$savedTime))\n      paste(\"Last saved at\", vals$savedTime)\n    else\n      \"\"\n  })\n\n  onBookmark(function(state) {\n    vals$savedTime <- Sys.time()\n    # state is a mutable reference object, and we can add arbitrary values\n    # to it.\n    state$values$time <- vals$savedTime\n  })\n  onRestore(function(state) {\n    vals$savedTime <- state$values$time\n  })\n}\nenableBookmarking(store = \"url\")\nshinyApp(ui, server)\n\n\n# Usable with dynamic UI (set the slider, then change the text input,\n# click the bookmark button)\nui <- function(request) {\n  fluidPage(\n    sliderInput(\"slider\", \"Slider\", 1, 100, 50),\n    uiOutput(\"ui\"),\n    bookmarkButton()\n  )\n}\nserver <- function(input, output, session) {\n  output$ui <- renderUI({\n    textInput(\"txt\", \"Text\", input$slider)\n  })\n}\nenableBookmarking(\"url\")\nshinyApp(ui, server)\n\n\n# Exclude specific inputs (The only input that will be saved in this\n# example is chk)\nui <- function(request) {\n  fluidPage(\n    passwordInput(\"pw\", \"Password\"), # Passwords are never saved\n    sliderInput(\"slider\", \"Slider\", 1, 100, 50), # Manually excluded below\n    checkboxInput(\"chk\", \"Checkbox\"),\n    bookmarkButton()\n  )\n}\nserver <- function(input, output, session) {\n  setBookmarkExclude(\"slider\")\n}\nenableBookmarking(\"url\")\nshinyApp(ui, server)\n\n\n# Update the browser's location bar every time an input changes. This should\n# not be used with enableBookmarking(\"server\"), because that would create a\n# new saved state on disk every time the user changes an input.\nui <- function(req) {\n  fluidPage(\n    textInput(\"txt\", \"Text\"),\n    checkboxInput(\"chk\", \"Checkbox\")\n  )\n}\nserver <- function(input, output, session) {\n  observe({\n    # Trigger this observer every time an input changes\n    reactiveValuesToList(input)\n    session$doBookmark()\n  })\n  onBookmarked(function(url) {\n    updateQueryString(url)\n  })\n}\nenableBookmarking(\"url\")\nshinyApp(ui, server)\n\n\n# Save/restore uploaded files\nui <- function(request) {\n  fluidPage(\n    sidebarLayout(\n      sidebarPanel(\n        fileInput(\"file1\", \"Choose CSV File\", multiple = TRUE,\n          accept = c(\n            \"text/csv\",\n            \"text/comma-separated-values,text/plain\",\n            \".csv\"\n          )\n        ),\n        tags$hr(),\n        checkboxInput(\"header\", \"Header\", TRUE),\n        bookmarkButton()\n      ),\n      mainPanel(\n        tableOutput(\"contents\")\n      )\n    )\n  )\n}\nserver <- function(input, output) {\n  output$contents <- renderTable({\n    inFile <- input$file1\n    if (is.null(inFile))\n      return(NULL)\n\n    if (nrow(inFile) == 1) {\n      read.csv(inFile$datapath, header = input$header)\n    } else {\n      data.frame(x = \"multiple files\")\n    }\n  })\n}\nenableBookmarking(\"server\")\nshinyApp(ui, server)\n\n}",
            "exportTestValues": "## Only run this example in interactive R sessions\nif (interactive()) {\n\noptions(shiny.testmode = TRUE)\n\n# This application shows the test snapshot URL; clicking on it will\n# fetch the input, output, and exported values in JSON format.\nshinyApp(\n  ui = basicPage(\n    h4(\"Snapshot URL: \"),\n    uiOutput(\"url\"),\n    h4(\"Current values:\"),\n    verbatimTextOutput(\"values\"),\n    actionButton(\"inc\", \"Increment x\")\n  ),\n\n  server = function(input, output, session) {\n    vals <- reactiveValues(x = 1)\n    y <- reactive({ vals$x + 1 })\n\n    observeEvent(input$inc, {\n      vals$x <<- vals$x + 1\n    })\n\n    exportTestValues(\n      x = vals$x,\n      y = y()\n    )\n\n    output$url <- renderUI({\n      url <- session$getTestSnapshotUrl(format=\"json\")\n      a(href = url, url)\n    })\n\n    output$values <- renderText({\n      paste0(\"vals$x: \", vals$x, \"\\ny: \", y())\n    })\n  }\n)\n}",
            "fileInput": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file1\", \"Choose CSV File\", accept = \".csv\"),\n      checkboxInput(\"header\", \"Header\", TRUE)\n    ),\n    mainPanel(\n      tableOutput(\"contents\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  output$contents <- renderTable({\n    file <- input$file1\n    ext <- tools::file_ext(file$datapath)\n\n    req(file)\n    validate(need(ext == \"csv\", \"Please upload a csv file\"))\n\n    read.csv(file$datapath, header = input$header)\n  })\n}\n\nshinyApp(ui, server)\n}",
            "fillPage": "fillPage(\n  tags$style(type = \"text/css\",\n    \".half-fill { width: 50\\%; height: 100\\%; }\",\n    \"#one { float: left; background-color: #ddddff; }\",\n    \"#two { float: right; background-color: #ccffcc; }\"\n  ),\n  div(id = \"one\", class = \"half-fill\",\n    \"Left half\"\n  ),\n  div(id = \"two\", class = \"half-fill\",\n    \"Right half\"\n  ),\n  padding = 10\n)\n\nfillPage(\n  fillRow(\n    div(style = \"background-color: red; width: 100\\%; height: 100\\%;\"),\n    div(style = \"background-color: blue; width: 100\\%; height: 100\\%;\")\n  )\n)",
            "fillRow": "# Only run this example in interactive R sessions.\nif (interactive()) {\n\nui <- fillPage(fillRow(\n  plotOutput(\"plotLeft\", height = \"100\\%\"),\n  fillCol(\n    plotOutput(\"plotTopRight\", height = \"100\\%\"),\n    plotOutput(\"plotBottomRight\", height = \"100\\%\")\n  )\n))\n\nserver <- function(input, output, session) {\n  output$plotLeft <- renderPlot(plot(cars))\n  output$plotTopRight <- renderPlot(plot(pressure))\n  output$plotBottomRight <- renderPlot(plot(AirPassengers))\n}\n\nshinyApp(ui, server)\n\n}",
            "fixedPage": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fixedPage(\n  title = \"Hello, Shiny!\",\n  fixedRow(\n    column(width = 4,\n      \"4\"\n    ),\n    column(width = 3, offset = 2,\n      \"3 offset 2\"\n    )\n  )\n)\n\nshinyApp(ui, server = function(input, output) { })\n}",
            "flowLayout": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- flowLayout(\n  numericInput(\"rows\", \"How many rows?\", 5),\n  selectInput(\"letter\", \"Which letter?\", LETTERS),\n  sliderInput(\"value\", \"What value?\", 0, 100, 50)\n)\nshinyApp(ui, server = function(input, output) { })\n}",
            "fluidPage": "## Only run examples in interactive R sessions\nif (interactive()) {\n\n# Example of UI with fluidPage\nui <- fluidPage(\n\n  # Application title\n  titlePanel(\"Hello Shiny!\"),\n\n  sidebarLayout(\n\n    # Sidebar with a slider input\n    sidebarPanel(\n      sliderInput(\"obs\",\n                  \"Number of observations:\",\n                  min = 0,\n                  max = 1000,\n                  value = 500)\n    ),\n\n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Server logic\nserver <- function(input, output) {\n  output$distPlot <- renderPlot({\n    hist(rnorm(input$obs))\n  })\n}\n\n# Complete app with UI and server components\nshinyApp(ui, server)\n\n\n# UI demonstrating column layouts\nui <- fluidPage(\n  title = \"Hello Shiny!\",\n  fluidRow(\n    column(width = 4,\n      \"4\"\n    ),\n    column(width = 3, offset = 2,\n      \"3 offset 2\"\n    )\n  )\n)\n\nshinyApp(ui, server = function(input, output) { })\n}",
            "freezeReactiveValue": "## Only run this examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  selectInput(\"data\", \"Data Set\", c(\"mtcars\", \"pressure\")),\n  checkboxGroupInput(\"cols\", \"Columns (select 2)\", character(0)),\n  plotOutput(\"plot\")\n)\n\nserver <- function(input, output, session) {\n  observe({\n    data <- get(input$data)\n    # Sets a flag on input$cols to essentially do req(FALSE) if input$cols\n    # is accessed. Without this, an error will momentarily show whenever a\n    # new data set is selected.\n    freezeReactiveValue(input, \"cols\")\n    updateCheckboxGroupInput(session, \"cols\", choices = names(data))\n  })\n\n  output$plot <- renderPlot({\n    # When a new data set is selected, input$cols will have been invalidated\n    # above, and this will essentially do the same as req(FALSE), causing\n    # this observer to stop and raise a silent exception.\n    cols <- input$cols\n    data <- get(input$data)\n\n    if (length(cols) == 2) {\n      plot(data[[ cols[1] ]], data[[ cols[2] ]])\n    }\n  })\n}\n\nshinyApp(ui, server)\n}",
            "getCurrentOutputInfo": "if (interactive()) {\n  shinyApp(\n    fluidPage(\n      tags$style(HTML(\"body {background-color: black; color: white; }\")),\n      tags$style(HTML(\"body a {color: purple}\")),\n      tags$style(HTML(\"#info {background-color: teal; color: orange; }\")),\n      plotOutput(\"p\"),\n      \"Computed CSS styles for the output named info:\",\n      tagAppendAttributes(\n        textOutput(\"info\"),\n        class = \"shiny-report-theme\"\n      )\n    ),\n    function(input, output) {\n      output$p <- renderPlot({\n        info <- getCurrentOutputInfo()\n        par(bg = info$bg(), fg = info$fg(), col.axis = info$fg(), col.main = info$fg())\n        plot(1:10, col = info$accent(), pch = 19)\n        title(\"A simple R plot that uses its CSS styling\")\n      })\n      output$info <- renderText({\n        info <- getCurrentOutputInfo()\n        jsonlite::toJSON(\n          list(\n            bg = info$bg(),\n            fg = info$fg(),\n            accent = info$accent(),\n            font = info$font()\n          ),\n          auto_unbox = TRUE\n        )\n      })\n    }\n  )\n}",
            "getQueryString": "## Only run this example in interactive R sessions\nif (interactive()) {\n\n  ## App 1: getQueryString\n  ## Printing the value of the query string\n  ## (Use the back and forward buttons to see how the browser\n  ## keeps a record of each state)\n  shinyApp(\n    ui = fluidPage(\n      textInput(\"txt\", \"Enter new query string\"),\n      helpText(\"Format: ?param1=val1&param2=val2\"),\n      actionButton(\"go\", \"Update\"),\n      hr(),\n      verbatimTextOutput(\"query\")\n    ),\n    server = function(input, output, session) {\n      observeEvent(input$go, {\n        updateQueryString(input$txt, mode = \"push\")\n      })\n      output$query <- renderText({\n        query <- getQueryString()\n        queryText <- paste(names(query), query,\n                       sep = \"=\", collapse=\", \")\n        paste(\"Your query string is:\\n\", queryText)\n      })\n    }\n  )\n\n  ## App 2: getUrlHash\n  ## Printing the value of the URL hash\n  ## (Use the back and forward buttons to see how the browser\n  ## keeps a record of each state)\n  shinyApp(\n    ui = fluidPage(\n      textInput(\"txt\", \"Enter new hash\"),\n      helpText(\"Format: #hash\"),\n      actionButton(\"go\", \"Update\"),\n      hr(),\n      verbatimTextOutput(\"hash\")\n    ),\n    server = function(input, output, session) {\n      observeEvent(input$go, {\n        updateQueryString(input$txt, mode = \"push\")\n      })\n      output$hash <- renderText({\n        hash <- getUrlHash()\n        paste(\"Your hash is:\\n\", hash)\n      })\n    }\n  )\n}",
            "helpText": "helpText(\"Note: while the data view will show only\",\n         \"the specified number of observations, the\",\n         \"summary will be based on the full dataset.\")",
            "htmlOutput": "htmlOutput(\"summary\")\n\n# Using a custom container and class\ntags$ul(\n  htmlOutput(\"summary\", container = tags$li, class = \"custom-li-output\")\n)",
            "httpResponse": "httpResponse(status = 405L,\n  content_type = \"text/plain\",\n  content = \"The requested method was not allowed\"\n)",
            "icon": "# add an icon to a submit button\nsubmitButton(\"Update View\", icon = icon(\"redo\"))\n\nnavbarPage(\"App Title\",\n  tabPanel(\"Plot\", icon = icon(\"bar-chart-o\")),\n  tabPanel(\"Summary\", icon = icon(\"list-alt\")),\n  tabPanel(\"Table\", icon = icon(\"table\"))\n)",
            "insertTab": "## Only run this example in interactive R sessions\nif (interactive()) {\n\n# example app for inserting/removing a tab\nui <- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      actionButton(\"add\", \"Add 'Dynamic' tab\"),\n      actionButton(\"remove\", \"Remove 'Foo' tab\")\n    ),\n    mainPanel(\n      tabsetPanel(id = \"tabs\",\n        tabPanel(\"Hello\", \"This is the hello tab\"),\n        tabPanel(\"Foo\", \"This is the foo tab\"),\n        tabPanel(\"Bar\", \"This is the bar tab\")\n      )\n    )\n  )\n)\nserver <- function(input, output, session) {\n  observeEvent(input$add, {\n    insertTab(inputId = \"tabs\",\n      tabPanel(\"Dynamic\", \"This a dynamically-added tab\"),\n      target = \"Bar\"\n    )\n  })\n  observeEvent(input$remove, {\n    removeTab(inputId = \"tabs\", target = \"Foo\")\n  })\n}\n\nshinyApp(ui, server)\n\n\n# example app for prepending/appending a navbarMenu\nui <- navbarPage(\"Navbar page\", id = \"tabs\",\n  tabPanel(\"Home\",\n    actionButton(\"prepend\", \"Prepend a navbarMenu\"),\n    actionButton(\"append\", \"Append a navbarMenu\")\n  )\n)\nserver <- function(input, output, session) {\n  observeEvent(input$prepend, {\n    id <- paste0(\"Dropdown\", input$prepend, \"p\")\n    prependTab(inputId = \"tabs\",\n      navbarMenu(id,\n        tabPanel(\"Drop1\", paste(\"Drop1 page from\", id)),\n        tabPanel(\"Drop2\", paste(\"Drop2 page from\", id)),\n        \"------\",\n        \"Header\",\n        tabPanel(\"Drop3\", paste(\"Drop3 page from\", id))\n      )\n    )\n  })\n  observeEvent(input$append, {\n    id <- paste0(\"Dropdown\", input$append, \"a\")\n    appendTab(inputId = \"tabs\",\n      navbarMenu(id,\n        tabPanel(\"Drop1\", paste(\"Drop1 page from\", id)),\n        tabPanel(\"Drop2\", paste(\"Drop2 page from\", id)),\n        \"------\",\n        \"Header\",\n        tabPanel(\"Drop3\", paste(\"Drop3 page from\", id))\n      )\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n}",
            "insertUI": "## Only run this example in interactive R sessions\nif (interactive()) {\n# Define UI\nui <- fluidPage(\n  actionButton(\"add\", \"Add UI\")\n)\n\n# Server logic\nserver <- function(input, output, session) {\n  observeEvent(input$add, {\n    insertUI(\n      selector = \"#add\",\n      where = \"afterEnd\",\n      ui = textInput(paste0(\"txt\", input$add),\n                     \"Insert some text\")\n    )\n  })\n}\n\n# Complete app with UI and server components\nshinyApp(ui, server)\n}\n\nif (interactive()) {\n# Define UI\nui <- fluidPage(\n  actionButton(\"rmv\", \"Remove UI\"),\n  textInput(\"txt\", \"This is no longer useful\")\n)\n\n# Server logic\nserver <- function(input, output, session) {\n  observeEvent(input$rmv, {\n    removeUI(\n      selector = \"div:has(> #txt)\"\n    )\n  })\n}\n\n# Complete app with UI and server components\nshinyApp(ui, server)\n}",
            "invalidateLater": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  sliderInput(\"n\", \"Number of observations\", 2, 1000, 500),\n  plotOutput(\"plot\")\n)\n\nserver <- function(input, output, session) {\n\n  observe({\n    # Re-execute this reactive expression after 1000 milliseconds\n    invalidateLater(1000, session)\n\n    # Do something each time this is invalidated.\n    # The isolate() makes this observer _not_ get invalidated and re-executed\n    # when input$n changes.\n    print(paste(\"The value of input$n is\", isolate(input$n)))\n  })\n\n  # Generate a new histogram at timed intervals, but not when\n  # input$n changes.\n  output$plot <- renderPlot({\n    # Re-execute this reactive expression after 2000 milliseconds\n    invalidateLater(2000)\n    hist(rnorm(isolate(input$n)))\n  })\n}\n\nshinyApp(ui, server)\n}",
            "isolate": "\\dontrun{\nobserve({\n  input$saveButton  # Do take a dependency on input$saveButton\n\n  # isolate a simple expression\n  data <- get(isolate(input$dataset))  # No dependency on input$dataset\n  writeToDatabase(data)\n})\n\nobserve({\n  input$saveButton  # Do take a dependency on input$saveButton\n\n  # isolate a whole block\n  data <- isolate({\n    a <- input$valueA   # No dependency on input$valueA or input$valueB\n    b <- input$valueB\n    c(a=a, b=b)\n  })\n  writeToDatabase(data)\n})\n\nobserve({\n  x <- 1\n  # x outside of isolate() is affected\n  isolate(x <- 2)\n  print(x) # 2\n\n  y <- 1\n  # Use local() to avoid affecting calling environment\n  isolate(local(y <- 2))\n  print(y) # 1\n})\n\n}\n\n# Can also use isolate to call reactive expressions from the R console\nvalues <- reactiveValues(A=1)\nfun <- reactive({ as.character(values$A) })\nisolate(fun())\n# \"1\"\n\n# isolate also works if the reactive expression accesses values from the\n# input object, like input$x",
            "makeReactiveBinding": "reactiveConsole(TRUE)\n\na <- 10\nmakeReactiveBinding(\"a\")\n\nb <- reactive(a * -1)\nobserve(print(b()))\n\na <- 20\na <- 30\n\nreactiveConsole(FALSE)",
            "markdown": "ui <- fluidPage(\n  markdown(\"\n    # Markdown Example\n\n    This is a markdown paragraph, and will be contained within a `<p>` tag\n    in the UI.\n\n    The following is an unordered list, which will be represented in the UI as\n    a `<ul>` with `<li>` children:\n\n    * a bullet\n    * another\n\n    [Links](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/a) work;\n    so does *emphasis*.\n\n    To see more of what's possible, check out [commonmark.org/help](https://commonmark.org/help).\n    \")\n)",
            "modalDialog": "if (interactive()) {\n# Display an important message that can be dismissed only by clicking the\n# dismiss button.\nshinyApp(\n  ui = basicPage(\n    actionButton(\"show\", \"Show modal dialog\")\n  ),\n  server = function(input, output) {\n    observeEvent(input$show, {\n      showModal(modalDialog(\n        title = \"Important message\",\n        \"This is an important message!\"\n      ))\n    })\n  }\n)\n\n\n# Display a message that can be dismissed by clicking outside the modal dialog,\n# or by pressing Esc.\nshinyApp(\n  ui = basicPage(\n    actionButton(\"show\", \"Show modal dialog\")\n  ),\n  server = function(input, output) {\n    observeEvent(input$show, {\n      showModal(modalDialog(\n        title = \"Somewhat important message\",\n        \"This is a somewhat important message.\",\n        easyClose = TRUE,\n        footer = NULL\n      ))\n    })\n  }\n)\n\n\n# Display a modal that requires valid input before continuing.\nshinyApp(\n  ui = basicPage(\n    actionButton(\"show\", \"Show modal dialog\"),\n    verbatimTextOutput(\"dataInfo\")\n  ),\n\n  server = function(input, output) {\n    # reactiveValues object for storing current data set.\n    vals <- reactiveValues(data = NULL)\n\n    # Return the UI for a modal dialog with data selection input. If 'failed' is\n    # TRUE, then display a message that the previous value was invalid.\n    dataModal <- function(failed = FALSE) {\n      modalDialog(\n        textInput(\"dataset\", \"Choose data set\",\n          placeholder = 'Try \"mtcars\" or \"abc\"'\n        ),\n        span('(Try the name of a valid data object like \"mtcars\", ',\n             'then a name of a non-existent object like \"abc\")'),\n        if (failed)\n          div(tags$b(\"Invalid name of data object\", style = \"color: red;\")),\n\n        footer = tagList(\n          modalButton(\"Cancel\"),\n          actionButton(\"ok\", \"OK\")\n        )\n      )\n    }\n\n    # Show modal when button is clicked.\n    observeEvent(input$show, {\n      showModal(dataModal())\n    })\n\n    # When OK button is pressed, attempt to load the data set. If successful,\n    # remove the modal. If not show another modal, but this time with a failure\n    # message.\n    observeEvent(input$ok, {\n      # Check that data object exists and is data frame.\n      if (!is.null(input$dataset) && nzchar(input$dataset) &&\n          exists(input$dataset) && is.data.frame(get(input$dataset))) {\n        vals$data <- get(input$dataset)\n        removeModal()\n      } else {\n        showModal(dataModal(failed = TRUE))\n      }\n    })\n\n    # Display information about selected data\n    output$dataInfo <- renderPrint({\n      if (is.null(vals$data))\n        \"No data selected\"\n      else\n        summary(vals$data)\n    })\n  }\n)\n}",
            "moduleServer": "# Define the UI for a module\ncounterUI <- function(id, label = \"Counter\") {\n  ns <- NS(id)\n  tagList(\n    actionButton(ns(\"button\"), label = label),\n    verbatimTextOutput(ns(\"out\"))\n  )\n}\n\n# Define the server logic for a module\ncounterServer <- function(id) {\n  moduleServer(\n    id,\n    function(input, output, session) {\n      count <- reactiveVal(0)\n      observeEvent(input$button, {\n        count(count() + 1)\n      })\n      output$out <- renderText({\n        count()\n      })\n      count\n    }\n  )\n}\n\n# Use the module in an app\nui <- fluidPage(\n  counterUI(\"counter1\", \"Counter #1\"),\n  counterUI(\"counter2\", \"Counter #2\")\n)\nserver <- function(input, output, session) {\n  counterServer(\"counter1\")\n  counterServer(\"counter2\")\n}\nif (interactive()) {\n  shinyApp(ui, server)\n}\n\n\n\n# If you want to pass extra parameters to the module's server logic, you can\n# add them to your function. In this case `prefix` is text that will be\n# printed before the count.\ncounterServer2 <- function(id, prefix = NULL) {\n  moduleServer(\n    id,\n    function(input, output, session) {\n      count <- reactiveVal(0)\n      observeEvent(input$button, {\n        count(count() + 1)\n      })\n      output$out <- renderText({\n        paste0(prefix, count())\n      })\n      count\n    }\n  )\n}\n\nui <- fluidPage(\n  counterUI(\"counter\", \"Counter\"),\n)\nserver <- function(input, output, session) {\n  counterServer2(\"counter\", \"The current count is: \")\n}\nif (interactive()) {\n  shinyApp(ui, server)\n}",
            "navbarPage": "navbarPage(\"App Title\",\n  tabPanel(\"Plot\"),\n  tabPanel(\"Summary\"),\n  tabPanel(\"Table\")\n)\n\nnavbarPage(\"App Title\",\n  tabPanel(\"Plot\"),\n  navbarMenu(\"More\",\n    tabPanel(\"Summary\"),\n    \"----\",\n    \"Section header\",\n    tabPanel(\"Table\")\n  )\n)",
            "navlistPanel": "fluidPage(\n\n  titlePanel(\"Application Title\"),\n\n  navlistPanel(\n    \"Header\",\n    tabPanel(\"First\"),\n    tabPanel(\"Second\"),\n    tabPanel(\"Third\")\n  )\n)",
            "numericInput": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  numericInput(\"obs\", \"Observations:\", 10, min = 1, max = 100),\n  verbatimTextOutput(\"value\")\n)\nserver <- function(input, output) {\n  output$value <- renderText({ input$obs })\n}\nshinyApp(ui, server)\n}",
            "observe": "values <- reactiveValues(A=1)\n\nobsB <- observe({\n  print(values$A + 1)\n})\n\n# To store expressions for later conversion to observe, use rlang::quo()\nmyquo <- rlang::quo({ print(values$A + 3) })\nobsC <- rlang::inject(observe(!!myquo))\n\n# (Legacy) Can use quoted expressions\nobsD <- observe(quote({ print(values$A + 2) }), quoted = TRUE)\n\n# In a normal Shiny app, the web client will trigger flush events. If you\n# are at the console, you can force a flush with flushReact()\nshiny:::flushReact()",
            "observeEvent": "## Only run examples in interactive R sessions\nif (interactive()) {\n\n  ## App 1: Sample usage\n  shinyApp(\n    ui = fluidPage(\n      column(4,\n        numericInput(\"x\", \"Value\", 5),\n        br(),\n        actionButton(\"button\", \"Show\")\n      ),\n      column(8, tableOutput(\"table\"))\n    ),\n    server = function(input, output) {\n      # Take an action every time button is pressed;\n      # here, we just print a message to the console\n      observeEvent(input$button, {\n        cat(\"Showing\", input$x, \"rows\\n\")\n      })\n      # The observeEvent() above is equivalent to:\n      # observe({\n      #    cat(\"Showing\", input$x, \"rows\\n\")\n      #   }) \\%>\\%\n      #   bindEvent(input$button)\n\n      # Take a reactive dependency on input$button, but\n      # not on any of the stuff inside the function\n      df <- eventReactive(input$button, {\n        head(cars, input$x)\n      })\n      output$table <- renderTable({\n        df()\n      })\n    }\n  )\n\n  ## App 2: Using `once`\n  shinyApp(\n    ui = basicPage( actionButton(\"go\", \"Go\")),\n    server = function(input, output, session) {\n      observeEvent(input$go, {\n        print(paste(\"This will only be printed once; all\",\n              \"subsequent button clicks won't do anything\"))\n      }, once = TRUE)\n      # The observeEvent() above is equivalent to:\n      # observe({\n      #   print(paste(\"This will only be printed once; all\",\n      #         \"subsequent button clicks won't do anything\"))\n      #   }) \\%>\\%\n      #   bindEvent(input$go, once = TRUE)\n    }\n  )\n\n  ## App 3: Using `ignoreInit` and `once`\n  shinyApp(\n    ui = basicPage(actionButton(\"go\", \"Go\")),\n    server = function(input, output, session) {\n      observeEvent(input$go, {\n        insertUI(\"#go\", \"afterEnd\",\n                 actionButton(\"dynamic\", \"click to remove\"))\n\n        # set up an observer that depends on the dynamic\n        # input, so that it doesn't run when the input is\n        # created, and only runs once after that (since\n        # the side effect is remove the input from the DOM)\n        observeEvent(input$dynamic, {\n          removeUI(\"#dynamic\")\n        }, ignoreInit = TRUE, once = TRUE)\n      })\n    }\n  )\n}",
            "onBookmark": "## Only run these examples in interactive sessions\nif (interactive()) {\n\n# Basic use of onBookmark and onRestore: This app saves the time in its\n# arbitrary values, and restores that time when the app is restored.\nui <- function(req) {\n  fluidPage(\n    textInput(\"txt\", \"Input text\"),\n    bookmarkButton()\n  )\n}\nserver <- function(input, output) {\n  onBookmark(function(state) {\n    savedTime <- as.character(Sys.time())\n    cat(\"Last saved at\", savedTime, \"\\n\")\n    # state is a mutable reference object, and we can add arbitrary values to\n    # it.\n    state$values$time <- savedTime\n  })\n\n  onRestore(function(state) {\n    cat(\"Restoring from state bookmarked at\", state$values$time, \"\\n\")\n  })\n}\nenableBookmarking(\"url\")\nshinyApp(ui, server)\n\n\n\nui <- function(req) {\n  fluidPage(\n    textInput(\"txt\", \"Input text\"),\n    bookmarkButton()\n  )\n}\nserver <- function(input, output, session) {\n  lastUpdateTime <- NULL\n\n  observeEvent(input$txt, {\n    updateTextInput(session, \"txt\",\n      label = paste0(\"Input text (Changed \", as.character(Sys.time()), \")\")\n    )\n  })\n\n  onBookmark(function(state) {\n    # Save content to a file\n    messageFile <- file.path(state$dir, \"message.txt\")\n    cat(as.character(Sys.time()), file = messageFile)\n  })\n\n  onRestored(function(state) {\n    # Read the file\n    messageFile <- file.path(state$dir, \"message.txt\")\n    timeText <- readChar(messageFile, 1000)\n\n    # updateTextInput must be called in onRestored, as opposed to onRestore,\n    # because onRestored happens after the client browser is ready.\n    updateTextInput(session, \"txt\",\n      label = paste0(\"Input text (Changed \", timeText, \")\")\n    )\n  })\n}\n# \"server\" bookmarking is needed for writing to disk.\nenableBookmarking(\"server\")\nshinyApp(ui, server)\n\n\n# This app has a module, and both the module and the main app code have\n# onBookmark and onRestore functions which write and read state$values$hash. The\n# module's version of state$values$hash does not conflict with the app's version\n# of state$values$hash.\n#\n# A basic module that captializes text.\ncapitalizerUI <- function(id) {\n  ns <- NS(id)\n  wellPanel(\n    h4(\"Text captializer module\"),\n    textInput(ns(\"text\"), \"Enter text:\"),\n    verbatimTextOutput(ns(\"out\"))\n  )\n}\ncapitalizerServer <- function(input, output, session) {\n  output$out <- renderText({\n    toupper(input$text)\n  })\n  onBookmark(function(state) {\n    state$values$hash <- rlang::hash(input$text)\n  })\n  onRestore(function(state) {\n    if (identical(rlang::hash(input$text), state$values$hash)) {\n      message(\"Module's input text matches hash \", state$values$hash)\n    } else {\n      message(\"Module's input text does not match hash \", state$values$hash)\n    }\n  })\n}\n# Main app code\nui <- function(request) {\n  fluidPage(\n    sidebarLayout(\n      sidebarPanel(\n        capitalizerUI(\"tc\"),\n        textInput(\"text\", \"Enter text (not in module):\"),\n        bookmarkButton()\n      ),\n      mainPanel()\n    )\n  )\n}\nserver <- function(input, output, session) {\n  callModule(capitalizerServer, \"tc\")\n  onBookmark(function(state) {\n    state$values$hash <- rlang::hash(input$text)\n  })\n  onRestore(function(state) {\n    if (identical(rlang::hash(input$text), state$values$hash)) {\n      message(\"App's input text matches hash \", state$values$hash)\n    } else {\n      message(\"App's input text does not match hash \", state$values$hash)\n    }\n  })\n}\nenableBookmarking(store = \"url\")\nshinyApp(ui, server)\n}",
            "onFlush": "\\dontshow{if (interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nlibrary(shiny)\n\nui <- fixedPage(\n  markdown(c(\n    \"Set the number to 8 or higher to cause an error\",\n    \"in the `renderText()` output.\"\n  )),\n  sliderInput(\"number\", \"Number\", 0, 10, 4),\n  textOutput(\"text\"),\n  hr(),\n  markdown(c(\n    \"Click the button below to crash the app with an unhandled error\",\n    \"in an `observe()` block.\"\n  )),\n  actionButton(\"crash\", \"Crash the app!\")\n)\n\nlog_event <- function(level, ...) {\n  ts <- strftime(Sys.time(), \" [\\%F \\%T] \")\n  message(level, ts, ...)\n}\n\nserver <- function(input, output, session) {\n  log_event(\"INFO\", \"Session started\")\n\n  onUnhandledError(function(err) {\n    # log the unhandled error\n    level <- if (inherits(err, \"shiny.error.fatal\")) \"FATAL\" else \"ERROR\"\n    log_event(level, conditionMessage(err))\n  })\n\n  onStop(function() {\n    log_event(\"INFO\", \"Session ended\")\n  })\n\n  observeEvent(input$crash, stop(\"Oops, an unhandled error happened!\"))\n\n  output$text <- renderText({\n    if (input$number > 7) {\n      stop(\"that's too high!\")\n    }\n    sprintf(\"You picked number \\%d.\", input$number)\n  })\n}\n\nshinyApp(ui, server)\n\\dontshow{\\}) # examplesIf}",
            "onStop": "## Only run this example in interactive R sessions\nif (interactive()) {\n  # Open this application in multiple browsers, then close the browsers.\n  shinyApp(\n    ui = basicPage(\"onStop demo\"),\n\n    server = function(input, output, session) {\n      onStop(function() cat(\"Session stopped\\n\"))\n    },\n\n    onStart = function() {\n      cat(\"Doing application setup\\n\")\n\n      onStop(function() {\n        cat(\"Doing application cleanup\\n\")\n      })\n    }\n  )\n}\n# In the example above, onStop() is called inside of onStart(). This is\n# the pattern that should be used when creating a shinyApp() object from\n# a function, or at the console. If instead you are writing an app.R which\n# will be invoked with runApp(), you can do it that way, or put the onStop()\n# before the shinyApp() call, as shown below.\n\n\\dontrun{\n# ==== app.R ====\ncat(\"Doing application setup\\n\")\nonStop(function() {\n  cat(\"Doing application cleanup\\n\")\n})\n\nshinyApp(\n  ui = basicPage(\"onStop demo\"),\n\n  server = function(input, output, session) {\n    onStop(function() cat(\"Session stopped\\n\"))\n  }\n)\n# ==== end app.R ====\n\n\n# Similarly, if you have a global.R, you can call onStop() from there.\n# ==== global.R ====\ncat(\"Doing application setup\\n\")\nonStop(function() {\n  cat(\"Doing application cleanup\\n\")\n})\n# ==== end global.R ====\n}",
            "outputOptions": "\\dontrun{\n# Get the list of options for all observers within output\noutputOptions(output)\n\n# Disable suspend for output$myplot\noutputOptions(output, \"myplot\", suspendWhenHidden = FALSE)\n\n# Change priority for output$myplot\noutputOptions(output, \"myplot\", priority = 10)\n\n# Get the list of options for output$myplot\noutputOptions(output, \"myplot\")\n}",
            "parseQueryString": "parseQueryString(\"?foo=1&bar=b\\%20a\\%20r\")\n\n\\dontrun{\n# Example of usage within a Shiny app\nfunction(input, output, session) {\n\n  output$queryText <- renderText({\n    query <- parseQueryString(session$clientData$url_search)\n\n    # Ways of accessing the values\n    if (as.numeric(query$foo) == 1) {\n      # Do something\n    }\n    if (query[[\"bar\"]] == \"targetstring\") {\n      # Do something else\n    }\n\n    # Return a string with key-value pairs\n    paste(names(query), query, sep = \"=\", collapse=\", \")\n  })\n}\n}",
            "passwordInput": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  passwordInput(\"password\", \"Password:\"),\n  actionButton(\"go\", \"Go\"),\n  verbatimTextOutput(\"value\")\n)\nserver <- function(input, output) {\n  output$value <- renderText({\n    req(input$go)\n    isolate(input$password)\n  })\n}\nshinyApp(ui, server)\n}",
            "plotOutput": "# Only run these examples in interactive R sessions\nif (interactive()) {\n\n# A basic shiny app with a plotOutput\nshinyApp(\n  ui = fluidPage(\n    sidebarLayout(\n      sidebarPanel(\n        actionButton(\"newplot\", \"New plot\")\n      ),\n      mainPanel(\n        plotOutput(\"plot\")\n      )\n    )\n  ),\n  server = function(input, output) {\n    output$plot <- renderPlot({\n      input$newplot\n      # Add a little noise to the cars data\n      cars2 <- cars + rnorm(nrow(cars))\n      plot(cars2)\n    })\n  }\n)\n\n\n# A demonstration of clicking, hovering, and brushing\nshinyApp(\n  ui = basicPage(\n    fluidRow(\n      column(width = 4,\n        plotOutput(\"plot\", height=300,\n          click = \"plot_click\",  # Equiv, to click=clickOpts(id=\"plot_click\")\n          hover = hoverOpts(id = \"plot_hover\", delayType = \"throttle\"),\n          brush = brushOpts(id = \"plot_brush\")\n        ),\n        h4(\"Clicked points\"),\n        tableOutput(\"plot_clickedpoints\"),\n        h4(\"Brushed points\"),\n        tableOutput(\"plot_brushedpoints\")\n      ),\n      column(width = 4,\n        verbatimTextOutput(\"plot_clickinfo\"),\n        verbatimTextOutput(\"plot_hoverinfo\")\n      ),\n      column(width = 4,\n        wellPanel(actionButton(\"newplot\", \"New plot\")),\n        verbatimTextOutput(\"plot_brushinfo\")\n      )\n    )\n  ),\n  server = function(input, output, session) {\n    data <- reactive({\n      input$newplot\n      # Add a little noise to the cars data so the points move\n      cars + rnorm(nrow(cars))\n    })\n    output$plot <- renderPlot({\n      d <- data()\n      plot(d$speed, d$dist)\n    })\n    output$plot_clickinfo <- renderPrint({\n      cat(\"Click:\\n\")\n      str(input$plot_click)\n    })\n    output$plot_hoverinfo <- renderPrint({\n      cat(\"Hover (throttled):\\n\")\n      str(input$plot_hover)\n    })\n    output$plot_brushinfo <- renderPrint({\n      cat(\"Brush (debounced):\\n\")\n      str(input$plot_brush)\n    })\n    output$plot_clickedpoints <- renderTable({\n      # For base graphics, we need to specify columns, though for ggplot2,\n      # it's usually not necessary.\n      res <- nearPoints(data(), input$plot_click, \"speed\", \"dist\")\n      if (nrow(res) == 0)\n        return()\n      res\n    })\n    output$plot_brushedpoints <- renderTable({\n      res <- brushedPoints(data(), input$plot_brush, \"speed\", \"dist\")\n      if (nrow(res) == 0)\n        return()\n      res\n    })\n  }\n)\n\n\n# Demo of clicking, hovering, brushing with imageOutput\n# Note that coordinates are in pixels\nshinyApp(\n  ui = basicPage(\n    fluidRow(\n      column(width = 4,\n        imageOutput(\"image\", height=300,\n          click = \"image_click\",\n          hover = hoverOpts(\n            id = \"image_hover\",\n            delay = 500,\n            delayType = \"throttle\"\n          ),\n          brush = brushOpts(id = \"image_brush\")\n        )\n      ),\n      column(width = 4,\n        verbatimTextOutput(\"image_clickinfo\"),\n        verbatimTextOutput(\"image_hoverinfo\")\n      ),\n      column(width = 4,\n        wellPanel(actionButton(\"newimage\", \"New image\")),\n        verbatimTextOutput(\"image_brushinfo\")\n      )\n    )\n  ),\n  server = function(input, output, session) {\n    output$image <- renderImage({\n      input$newimage\n\n      # Get width and height of image output\n      width  <- session$clientData$output_image_width\n      height <- session$clientData$output_image_height\n\n      # Write to a temporary PNG file\n      outfile <- tempfile(fileext = \".png\")\n\n      png(outfile, width=width, height=height)\n      plot(rnorm(200), rnorm(200))\n      dev.off()\n\n      # Return a list containing information about the image\n      list(\n        src = outfile,\n        contentType = \"image/png\",\n        width = width,\n        height = height,\n        alt = \"This is alternate text\"\n      )\n    })\n    output$image_clickinfo <- renderPrint({\n      cat(\"Click:\\n\")\n      str(input$image_click)\n    })\n    output$image_hoverinfo <- renderPrint({\n      cat(\"Hover (throttled):\\n\")\n      str(input$image_hover)\n    })\n    output$image_brushinfo <- renderPrint({\n      cat(\"Brush (debounced):\\n\")\n      str(input$image_brush)\n    })\n  }\n)\n\n}",
            "radioButtons": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  radioButtons(\"dist\", \"Distribution type:\",\n               c(\"Normal\" = \"norm\",\n                 \"Uniform\" = \"unif\",\n                 \"Log-normal\" = \"lnorm\",\n                 \"Exponential\" = \"exp\")),\n  plotOutput(\"distPlot\")\n)\n\nserver <- function(input, output) {\n  output$distPlot <- renderPlot({\n    dist <- switch(input$dist,\n                   norm = rnorm,\n                   unif = runif,\n                   lnorm = rlnorm,\n                   exp = rexp,\n                   rnorm)\n\n    hist(dist(500))\n  })\n}\n\nshinyApp(ui, server)\n\nui <- fluidPage(\n  radioButtons(\"rb\", \"Choose one:\",\n               choiceNames = list(\n                 icon(\"calendar\"),\n                 HTML(\"<p style='color:red;'>Red Text</p>\"),\n                 \"Normal text\"\n               ),\n               choiceValues = list(\n                 \"icon\", \"html\", \"text\"\n               )),\n  textOutput(\"txt\")\n)\n\nserver <- function(input, output) {\n  output$txt <- renderText({\n    paste(\"You chose\", input$rb)\n  })\n}\n\nshinyApp(ui, server)\n}",
            "reactive": "library(rlang)\nvalues <- reactiveValues(A=1)\n\nreactiveB <- reactive({\n  values$A + 1\n})\n# View the values from the R console with isolate()\nisolate(reactiveB())\n# 2\n\n# To store expressions for later conversion to reactive, use quote()\nmyquo <- rlang::quo(values$A + 2)\n# Unexpected value! Sending a quosure directly will not work as expected.\nreactiveC <- reactive(myquo)\n# We'd hope for `3`, but instead we get the quosure that was supplied.\nisolate(reactiveC())\n\n# Instead, the quosure should be `rlang::inject()`ed\nreactiveD <- rlang::inject(reactive(!!myquo))\nisolate(reactiveD())\n# 3\n\n# (Legacy) Can use quoted expressions\nexpr <- quote({ values$A + 3 })\nreactiveE <- reactive(expr, quoted = TRUE)\nisolate(reactiveE())\n# 4",
            "reactiveConsole": "reactiveConsole(TRUE)\nx <- reactiveVal(10)\ny <- observe({\n  message(\"The value of x is \", x())\n})\nx(20)\nx(30)\nreactiveConsole(FALSE)",
            "reactiveFileReader": "\\dontrun{\n# Per-session reactive file reader\nfunction(input, output, session) {\n  fileData <- reactiveFileReader(1000, session, 'data.csv', read.csv)\n\n  output$data <- renderTable({\n    fileData()\n  })\n}\n\n# Cross-session reactive file reader. In this example, all sessions share\n# the same reader, so read.csv only gets executed once no matter how many\n# user sessions are connected.\nfileData <- reactiveFileReader(1000, NULL, 'data.csv', read.csv)\nfunction(input, output, session) {\n  output$data <- renderTable({\n    fileData()\n  })\n}\n}",
            "reactivePoll": "function(input, output, session) {\n\n  data <- reactivePoll(1000, session,\n    # This function returns the time that log_file was last modified\n    checkFunc = function() {\n      if (file.exists(log_file))\n        file.info(log_file)$mtime[1]\n      else\n        \"\"\n    },\n    # This function returns the content of log_file\n    valueFunc = function() {\n      read.csv(log_file)\n    }\n  )\n\n  output$dataTable <- renderTable({\n    data()\n  })\n}",
            "reactiveTimer": "## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  sliderInput(\"n\", \"Number of observations\", 2, 1000, 500),\n  plotOutput(\"plot\")\n)\n\nserver <- function(input, output) {\n\n  # Anything that calls autoInvalidate will automatically invalidate\n  # every 2 seconds.\n  autoInvalidate <- reactiveTimer(2000)\n\n  observe({\n    # Invalidate and re-execute this reactive expression every time the\n    # timer fires.\n    autoInvalidate()\n\n    # Do something each time this is invalidated.\n    # The isolate() makes this observer _not_ get invalidated and re-executed\n    # when input$n changes.\n    print(paste(\"The value of input$n is\", isolate(input$n)))\n  })\n\n  # Generate a new histogram each time the timer fires, but not when\n  # input$n changes.\n  output$plot <- renderPlot({\n    autoInvalidate()\n    hist(rnorm(isolate(input$n)))\n  })\n}\n\nshinyApp(ui, server)\n}",
            "reactiveVal": "\\dontrun{\n\n# Create the object by calling reactiveVal\nr <- reactiveVal()\n\n# Set the value by calling with an argument\nr(10)\n\n# Read the value by calling without arguments\nr()\n\n}\n\n## Only run examples in interactive R sessions\nif (interactive()) {\n\nui <- fluidPage(\n  actionButton(\"minus\", \"-1\"),\n  actionButton(\"plus\", \"+1\"),\n  br(),\n  textOutput(\"value\")\n)\n\n# The comments below show the equivalent logic using reactiveValues()\nserver <- function(input, output, session) {\n  value <- reactiveVal(0)       # rv <- reactiveValues(value = 0)\n\n  observeEvent(input$minus, {\n    newValue <- value() - 1     # newValue <- rv$value - 1\n    value(newValue)             # rv$value <- newValue\n  })\n\n  observeEvent(input$plus, {\n    newValue <- value() + 1     # newValue <- rv$value + 1\n    value(newValue)             # rv$value <- newValue\n  })\n\n  output$value <- renderText({\n    value()                     # rv$value\n  })\n}\n\nshinyApp(ui, server)\n\n}",
            "reactiveValues": "# Create the object with no values\nvalues <- reactiveValues()\n\n# Assign values to 'a' and 'b'\nvalues$a <- 3\nvalues[['b']] <- 4\n\n\\dontrun{\n# From within a reactive context, you can access values with:\nvalues$a\nvalues[['a']]\n}\n\n# If not in a reactive context (e.g., at the console), you can use isolate()\n# to retrieve the value:\nisolate(values$a)\nisolate(values[['a']])\n\n# Set values upon creation\nvalues <- reactiveValues(a = 1, b = 2)\nisolate(values$a)",
            "reactiveValuesToList": "values <- reactiveValues(a = 1)\n\\dontrun{\nreactiveValuesToList(values)\n}\n\n# To get the objects without taking dependencies on them, use isolate().\n# isolate() can also be used when calling from outside a reactive context (e.g.\n# at the console)\nisolate(reactiveValuesToList(values))",
            "registerInputHandler": "\\dontrun{\n# Register an input handler which rounds a input number to the nearest integer\n# In a package, this should be called from the .onLoad function.\nregisterInputHandler(\"mypackage.validint\", function(x, shinysession, name) {\n  if (is.null(x)) return(NA)\n  round(x)\n})\n\n## On the Javascript side, the associated input binding must have a corresponding getType method:\n# getType: function(el) {\n#   return \"mypackage.validint\";\n# }\n\n}",
            "renderCachedPlot": "## Only run examples in interactive R sessions\nif (interactive()) {\n\n# A basic example that uses the default app-scoped memory cache.\n# The cache will be shared among all simultaneous users of the application.\nshinyApp(\n  fluidPage(\n    sidebarLayout(\n      sidebarPanel(\n        sliderInput(\"n\", \"Number of points\", 4, 32, value = 8, step = 4)\n      ),\n      mainPanel(plotOutput(\"plot\"))\n    )\n  ),\n  function(input, output, session) {\n    output$plot <- renderCachedPlot({\n        Sys.sleep(2)  # Add an artificial delay\n        seqn <- seq_len(input$n)\n        plot(mtcars$wt[seqn], mtcars$mpg[seqn],\n             xlim = range(mtcars$wt), ylim = range(mtcars$mpg))\n      },\n      cacheKeyExpr = { list(input$n) }\n    )\n  }\n)\n\n\n\n# An example uses a data object shared across sessions. mydata() is part of\n# the cache key, so when its value changes, plots that were previously\n# stored in the cache will no longer be used (unless mydata() changes back\n# to its previous value).\nmydata <- reactiveVal(data.frame(x = rnorm(400), y = rnorm(400)))\n\nui <- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"n\", \"Number of points\", 50, 400, 100, step = 50),\n      actionButton(\"newdata\", \"New data\")\n    ),\n    mainPanel(\n      plotOutput(\"plot\")\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  observeEvent(input$newdata, {\n    mydata(data.frame(x = rnorm(400), y = rnorm(400)))\n  })\n\n  output$plot <- renderCachedPlot(\n    {\n      Sys.sleep(2)\n      d <- mydata()\n      seqn <- seq_len(input$n)\n      plot(d$x[seqn], d$y[seqn], xlim = range(d$x), ylim = range(d$y))\n    },\n    cacheKeyExpr = { list(input$n, mydata()) },\n  )\n}\n\nshinyApp(ui, server)\n\n\n# A basic application with two plots, where each plot in each session has\n# a separate cache.\nshinyApp(\n  fluidPage(\n    sidebarLayout(\n      sidebarPanel(\n        sliderInput(\"n\", \"Number of points\", 4, 32, value = 8, step = 4)\n      ),\n      mainPanel(\n        plotOutput(\"plot1\"),\n        plotOutput(\"plot2\")\n      )\n    )\n  ),\n  function(input, output, session) {\n    output$plot1 <- renderCachedPlot({\n        Sys.sleep(2)  # Add an artificial delay\n        seqn <- seq_len(input$n)\n        plot(mtcars$wt[seqn], mtcars$mpg[seqn],\n             xlim = range(mtcars$wt), ylim = range(mtcars$mpg))\n      },\n      cacheKeyExpr = { list(input$n) },\n      cache = cachem::cache_mem()\n    )\n    output$plot2 <- renderCachedPlot({\n        Sys.sleep(2)  # Add an artificial delay\n        seqn <- seq_len(input$n)\n        plot(mtcars$wt[seqn], mtcars$mpg[seqn],\n             xlim = range(mtcars$wt), ylim = range(mtcars$mpg))\n      },\n      cacheKeyExpr = { list(input$n) },\n      cache = cachem::cache_mem()\n    )\n  }\n)\n\n}\n\n\\dontrun{\n# At the top of app.R, this set the application-scoped cache to be a memory\n# cache that is 20 MB in size, and where cached objects expire after one\n# hour.\nshinyOptions(cache = cachem::cache_mem(max_size = 20e6, max_age = 3600))\n\n# At the top of app.R, this set the application-scoped cache to be a disk\n# cache that can be shared among multiple concurrent R processes, and is\n# deleted when the system reboots.\nshinyOptions(cache = cachem::cache_disk(file.path(dirname(tempdir()), \"myapp-cache\")))\n\n# At the top of app.R, this set the application-scoped cache to be a disk\n# cache that can be shared among multiple concurrent R processes, and\n# persists on disk across reboots.\nshinyOptions(cache = cachem::cache_disk(\"./myapp-cache\"))\n\n# At the top of the server function, this set the session-scoped cache to be\n# a memory cache that is 5 MB in size.\nserver <- function(input, output, session) {\n  shinyOptions(cache = cachem::cache_mem(max_size = 5e6))\n\n  output$plot <- renderCachedPlot(\n    ...,\n    cache = \"session\"\n  )\n}\n\n}",
            "renderDataTable": "## Only run this example in interactive R sessions\nif (interactive()) {\n  # pass a callback function to DataTables using I()\n  shinyApp(\n    ui = fluidPage(\n      fluidRow(\n        column(12,\n          dataTableOutput('table')\n        )\n      )\n    ),\n    server = function(input, output) {\n      output$table <- renderDataTable(iris,\n        options = list(\n          pageLength = 5,\n          initComplete = I(\"function(settings, json) {alert('Done.');}\")\n        )\n      )\n    }\n  )\n}",
            "renderImage": "## Only run examples in interactive R sessions\nif (interactive()) {\noptions(device.ask.default = FALSE)\n\nui <- fluidPage(\n  sliderInput(\"n\", \"Number of observations\", 2, 1000, 500),\n  plotOutput(\"plot1\"),\n  plotOutput(\"plot2\"),\n  plotOutput(\"plot3\")\n)\n\nserver <- function(input, output, session) {\n\n  # A plot of fixed size\n  output$plot1 <- renderImage({\n    # A temp file to save the output. It will be deleted after renderImage\n    # sends it, because deleteFile=TRUE.\n    outfile <- tempfile(fileext='.png')\n\n    # Generate a png\n    png(outfile, width=400, height=400)\n    hist(rnorm(input$n))\n    dev.off()\n\n    # Return a list\n    list(src = outfile,\n         alt = \"This is alternate text\")\n  }, deleteFile = TRUE)\n\n  # A dynamically-sized plot\n  output$plot2 <- renderImage({\n    # Read plot2's width and height. These are reactive values, so this\n    # expression will re-run whenever these values change.\n    width  <- session$clientData$output_plot2_width\n    height <- session$clientData$output_plot2_height\n\n    # A temp file to save the output.\n    outfile <- tempfile(fileext='.png')\n\n    png(outfile, width=width, height=height)\n    hist(rnorm(input$n))\n    dev.off()\n\n    # Return a list containing the filename\n    list(src = outfile,\n         width = width,\n         height = height,\n         alt = \"This is alternate text\")\n  }, deleteFile = TRUE)\n\n  # Send a pre-rendered image, and don't delete the image after sending it\n  # NOTE: For this example to work, it would require files in a subdirectory\n  # named images/\n  output$plot3 <- renderImage({\n    # When input$n is 1, filename is ./images/image1.jpeg\n    filename <- normalizePath(file.path('./images',\n                              paste('image', input$n, '.jpeg', sep='')))\n\n    # Return a list containing the filename\n    list(src = filename)\n  }, deleteFile = FALSE)\n}\n\nshinyApp(ui, server)\n}"
        }
    },
    "RcppArmadillo": {
        "description": "'Armadillo' is a templated C++ linear algebra library (by Conrad\n Sanderson) that aims towards a good balance between speed and ease of\n use. Integer, floating point and complex numbers are supported, as\n well as a subset of trigonometric and statistics functions. Various\n matrix decompositions are provided through optional integration with\n LAPACK and ATLAS libraries.  The 'RcppArmadillo' package includes the\n header files from the templated 'Armadillo' library. Thus users do\n not need to install 'Armadillo' itself in order to use\n 'RcppArmadillo'. From release 7.800.0 on, 'Armadillo' is licensed\n under Apache License 2; previous releases were under licensed as MPL\n 2.0 from version 3.800.0 onwards and LGPL-3 prior to that;\n 'RcppArmadillo' (the 'Rcpp' bindings/bridge to Armadillo) is licensed\n under the GNU GPL version 2 or later, as is the rest of 'Rcpp'.",
        "examples": {
            "RcppArmadillo.package.skeleton": "\\dontrun{\nRcppArmadillo.package.skeleton( \"foobar\" )\n}",
            "fastLm": "\\dontshow{\n      ## as an illustration, the example is computationally inexpensive\n      ## and does not require this per se\n      armadillo_throttle_cores(2)\n  }\n  data(trees, package=\"datasets\")\n\n  ## bare-bones direct interface\n  flm <- fastLmPure( cbind(1, log(trees$Girth)), log(trees$Volume) )\n  print(flm)\n\n  ## standard R interface for formula or data returning object of class fastLm\n  flmmod <- fastLm( log(Volume) ~ log(Girth), data=trees)\n  summary(flmmod)\n\n  ## case where fastLm breaks down\n  dd <- data.frame(f1 = gl(4, 6, labels = LETTERS[1:4]),\n                   f2 = gl(3, 2, labels = letters[1:3]))[-(7:8), ]\n  xtabs(~ f2 + f1, dd)     # one missing cell\n  mm <- model.matrix(~ f1 * f2, dd)\n  kappa(mm)                # large, indicating rank deficiency\n  set.seed(1)\n  dd$y <- mm \\%*\\% seq_len(ncol(mm)) + rnorm(nrow(mm), sd = 0.1)\n  summary(lm(y ~ f1 * f2, dd))     # detects rank deficiency\n  summary(fastLm(y ~ f1 * f2, dd)) # some huge coefficients\n\n  \\dontshow{armadillo_reset_cores()}"
        }
    },
    "pkgload": {
        "description": "Simulates the process of installing a package and then\n    attaching it. This is a key part of the 'devtools' package as it\n    allows you to rapidly iterate while developing a package.",
        "examples": {
            "dev_example": "\\dontrun{\n# Runs installed example:\nlibrary(\"ggplot2\")\nexample(\"ggplot\")\n\n# Runs development example:\ndev_example(\"ggplot\")\n}",
            "dev_help": "\\dontrun{\nlibrary(\"ggplot2\")\nhelp(\"ggplot\") # loads installed documentation for ggplot\n\nload_all(\"ggplot2\")\ndev_help(\"ggplot\") # loads development documentation for ggplot\n}",
            "dev_meta": "dev_meta(\"stats\") # NULL\n\nif (has_tests()) {\n# Load the test package in directory \"testLoadHooks\"\nload_all(pkgtest(\"testLoadHooks\"))\n\n# Get metadata for the package\nx <- dev_meta(\"testLoadHooks\")\nas.list(x)\n\n# Clean up.\nunload(\"testLoadHooks\")\n}",
            "help": "\\dontrun{\n# This would load devtools and look at the help for load_all, if currently\n# in the devtools source directory.\nload_all()\n?load_all\nhelp(\"load_all\")\n}\n\n# To see the help pages for utils::help and utils::`?`:\nhelp(\"help\", \"utils\")\nhelp(\"?\", \"utils\")\n\n\\dontrun{\n# Examples demonstrating the multiple ways of supplying arguments\n# NB: you can't do pkg <- \"ggplot2\"; help(\"ggplot2\", pkg)\nhelp(lm)\nhelp(lm, stats)\nhelp(lm, 'stats')\nhelp('lm')\nhelp('lm', stats)\nhelp('lm', 'stats')\nhelp(package = stats)\nhelp(package = 'stats')\ntopic <- \"lm\"\nhelp(topic)\nhelp(topic, stats)\nhelp(topic, 'stats')\n}",
            "inst": "inst(\"pkgload\")\ninst(\"grid\")",
            "load_all": "\\dontrun{\n# Load the package in the current directory\nload_all(\"./\")\n\n# Running again loads changed files\nload_all(\"./\")\n\n# With export_all=FALSE, only objects listed as exports in NAMESPACE\n# are exported\nload_all(\"./\", export_all = FALSE)\n}",
            "package_file": "\\dontrun{\npackage_file(\"figures\", \"figure_1\")\n}",
            "parse_deps": "parse_deps(\"httr (< 2.1),\\nRCurl (>= 3)\")\n# only package dependencies are returned\nparse_deps(\"utils (== 2.12.1),\\ntools,\\nR (>= 2.10),\\nmemoise\")",
            "parse_ns_file": "if (has_tests()) {\nparse_ns_file(pkgtest(\"testLoadHooks\"))\n}",
            "pkgtest": "if (has_tests()) {\npkgtest(\"testData\")\n}",
            "unload": "\\dontrun{\n# Unload package that is in current directory\nunload()\n\n# Unload package that is in ./ggplot2/\nunload(pkg_name(\"ggplot2/\"))\n\nlibrary(ggplot2)\n# unload the ggplot2 package directly by name\nunload(\"ggplot2\")\n}"
        }
    },
    "dbplyr": {
        "description": "A 'dplyr' back end for databases that allows you to work with\n    remote database tables as if they are in-memory data frames.  Basic\n    features works with any database that has a 'DBI' back end; more\n    advanced features require 'SQL' translation to be provided by the\n    package author.",
        "examples": {
            "arrange.tbl_lazy": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(a = c(3, 4, 1, 2), b = c(5, 1, 2, NA))\ndb \\%>\\% arrange(a) \\%>\\% show_query()\n\n# Note that NAs are sorted first\ndb \\%>\\% arrange(b)\n# override by sorting on is.na() first\ndb \\%>\\% arrange(is.na(b), b)",
            "backend-access": "library(dplyr, warn.conflicts = FALSE)\nlf <- lazy_frame(x = 1, y = 2, z = \"a\", con = simulate_access())\n\nlf \\%>\\% head()\nlf \\%>\\% mutate(y = as.numeric(y), z = sqrt(x^2 + 10))\nlf \\%>\\% mutate(a = paste0(z, \" times\"))",
            "backend-hana": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, c = 2, d = \"z\", con = simulate_hana())\nlf \\%>\\% transmute(x = paste0(d, \" times\"))",
            "backend-hive": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, d = 2, c = \"z\", con = simulate_hive())\nlf \\%>\\% transmute(x = cot(b))\nlf \\%>\\% transmute(x = bitwShiftL(c, 1L))\nlf \\%>\\% transmute(x = str_replace_all(c, \"a\", \"b\"))\n\nlf \\%>\\% summarise(x = median(d, na.rm = TRUE))\nlf \\%>\\% summarise(x = var(c, na.rm = TRUE))",
            "backend-impala": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, c = 2, d = \"z\", con = simulate_impala())\nlf \\%>\\% transmute(X = bitwNot(bitwOr(b, c)))",
            "backend-mssql": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, c = 2, d = \"z\", con = simulate_mssql())\nlf \\%>\\% head()\nlf \\%>\\% transmute(x = paste(b, c, d))\n\n# Can use boolean as is:\nlf \\%>\\% filter(c > d)\n# Need to convert from boolean to bit:\nlf \\%>\\% transmute(x = c > d)\n# Can use boolean as is:\nlf \\%>\\% transmute(x = ifelse(c > d, \"c\", \"d\"))",
            "backend-mysql": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, c = 2, d = \"z\", con = simulate_mysql())\nlf \\%>\\% transmute(x = paste0(d, \" times\"))",
            "backend-odbc": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, d = 2, c = \"z\", con = simulate_odbc())\nlf \\%>\\% transmute(x = as.numeric(b))\nlf \\%>\\% transmute(x = as.integer(b))\nlf \\%>\\% transmute(x = as.character(b))",
            "backend-oracle": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, c = 2, d = \"z\", con = simulate_oracle())\nlf \\%>\\% transmute(x = paste0(c, \" times\"))\nlf \\%>\\% setdiff(lf)",
            "backend-postgres": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, c = 2, d = \"z\", con = simulate_postgres())\nlf \\%>\\% summarise(x = sd(b, na.rm = TRUE))\nlf \\%>\\% summarise(y = cor(b, c), z = cov(b, c))",
            "backend-redshift": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, c = 2, d = \"z\", con = simulate_redshift())\nlf \\%>\\% transmute(x = paste(c, \" times\"))\nlf \\%>\\% transmute(x = substr(c, 2, 3))\nlf \\%>\\% transmute(x = str_replace_all(c, \"a\", \"z\"))",
            "backend-snowflake": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, c = 2, d = \"z\", con = simulate_snowflake())\nlf \\%>\\% transmute(x = paste0(d, \" times\"))",
            "backend-spark-sql": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, d = 2, c = \"z\", con = simulate_spark_sql())\n\nlf \\%>\\% summarise(x = median(d, na.rm = TRUE))\nlf \\%>\\% summarise(x = var(c, na.rm = TRUE), .by = d)\n\nlf \\%>\\% mutate(x = first(c))\nlf \\%>\\% mutate(x = first(c), .by = d)",
            "backend-sqlite": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, c = 2, d = \"z\", con = simulate_sqlite())\nlf \\%>\\% transmute(x = paste(c, \" times\"))\nlf \\%>\\% transmute(x = log(b), y = log(b, base = 2))",
            "backend-teradata": "library(dplyr, warn.conflicts = FALSE)\n\nlf <- lazy_frame(a = TRUE, b = 1, c = 2, d = \"z\", con = simulate_teradata())\nlf \\%>\\% head()",
            "build_sql": "con <- simulate_dbi()\nbuild_sql(\"SELECT * FROM TABLE\", con = con)\nx <- \"TABLE\"\nbuild_sql(\"SELECT * FROM \", x, con = con)\nbuild_sql(\"SELECT * FROM \", ident(x), con = con)\nbuild_sql(\"SELECT * FROM \", sql(x), con = con)\n\n# http://xkcd.com/327/\nname <- \"Robert'); DROP TABLE Students;--\"\nbuild_sql(\"INSERT INTO Students (Name) VALUES (\", name, \")\", con = con)",
            "collapse.tbl_sql": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(a = c(3, 4, 1, 2), b = c(5, 1, 2, NA))\ndb \\%>\\% filter(a <= 2) \\%>\\% collect()",
            "complete.tbl_lazy": "\\dontshow{if (rlang::is_installed(\"tidyr\", version = \"1.0.0\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\ndf <- memdb_frame(\n  group = c(1:2, 1),\n  item_id = c(1:2, 2),\n  item_name = c(\"a\", \"b\", \"b\"),\n  value1 = 1:3,\n  value2 = 4:6\n)\n\ndf \\%>\\% tidyr::complete(group, nesting(item_id, item_name))\n\n# You can also choose to fill in missing values\ndf \\%>\\% tidyr::complete(group, nesting(item_id, item_name), fill = list(value1 = 0))\n\\dontshow{\\}) # examplesIf}",
            "copy_inline": "df <- data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\"))\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\n\ncopy_inline(con, df)\n\ncopy_inline(con, df) \\%>\\% dplyr::show_query()",
            "copy_to.src_sql": "library(dplyr, warn.conflicts = FALSE)\n\ndf <- data.frame(x = 1:5, y = letters[5:1])\ndb <- copy_to(src_memdb(), df)\ndb\n\ndf2 <- data.frame(y = c(\"a\", \"d\"), fruit = c(\"apple\", \"date\"))\n# copy_to() is called automatically if you set copy = TRUE\n# in the join functions\ndb \\%>\\% left_join(df2, copy = TRUE)",
            "count.tbl_lazy": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(g = c(1, 1, 1, 2, 2), x = c(4, 3, 6, 9, 2))\ndb \\%>\\% count(g) \\%>\\% show_query()\ndb \\%>\\% count(g, wt = x) \\%>\\% show_query()\ndb \\%>\\% count(g, wt = x, sort = TRUE) \\%>\\% show_query()",
            "db-quote": "con <- simulate_dbi()\nsql_escape_logical(con, c(TRUE, FALSE, NA))\nsql_escape_date(con, Sys.Date())\nsql_escape_date(con, Sys.time())\nsql_escape_raw(con, charToRaw(\"hi\"))",
            "dbplyr-slice": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(x = 1:3, y = c(1, 1, 2))\ndb \\%>\\% slice_min(x) \\%>\\% show_query()\ndb \\%>\\% slice_max(x) \\%>\\% show_query()\ndb \\%>\\% slice_sample() \\%>\\% show_query()\n\ndb \\%>\\% group_by(y) \\%>\\% slice_min(x) \\%>\\% show_query()\n\n# By default, ties are includes so you may get more rows\n# than you expect\ndb \\%>\\% slice_min(y, n = 1)\ndb \\%>\\% slice_min(y, n = 1, with_ties = FALSE)\n\n# Non-integer group sizes are rounded down\ndb \\%>\\% slice_min(x, prop = 0.5)",
            "dbplyr_uncount": "df <- memdb_frame(x = c(\"a\", \"b\"), n = c(1, 2))\ndbplyr_uncount(df, n)\ndbplyr_uncount(df, n, .id = \"id\")\n\n# You can also use constants\ndbplyr_uncount(df, 2)\n\n# Or expressions\ndbplyr_uncount(df, 2 / n)",
            "distinct.tbl_lazy": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(x = c(1, 1, 2, 2), y = c(1, 2, 1, 1))\ndb \\%>\\% distinct() \\%>\\% show_query()\ndb \\%>\\% distinct(x) \\%>\\% show_query()",
            "escape": "# Doubles vs. integers\nescape_ansi(1:5)\nescape_ansi(c(1, 5.4))\n\n# String vs known sql vs. sql identifier\nescape_ansi(\"X\")\nescape_ansi(sql(\"X\"))\nescape_ansi(ident(\"X\"))\n\n# Escaping is idempotent\nescape_ansi(\"X\")\nescape_ansi(escape_ansi(\"X\"))\nescape_ansi(escape_ansi(escape_ansi(\"X\")))",
            "expand.tbl_lazy": "\\dontshow{if (rlang::is_installed(\"tidyr\", version = \"1.0.0\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nfruits <- memdb_frame(\n  type   = c(\"apple\", \"orange\", \"apple\", \"orange\", \"orange\", \"orange\"),\n  year   = c(2010, 2010, 2012, 2010, 2010, 2012),\n  size = c(\"XS\", \"S\",  \"M\", \"S\", \"S\", \"M\"),\n  weights = rnorm(6)\n)\n\n# All possible combinations ---------------------------------------\nfruits \\%>\\% tidyr::expand(type)\nfruits \\%>\\% tidyr::expand(type, size)\n\n# Only combinations that already appear in the data ---------------\nfruits \\%>\\% tidyr::expand(nesting(type, size))\n\\dontshow{\\}) # examplesIf}",
            "fill.tbl_lazy": "\\dontshow{if (rlang::is_installed(\"tidyr\", version = \"1.0.0\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nsquirrels <- tibble::tribble(\n  ~group,    ~name,     ~role,     ~n_squirrels, ~ n_squirrels2,\n  1,      \"Sam\",    \"Observer\",   NA,                 1,\n  1,     \"Mara\", \"Scorekeeper\",    8,                NA,\n  1,    \"Jesse\",    \"Observer\",   NA,                NA,\n  1,      \"Tom\",    \"Observer\",   NA,                 4,\n  2,     \"Mike\",    \"Observer\",   NA,                NA,\n  2,  \"Rachael\",    \"Observer\",   NA,                 6,\n  2,  \"Sydekea\", \"Scorekeeper\",   14,                NA,\n  2, \"Gabriela\",    \"Observer\",   NA,                NA,\n  3,  \"Derrick\",    \"Observer\",   NA,                NA,\n  3,     \"Kara\", \"Scorekeeper\",    9,                 10,\n  3,    \"Emily\",    \"Observer\",   NA,                NA,\n  3, \"Danielle\",    \"Observer\",   NA,                NA\n)\nsquirrels$id <- 1:12\n\ntbl_memdb(squirrels) \\%>\\%\n  window_order(id) \\%>\\%\n  tidyr::fill(\n    n_squirrels,\n    n_squirrels2,\n  )\n\\dontshow{\\}) # examplesIf}",
            "filter.tbl_lazy": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(x = c(2, NA, 5, NA, 10), y = 1:5)\ndb \\%>\\% filter(x < 5) \\%>\\% show_query()\ndb \\%>\\% filter(is.na(x)) \\%>\\% show_query()",
            "get_returned_rows": "library(dplyr)\n\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\nDBI::dbExecute(con, \"CREATE TABLE Info (\n   id INTEGER PRIMARY KEY AUTOINCREMENT,\n   number INTEGER\n)\")\ninfo <- tbl(con, \"Info\")\n\nrows1 <- copy_inline(con, data.frame(number = c(1, 5)))\nrows_insert(info, rows1, conflict = \"ignore\", in_place = TRUE)\ninfo\n\n# If the table has an auto incrementing primary key, you can use\n# the returning argument + `get_returned_rows()` its value\nrows2 <- copy_inline(con, data.frame(number = c(13, 27)))\ninfo <- rows_insert(\n  info,\n  rows2,\n  conflict = \"ignore\",\n  in_place = TRUE,\n  returning = id\n)\ninfo\nget_returned_rows(info)",
            "group_by.tbl_lazy": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(g = c(1, 1, 1, 2, 2), x = c(4, 3, 6, 9, 2))\ndb \\%>\\%\n  group_by(g) \\%>\\%\n  summarise(n()) \\%>\\%\n  show_query()\n\ndb \\%>\\%\n  group_by(g) \\%>\\%\n  mutate(x2 = x / sum(x, na.rm = TRUE)) \\%>\\%\n  show_query()",
            "head.tbl_lazy": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(x = 1:100)\ndb \\%>\\% head() \\%>\\% show_query()\n\n# Pretend we have data in a SQL server database\ndb2 <- lazy_frame(x = 1:100, con = simulate_mssql())\ndb2 \\%>\\% head() \\%>\\% show_query()",
            "ident": "# SQL92 quotes strings with '\nescape_ansi(\"x\")\n\n# And identifiers with \"\nident(\"x\")\nescape_ansi(ident(\"x\"))\n\n# You can supply multiple inputs\nident(a = \"x\", b = \"y\")\nident_q(a = \"x\", b = \"y\")",
            "in_schema": "# Previously:\nin_schema(\"my_schema\", \"my_table\")\nin_catalog(\"my_catalog\", \"my_schema\", \"my_table\")\nin_schema(sql(\"my_schema\"), sql(\"my_table\"))\n\n# Now\nI(\"my_schema.my_table\")\nI(\"my_catalog.my_schema.my_table\")\nI(\"my_schema.my_table\")\n\n# Example using schemas with SQLite\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\n\n# Add auxiliary schema\ntmp <- tempfile()\nDBI::dbExecute(con, paste0(\"ATTACH '\", tmp, \"' AS aux\"))\n\nlibrary(dplyr, warn.conflicts = FALSE)\ncopy_to(con, iris, \"df\", temporary = FALSE)\ncopy_to(con, mtcars, I(\"aux.df\"), temporary = FALSE)\n\ncon \\%>\\% tbl(\"df\")\ncon \\%>\\% tbl(I(\"aux.df\"))",
            "join.tbl_sql": "library(dplyr, warn.conflicts = FALSE)\n\nband_db <- tbl_memdb(dplyr::band_members)\ninstrument_db <- tbl_memdb(dplyr::band_instruments)\nband_db \\%>\\% left_join(instrument_db) \\%>\\% show_query()\n\n# Can join with local data frames by setting copy = TRUE\nband_db \\%>\\%\n  left_join(dplyr::band_instruments, copy = TRUE)\n\n# Unlike R, joins in SQL don't usually match NAs (NULLs)\ndb <- memdb_frame(x = c(1, 2, NA))\nlabel <- memdb_frame(x = c(1, NA), label = c(\"one\", \"missing\"))\ndb \\%>\\% left_join(label, by = \"x\")\n# But you can activate R's usual behaviour with the na_matches argument\ndb \\%>\\% left_join(label, by = \"x\", na_matches = \"na\")\n\n# By default, joins are equijoins, but you can use `sql_on` to\n# express richer relationships\ndb1 <- memdb_frame(x = 1:5)\ndb2 <- memdb_frame(x = 1:3, y = letters[1:3])\ndb1 \\%>\\% left_join(db2) \\%>\\% show_query()\ndb1 \\%>\\% left_join(db2, sql_on = \"LHS.x < RHS.x\") \\%>\\% show_query()",
            "lahman": "# Connect to a local sqlite database, if already created\n\\donttest{\nlibrary(dplyr)\n\nif (has_lahman(\"sqlite\")) {\n  lahman_sqlite()\n  batting <- tbl(lahman_sqlite(), \"Batting\")\n  batting\n}\n\n# Connect to a local postgres database with lahman database, if available\nif (has_lahman(\"postgres\")) {\n  lahman_postgres()\n  batting <- tbl(lahman_postgres(), \"Batting\")\n}\n}",
            "memdb_frame": "library(dplyr)\ndf <- memdb_frame(x = runif(100), y = runif(100))\ndf \\%>\\% arrange(x)\ndf \\%>\\% arrange(x) \\%>\\% show_query()\n\nmtcars_db <- tbl_memdb(mtcars)\nmtcars_db \\%>\\% group_by(cyl) \\%>\\% summarise(n = n()) \\%>\\% show_query()",
            "mutate.tbl_lazy": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(x = 1:5, y = 5:1)\ndb \\%>\\%\n  mutate(a = (x + y) / 2, b = sqrt(x^2L + y^2L)) \\%>\\%\n  show_query()\n\n# dbplyr automatically creates subqueries as needed\ndb \\%>\\%\n  mutate(x1 = x + 1, x2 = x1 * 2) \\%>\\%\n  show_query()",
            "partial_eval": "lf <- lazy_frame(year = 1980, id = 1)\npartial_eval(quote(year > 1980), data = lf)\n\nids <- c(\"ansonca01\", \"forceda01\", \"mathebo01\")\npartial_eval(quote(id \\%in\\% ids), lf)\n\n# cf.\npartial_eval(quote(id == .data$id), lf)\n\n# You can use local() or .env to disambiguate between local and remote\n# variables: otherwise remote is always preferred\nyear <- 1980\npartial_eval(quote(year > year), lf)\npartial_eval(quote(year > local(year)), lf)\npartial_eval(quote(year > .env$year), lf)\n\n# Functions are always assumed to be remote. Use local to force evaluation\n# in R.\nf <- function(x) x + 1\npartial_eval(quote(year > f(1980)), lf)\npartial_eval(quote(year > local(f(1980))), lf)",
            "pivot_longer.tbl_lazy": "\\dontshow{if (rlang::is_installed(\"tidyr\", version = \"1.0.0\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n# See vignette(\"pivot\") for examples and explanation\n\n# Simplest case where column names are character data\nmemdb_frame(\n  id = c(\"a\", \"b\"),\n  x = 1:2,\n  y = 3:4\n) \\%>\\%\n  tidyr::pivot_longer(-id)\n\\dontshow{\\}) # examplesIf}",
            "pivot_wider.tbl_lazy": "\\dontshow{if (rlang::is_installed(\"tidyr\", version = \"1.0.0\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nmemdb_frame(\n  id = 1,\n  key = c(\"x\", \"y\"),\n  value = 1:2\n) \\%>\\%\n  tidyr::pivot_wider(\n    id_cols = id,\n    names_from = key,\n    values_from = value\n  )\n\\dontshow{\\}) # examplesIf}",
            "pull.tbl_sql": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(x = 1:5, y = 5:1)\ndb \\%>\\%\n  mutate(z = x + y * 2) \\%>\\%\n  pull()",
            "remote_name": "mf <- memdb_frame(x = 1:5, y = 5:1, .name = \"blorp\")\nremote_name(mf)\nremote_src(mf)\nremote_con(mf)\nremote_query(mf)\n\nmf2 <- dplyr::filter(mf, x > 3)\nremote_name(mf2)\nremote_src(mf2)\nremote_con(mf2)\nremote_query(mf2)",
            "replace_na.tbl_lazy": "\\dontshow{if (rlang::is_installed(\"tidyr\", version = \"1.0.0\")) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\ndf <- memdb_frame(x = c(1, 2, NA), y = c(\"a\", NA, \"b\"))\ndf \\%>\\% tidyr::replace_na(list(x = 0, y = \"unknown\"))\n\\dontshow{\\}) # examplesIf}",
            "rows-db": "library(dplyr)\n\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\nDBI::dbExecute(con, \"CREATE TABLE Ponies (\n   id INTEGER PRIMARY KEY AUTOINCREMENT,\n   name TEXT,\n   cutie_mark TEXT\n)\")\n\nponies <- tbl(con, \"Ponies\")\n\napplejack <- copy_inline(con, data.frame(\n  name = \"Apple Jack\",\n  cutie_mark = \"three apples\"\n))\n\n# The default behavior is to generate a SELECT query\nrows_insert(ponies, applejack, conflict = \"ignore\")\n# And the original table is left unchanged:\nponies\n\n# You can also choose to modify the table with in_place = TRUE:\nrows_insert(ponies, applejack, conflict = \"ignore\", in_place = TRUE)\n# In this case `rows_insert()` returns nothing and the underlying\n# data is modified\nponies",
            "select.tbl_lazy": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(x = 1, y = 2, z = 3)\ndb \\%>\\% select(-y) \\%>\\% show_query()\ndb \\%>\\% relocate(z) \\%>\\% show_query()\ndb \\%>\\% rename(first = x, last = z) \\%>\\% show_query()",
            "sql_expr": "con <- simulate_dbi() # not necessary when writing translations\n\nsql_expr(f(x + 1), con = con)\nsql_expr(f(\"x\", \"y\"), con = con)\nsql_expr(f(x, y), con = con)\n\nx <- ident(\"x\")\nsql_expr(f(!!x, y), con = con)\n\nsql_expr(cast(\"x\" \\%as\\% DECIMAL), con = con)\nsql_expr(round(x) \\%::\\% numeric, con = con)\n\nsql_call2(\"+\", quote(x), 1, con = con)\nsql_call2(\"+\", \"x\", 1, con = con)",
            "sql_options": "library(dplyr, warn.conflicts = FALSE)\nlf1 <- lazy_frame(key = 1, a = 1, b = 2)\nlf2 <- lazy_frame(key = 1, a = 1, c = 3)\n\nresult <- left_join(lf1, lf2, by = \"key\") \\%>\\%\n  filter(c >= 3)\n\nshow_query(result)\nsql_options <- sql_options(cte = TRUE, qualify_all_columns = TRUE)\nshow_query(result, sql_options = sql_options)",
            "sql_query_insert": "sql_query_upsert(\n  con = simulate_postgres(),\n  table = ident(\"airlines\"),\n  from = ident(\"df\"),\n  by = \"carrier\",\n  update_cols = \"name\"\n)",
            "sql_quote": "sql_quote(\"abc\", \"'\")\nsql_quote(\"I've had a good day\", \"'\")\nsql_quote(c(\"abc\", NA), \"'\")",
            "sql_variant": "# An example of adding some mappings for the statistical functions that\n# postgresql provides: http://bit.ly/K5EdTn\n\npostgres_agg <- sql_translator(.parent = base_agg,\n  cor = sql_aggregate_2(\"CORR\"),\n  cov = sql_aggregate_2(\"COVAR_SAMP\"),\n  sd =  sql_aggregate(\"STDDEV_SAMP\", \"sd\"),\n  var = sql_aggregate(\"VAR_SAMP\", \"var\")\n)\n\n# Next we have to simulate a connection that uses this variant\ncon <- simulate_dbi(\"TestCon\")\nsql_translation.TestCon <- function(x) {\n  sql_variant(\n    base_scalar,\n    postgres_agg,\n    base_no_win\n  )\n}\n\ntranslate_sql(cor(x, y), con = con, window = FALSE)\ntranslate_sql(sd(income / years), con = con, window = FALSE)\n\n# Any functions not explicitly listed in the converter will be translated\n# to sql as is, so you don't need to convert all functions.\ntranslate_sql(regr_intercept(y, x), con = con)",
            "summarise.tbl_lazy": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(g = c(1, 1, 1, 2, 2), x = c(4, 3, 6, 9, 2))\ndb \\%>\\%\n  summarise(n()) \\%>\\%\n  show_query()\n\ndb \\%>\\%\n  group_by(g) \\%>\\%\n  summarise(n()) \\%>\\%\n  show_query()",
            "tbl.src_dbi": "library(dplyr)\n\n# Connect to a temporary in-memory SQLite database\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\n\n# Add some data\ncopy_to(con, mtcars)\nDBI::dbListTables(con)\n\n# To retrieve a single table from a source, use `tbl()`\ncon \\%>\\% tbl(\"mtcars\")\n\n# Use `I()` for qualified table names\ncon \\%>\\% tbl(I(\"temp.mtcars\")) \\%>\\% head(1)\n\n# You can also use pass raw SQL if you want a more sophisticated query\ncon \\%>\\% tbl(sql(\"SELECT * FROM mtcars WHERE cyl = 8\"))\n\n# If you just want a temporary in-memory database, use src_memdb()\nsrc2 <- src_memdb()\n\n# To show off the full features of dplyr's database integration,\n# we'll use the Lahman database. lahman_sqlite() takes care of\n# creating the database.\n\nif (requireNamespace(\"Lahman\", quietly = TRUE)) {\nbatting <- copy_to(con, Lahman::Batting)\nbatting\n\n# Basic data manipulation verbs work in the same way as with a tibble\nbatting \\%>\\% filter(yearID > 2005, G > 130)\nbatting \\%>\\% select(playerID:lgID)\nbatting \\%>\\% arrange(playerID, desc(yearID))\nbatting \\%>\\% summarise(G = mean(G), n = n())\n\n# There are a few exceptions. For example, databases give integer results\n# when dividing one integer by another. Multiply by 1 to fix the problem\nbatting \\%>\\%\n  select(playerID:lgID, AB, R, G) \\%>\\%\n  mutate(\n   R_per_game1 = R / G,\n   R_per_game2 = R * 1.0 / G\n )\n\n# All operations are lazy: they don't do anything until you request the\n# data, either by `print()`ing it (which shows the first ten rows),\n# or by `collect()`ing the results locally.\nsystem.time(recent <- filter(batting, yearID > 2010))\nsystem.time(collect(recent))\n\n# You can see the query that dplyr creates with show_query()\nbatting \\%>\\%\n  filter(G > 0) \\%>\\%\n  group_by(playerID) \\%>\\%\n  summarise(n = n()) \\%>\\%\n  show_query()\n}",
            "tbl_lazy": "library(dplyr)\ndf <- data.frame(x = 1, y = 2)\n\ndf_sqlite <- tbl_lazy(df, con = simulate_sqlite())\ndf_sqlite \\%>\\% summarise(x = sd(x, na.rm = TRUE)) \\%>\\% show_query()",
            "testing": "\\dontrun{\ntest_register_src(\"sqlite\", {\n  DBI::dbConnect(RSQLite::SQLite(), \":memory:\", create = TRUE)\n})\n\ntest_frame(x = 1:3, y = 3:1)\ntest_load(mtcars)\n}",
            "translate_sql": "con <- simulate_dbi()\n\n# Regular maths is translated in a very straightforward way\ntranslate_sql(x + 1, con = con)\ntranslate_sql(sin(x) + tan(y), con = con)\n\n# Note that all variable names are escaped\ntranslate_sql(like == \"x\", con = con)\n# In ANSI SQL: \"\" quotes variable _names_, '' quotes strings\n\n# Logical operators are converted to their sql equivalents\ntranslate_sql(x < 5 & !(y >= 5), con = con)\n# xor() doesn't have a direct SQL equivalent\ntranslate_sql(xor(x, y), con = con)\n\n# If is translated into case when\ntranslate_sql(if (x > 5) \"big\" else \"small\", con = con)\n\n# Infix functions are passed onto SQL with \\% removed\ntranslate_sql(first \\%like\\% \"Had\\%\", con = con)\ntranslate_sql(first \\%is\\% NA, con = con)\ntranslate_sql(first \\%in\\% c(\"John\", \"Roger\", \"Robert\"), con = con)\n\n# And be careful if you really want integers\ntranslate_sql(x == 1, con = con)\ntranslate_sql(x == 1L, con = con)\n\n# If you have an already quoted object, use translate_sql_:\nx <- quote(y + 1 / sin(t))\ntranslate_sql_(list(x), con = simulate_dbi())\n\n# Windowed translation --------------------------------------------\n# Known window functions automatically get OVER()\ntranslate_sql(mpg > mean(mpg), con = con)\n\n# Suppress this with window = FALSE\ntranslate_sql(mpg > mean(mpg), window = FALSE, con = con)\n\n# vars_group controls partition:\ntranslate_sql(mpg > mean(mpg), vars_group = \"cyl\", con = con)\n\n# and vars_order controls ordering for those functions that need it\ntranslate_sql(cumsum(mpg), con = con)\ntranslate_sql(cumsum(mpg), vars_order = \"mpg\", con = con)",
            "win_over": "con <- simulate_dbi()\n\nwin_over(sql(\"avg(x)\"), con = con)\nwin_over(sql(\"avg(x)\"), \"y\", con = con)\nwin_over(sql(\"avg(x)\"), order = \"y\", con = con)\nwin_over(sql(\"avg(x)\"), order = c(\"x\", \"y\"), con = con)\nwin_over(sql(\"avg(x)\"), frame = c(-Inf, 0), order = \"y\", con = con)",
            "window_order": "library(dplyr, warn.conflicts = FALSE)\n\ndb <- memdb_frame(g = rep(1:2, each = 5), y = runif(10), z = 1:10)\ndb \\%>\\%\n  window_order(y) \\%>\\%\n  mutate(z = cumsum(y)) \\%>\\%\n  show_query()\n\ndb \\%>\\%\n  group_by(g) \\%>\\%\n  window_frame(-3, 0) \\%>\\%\n  window_order(z) \\%>\\%\n  mutate(z = sum(y)) \\%>\\%\n  show_query()"
        }
    },
    "htmlwidgets": {
        "description": "A framework for creating HTML widgets that render in various\n    contexts including the R console, 'R Markdown' documents, and 'Shiny'\n    web applications.",
        "examples": {
            "JS": "library(htmlwidgets)\nJS('1 + 1')\nlist(x = JS('function(foo) {return foo;}'), y = 1:10)\nJS('function(x) {', 'return x + 1;', '}')",
            "htmlwidgets-shiny": "# shiny output binding for a widget named 'foo'\nfooOutput <- function(outputId, width = \"100\\%\", height = \"400px\") {\n  htmlwidgets::shinyWidgetOutput(outputId, \"foo\", width, height)\n}\n\n# shiny render function for a widget named 'foo'\nrenderFoo <- function(expr, env = parent.frame(), quoted = FALSE) {\n  if (!quoted) { expr <- substitute(expr) } # force quoted\n  htmlwidgets::shinyRenderWidget(expr, fooOutput, env, quoted = TRUE)\n}",
            "onRender": "\\dontrun{\nlibrary(leaflet)\n\n# This example uses browser geolocation. RStudio users:\n# this won't work in the Viewer pane; try popping it\n# out into your system web browser.\nleaflet() \\%>\\% addTiles() \\%>\\%\n  onRender(\"\n    function(el, x) {\n      // Navigate the map to the user's location\n      this.locate({setView: true});\n    }\n  \")\n\n\n# This example shows how you can make an R data frame available\n# to your JavaScript code.\n\nmeh <- \"&#x1F610;\";\nyikes <- \"&#x1F628;\";\n\ndf <- data.frame(\n  lng = quakes$long,\n  lat = quakes$lat,\n  html = ifelse(quakes$mag < 5.5, meh, yikes),\n  stringsAsFactors = FALSE\n)\n\nleaflet() \\%>\\% addTiles() \\%>\\%\n  fitBounds(min(df$lng), min(df$lat), max(df$lng), max(df$lat)) \\%>\\%\n  onRender(\"\n    function(el, x, data) {\n      for (var i = 0; i < data.lng.length; i++) {\n        var icon = L.divIcon({className: '', html: data.html[i]});\n        L.marker([data.lat[i], data.lng[i]], {icon: icon}).addTo(this);\n      }\n    }\n  \", data = df)\n}",
            "onStaticRenderComplete": "\\dontrun{\nlibrary(leaflet)\nlibrary(htmltools)\nlibrary(htmlwidgets)\n\npage <- tagList(\n  leaflet() \\%>\\% addTiles(),\n  onStaticRenderComplete(\n    \"HTMLWidgets.find('.leaflet').setZoom(4);\"\n  )\n)\nprint(page, browse = TRUE)\n}"
        }
    },
    "forcats": {
        "description": "Helpers for reordering factor levels (including moving\n    specified levels to front, ordering by first appearance, reversing,\n    and randomly shuffling), and tools for modifying factor levels\n    (including collapsing rare levels into other, 'anonymising', and\n    manually 'recoding').",
        "examples": {
            "as_factor": "# Character object\nx <- c(\"a\", \"z\", \"g\")\nas_factor(x)\nas.factor(x)\n\n# Character object containing numbers\ny <- c(\"1.1\", \"11\", \"2.2\", \"22\")\nas_factor(y)\nas.factor(y)\n\n# Numeric object\nz <- as.numeric(y)\nas_factor(z)\nas.factor(z)",
            "fct": "# Use factors when you know the set of possible values a variable might take\nx <- c(\"A\", \"O\", \"O\", \"AB\", \"A\")\nfct(x, levels = c(\"O\", \"A\", \"B\", \"AB\"))\n\n# If you don't specify the levels, fct will create from the data\n# in the order that they're seen\nfct(x)\n\n\n# Differences with base R -----------------------------------------------\n# factor() silently generates NAs\nx <- c(\"a\", \"b\", \"c\")\nfactor(x, levels = c(\"a\", \"b\"))\n# fct() errors\ntry(fct(x, levels = c(\"a\", \"b\")))\n# Unless you explicitly supply NA:\nfct(x, levels = c(\"a\", \"b\"), na = \"c\")\n\n# factor() sorts default levels:\nfactor(c(\"y\", \"x\"))\n# fct() uses in order of appearance:\nfct(c(\"y\", \"x\"))",
            "fct_anon": "gss_cat$relig \\%>\\% fct_count()\ngss_cat$relig \\%>\\%\n  fct_anon() \\%>\\%\n  fct_count()\ngss_cat$relig \\%>\\%\n  fct_anon(\"X\") \\%>\\%\n  fct_count()",
            "fct_c": "fa <- factor(\"a\")\nfb <- factor(\"b\")\nfab <- factor(c(\"a\", \"b\"))\n\nc(fa, fb, fab)\nfct_c(fa, fb, fab)\n\n# You can also pass a list of factors with !!!\nfs <- list(fa, fb, fab)\nfct_c(!!!fs)",
            "fct_collapse": "fct_count(gss_cat$partyid)\n\npartyid2 <- fct_collapse(gss_cat$partyid,\n  missing = c(\"No answer\", \"Don't know\"),\n  other = \"Other party\",\n  rep = c(\"Strong republican\", \"Not str republican\"),\n  ind = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n  dem = c(\"Not str democrat\", \"Strong democrat\")\n)\nfct_count(partyid2)",
            "fct_count": "f <- factor(sample(letters)[rpois(1000, 10)])\ntable(f)\nfct_count(f)\nfct_count(f, sort = TRUE)\nfct_count(f, sort = TRUE, prop = TRUE)",
            "fct_cross": "fruit <- factor(c(\"apple\", \"kiwi\", \"apple\", \"apple\"))\ncolour <- factor(c(\"green\", \"green\", \"red\", \"green\"))\neaten <- c(\"yes\", \"no\", \"yes\", \"no\")\nfct_cross(fruit, colour)\nfct_cross(fruit, colour, eaten)\nfct_cross(fruit, colour, keep_empty = TRUE)",
            "fct_drop": "f <- factor(c(\"a\", \"b\"), levels = c(\"a\", \"b\", \"c\"))\nf\nfct_drop(f)\n\n# Set only to restrict which levels to drop\nfct_drop(f, only = \"a\")\nfct_drop(f, only = \"c\")",
            "fct_expand": "f <- factor(sample(letters[1:3], 20, replace = TRUE))\nf\nfct_expand(f, \"d\", \"e\", \"f\")\nfct_expand(f, letters[1:6])\nfct_expand(f, \"Z\", after = 0)",
            "fct_explicit_na": "f1 <- factor(c(\"a\", \"a\", NA, NA, \"a\", \"b\", NA, \"c\", \"a\", \"c\", \"b\"))\nfct_count(f1)\ntable(f1)\nsum(is.na(f1))\n\n# previously\nf2 <- fct_explicit_na(f1)\n# now\nf2 <- fct_na_value_to_level(f1)\n\nfct_count(f2)\ntable(f2)\nsum(is.na(f2))",
            "fct_inorder": "f <- factor(c(\"b\", \"b\", \"a\", \"c\", \"c\", \"c\"))\nf\nfct_inorder(f)\nfct_infreq(f)\n\nf <- factor(1:3, levels = c(\"3\", \"2\", \"1\"))\nf\nfct_inseq(f)",
            "fct_lump": "x <- factor(rep(LETTERS[1:9], times = c(40, 10, 5, 27, 1, 1, 1, 1, 1)))\nx \\%>\\% table()\nx \\%>\\%\n  fct_lump_n(3) \\%>\\%\n  table()\nx \\%>\\%\n  fct_lump_prop(0.10) \\%>\\%\n  table()\nx \\%>\\%\n  fct_lump_min(5) \\%>\\%\n  table()\nx \\%>\\%\n  fct_lump_lowfreq() \\%>\\%\n  table()\n\nx <- factor(letters[rpois(100, 5)])\nx\ntable(x)\ntable(fct_lump_lowfreq(x))\n\n# Use positive values to collapse the rarest\nfct_lump_n(x, n = 3)\nfct_lump_prop(x, prop = 0.1)\n\n# Use negative values to collapse the most common\nfct_lump_n(x, n = -3)\nfct_lump_prop(x, prop = -0.1)\n\n# Use weighted frequencies\nw <- c(rep(2, 50), rep(1, 50))\nfct_lump_n(x, n = 5, w = w)\n\n# Use ties.method to control how tied factors are collapsed\nfct_lump_n(x, n = 6)\nfct_lump_n(x, n = 6, ties.method = \"max\")\n\n# Use fct_lump_min() to lump together all levels with fewer than `n` values\ntable(fct_lump_min(x, min = 10))\ntable(fct_lump_min(x, min = 15))",
            "fct_match": "table(fct_match(gss_cat$marital, c(\"Married\", \"Divorced\")))\n\n# Compare to \\%in\\%, misspelled levels throw an error\ntable(gss_cat$marital \\%in\\% c(\"Maried\", \"Davorced\"))\n\\dontrun{\ntable(fct_match(gss_cat$marital, c(\"Maried\", \"Davorced\")))\n}",
            "fct_na_value_to_level": "# Most factors store NAs in the values:\nf1 <- fct(c(\"a\", \"b\", NA, \"c\", \"b\", NA))\nlevels(f1)\nas.integer(f1)\nis.na(f1)\n\n# But it's also possible to store them in the levels\nf2 <- fct_na_value_to_level(f1)\nlevels(f2)\nas.integer(f2)\nis.na(f2)\n\n# If needed, you can convert back to NAs in the values:\nf3 <- fct_na_level_to_value(f2)\nlevels(f3)\nas.integer(f3)\nis.na(f3)",
            "fct_other": "x <- factor(rep(LETTERS[1:9], times = c(40, 10, 5, 27, 1, 1, 1, 1, 1)))\n\nfct_other(x, keep = c(\"A\", \"B\"))\nfct_other(x, drop = c(\"A\", \"B\"))",
            "fct_recode": "x <- factor(c(\"apple\", \"bear\", \"banana\", \"dear\"))\nfct_recode(x, fruit = \"apple\", fruit = \"banana\")\n\n# If you make a mistake you'll get a warning\nfct_recode(x, fruit = \"apple\", fruit = \"bananana\")\n\n# If you name the level NULL it will be removed\nfct_recode(x, NULL = \"apple\", fruit = \"banana\")\n\n# Wrap the left hand side in quotes if it contains special variables\nfct_recode(x, \"an apple\" = \"apple\", \"a bear\" = \"bear\")\n\n# When passing a named vector to rename levels use !!! to splice\nx <- factor(c(\"apple\", \"bear\", \"banana\", \"dear\"))\nlevels <- c(fruit = \"apple\", fruit = \"banana\")\nfct_recode(x, !!!levels)",
            "fct_relabel": "gss_cat$partyid \\%>\\% fct_count()\ngss_cat$partyid \\%>\\%\n  fct_relabel(~ gsub(\",\", \", \", .x)) \\%>\\%\n  fct_count()\n\nconvert_income <- function(x) {\n  regex <- \"^(?:Lt |)[$]([0-9]+).*$\"\n  is_range <- grepl(regex, x)\n  num_income <- as.numeric(gsub(regex, \"\\\\\\\\1\", x[is_range]))\n  num_income <- trunc(num_income / 5000) * 5000\n  x[is_range] <- paste0(\"Gt $\", num_income)\n  x\n}\nfct_count(gss_cat$rincome)\nconvert_income(levels(gss_cat$rincome))\nrincome2 <- fct_relabel(gss_cat$rincome, convert_income)\nfct_count(rincome2)",
            "fct_relevel": "f <- factor(c(\"a\", \"b\", \"c\", \"d\"), levels = c(\"b\", \"c\", \"d\", \"a\"))\nfct_relevel(f)\nfct_relevel(f, \"a\")\nfct_relevel(f, \"b\", \"a\")\n\n# Move to the third position\nfct_relevel(f, \"a\", after = 2)\n\n# Relevel to the end\nfct_relevel(f, \"a\", after = Inf)\nfct_relevel(f, \"a\", after = 3)\n\n# Relevel with a function\nfct_relevel(f, sort)\nfct_relevel(f, sample)\nfct_relevel(f, rev)\n\n# Using 'Inf' allows you to relevel to the end when the number\n# of levels is unknown or variable (e.g. vectorised operations)\ndf <- forcats::gss_cat[, c(\"rincome\", \"denom\")]\nlapply(df, levels)\n\ndf2 <- lapply(df, fct_relevel, \"Don't know\", after = Inf)\nlapply(df2, levels)\n\n# You'll get a warning if the levels don't exist\nfct_relevel(f, \"e\")",
            "fct_reorder": "# fct_reorder() -------------------------------------------------------------\n# Useful when a categorical variable is mapped to position\nboxplot(Sepal.Width ~ Species, data = iris)\nboxplot(Sepal.Width ~ fct_reorder(Species, Sepal.Width), data = iris)\n\n# or with\nlibrary(ggplot2)\nggplot(iris, aes(fct_reorder(Species, Sepal.Width), Sepal.Width)) +\n  geom_boxplot()\n\n# fct_reorder2() -------------------------------------------------------------\n# Useful when a categorical variable is mapped to color, size, shape etc\n\nchks <- subset(ChickWeight, as.integer(Chick) < 10)\nchks <- transform(chks, Chick = fct_shuffle(Chick))\n\n# Without reordering it's hard to match line to legend\nggplot(chks, aes(Time, weight, colour = Chick)) +\n  geom_point() +\n  geom_line()\n\n# With reordering it's much easier\nggplot(chks, aes(Time, weight, colour = fct_reorder2(Chick, Time, weight))) +\n  geom_point() +\n  geom_line() +\n  labs(colour = \"Chick\")",
            "fct_rev": "f <- factor(c(\"a\", \"b\", \"c\"))\nfct_rev(f)",
            "fct_shift": "x <- factor(\n  c(\"Mon\", \"Tue\", \"Wed\"),\n  levels = c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"),\n  ordered = TRUE\n)\nx\nfct_shift(x)\nfct_shift(x, 2)\nfct_shift(x, -1)",
            "fct_shuffle": "f <- factor(c(\"a\", \"b\", \"c\"))\nfct_shuffle(f)\nfct_shuffle(f)",
            "fct_unify": "fs <- list(factor(\"a\"), factor(\"b\"), factor(c(\"a\", \"b\")))\nfct_unify(fs)",
            "fct_unique": "f <- fct(letters[rpois(100, 10)])\nunique(f)     # in order of appearance\nfct_unique(f) # in order of levels\n\nf <- fct(letters[rpois(100, 2)], letters[1:20])\nunique(f)     # levels that appear in data\nfct_unique(f) # all possible levels",
            "gss_cat": "gss_cat\n\nfct_count(gss_cat$relig)\nfct_count(fct_lump(gss_cat$relig))",
            "lvls": "f <- factor(c(\"a\", \"b\", \"c\"))\nlvls_reorder(f, 3:1)\nlvls_revalue(f, c(\"apple\", \"banana\", \"carrot\"))\nlvls_expand(f, c(\"a\", \"b\", \"c\", \"d\"))",
            "lvls_union": "fs <- list(factor(\"a\"), factor(\"b\"), factor(c(\"a\", \"b\")))\nlvls_union(fs)"
        }
    },
    "clipr": {
        "description": "Simple utility functions to read from and write to\n    the Windows, OS X, and X11 clipboards.",
        "examples": {
            "clipr_available": "\\dontrun{\n# When using testthat:\nlibrary(testthat)\nskip_if_not(clipr_available())\n}",
            "read_clip": "\\dontrun{\nclip_text <- read_clip()\n}",
            "write_clip": "\\dontrun{\ntext <- \"Write to clipboard\"\nwrite_clip(text)\n\nmultiline <- c(\"Write\", \"to\", \"clipboard\")\nwrite_clip(multiline)\n# Write\n# to\n# clipboard\n\nwrite_clip(multiline, breaks = \",\")\n# write,to,clipboard\n\ntbl <- data.frame(a=c(1,2,3), b=c(4,5,6))\nwrite_clip(tbl)\n}"
        }
    },
    "memoise": {
        "description": "Cache the results of a function so that when you\n    call it again with the same arguments it returns the previously computed\n    value.",
        "examples": {
            "cache_filesystem": "\\dontrun{\n# Use with Dropbox\n\ndb <- cache_filesystem(\"~/Dropbox/.rcache\")\n\nmem_runif <- memoise(runif, cache = db)\n\n# Use with Google Drive\n\ngd <- cache_filesystem(\"~/Google Drive/.rcache\")\n\nmem_runif <- memoise(runif, cache = gd)\n\n}",
            "cache_gcs": "\\dontrun{\nlibrary(googleCloudStorageR)\n# Set GCS credentials.\nSys.setenv(\"GCS_AUTH_FILE\"=\"<google-service-json>\",\n           \"GCS_DEFAULT_BUCKET\"=\"unique-bucket-name\")\n\ngcs <- cache_gcs(\"unique-bucket-name\")\nmem_runif <- memoise(runif, cache = gcs)\n}",
            "cache_s3": "\\dontrun{\n# Set AWS credentials.\nSys.setenv(\"AWS_ACCESS_KEY_ID\" = \"<access key>\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"<access secret>\")\n\n# Set up a unique bucket name.\ns3 <- cache_s3(\"unique-bucket-name\")\nmem_runif <- memoise(runif, cache = s3)\n}",
            "drop_cache": "mem_sum <- memoise(sum)\nmem_sum(1, 2, 3)\nmem_sum(2, 3, 4)\nhas_cache(mem_sum)(1, 2, 3) # TRUE\nhas_cache(mem_sum)(2, 3, 4) # TRUE\ndrop_cache(mem_sum)(1, 2, 3) # TRUE\nhas_cache(mem_sum)(1, 2, 3) # FALSE\nhas_cache(mem_sum)(2, 3, 4) # TRUE",
            "forget": "memX <- memoise(function() { Sys.sleep(1); runif(1) })\n# The forget() function\nsystem.time(print(memX()))\nsystem.time(print(memX()))\nforget(memX)\nsystem.time(print(memX()))",
            "has_cache": "mem_sum <- memoise(sum)\nhas_cache(mem_sum)(1, 2, 3) # FALSE\nmem_sum(1, 2, 3)\nhas_cache(mem_sum)(1, 2, 3) # TRUE",
            "is.memoised": "mem_lm <- memoise(lm)\nis.memoised(lm) # FALSE\nis.memoised(mem_lm) # TRUE",
            "memoise": "# a() is evaluated anew each time. memA() is only re-evaluated\n# when you call it with a new set of parameters.\na <- function(n) { runif(n) }\nmemA <- memoise(a)\nreplicate(5, a(2))\nreplicate(5, memA(2))\n\n# Caching is done based on parameters' value, so same-name-but-\n# changed-value correctly produces two different outcomes...\nN <- 4; memA(N)\nN <- 5; memA(N)\n# ... and same-value-but-different-name correctly produces\n#     the same cached outcome.\nN <- 4; memA(N)\nN2 <- 4; memA(N2)\n\n# memoise() knows about default parameters.\nb <- function(n, dummy=\"a\") { runif(n) }\nmemB <- memoise(b)\nmemB(2)\nmemB(2, dummy=\"a\")\n# This works, because the interface of the memoised function is the same as\n# that of the original function.\nformals(b)\nformals(memB)\n# However, it doesn't know about parameter relevance.\n# Different call means different caching, no matter\n# that the outcome is the same.\nmemB(2, dummy=\"b\")\n\n# You can create multiple memoisations of the same function,\n# and they'll be independent.\nmemA(2)\nmemA2 <- memoise(a)\nmemA(2)  # Still the same outcome\nmemA2(2) # Different cache, different outcome\n\n# Multiple memoized functions can share a cache.\ncm <- cachem::cache_mem(max_size = 50 * 1024^2)\nmemA <- memoise(a, cache = cm)\nmemB <- memoise(b, cache = cm)\n\n# Don't do the same memoisation assignment twice: a brand-new\n# memoised function also means a brand-new cache, and *that*\n# you could as easily and more legibly achieve using forget().\n# (If you're not sure whether you already memoised something,\n#  use is.memoised() to check.)\nmemA(2)\nmemA <- memoise(a)\nmemA(2)\n\n# Make a memoized result automatically time out after 10 seconds.\nmemA3 <- memoise(a, cache = cachem::cache_mem(max_age = 10))\nmemA3(2)",
            "timeout": "a <- function(n) { runif(n) }\nmemA <- memoise(a, ~timeout(10))\nmemA(2)"
        }
    },
    "lme4": {
        "description": "Fit linear and generalized linear mixed-effects models.\n    The models and their components are represented using S4 classes and\n    methods.  The core computational algorithms are implemented using the\n    'Eigen' C++ library for numerical linear algebra and 'RcppEigen' \"glue\".",
        "examples": {
            "Arabidopsis": "data(Arabidopsis)\nsummary(Arabidopsis[,\"total.fruits\"])\ntable(gsub(\"[0-9].\",\"\",levels(Arabidopsis[,\"popu\"])))\nlibrary(lattice)\nstripplot(log(total.fruits+1) ~ amd|nutrient, data = Arabidopsis,\n          groups = gen,\n          strip=strip.custom(strip.names=c(TRUE,TRUE)),\n          type=c('p','a'), ## points and panel-average value --\n          ## see ?panel.xyplot\n          scales=list(x=list(rot=90)),\n          main=\"Panel: nutrient, Color: genotype\")",
            "Dyestuff": "\\dontshow{ # useful for the lme4-authors --- development, debugging, etc:\n commandArgs()[-1]\n if(FALSE) ## R environment variables:\n local({ ne <- names(e <- Sys.getenv())\n         list(R    = e[grep(\"^R\", ne)],\n              \"_R\" = e[grep(\"^_R\",ne)]) })\n Sys.getenv(\"R_ENVIRON\")\n Sys.getenv(\"R_PROFILE\")\n cat(\"R_LIBS:\\\\n\"); (RL <- strsplit(Sys.getenv(\"R_LIBS\"), \":\")[[1]])\n nRL <- normalizePath(RL)\n cat(\"and extra(:= not in R_LIBS) .libPaths():\\\\n\")\n .libPaths()[is.na(match(.libPaths(), nRL))]\n\n structure(Sys.info()[c(4,5,1:3)], class=\"simple.list\") #-> 'nodename' ..\n sessionInfo()\n searchpaths()\n pkgI <- function(pkgname) {\n   pd <- tryCatch(packageDescription(pkgname),\n                  error=function(e)e, warning=function(w)w)\n   if(inherits(pd, \"error\") || inherits(pd, \"warning\"))\n     cat(sprintf(\"packageDescription(\\\\\"\\%s\\\\\") \\%s: \\%s\\\\n\",\n                 pkgname, class(pd)[2], pd$message))\n   else\n     cat(sprintf(\"\\%s -- built: \\%s\\\\n\\%*s -- dir  : \\%s\\\\n\",\n                 pkgname, pd$Built, nchar(pkgname), \"\",\n                 dirname(dirname(attr(pd, \"file\")))))\n }\n pkgI(\"Matrix\")\n pkgI(\"Rcpp\")\n ## 2012-03-12{MM}: fails with --as-cran\n pkgI(\"RcppEigen\")\n pkgI(\"minqa\")\n pkgI(\"lme4\")\n}\nrequire(lattice)\nstr(Dyestuff)\ndotplot(reorder(Batch, Yield) ~ Yield, Dyestuff,\n        ylab = \"Batch\", jitter.y = TRUE, aspect = 0.3,\n        type = c(\"p\", \"a\"))\ndotplot(reorder(Batch, Yield) ~ Yield, Dyestuff2,\n        ylab = \"Batch\", jitter.y = TRUE, aspect = 0.3,\n        type = c(\"p\", \"a\"))\n(fm1 <- lmer(Yield ~ 1|Batch, Dyestuff))\n(fm2 <- lmer(Yield ~ 1|Batch, Dyestuff2))",
            "GHrule": "(r5  <- GHrule( 5, asMatrix=FALSE))\n(r12 <- GHrule(12, asMatrix=FALSE))\n\n## second, fourth, sixth, eighth and tenth central moments of the\n## standard Gaussian N(0,1) density:\nps <- seq(2, 10, by = 2)\ncbind(p = ps, \"E[X^p]\" = with(r5,  sapply(ps, function(p) sum(w * z^p)))) # p=10 is wrong for 5-rule\np <- 1:15\nGQ12 <- with(r12, sapply(p, function(p) sum(w * z^p)))\ncbind(p = p, \"E[X^p]\" = zapsmall(GQ12))\n## standard R numerical integration can do it too:\nintL <- lapply(p, function(p) integrate(function(x) x^p * dnorm(x),\n                                        -Inf, Inf, rel.tol=1e-11))\nintegR <- sapply(intL, `[[`, \"value\")\ncbind(p, \"E[X^p]\" = integR)# no zapsmall() needed here\nall.equal(GQ12, integR, tol=0)# => shows small difference\nstopifnot(all.equal(GQ12, integR, tol = 1e-10))\n(xactMom <- cumprod(seq(1,13, by=2)))\nstopifnot(all.equal(xactMom, GQ12[2*(1:7)], tol=1e-14))\n## mean relative errors :\nmean(abs(GQ12  [2*(1:7)] / xactMom - 1)) # 3.17e-16\nmean(abs(integR[2*(1:7)] / xactMom - 1)) # 9.52e-17 {even better}",
            "GQdk": "GQdk(2,5) # 53 x 3\n\nGQN[[3]][[5]] # a 14 x 4 matrix",
            "InstEval": "%% lots in ../tests/vcov-etc.R\nstr(InstEval)\n\nhead(InstEval, 16)\nxtabs(~ service + dept, InstEval)",
            "NelderMead-class": "showClass(\"NelderMead\")",
            "Nelder_Mead": "fr <- function(x) {   ## Rosenbrock Banana function\n    x1 <- x[1]\n    x2 <- x[2]\n    100 * (x2 - x1 * x1)^2 + (1 - x1)^2\n}\np0 <- c(-1.2, 1)\n\noo  <- optim(p0, fr) ## also uses Nelder-Mead by default\no.  <- Nelder_Mead(fr, p0)\no.1 <- Nelder_Mead(fr, p0, control=list(verbose=1))# -> some iteration output\nstopifnot(identical(o.[1:4], o.1[1:4]),\n          all.equal(o.$par, oo$par, tolerance=1e-3))# diff: 0.0003865\n\n%%## but this shows that something \"does not work\"\no.2 <- Nelder_Mead(fr, p0, control=list(verbose=3, XtolRel=1e-15, FtolAbs= 1e-14))\nall.equal(o.2[-5],o.1[-5], tolerance=1e-15)# TRUE, unexpectedly",
            "Pastes": "str(Pastes)\nrequire(lattice)\ndotplot(cask ~ strength | reorder(batch, strength), Pastes,\n        strip = FALSE, strip.left = TRUE, layout = c(1, 10),\n        ylab = \"Cask within batch\",\n        xlab = \"Paste strength\", jitter.y = TRUE)\n## Modifying the factors to enhance the plot\nPastes <- within(Pastes, batch <- reorder(batch, strength))\nPastes <- within(Pastes, sample <- reorder(reorder(sample, strength),\n          as.numeric(batch)))\ndotplot(sample ~ strength | batch, Pastes,\n        strip = FALSE, strip.left = TRUE, layout = c(1, 10),\n        scales = list(y = list(relation = \"free\")),\n        ylab = \"Sample within batch\",\n        xlab = \"Paste strength\", jitter.y = TRUE)\n## Four equivalent models differing only in specification\n(fm1 <- lmer(strength ~ (1|batch) + (1|sample), Pastes))\n(fm2 <- lmer(strength ~ (1|batch/cask), Pastes))\n(fm3 <- lmer(strength ~ (1|batch) + (1|batch:cask), Pastes))\n(fm4 <- lmer(strength ~ (1|batch/sample), Pastes))\n## fm4 results in redundant labels on the sample:batch interaction\nhead(ranef(fm4)[[1]])\n## compare to fm1\nhead(ranef(fm1)[[1]])\n## This model is different and NOT appropriate for these data\n(fm5 <- lmer(strength ~ (1|batch) + (1|cask), Pastes))\n\nL <- getME(fm1, \"L\")\nMatrix::image(L, sub = \"Structure of random effects interaction in pastes model\")",
            "Penicillin": "str(Penicillin)\nrequire(lattice)\ndotplot(reorder(plate, diameter) ~ diameter, Penicillin, groups = sample,\n        ylab = \"Plate\", xlab = \"Diameter of growth inhibition zone (mm)\",\n        type = c(\"p\", \"a\"), auto.key = list(columns = 3, lines = TRUE,\n        title = \"Penicillin sample\"))\n(fm1 <- lmer(diameter ~ (1|plate) + (1|sample), Penicillin))\n\nL <- getME(fm1, \"L\")\nMatrix::image(L, main = \"L\",\n              sub = \"Penicillin: Structure of random effects interaction\")",
            "VarCorr": "data(Orthodont, package=\"nlme\")\nfm1 <- lmer(distance ~ age + (age|Subject), data = Orthodont)\nprint(vc <- VarCorr(fm1))  ## default print method: standard dev and corr\n## both variance and std.dev.\nprint(vc,comp=c(\"Variance\",\"Std.Dev.\"), digits=2)\n## variance only\nprint(vc, comp=c(\"Variance\"))\n## standard deviations only, but covariances rather than correlations\nprint(vc, corr = FALSE)\nas.data.frame(vc)\nas.data.frame(vc, order=\"lower.tri\")",
            "VerbAgg": "str(VerbAgg)\n## Show how  r2 := h(resp) is defined:\nwith(VerbAgg, stopifnot( identical(r2, {\n     r <- factor(resp, ordered=FALSE); levels(r) <- c(\"N\",\"Y\",\"Y\"); r})))\n\nxtabs(~ item + resp, VerbAgg)\nxtabs(~ btype + resp, VerbAgg)\nround(100 * ftable(prop.table(xtabs(~ situ + mode + resp, VerbAgg), 1:2), 1))\nperson <- unique(subset(VerbAgg, select = c(id, Gender, Anger)))\nrequire(lattice)\ndensityplot(~ Anger, person, groups = Gender, auto.key = list(columns = 2),\n            xlab = \"Trait Anger score (STAXI)\")\n\nif(lme4:::testLevel() >= 3) { ## takes about 15 sec\n    print(fmVA <- glmer(r2 ~ (Anger + Gender + btype + situ)^2 +\n \t\t   (1|id) + (1|item), family = binomial, data =\n\t\t   VerbAgg), corr=FALSE)\n} ## testLevel() >= 3\nif (interactive()) {\n## much faster but less accurate\n    print(fmVA0 <- glmer(r2 ~ (Anger + Gender + btype + situ)^2 +\n                             (1|id) + (1|item), family = binomial,\n                         data = VerbAgg, nAGQ=0L), corr=FALSE)\n} ## interactive()",
            "allFit": "if (interactive()) {\nlibrary(lme4)\n  gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n             data = cbpp, family = binomial)\n  ## show available methods\n  allFit(show.meth.tab=TRUE) \n  gm_all <- allFit(gm1)\n  ss <- summary(gm_all)\n  ss$which.OK            ## logical vector: which optimizers worked?\n  ## the other components only contain values for the optimizers that worked\n  ss$llik                ## vector of log-likelihoods\n  ss$fixef               ## table of fixed effects\n  ss$sdcor               ## table of random effect SDs and correlations\n  ss$theta               ## table of random effects parameters, Cholesky scale\n} %% interactive()\n\\dontrun{\n  ## Parallel examples for Windows\n  nc <- detectCores()-1\n  optCls <- makeCluster(nc, type = \"SOCK\")\n  clusterEvalQ(optCls,library(\"lme4\"))\n  ### not necessary here because using a built-in\n  ## data set, but in general you should clusterExport() your data\n  clusterExport(optCls, \"cbpp\")\n  system.time(af1 <- allFit(m0, parallel = 'snow', \n                          ncpus = nc, cl=optCls))\n  stopCluster(optCls)\n} %% dontrun",
            "bootMer": "if (interactive()) {\nfm01ML <- lmer(Yield ~ 1|Batch, Dyestuff, REML = FALSE)\n## see ?\"profile-methods\"\nmySumm <- function(.) { s <- sigma(.)\n    c(beta =getME(., \"beta\"), sigma = s, sig01 = unname(s * getME(., \"theta\"))) }\n(t0 <- mySumm(fm01ML)) # just three parameters\n## alternatively:\nmySumm2 <- function(.) {\n    c(beta=fixef(.),sigma=sigma(.), sig01=sqrt(unlist(VarCorr(.))))\n}\n\nset.seed(101)\n## 3.8s (on a 5600 MIPS 64bit fast(year 2009) desktop \"AMD Phenom(tm) II X4 925\"):\nsystem.time( boo01 <- bootMer(fm01ML, mySumm, nsim = 100) )\n\n## to \"look\" at it\nif (requireNamespace(\"boot\")) {\n    boo01\n    ## note large estimated bias for sig01\n    ## (~30\\% low, decreases _slightly_ for nsim = 1000)\n\n    ## extract the bootstrapped values as a data frame ...\n    head(as.data.frame(boo01))\n\n    ## ------ Bootstrap-based confidence intervals ------------\n\n    ## warnings about \"Some ... intervals may be unstable\" go away\n    ##   for larger bootstrap samples, e.g. nsim=500\n\n    ## intercept\n    (bCI.1 <- boot::boot.ci(boo01, index=1, type=c(\"norm\", \"basic\", \"perc\")))# beta\n\n    ## Residual standard deviation - original scale:\n    (bCI.2  <- boot::boot.ci(boo01, index=2, type=c(\"norm\", \"basic\", \"perc\")))\n    ## Residual SD - transform to log scale:\n    (bCI.2L <- boot::boot.ci(boo01, index=2, type=c(\"norm\", \"basic\", \"perc\"),\n                       h = log, hdot = function(.) 1/., hinv = exp))\n\n    ## Among-batch variance:\n    (bCI.3 <- boot::boot.ci(boo01, index=3, type=c(\"norm\", \"basic\", \"perc\"))) # sig01\n\n    \n    confint(boo01)\n    confint(boo01,type=\"norm\")\n    confint(boo01,type=\"basic\")\n\n    ## Graphical examination:\n    plot(boo01,index=3)\n\n    ## Check stored values from a longer (1000-replicate) run:\n    (load(system.file(\"testdata\",\"boo01L.RData\", package=\"lme4\")))# \"boo01L\"\n    plot(boo01L, index=3)\n    mean(boo01L$t[,\"sig01\"]==0) ## note point mass at zero!\n} %% if boot package available\n} %% interactive",
            "cake": "str(cake)\n## 'temp' is continuous, 'temperature' an ordered factor with 6 levels\n\n(fm1 <- lmer(angle ~ recipe * temperature + (1|recipe:replicate), cake, REML= FALSE))\n(fm2 <- lmer(angle ~ recipe + temperature + (1|recipe:replicate), cake, REML= FALSE))\n(fm3 <- lmer(angle ~ recipe + temp        + (1|recipe:replicate), cake, REML= FALSE))\n\n## and now \"choose\" :\nanova(fm3, fm2, fm1)",
            "cbpp": "## response as a matrix\n(m1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n             family = binomial, data = cbpp))\n## response as a vector of probabilities and usage of argument \"weights\"\nm1p <- glmer(incidence / size ~ period + (1 | herd), weights = size,\n             family = binomial, data = cbpp)\n## Confirm that these are equivalent:\nstopifnot(all.equal(fixef(m1), fixef(m1p), tolerance = 1e-5),\n          all.equal(ranef(m1), ranef(m1p), tolerance = 1e-5))\n%% more extensive variations of the above --> ../tests/glmer-1.R\n\n## GLMM with individual-level variability (accounting for overdispersion)\ncbpp$obs <- 1:nrow(cbpp)\n(m2 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd) +  (1|obs),\n              family = binomial, data = cbpp))",
            "confint.merMod": "if (interactive() || lme4_testlevel() >= 3) {\nfm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)\nfm1W <- confint(fm1, method=\"Wald\")# very fast, but not useful for \"sigmas\" = var-cov pars\nfm1W\n(fm2 <- lmer(Reaction ~ Days + (Days || Subject), sleepstudy))\n(CI2 <- confint(fm2, maxpts = 8)) # method = \"profile\"; 8: to be much faster\n\\dontshow{ stopifnot(all.equal(tolerance = 5e-6, signif(unname(CI2), 7),\n               array(c(15.25847, 3.964157, 22.88062, 237.5732,  7.33431,\n                       37.78184, 8.768238, 28.78768, 265.2383, 13.60057),\n                     dim = c(5L, 2L))))\n}\nif (lme4_testlevel() >= 3) {\n  system.time(fm1P <- confint(fm1, method=\"profile\", ## <- default\n                              oldNames = FALSE))\n  ## --> ~ 2.2 seconds (2022)\n  set.seed(123) # (reproducibility when using bootstrap)\n  system.time(fm1B <- confint(fm1, method=\"boot\", oldNames=FALSE,\n                              .progress=\"txt\", PBargs= list(style=3)))\n  ## --> ~ 6.2 seconds (2022) and warning, messages\n} else {\n    load(system.file(\"testdata\",\"confint_ex.rda\",package=\"lme4\"))\n}\nfm1P\nfm1B\n} ## if interactive && testlevel>=3",
            "convergence": "if (interactive()) {\nfm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\n\n## 1. decrease stopping tolerances\nstrict_tol <- lmerControl(optCtrl=list(xtol_abs=1e-8, ftol_abs=1e-8))\nif (all(fm1@optinfo$optimizer==\"nloptwrap\")) {\n    fm1.tol <- update(fm1, control=strict_tol)\n}\n\n## 2. center and scale predictors:\nss.CS <- transform(sleepstudy, Days=scale(Days))\nfm1.CS <- update(fm1, data=ss.CS)\n\n## 3. recompute gradient and Hessian with Richardson extrapolation\ndevfun <- update(fm1, devFunOnly=TRUE)\nif (isLMM(fm1)) {\n    pars <- getME(fm1,\"theta\")\n} else {\n    ## GLMM: requires both random and fixed parameters\n    pars <- getME(fm1, c(\"theta\",\"fixef\"))\n}\nif (require(\"numDeriv\")) {\n    cat(\"hess:\\n\"); print(hess <- hessian(devfun, unlist(pars)))\n    cat(\"grad:\\n\"); print(grad <- grad(devfun, unlist(pars)))\n    cat(\"scaled gradient:\\n\")\n    print(scgrad <- solve(chol(hess), grad))\n}\n## compare with internal calculations:\nfm1@optinfo$derivs\n\n## compute reciprocal condition number of Hessian\nH <- fm1@optinfo$derivs$Hessian\nMatrix::rcond(H)\n\n## 4. restart the fit from the original value (or\n## a slightly perturbed value):\nfm1.restart <- update(fm1, start=pars)\nset.seed(101)\npars_x <- runif(length(pars),pars/1.01,pars*1.01)\nfm1.restart2 <- update(fm1, start=pars_x,\n                       control=strict_tol)\n\n## 5. try all available optimizers\n\n  fm1.all <- allFit(fm1)\n  ss <- summary(fm1.all)\n  ss$ fixef               ## fixed effects\n  ss$ llik                ## log-likelihoods\n  ss$ sdcor               ## SDs and correlations\n  ss$ theta               ## Cholesky factors\n  ss$ which.OK            ## which fits worked\n\n} %% interactive()",
            "devfun2": "m1 <- lmer(Reaction~Days+(Days|Subject),sleepstudy)\ndd <- devfun2(m1, useSc=TRUE)\npp <- attr(dd,\"optimum\")\n## extract variance-covariance and residual std dev parameters\nsigpars <- pp[grepl(\"^\\\\\\\\.sig\",names(pp))]\nall.equal(unname(dd(sigpars)),deviance(refitML(m1)))",
            "drop1.merMod": "fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)\n## likelihood ratio tests\ndrop1(fm1,test=\"Chisq\")\n## use Kenward-Roger corrected F test, or parametric bootstrap,\n## to test the significance of each dropped predictor\nif (require(pbkrtest) && packageVersion(\"pbkrtest\")>=\"0.3.8\") {\n   KRSumFun <- function(object, objectDrop, ...) {\n      krnames <- c(\"ndf\",\"ddf\",\"Fstat\",\"p.value\",\"F.scaling\")\n      r <- if (missing(objectDrop)) {\n          setNames(rep(NA,length(krnames)),krnames)\n      } else {\n         krtest <- KRmodcomp(object,objectDrop)\n         unlist(krtest$stats[krnames])\n      }\n      attr(r,\"method\") <- c(\"Kenward-Roger via pbkrtest package\")\n      r\n   }\n   drop1(fm1, test=\"user\", sumFun=KRSumFun)\n\n   if(lme4:::testLevel() >= 3) { ## takes about 16 sec\n     nsim <- 100\n     PBSumFun <- function(object, objectDrop, ...) {\n\tpbnames <- c(\"stat\",\"p.value\")\n\tr <- if (missing(objectDrop)) {\n\t    setNames(rep(NA,length(pbnames)),pbnames)\n\t} else {\n\t   pbtest <- PBmodcomp(object,objectDrop,nsim=nsim)\n\t   unlist(pbtest$test[2,pbnames])\n\t}\n\tattr(r,\"method\") <- c(\"Parametric bootstrap via pbkrtest package\")\n\tr\n     }\n     system.time(drop1(fm1, test=\"user\", sumFun=PBSumFun))\n   }\n}\n## workaround for creating a formula in a separate environment\ncreateFormula <- function(resp, fixed, rand) {  \n    f <- reformulate(c(fixed,rand),response=resp)\n    ## use the parent (createModel) environment, not the\n    ## environment of this function (which does not contain 'data')\n    environment(f) <- parent.frame()\n    f\n}\ncreateModel <- function(data) {\n    mf.final <- createFormula(\"Reaction\", \"Days\", \"(Days|Subject)\")\n    lmer(mf.final, data=data)\n}\ndrop1(createModel(data=sleepstudy))",
            "dummy": "data(Orthodont,package=\"nlme\")\nlmer(distance ~ age + (age|Subject) +\n     (0+dummy(Sex, \"Female\")|Subject), data = Orthodont)",
            "expandDoubleVerts": "m <- ~ x + (x || g)\n  expandDoubleVerts(m)\n  set.seed(101)\n  dd <- expand.grid(f=factor(letters[1:3]),g=factor(1:200),rep=1:3)\n  dd$y <- simulate(~f + (1|g) + (0+dummy(f,\"b\")|g) + (0+dummy(f,\"c\")|g),\n          newdata=dd,\n          newparams=list(beta=rep(0,3),\n                         theta=c(1,2,1),\n                         sigma=1),\n          family=gaussian)[[1]]\n  m1 <- lmer(y~f+(f|g),data=dd)\n  VarCorr(m1)\n  m2 <- lmer(y~f+(1|g) + (0+dummy(f,\"b\")|g) + (0+dummy(f,\"c\")|g),\n               data=dd)\n  VarCorr(m2)",
            "findbars": "findbars(f1 <- Reaction ~ Days + (Days | Subject))\n## => list( Days | Subject )\n## These two are equivalent:% tests in ../inst/tests/test-doubleVertNotation.R\nfindbars(y ~ Days + (1 | Subject) + (0 + Days | Subject))\nfindbars(y ~ Days + (Days || Subject))\n## => list of length 2:  list ( 1 | Subject ,  0 + Days | Subject)\nfindbars(~ 1 + (1 | batch / cask))\n## => list of length 2:  list ( 1 | cask:batch ,  1 | batch)\n\\dontshow{\nstopifnot(identical(findbars(f1),\n                    list(quote(Days | Subject))))\n}",
            "fixef": "fixef(lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy))\nfm2 <- lmer(Reaction ~ Days + Days2 + (1|Subject),\n            data=transform(sleepstudy,Days2=Days))\nfixef(fm2,add.dropped=TRUE)\n## first two parameters are the same ...\nstopifnot(all.equal(fixef(fm2,add.dropped=TRUE)[1:2],\n                    fixef(fm2)))",
            "fortify": "fm1 <- lmer(Reaction~Days+(1|Subject),sleepstudy)\n  names(fortify.merMod(fm1))",
            "getME": "## shows many methods you should consider *before* using getME():\nmethods(class = \"merMod\")\n\n(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))\nZ <- getME(fm1, \"Z\")\nstopifnot(is(Z, \"CsparseMatrix\"),\n          c(180,36) == dim(Z),\n\t  all.equal(fixef(fm1), b1 <- getME(fm1, \"beta\"),\n\t\t    check.attributes=FALSE, tolerance = 0))\n\n## A way to get *all* getME()s :\n## internal consistency check ensuring that all work:\nparts <- getME(fm1, \"ALL\")\nstr(parts, max=2)\nstopifnot(identical(Z,  parts $ Z),\n          identical(b1, parts $ beta))",
            "glmFamily-class": "str(glmFamily$new(family=poisson()))",
            "glmer": "## generalized linear mixed model\nlibrary(lattice)\nxyplot(incidence/size ~ period|herd, cbpp, type=c('g','p','l'),\n       layout=c(3,5), index.cond = function(x,y)max(y))\n(gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n              data = cbpp, family = binomial))\n## using nAGQ=0 only gets close to the optimum\n(gm1a <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n               cbpp, binomial, nAGQ = 0))\n## using  nAGQ = 9  provides a better evaluation of the deviance\n## Currently the internal calculations use the sum of deviance residuals,\n## which is not directly comparable with the nAGQ=0 or nAGQ=1 result.\n## 'verbose = 1' monitors iteratin a bit; (verbose = 2 does more):\n(gm1a <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n               cbpp, binomial, verbose = 1, nAGQ = 9))\n\n## GLMM with individual-level variability (accounting for overdispersion)\n## For this data set the model is the same as one allowing for a period:herd\n## interaction, which the plot indicates could be needed.\ncbpp$obs <- 1:nrow(cbpp)\n(gm2 <- glmer(cbind(incidence, size - incidence) ~ period +\n    (1 | herd) +  (1|obs),\n              family = binomial, data = cbpp))\nanova(gm1,gm2)\n\n## glmer and glm log-likelihoods are consistent\ngm1Devfun <- update(gm1,devFunOnly=TRUE)\ngm0 <- glm(cbind(incidence, size - incidence) ~ period,\n           family = binomial, data = cbpp)\n## evaluate GLMM deviance at RE variance=theta=0, beta=(GLM coeffs)\ngm1Dev0 <- gm1Devfun(c(0,coef(gm0)))\n## compare\nstopifnot(all.equal(gm1Dev0,c(-2*logLik(gm0))))\n## the toenail oncholysis data from Backer et al 1998\n## these data are notoriously difficult to fit\n\\dontrun{\nif (require(\"HSAUR3\")) {\n    gm2 <- glmer(outcome~treatment*visit+(1|patientID),\n                 data=toenail,\n                 family=binomial,nAGQ=20)\n}\n}",
            "glmer.nb": "set.seed(101)\ndd <- expand.grid(f1 = factor(1:3),\n                  f2 = LETTERS[1:2], g=1:9, rep=1:15,\n          KEEP.OUT.ATTRS=FALSE)\nsummary(mu <- 5*(-4 + with(dd, as.integer(f1) + 4*as.numeric(f2))))\ndd$y <- rnbinom(nrow(dd), mu = mu, size = 0.5)\nstr(dd)\nrequire(\"MASS\")## and use its glm.nb() - as indeed we have zero random effect:\n\\dontrun{\nm.glm <- glm.nb(y ~ f1*f2, data=dd, trace=TRUE)\nsummary(m.glm)\nm.nb <- glmer.nb(y ~ f1*f2 + (1|g), data=dd, verbose=TRUE)\nm.nb\n## The neg.binomial theta parameter:\ngetME(m.nb, \"glmer.nb.theta\")\nLL <- logLik(m.nb)\n## mixed model has 1 additional parameter (RE variance)\nstopifnot(attr(LL,\"df\")==attr(logLik(m.glm),\"df\")+1)\nplot(m.nb, resid(.) ~ g)# works, as long as data 'dd' is found\n}",
            "golden-class": "showClass(\"golden\")\n\ngolden(lower= -100, upper= 1e100)",
            "grouseticks": "if (interactive()) {\ndata(grouseticks)\n## Figure 1a from Elston et al\npar(las=1,bty=\"l\")\ntvec <- c(0,1,2,5,20,40,80)\npvec <- c(4,1,3)\nwith(grouseticks_agg,plot(1+meanTICKS~HEIGHT,\n                  pch=pvec[factor(YEAR)],\n                  log=\"y\",axes=FALSE,\n                  xlab=\"Altitude (m)\",\n                  ylab=\"Brood mean ticks\"))\naxis(side=1)\naxis(side=2,at=tvec+1,label=tvec)\nbox()\nabline(v=405,lty=2)\n## Figure 1b\nwith(grouseticks_agg,plot(varTICKS~meanTICKS,\n                  pch=4,\n                  xlab=\"Brood mean ticks\",\n                  ylab=\"Within-brood variance\"))\ncurve(1*x,from=0,to=70,add=TRUE)\n## Model fitting\nform <- TICKS~YEAR+HEIGHT+(1|BROOD)+(1|INDEX)+(1|LOCATION)\n(full_mod1  <- glmer(form, family=\"poisson\",data=grouseticks))\n}",
            "hatvalues.merMod": "m <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\nhatvalues(m)",
            "influence.merMod": "if (interactive()) {\n  fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\n  inf_fm1 <- influence(fm1, \"Subject\")\n  if (require(\"car\")) {\n    infIndexPlot(inf_fm1)\n  }\n  dfbeta(inf_fm1)\n  dfbetas(inf_fm1)\n  gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n               data = cbpp, family = binomial)\n  inf_gm1 <- influence(gm1, \"herd\", maxfun=100)\n  gm1.11 <- update(gm1, subset = herd != 11) # check deleting herd 11\n  if (require(\"car\")) {\n    infIndexPlot(inf_gm1)\n    compareCoefs(gm1, gm1.11)\n  }\n  if(packageVersion(\"car\") >= \"3.0.10\") {\n    dfbeta(inf_gm1)\n    dfbetas(inf_gm1)\n  }\n } %% interactive()",
            "isNested": "with(Pastes, isNested(cask, batch))   ## => FALSE\nwith(Pastes, isNested(sample, batch))  ## => TRUE",
            "isREML": "fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)\ngm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n              data = cbpp, family = binomial)\nnm1 <- nlmer(circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym|Tree,\n             Orange, start = c(Asym = 200, xmid = 725, scal = 350))\n\nisLMM(fm1)\nisGLMM(gm1)\n## check all :\nis.MM <- function(x) c(LMM = isLMM(x), GLMM= isGLMM(x), NLMM= isNLMM(x))\nstopifnot(cbind(is.MM(fm1), is.MM(gm1), is.MM(nm1))\n\t  == diag(rep(TRUE,3)))",
            "lmList": "fm.plm  <- lmList(Reaction ~ Days | Subject, sleepstudy)\ncoef(fm.plm)\nfm.2  <- update(fm.plm, pool = FALSE)\n## coefficients are the same, \"pooled or unpooled\":\nstopifnot( all.equal(coef(fm.2), coef(fm.plm)) )\n\n(ci <- confint(fm.plm)) # print and rather *see* :\nplot(ci)                # how widely they vary for the individuals",
            "lmList4-class": "if(getRversion() >= \"3.2.0\") {\n  (mm <- methods(class = \"lmList4\"))\n  ## The S3 (\"not S4\") ones :\n  mm[!attr(mm,\"info\")[,\"isS4\"]]\n}\n## For more examples:  example(lmList)  i.e., ?lmList",
            "lmResp-class": "showClass(\"lmResp\")\nstr(lmResp$new(y=1:4))\nshowClass(\"glmResp\")\nstr(glmResp$new(family=poisson(), y=1:4))\nshowClass(\"nlsResp\")\nshowClass(\"lmerResp\")\nstr(lmerResp$new(y=1:4))",
            "lmer": "## linear mixed models - reference values from older code\n(fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy))\nsummary(fm1)# (with its own print method; see class?merMod % ./merMod-class.Rd\n\nstr(terms(fm1))\nstopifnot(identical(terms(fm1, fixed.only=FALSE),\n                    terms(model.frame(fm1))))\nattr(terms(fm1, FALSE), \"dataClasses\") # fixed.only=FALSE needed for dataCl.\n\n## Maximum Likelihood (ML), and \"monitor\" iterations via 'verbose':\nfm1_ML <- update(fm1, REML=FALSE, verbose = 1)\n(fm2 <- lmer(Reaction ~ Days + (Days || Subject), sleepstudy))\nanova(fm1, fm2)\nsm2 <- summary(fm2)\nprint(fm2, digits=7, ranef.comp=\"Var\") # the print.merMod()         method\nprint(sm2, digits=3, corr=FALSE)       # the print.summary.merMod() method\n\n## Fit sex-specific variances by constructing numeric dummy variables\n## for sex and sex:age; in this case the estimated variance differences\n## between groups in both intercept and slope are zero ...\ndata(Orthodont,package=\"nlme\")\nOrthodont$nsex <- as.numeric(Orthodont$Sex==\"Male\")\nOrthodont$nsexage <- with(Orthodont, nsex*age)\nlmer(distance ~ age + (age|Subject) + (0+nsex|Subject) +\n     (0 + nsexage|Subject), data=Orthodont)",
            "lmerControl": "str(lmerControl())\nstr(glmerControl())\n## fit with default algorithm [nloptr version of BOBYQA] ...\nfm0 <- lmer(Reaction ~ Days +   ( 1 | Subject), sleepstudy)\nfm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\n## or with \"bobyqa\" (default 2013 - 2019-02) ...\nfm1_bobyqa <- update(fm1, control = lmerControl(optimizer=\"bobyqa\"))\n## or with \"Nelder_Mead\" (the default till 2013) ...\nfm1_NMead <- update(fm1, control = lmerControl(optimizer=\"Nelder_Mead\"))\n## or with the nlminb function used in older (<1.0) versions of lme4;\n## this will usually replicate older results\nif (require(optimx)) {\n    fm1_nlminb <- update(fm1,\n                         control = lmerControl(optimizer= \"optimx\",\n                                               optCtrl  = list(method=\"nlminb\")))\n    ## The other option here is method=\"L-BFGS-B\".\n}\n\n## Or we can wrap base::optim():\noptimwrap <- function(fn,par,lower,upper,control=list(),\n                      ...) {\n    if (is.null(control$method)) stop(\"must specify method in optCtrl\")\n    method <- control$method\n    control$method <- NULL\n    ## \"Brent\" requires finite upper values (lower bound will always\n    ##  be zero in this case)\n    if (method==\"Brent\") upper <- pmin(1e4,upper)\n    res <- optim(par=par, fn=fn, lower=lower,upper=upper,\n                 control=control,method=method,...)\n    with(res, list(par  = par,\n                   fval = value,\n                   feval= counts[1],\n                   conv = convergence,\n                   message = message))\n}\nfm0_brent <- update(fm0,\n                    control = lmerControl(optimizer = \"optimwrap\",\n                                          optCtrl = list(method=\"Brent\")))\n\n## You can also use functions (in addition to the lmerControl() default \"NLOPT_BOBYQA\")\n## from the 'nloptr' package, see also  '?nloptwrap' :\nif (require(nloptr)) {\n    fm1_nloptr_NM <- update(fm1, control=lmerControl(optimizer=\"nloptwrap\",\n                                      optCtrl=list(algorithm=\"NLOPT_LN_NELDERMEAD\")))\n    fm1_nloptr_COBYLA <- update(fm1, control=lmerControl(optimizer=\"nloptwrap\",\n                                      optCtrl=list(algorithm=\"NLOPT_LN_COBYLA\",\n                                                   xtol_rel=1e-6,\n                                                   xtol_abs=1e-10,\n                                                   ftol_abs=1e-10)))\n}\n## other algorithm options include NLOPT_LN_SBPLX",
            "merMod-class": "showClass(\"merMod\")\nmethods(class=\"merMod\")## over 30  (S3) methods available\n\nm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\nprint(m1, ranef.corr = TRUE)   ## print correlations of REs\nprint(m1, ranef.corr = FALSE)  ## print covariances of REs",
            "merPredD-class": "showClass(\"merPredD\")\npp <- slot(lmer(Yield ~ 1|Batch, Dyestuff), \"pp\")\nstopifnot(is(pp, \"merPredD\"))\nstr(pp) # an overview of all fields and methods' names.",
            "mkMerMod": "%% ## 1) An \"empty\"   merMod object :\n%% ## 2) A  \"lmer()-like\" merMod object, using our \"modular\" approach instead of lmer()\n%%",
            "mkReTrms": "data(\"Pixel\", package=\"nlme\")\nmform <- pixel ~ day + I(day^2) + (day | Dog) + (1 | Side/Dog)\n(bar.f <- findbars(mform)) # list with 3 terms\nmf <- model.frame(subbars(mform),data=Pixel)\nrt <- mkReTrms(bar.f,mf)\nnames(rt)",
            "modular": "### Fitting a linear mixed model in 4 modularized steps\n\n## 1.  Parse the data and formula:\nlmod <- lFormula(Reaction ~ Days + (Days|Subject), sleepstudy)\nnames(lmod)\n## 2.  Create the deviance function to be optimized:\n(devfun <- do.call(mkLmerDevfun, lmod))\nls(environment(devfun)) # the environment of 'devfun' contains objects\n                        # required for its evaluation\n## 3.  Optimize the deviance function:\nopt <- optimizeLmer(devfun)\nopt[1:3]\n## 4.  Package up the results:\nmkMerMod(environment(devfun), opt, lmod$reTrms, fr = lmod$fr)\n\n\n### Same model in one line\nlmer(Reaction ~ Days + (Days|Subject), sleepstudy)\n\n\n### Fitting a generalized linear mixed model in six modularized steps\n\n## 1.  Parse the data and formula:\nglmod <- glFormula(cbind(incidence, size - incidence) ~ period + (1 | herd),\n                   data = cbpp, family = binomial)\n    #.... see what've got :\nstr(glmod, max=1, give.attr=FALSE)\n## 2.  Create the deviance function for optimizing over theta:\n(devfun <- do.call(mkGlmerDevfun, glmod))\nls(environment(devfun)) # the environment of devfun contains lots of info\n## 3.  Optimize over theta using a rough approximation (i.e. nAGQ = 0):\n(opt <- optimizeGlmer(devfun))\n## 4.  Update the deviance function for optimizing over theta and beta:\n(devfun <- updateGlmerDevfun(devfun, glmod$reTrms))\n## 5.  Optimize over theta and beta:\nopt <- optimizeGlmer(devfun, stage=2)\nstr(opt, max=1) # seeing what we'got\n## 6.  Package up the results:\n(fMod <- mkMerMod(environment(devfun), opt, glmod$reTrms, fr = glmod$fr))\n\n### Same model in one line\nfM <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n            data = cbpp, family = binomial)\nall.equal(fMod, fM, check.attributes=FALSE, tolerance = 1e-12)\n        # ----  --  even tolerance = 0  may work",
            "namedList": "a <- 1\nb <- 2\nc <- 3\nstr(namedList(a, b, d = c))",
            "ngrps": "ngrps(factor(seq(1,10,2)))\nngrps(lmer(Reaction ~ 1|Subject, sleepstudy))\n\n## A named vector if there's more than one grouping factor :\nngrps(lmer(strength ~ (1|batch/cask), Pastes))\n## cask:batch      batch\n##         30         10\n\nmethods(ngrps) # -> \"factor\" and \"merMod\"",
            "nlmer": "## nonlinear mixed models --- 3-part formulas ---\n## 1. basic nonlinear fit. Use stats::SSlogis for its\n## implementation of the 3-parameter logistic curve.\n## \"SS\" stands for \"self-starting logistic\", but the\n## \"self-starting\" part is not currently used by nlmer ... 'start' is\n## necessary\nstartvec <- c(Asym = 200, xmid = 725, scal = 350)\n(nm1 <- nlmer(circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym|Tree,\n             Orange, start = startvec))\n## 2. re-run with \"quick and dirty\" PIRLS step\n(nm1a <- update(nm1, nAGQ = 0L))\n\n## 3. Fit the same model with a user-built function:\n## a. Define formula\nnform <- ~Asym/(1+exp((xmid-input)/scal))\n## b. Use deriv() to construct function:\nnfun <- deriv(nform,namevec=c(\"Asym\",\"xmid\",\"scal\"),\n              function.arg=c(\"input\",\"Asym\",\"xmid\",\"scal\"))\nnm1b <- update(nm1,circumference ~ nfun(age, Asym, xmid, scal)  ~ Asym | Tree)\n\n## 4. User-built function without using deriv():\n##    derivatives could be computed more efficiently\n##    by pre-computing components, but these are essentially\n##    the gradients as one would derive them by hand\nnfun2 <- function(input, Asym, xmid, scal) {\n    value <- Asym/(1+exp((xmid-input)/scal))\n    grad <- cbind(Asym=1/(1+exp((xmid-input)/scal)),\n              xmid=-Asym/(1+exp((xmid-input)/scal))^2*1/scal*\n                    exp((xmid-input)/scal),\n              scal=-Asym/(1+exp((xmid-input)/scal))^2*\n                     -(xmid-input)/scal^2*exp((xmid-input)/scal))\n    attr(value,\"gradient\") <- grad\n    value\n}\nstopifnot(all.equal(attr(nfun(2,1,3,4),\"gradient\"),\n                    attr(nfun(2,1,3,4),\"gradient\")))\nnm1c <- update(nm1,circumference ~ nfun2(age, Asym, xmid, scal)  ~ Asym | Tree)",
            "nloptwrap": "library(lme4)\nls.str(environment(nloptwrap)) # 'defaultControl' algorithm \"NLOPT_LN_BOBYQA\"\n## Note that  'optimizer =  \"nloptwrap\"' is now the default for lmer() :\nfm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)\n## tighten tolerances\nfm1B <- update(fm1, control= lmerControl(optCtrl = list(xtol_abs=1e-8, ftol_abs=1e-8)))\n## run for longer (no effect in this case)\nfm1C <- update(fm1,control = lmerControl(optCtrl = list(maxeval=10000)))\n\n  logLik(fm1B) - logLik(fm1)  ## small difference in log likelihood\nc(logLik(fm1C) - logLik(fm1)) ## no difference in LL\n## Nelder-Mead\nfm1_nloptr_NM <- update(fm1, control=\n                  lmerControl(optimizer = \"nloptwrap\",\n                              optCtrl = list(algorithm = \"NLOPT_LN_NELDERMEAD\")))\n## other nlOpt algorithm options include NLOPT_LN_COBYLA, NLOPT_LN_SBPLX, see\nif(interactive())% (the package *is* installed w/ 'lme4')\n  nloptr::nloptr.print.options()\n\nfm1_nlminb <- update(fm1, control=lmerControl(optimizer = \"nlminbwrap\"))\nif (require(optimx)) { ## the 'optimx'-based nlminb :\n  fm1_nlminb2 <- update(fm1, control=\n                lmerControl(optimizer = \"optimx\",\n                            optCtrl = list(method=\"nlminb\", kkt=FALSE)))\n  cat(\"Likelihood difference (typically zero):  \",\n      c(logLik(fm1_nlminb) - logLik(fm1_nlminb2)), \"\\n\")\n}",
            "nobars": "nobars(Reaction ~ Days + (Days|Subject)) ## => Reaction ~ Days",
            "plot.merMod": "data(Orthodont,package=\"nlme\")\nfm1 <- lmer(distance ~ age + (age|Subject), data=Orthodont)\n## standardized residuals versus fitted values by gender\nplot(fm1, resid(., scaled=TRUE) ~ fitted(.) | Sex, abline = 0)\n## box-plots of residuals by Subject\nplot(fm1, Subject ~ resid(., scaled=TRUE))\n## observed versus fitted values by Subject\nplot(fm1, distance ~ fitted(.) | Subject, abline = c(0,1))\n## residuals by age, separated by Subject\nplot(fm1, resid(., scaled=TRUE) ~ age | Sex, abline = 0)\n## scale-location plot, with red smoothed line\nscale_loc_plot <- function(m, line.col = \"red\", line.lty = 1,\n                           line.lwd = 2) {\n  plot(fm1, sqrt(abs(resid(.))) ~ fitted(.),\n       type = c(\"p\", \"smooth\"),\n       par.settings = list(plot.line =\n                             list(alpha=1, col = line.col,\n                                  lty = line.lty, lwd = line.lwd)))\n}\nscale_loc_plot(fm1)\n## Q-Q plot\nlattice::qqmath(fm1, id=0.05)\nggp.there <- \"package:ggplot2\" \\%in\\% search()\nif (ggp.there || require(\"ggplot2\")) {\n    ## we can create the same plots using ggplot2 and the fortify() function\n    fm1F <- fortify.merMod(fm1)\n    ggplot(fm1F, aes(.fitted, .resid)) + geom_point(colour=\"blue\") +\n           facet_grid(. ~ Sex) + geom_hline(yintercept=0)\n    ## note: Subjects are ordered by mean distance\n    ggplot(fm1F, aes(Subject,.resid)) + geom_boxplot() + coord_flip()\n    ggplot(fm1F, aes(.fitted,distance)) + geom_point(colour=\"blue\") +\n        facet_wrap(~Subject) +geom_abline(intercept=0,slope=1)\n    ggplot(fm1F, aes(age,.resid)) + geom_point(colour=\"blue\") + facet_grid(.~Sex) +\n        geom_hline(yintercept=0)+ geom_line(aes(group=Subject),alpha=0.4) +\n        geom_smooth(method=\"loess\")\n    ## (warnings about loess are due to having only 4 unique x values)\n    if(!ggp.there) detach(\"package:ggplot2\")\n}",
            "plots.thpr": "## see   example(\"profile.merMod\")",
            "predict.merMod": "(gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 |herd), cbpp, binomial))\nstr(p0 <- predict(gm1))            # fitted values\nstr(p1 <- predict(gm1,re.form=NA))  # fitted values, unconditional (level-0)\nnewdata <- with(cbpp, expand.grid(period=unique(period), herd=unique(herd)))\nstr(p2 <- predict(gm1,newdata))    # new data, all RE\nstr(p3 <- predict(gm1,newdata,re.form=NA)) # new data, level-0\nstr(p4 <- predict(gm1,newdata,re.form= ~(1|herd))) # explicitly specify RE\nstopifnot(identical(p2, p4))\n\\dontshow{\n\n## predict() should work with variable names with spaces [as lm() does]:\ndd <- expand.grid(y=1:3, \"Animal ID\" = 1:9)\nfm <- lmer(y ~ 1 + (1 | `Animal ID`),  dd)\nsummary(fm)\nisel <- c(7, 9, 11, 13:17, 20:22)\nstopifnot(all.equal(vcov(fm)[1,1], 0.02564102564),\n\t  all.equal(unname(predict(fm, newdata = dd[isel,])),\n\t\t    unname( fitted(fm) [isel])))\n} % dontshow",
            "profile-methods": "fm01ML <- lmer(Yield ~ 1|Batch, Dyestuff, REML = FALSE)\nsystem.time(\n  tpr  <- profile(fm01ML, optimizer=\"Nelder_Mead\", which=\"beta_\")\n)## fast; as only *one* beta parameter is profiled over -> 0.09s (2022)\n\n## full profiling (default which means 'all') needs longer:\nsystem.time( tpr  <- profile(fm01ML, signames=FALSE))\n## ~ 0.26s (2022) + possible warning about convergence\n(confint(tpr) -> CIpr)\n\\donttest{# too much precision (etc). but just FYI:\n trgt <- array(c(12.19854, 38.22998, 1486.451,\n                 84.06305, 67.6577,  1568.548), dim = 3:2)\n stopifnot(all.equal(trgt, unname(CIpr), tol = .0001)) # had 3.1e-7\n}\nif (interactive()) {\n library(\"lattice\")\n xyplot(tpr)\n xyplot(tpr, absVal=TRUE) # easier to see conf.int.s (and check symmetry)\n xyplot(tpr, conf = c(0.95, 0.99), # (instead of all five 50, 80,...)\n        main = \"95\\% and 99\\% profile() intervals\")\n xyplot(logProf(tpr, ranef=FALSE),\n        main = expression(\"lmer profile()s\"~~ log(sigma)*\" (only log)\"))\n densityplot(tpr, main=\"densityplot( profile(lmer(..)) )\")\n densityplot(varianceProf(tpr), main=\" varianceProf( profile(lmer(..)) )\")\n splom(tpr)\n splom(logProf(tpr, ranef=FALSE))\n doMore <- lme4:::testLevel() > 2 %% even more --> ../tests/profile.R\n if(doMore) { ## not typically, for time constraint reasons\n   ## Batch and residual variance only\n   system.time(tpr2 <- profile(fm01ML, which=1:2)) # , optimizer=\"Nelder_Mead\" gives warning\n   print( xyplot(tpr2) )\n   print( xyplot(log(tpr2)) )# log(sigma) is better\n   print( xyplot(logProf(tpr2, ranef=FALSE)) )\n\n   ## GLMM example\n   gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n               data = cbpp, family = binomial)\n   ## running ~ 10 seconds on a modern machine {-> \"verbose\" while you wait}:\n   print( system.time(pr4 <- profile(gm1, verbose=TRUE)) )\n   print( xyplot(pr4, layout=c(5,1), as.table=TRUE) )\n   print( xyplot(log(pr4), absVal=TRUE) ) # log(sigma_1)\n   print( splom(pr4) )\n   print( system.time( # quicker: only sig01 and one fixed effect\n       pr2 <- profile(gm1, which=c(\"theta_\", \"period2\"))))\n   print( confint(pr2) )\n   ## delta..: higher underlying resolution, only for 'sigma_1':\n   print( system.time(\n       pr4.hr <- profile(gm1, which=\"theta_\", delta.cutoff=1/16)))\n   print( xyplot(pr4.hr) )\n }\n} # only if interactive()",
            "ranef": "library(lattice) ## for dotplot, qqmath\nfm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)\nfm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy)\nfm3 <- lmer(diameter ~ (1|plate) + (1|sample), Penicillin)\nranef(fm1)\nstr(rr1 <- ranef(fm1))\ndotplot(rr1)  ## default\nqqmath(rr1)\n## specify free scales in order to make Day effects more visible\ndotplot(rr1,scales = list(x = list(relation = 'free')))[[\"Subject\"]]\n## plot options: ... can specify appearance of vertical lines with\n## lty.v, col.line.v, lwd.v, etc..\ndotplot(rr1, lty = 3, lty.v = 2, col.line.v = \"purple\",\n        col = \"red\", col.line.h = \"gray\")\nranef(fm2)\nop <- options(digits = 4)\nranef(fm3, drop = TRUE)\noptions(op)\n## as.data.frame() provides RE's and conditional standard deviations:\nstr(dd <- as.data.frame(rr1))\nif (require(ggplot2)) {\n    ggplot(dd, aes(y=grp,x=condval)) +\n        geom_point() + facet_wrap(~term,scales=\"free_x\") +\n        geom_errorbarh(aes(xmin=condval -2*condsd,\n                           xmax=condval +2*condsd), height=0)\n}",
            "rePCA": "fm1 <- lmer(Reaction~Days+(Days|Subject), sleepstudy)\n  rePCA(fm1)",
            "rePos-class": "showClass(\"rePos\")",
            "refit": "## Ex. 1: using refit() to fit each column in a matrix of responses -------\nset.seed(101)\nY <- matrix(rnorm(1000),ncol=10)\n## combine first column of responses with predictor variables\nd <- data.frame(y=Y[,1],x=rnorm(100),f=rep(1:10,10))\n## (use check.conv.grad=\"ignore\" to disable convergence checks because we\n##  are using a fake example)\n## fit first response\nfit1 <- lmer(y ~ x+(1|f), data = d,\n             control= lmerControl(check.conv.grad=\"ignore\",\n                                  check.conv.hess=\"ignore\"))\n## combine fit to first response with fits to remaining responses\nres <- c(fit1, lapply(as.data.frame(Y[,-1]), refit, object=fit1))\n\n## Ex. 2: refitting simulated data using data that contain NA values ------\nsleepstudyNA <- sleepstudy\nsleepstudyNA$Reaction[1:3] <- NA\nfm0 <- lmer(Reaction ~ Days + (1|Subject), sleepstudyNA)\n## the special case of refitting with a single simulation works ...\nss0 <- refit(fm0, simulate(fm0))\n## ... but if simulating multiple responses (for efficiency),\n## need to use na.action=na.exclude in order to have proper length of data\nfm1 <- lmer(Reaction ~ Days + (1|Subject), sleepstudyNA, na.action=na.exclude)\nss <- simulate(fm1, 5)\nres2 <- refit(fm1, ss[[5]])",
            "sigma": "methods(sigma)# from R 3.3.0 on, shows methods from pkgs 'stats' *and* 'lme4'",
            "simulate.merMod": "## test whether fitted models are consistent with the\n##  observed number of zeros in CBPP data set:\ngm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n             data = cbpp, family = binomial)\ngg <- simulate(gm1,1000)\nzeros <- sapply(gg,function(x) sum(x[,\"incidence\"]==0))\nplot(table(zeros))\nabline(v=sum(cbpp$incidence==0),col=2)\n##\n## simulate from a non-fitted model; in this case we are just\n## replicating the previous model, but starting from scratch\nparams <- list(theta=0.5,beta=c(2,-1,-2,-3))\nsimdat <- with(cbpp,expand.grid(herd=levels(herd),period=factor(1:4)))\nsimdat$size <- 15\nsimdat$incidence <- sample(0:1,size=nrow(simdat),replace=TRUE)\nform <- formula(gm1)[-2]  ## RHS of equation only\nsimulate(form,newdata=simdat,family=binomial,\n    newparams=params)\n## simulate from negative binomial distribution instead\nsimulate(form,newdata=simdat,family=negative.binomial(theta=2.5),\n    newparams=params)",
            "sleepstudy": "str(sleepstudy)\nrequire(lattice)\nxyplot(Reaction ~ Days | Subject, sleepstudy, type = c(\"g\",\"p\",\"r\"),\n       index = function(x,y) coef(lm(y ~ x))[1],\n       xlab = \"Days of sleep deprivation\",\n       ylab = \"Average reaction time (ms)\", aspect = \"xy\")\n(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy, subset=Days>=2))\n## independent model\n(fm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy, subset=Days>=2))",
            "subbars": "subbars(Reaction ~ Days + (Days|Subject)) ## => Reaction ~ Days + (Days + Subject)",
            "utilities": "## Create a few \"lme4 standard\" models ------------------------------\nfm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\nfmM <- update(fm1, REML=FALSE) # -> Maximum Likelihood\nfmQ <- update(fm1, . ~ Days + (Days | Subject))\n\ngm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n             data = cbpp, family = binomial)\ngmA <- update(gm1, nAGQ = 5)\n\n\n(lA1 <- llikAIC(fm1))\n(lAM <- llikAIC(fmM))\n(lAg <- llikAIC(gmA))\n\n(m1 <- methTitle(fm1 @ devcomp $ dims))\n(mM <- methTitle(fmM @ devcomp $ dims))\n(mG <- methTitle(gm1 @ devcomp $ dims))\n(mA <- methTitle(gmA @ devcomp $ dims))\n\n.prt.methTit(m1, class(fm1))\n.prt.methTit(mA, class(gmA))\n\n.prt.family(gaussian())\n.prt.family(binomial())\n.prt.family( poisson())\n\n.prt.resids(residuals(fm1), digits = 4)\n.prt.resids(residuals(fmM), digits = 2)\n\n.prt.call(getCall(fm1))\n.prt.call(getCall(gm1))\n\n.prt.aictab ( lA1 $ AICtab ) # REML\n.prt.aictab ( lAM $ AICtab ) # ML --> AIC, BIC, ...\n\nV1 <- VarCorr(fm1)\nm <- formatVC(V1)\nstopifnot(is.matrix(m), is.character(m), ncol(m) == 4)\nprint(m, quote = FALSE) ## prints all but the first line of .prt.VC() below:\n.prt.VC( V1, digits = 4)\n## Random effects:\n##  Groups   Name        Std.Dev. Corr\n##  Subject  (Intercept) 24.740\n##           Days         5.922   0.07\n##  Residual             25.592\np1 <- capture.output(V1)\np2 <- capture.output( print(m, quote=FALSE) )\npX <- capture.output( .prt.VC(V1, digits = max(3, getOption(\"digits\")-2)) )\nstopifnot(identical(p1, p2),\n          identical(p1, pX[-1])) # [-1] : dropping 1st line\n\n(Vq <- VarCorr(fmQ)) # default print()\nprint(Vq, comp = c(\"Std.Dev.\", \"Variance\"))\nprint(Vq, comp = c(\"Std.Dev.\", \"Variance\"), corr=FALSE)\nprint(Vq, comp = \"Variance\")\n\n.prt.grps(ngrps = ngrps(fm1),\n          nobs  = nobs (fm1))\n## --> Number of obs: 180, groups:  Subject, 18\n\n.prt.warn(fm1 @ optinfo) # nothing .. had no warnings\n.prt.warn(fmQ @ optinfo) # (ditto)",
            "vcconv": "vec2mlist(1:6)\nmlist2vec(vec2mlist(1:6)) # approximate inverse",
            "vcov.merMod": "fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\ngm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),\n             data = cbpp, family = binomial)\n(v1 <- vcov(fm1))\nv2 <- vcov(fm1, correlation = TRUE)\n# extract the hidden 'correlation' entry in @factors\nas(v2, \"corMatrix\")\nv3 <- vcov(gm1)\nv3X <- vcov(gm1, use.hessian  = FALSE)\nall.equal(v3, v3X)\n## full correlatiom matrix\ncv <- vcov(fm1, full = TRUE)\nimage(cv, xlab = \"\", ylab = \"\",\n      scales = list(y = list(labels = rownames(cv)),\n                    at = seq(nrow(cv)),\n                    x = list(labels = NULL)))"
        }
    },
    "bslib": {
        "description": "Simplifies custom 'CSS' styling of both 'shiny' and\n    'rmarkdown' via 'Bootstrap' 'Sass'. Supports 'Bootstrap' 3, 4 and 5 as\n    well as their various 'Bootswatch' themes. An interactive widget is\n    also provided for previewing themes in real time.",
        "examples": {
            "accordion": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nitems <- lapply(LETTERS, function(x) {\n  accordion_panel(paste(\"Section\", x), paste(\"Some narrative for section\", x))\n})\n\n# First shown by default\naccordion(!!!items)\n# Nothing shown by default\naccordion(!!!items, open = FALSE)\n# Everything shown by default\naccordion(!!!items, open = TRUE)\n\n# Show particular sections\naccordion(!!!items, open = \"Section B\")\naccordion(!!!items, open = c(\"Section A\", \"Section B\"))\n\n# Provide an id to create a shiny input binding\nlibrary(shiny)\n\nui <- page_fluid(\n  accordion(!!!items, id = \"acc\")\n)\n\nserver <- function(input, output) {\n  observe(print(input$acc))\n}\n\nshinyApp(ui, server)\n\\dontshow{\\}) # examplesIf}",
            "as_fill_carrier": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nlibrary(shiny)\nshinyApp(\n  page_fillable(\n    # without `as_fill_carrier()`, the plot won't fill the page because\n    # `uiOutput()` is neither a fillable container nor a fill item by default.\n    as_fill_carrier(uiOutput(\"ui\"))\n  ),\n  function(input, output) {\n    output$ui <- renderUI({\n      div(\n        class = \"bg-info text-white\",\n        as_fill_item(),\n        \"A fill item\"\n      )\n    })\n  }\n)\n\\dontshow{\\}) # examplesIf}",
            "bind_task_button": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(future)\nplan(multisession)\n\nui <- page_sidebar(\n  sidebar = sidebar(\n    input_task_button(\"recalc\", \"Recalculate\")\n  ),\n  textOutput(\"outval\")\n)\n\nserver <- function(input, output) {\n  rand_task <- ExtendedTask$new(function() {\n    future({\n      # Slow operation goes here\n      Sys.sleep(2)\n      runif(1)\n    }, seed = TRUE)\n  })\n\n  # Make button state reflect task.\n  # If using R >=4.1, you can do this instead:\n  # rand_task <- ExtendedTask$new(...) |> bind_task_button(\"recalc\")\n  bind_task_button(rand_task, \"recalc\")\n\n  observeEvent(input$recalc, {\n    rand_task$invoke()\n  })\n\n  output$outval <- renderText({\n    rand_task$result()\n  })\n}\n\nshinyApp(ui, server)\n\\dontshow{\\}) # examplesIf}",
            "breakpoints": "breakpoints(sm = c(4, 4, 4), md = c(3, 3, 6), lg = c(-2, 8, -2))",
            "bs_bundle": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# Function to preview the styling a (primary) Bootstrap button\nlibrary(htmltools)\nbutton <- tags$a(class = \"btn btn-primary\", href = \"#\", role = \"button\", \"Hello\")\npreview_button <- function(theme) {\n  browsable(tags$body(bs_theme_dependencies(theme), button))\n}\n\n# Here we start with a theme based on a Bootswatch theme,\n# then override some variable defaults\ntheme <- bs_add_variables(\n  bs_theme(bootswatch = \"sketchy\", primary = \"orange\"),\n  \"body-bg\" = \"#EEEEEE\",\n  \"font-family-base\" = \"monospace\",\n  \"font-size-base\" = \"1.4rem\",\n  \"btn-padding-y\" = \".16rem\",\n  \"btn-padding-x\" = \"2rem\"\n)\n\npreview_button(theme)\n\n# If you need to set a variable based on another Bootstrap variable\ntheme <- bs_add_variables(theme, \"body-color\" = \"$success\", .where = \"declarations\")\npreview_button(theme)\n\n# Start a new global theme and add some custom rules that\n# use Bootstrap variables to define a custom styling for a\n# 'person card'\nperson_rules <- system.file(\"custom\", \"person.scss\", package = \"bslib\")\ntheme <- bs_add_rules(bs_theme(), sass::sass_file(person_rules))\n\n# Include custom CSS that leverages bootstrap Sass variables\nperson <- function(name, title, company) {\n  tags$div(\n    class = \"person\",\n    h3(class = \"name\", name),\n    div(class = \"title\", title),\n    div(class = \"company\", company)\n  )\n}\n\npage_fluid(\n  theme = theme,\n  person(\"Andrew Carnegie\", \"Owner\", \"Carnegie Steel Company\"),\n  person(\"John D. Rockefeller\", \"Chairman\", \"Standard Oil\")\n)\n\\dontshow{\\}) # examplesIf}",
            "bs_dependency": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nmyWidgetVersion <- \"1.2.3\"\n\nmyWidgetDependency <- function() {\n  list(\n    bs_dependency_defer(myWidgetCss),\n    htmlDependency(\n      name = \"mywidget-js\",\n      version = myWidgetVersion,\n      src = system.file(package = \"mypackage\", \"js\"),\n      script = \"mywidget.js\"\n    )\n  )\n}\n\nmyWidgetCSS <- function(theme) {\n  if (!is_bs_theme(theme)) {\n    return(\n      htmlDependency(\n        name = \"mywidget-css\",\n        version = myWidgetVersion,\n        src = system.file(package = \"mypackage\", \"css\"),\n        stylesheet = \"mywidget.css\"\n      )\n    )\n  }\n\n  # Compile mywidget.scss using the variables and defaults from the theme\n  # object.\n  sass_input <- sass::sass_file(system.file(package = \"mypackage\", \"scss/mywidget.scss\"))\n\n  bs_dependency(\n    input = sass_input,\n    theme = theme,\n    name = \"mywidget\",\n    version = myWidgetVersion,\n    cache_key_extra = utils::packageVersion(\"mypackage\")\n  )\n}\n\n# Note that myWidgetDependency is not defined inside of myWidget. This is so\n# that, if `myWidget()` is called multiple times, Shiny can tell that the\n# function objects are identical and deduplicate them.\nmyWidget <- function(id) {\n  div(\n    id = id,\n    span(\"myWidget\"),\n    myWidgetDependency()\n  )\n}\n\\dontshow{\\}) # examplesIf}",
            "bs_get_variables": "vars <- c(\"body-bg\", \"body-color\", \"primary\", \"border-radius\")\nbs_get_variables(bs_theme(), varnames = vars)\nbs_get_variables(bs_theme(bootswatch = \"darkly\"), varnames = vars)\n\nbs_get_contrast(bs_theme(), c(\"primary\", \"dark\", \"light\"))\n\nlibrary(htmltools)\ndiv(\n  class = \"bg-primary\",\n  style = css(\n    color = bs_get_contrast(bs_theme(), \"primary\")\n  )\n)",
            "bs_global_theme": "# Remember the global state now (so we can restore later)\ntheme <- bs_global_get()\n\n# Use Bootstrap 3 (globally) with some theme customization\nbs_global_theme(3, bg = \"#444\", fg = \"#e4e4e4\", primary = \"#e39777\")\nif (rlang::is_interactive()) {\n  bs_theme_preview(with_themer = FALSE)\n}\n\n# If no global theme is active, bs_global_get() returns NULL\nbs_global_clear()\nbs_global_get()\n\n# Restore the original state\nbs_global_set(theme)",
            "bs_remove": "bs4 <- bs_theme(version = 4)\n\n# Retrieve sass bundle for print styles\nbs_retrieve(bs4, \"_print\", include_unnamed = FALSE)\n\n# Remove CSS rules for print and carousels\nbs4_no_print <- bs_remove(bs4, c(\"_print\", \"_carousel\"))\nsuppressWarnings(\n  bs_retrieve(bs4_no_print, \"_print\", include_unnamed = FALSE)\n)\n\n# Remove BS3 compatibility layer\nbs4_no_compat <- bs_remove(bs4, \"bs3compat\")",
            "bs_theme": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\ntheme <- bs_theme(\n  # Controls the default grayscale palette\n  bg = \"#202123\", fg = \"#B8BCC2\",\n  # Controls the accent (e.g., hyperlink, button, etc) colors\n  primary = \"#EA80FC\", secondary = \"#48DAC6\",\n  base_font = c(\"Grandstander\", \"sans-serif\"),\n  code_font = c(\"Courier\", \"monospace\"),\n  heading_font = \"'Helvetica Neue', Helvetica, sans-serif\",\n  # Can also add lower-level customization\n  \"input-border-color\" = \"#EA80FC\"\n)\n\nbs_theme_preview(theme)\n\n# Lower-level bs_add_*() functions allow you to work more\n# directly with the underlying Sass code\ntheme <- bs_add_variables(theme, \"my-class-color\" = \"red\")\ntheme <- bs_add_rules(theme, \".my-class { color: $my-class-color }\")\n\\dontshow{\\}) # examplesIf}",
            "bs_theme_dependencies": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\n# Function to preview the styling a (primary) Bootstrap button\nlibrary(htmltools)\nbutton <- tags$a(class = \"btn btn-primary\", href = \"#\", role = \"button\", \"Hello\")\npreview_button <- function(theme) {\n  browsable(tags$body(bs_theme_dependencies(theme), button))\n}\n\n# Latest Bootstrap\npreview_button(bs_theme())\n# Bootstrap 3\npreview_button(bs_theme(3))\n# Bootswatch 4 minty theme\npreview_button(bs_theme(4, bootswatch = \"minty\"))\n# Bootswatch 4 sketchy theme\npreview_button(bs_theme(4, bootswatch = \"sketchy\"))\n\\dontshow{\\}) # examplesIf}",
            "bs_theme_preview": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\ntheme <- bs_theme(bg = \"#6c757d\", fg = \"white\", primary = \"orange\")\nbs_theme_preview(theme)\n\\dontshow{\\}) # examplesIf}",
            "card": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nlibrary(htmltools)\n\ncard(\n  full_screen = TRUE,\n  card_header(\n    \"This is the header\"\n  ),\n  card_body(\n    p(\"This is the body.\"),\n    p(\"This is still the body.\")\n  ),\n  card_footer(\n    \"This is the footer\"\n  )\n)\n\\dontshow{\\}) # examplesIf}",
            "font_face": "# If you have an internet connection, running the following code\n# will download, cache, and import the relevant Google Font files\n# for local use\ntheme <- bs_theme(\n  base_font = font_google(\"Fira Sans\"),\n  code_font = font_google(\"Fira Code\"),\n  heading_font = font_google(\"Fredoka One\")\n)\nif (interactive()) {\n  bs_theme_preview(theme)\n}\n\n# Three different yet equivalent ways of importing a remotely-hosted Google Font\na <- font_google(\"Crimson Pro\", wght = \"200..900\", local = FALSE)\nb <- font_link(\n  \"Crimson Pro\",\n  href = \"https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@200..900\"\n)\nurl <- \"https://fonts.gstatic.com/s/crimsonpro/v13/q5uDsoa5M_tv7IihmnkabARboYF6CsKj.woff2\"\nc <- font_face(\n  family = \"Crimson Pro\",\n  style = \"normal\",\n  weight = \"200 900\",\n  src = paste0(\"url(\", url, \") format('woff2')\")\n)\ntheme <- bs_theme(base_font = c)\nif (interactive()) {\n  bs_theme_preview(theme)\n}",
            "input_switch": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nlibrary(shiny)\nlibrary(bslib)\n\nui <- page_fixed(\n  title = \"Keyboard Settings\",\n  h2(\"Keyboard Settings\"),\n  input_switch(\"auto_capitalization\", \"Auto-Capitalization\", TRUE),\n  input_switch(\"auto_correction\", \"Auto-Correction\", TRUE),\n  input_switch(\"check_spelling\", \"Check Spelling\", TRUE),\n  input_switch(\"smart_punctuation\", \"Smart Punctuation\"),\n  h2(\"Preview\"),\n  verbatimTextOutput(\"preview\")\n)\n\nserver <- function(input, output, session) {\n  output$preview <- renderPrint({\n    list(\n      auto_capitalization = input$auto_capitalization,\n      auto_correction = input$auto_correction,\n      check_spelling = input$check_spelling,\n      smart_punctuation = input$smart_punctuation\n    )\n  })\n}\n\nshinyApp(ui, server)\n\\dontshow{\\}) # examplesIf}",
            "input_task_button": "\\dontshow{if (interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nlibrary(shiny)\nlibrary(bslib)\n\nui <- page_sidebar(\n  sidebar = sidebar(\n    open = \"always\",\n    input_task_button(\"resample\", \"Resample\"),\n  ),\n  verbatimTextOutput(\"summary\")\n)\n\nserver <- function(input, output, session) {\n  sample <- eventReactive(input$resample, ignoreNULL=FALSE, {\n    Sys.sleep(2)  # Make this artificially slow\n    rnorm(100)\n  })\n\n  output$summary <- renderPrint({\n    summary(sample())\n  })\n}\n\nshinyApp(ui, server)\n\\dontshow{\\}) # examplesIf}",
            "layout_column_wrap": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nx <- card(\"A simple card\")\n\n# Always has 2 columns (on non-mobile)\nlayout_column_wrap(width = 1/2, x, x, x)\n\n# Automatically lays out three cards into columns\n# such that each column is at least 200px wide:\nlayout_column_wrap(x, x, x)\n\n# To use larger column widths by default, set `width`.\n# This example has 3 columns when the screen is at least 900px wide:\nlayout_column_wrap(width = \"300px\", x, x, x)\n\n# You can add a list of items, spliced with rlang's `!!!` operator\nlayout_column_wrap(!!!list(x, x, x))\n\\dontshow{\\}) # examplesIf}",
            "layout_columns": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nx <- card(\"A simple card\")\n\npage_fillable(\n  layout_columns(x, x, x, x)\n)\n\n# Or add a list of items, spliced with rlang's `!!!` operator\npage_fillable(\n layout_columns(!!!list(x, x, x))\n)\n\npage_fillable(\n  layout_columns(\n    col_widths = c(6, 6, 12),\n    x, x, x\n  )\n)\n\npage_fillable(\n  layout_columns(\n    col_widths = c(6, 6, -2, 8),\n    row_heights = c(1, 3),\n    x, x, x\n  )\n)\n\npage_fillable(\n  fillable_mobile = TRUE,\n  layout_columns(\n    col_widths = breakpoints(\n      sm = c(12, 12, 12),\n      md = c(6, 6, 12),\n      lg = c(4, 4, 4)\n    ),\n    x, x, x\n  )\n)\n\\dontshow{\\}) # examplesIf}",
            "nav_select": "can_browse <- function() rlang::is_interactive() && require(\"shiny\")\n\n# Selecting a tab\nif (can_browse()) {\n  shinyApp(\n    page_fluid(\n      radioButtons(\"item\", \"Choose\", c(\"A\", \"B\")),\n      navset_hidden(\n        id = \"container\",\n        nav_panel_hidden(\"A\", \"a\"),\n        nav_panel_hidden(\"B\", \"b\")\n      )\n    ),\n    function(input, output) {\n      observe(nav_select(\"container\", input$item))\n    }\n  )\n}\n\n# Inserting and removing\nif (can_browse()) {\n  ui <- page_fluid(\n    actionButton(\"add\", \"Add 'Dynamic' tab\"),\n    actionButton(\"remove\", \"Remove 'Foo' tab\"),\n    navset_tab(\n      id = \"tabs\",\n      nav_panel(\"Hello\", \"hello\"),\n      nav_panel(\"Foo\", \"foo\"),\n      nav_panel(\"Bar\", \"bar tab\")\n    )\n  )\n  server <- function(input, output) {\n    observeEvent(input$add, {\n      nav_insert(\n        \"tabs\", target = \"Bar\", select = TRUE,\n        nav_panel(\"Dynamic\", \"Dynamically added content\")\n      )\n    })\n    observeEvent(input$remove, {\n      nav_remove(\"tabs\", target = \"Foo\")\n    })\n  }\n  shinyApp(ui, server)\n}",
            "navbar_options": "navbar_options(position = \"static-top\", bg = \"#2e9f7d\", underline = FALSE)",
            "page_fillable": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(shiny)\nlibrary(ggplot2)\n\nui <- page_fillable(\n  h1(\"Example\", code(\"mtcars\"), \"dashboard\"),\n  layout_columns(\n    card(\n      full_screen = TRUE,\n      card_header(\"Number of forward gears\"),\n      plotOutput(\"gear\")\n    ),\n    card(\n      full_screen = TRUE,\n      card_header(\"Number of carburetors\"),\n      plotOutput(\"carb\")\n    )\n  ),\n  card(\n    full_screen = TRUE,\n    card_header(\"Weight vs. Quarter Mile Time\"),\n    layout_sidebar(\n      sidebar = sidebar(\n        varSelectInput(\"var_x\", \"Compare to qsec:\", mtcars[-7], \"wt\"),\n        varSelectInput(\"color\", \"Color by:\", mtcars[-7], \"cyl\"),\n        position = \"right\"\n      ),\n      plotOutput(\"var_vs_qsec\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  for (var in c(\"cyl\", \"vs\", \"am\", \"gear\", \"carb\")) {\n    mtcars[[var]] <- as.factor(mtcars[[var]])\n  }\n\n  output$gear <- renderPlot({\n    ggplot(mtcars, aes(gear)) + geom_bar()\n  })\n\n  output$carb <- renderPlot({\n    ggplot(mtcars, aes(carb)) + geom_bar()\n  })\n\n  output$var_vs_qsec <- renderPlot({\n    req(input$var_x, input$color)\n\n    ggplot(mtcars) +\n      aes(y = qsec, x = !!input$var_x, color = !!input$color) +\n      geom_point()\n  })\n}\n\nshinyApp(ui, server)\n\\dontshow{\\}) # examplesIf}",
            "page_navbar": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nlibrary(shiny)\nlibrary(bslib)\n\nlink_shiny <- tags$a(\n  shiny::icon(\"github\"), \"Shiny\",\n  href = \"https://github.com/rstudio/shiny\",\n  target = \"_blank\"\n)\nlink_posit <- tags$a(\n  shiny::icon(\"r-project\"), \"Posit\",\n  href = \"https://posit.co\",\n  target = \"_blank\"\n)\n\nui <- page_navbar(\n  title = \"My App\",\n  nav_panel(title = \"One\", p(\"First page content.\")),\n  nav_panel(title = \"Two\", p(\"Second page content.\")),\n  nav_panel(\"Three\", p(\"Third page content.\")),\n  nav_spacer(),\n  nav_menu(\n    title = \"Links\",\n    align = \"right\",\n    nav_item(link_shiny),\n    nav_item(link_posit)\n  )\n)\n\nserver <- function(...) { } # not used in this example\n\nshinyApp(ui, server)\n\\dontshow{\\}) # examplesIf}",
            "page_sidebar": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\nlibrary(shiny)\nlibrary(ggplot2)\n\nui <- page_sidebar(\n  title = \"Example dashboard\",\n  sidebar = sidebar(\n    varSelectInput(\"var\", \"Select variable\", mtcars)\n  ),\n  card(\n    full_screen = TRUE,\n    card_header(\"My plot\"),\n    plotOutput(\"p\")\n  )\n)\n\nserver <- function(input, output) {\n  output$p <- renderPlot({\n    ggplot(mtcars) + geom_histogram(aes(!!input$var))\n  })\n}\n\nshinyApp(ui, server)\n\\dontshow{\\}) # examplesIf}",
            "popover": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\npopover(\n  shiny::actionButton(\"btn\", \"A button\"),\n  \"Popover body content...\",\n  title = \"Popover title\"\n)\n\nlibrary(shiny)\n\nui <- page_fixed(\n  card(class = \"mt-5\",\n    card_header(\n      popover(\n        uiOutput(\"card_title\", inline = TRUE),\n        title = \"Provide a new title\",\n        textInput(\"card_title\", NULL, \"An editable title\")\n      )\n    ),\n    \"The card body...\"\n  )\n)\n\nserver <- function(input, output) {\n  output$card_title <- renderUI({\n    list(input$card_title, bsicons::bs_icon(\"pencil-square\"))\n  })\n}\n\nshinyApp(ui, server)\n\\dontshow{\\}) # examplesIf}",
            "run_with_themer": "library(shiny)\n\nui <- fluidPage(\n  theme = bs_theme(bg = \"black\", fg = \"white\"),\n  h1(\"Heading 1\"),\n  h2(\"Heading 2\"),\n  p(\n    \"Paragraph text;\",\n    tags$a(href = \"https://www.rstudio.com\", \"a link\")\n  ),\n  p(\n    actionButton(\"cancel\", \"Cancel\"),\n    actionButton(\"continue\", \"Continue\", class = \"btn-primary\")\n  ),\n  tabsetPanel(\n    tabPanel(\"First tab\",\n      \"The contents of the first tab\"\n    ),\n    tabPanel(\"Second tab\",\n      \"The contents of the second tab\"\n    )\n  )\n)\n\nif (interactive()) {\n  run_with_themer(shinyApp(ui, function(input, output) {}))\n}",
            "tooltip": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\n\ntooltip(\n  shiny::actionButton(\"btn\", \"A button\"),\n  \"A message\"\n)\n\ncard(\n  card_header(\n    tooltip(\n      span(\"Card title \", bsicons::bs_icon(\"question-circle-fill\")),\n      \"Additional info\",\n      placement = \"right\"\n    )\n  ),\n  \"Card body content...\"\n)\n\\dontshow{\\}) # examplesIf}",
            "value_box": "\\dontshow{if (rlang::is_interactive()) (if (getRversion() >= \"3.4\") withAutoprint else force)(\\{ # examplesIf}\nlibrary(htmltools)\n\nvalue_box(\n  \"KPI Title\",\n  h1(HTML(\"$1 <i>Billion</i> Dollars\")),\n  span(\n    bsicons::bs_icon(\"arrow-up\"),\n    \" 30\\% VS PREVIOUS 30 DAYS\"\n  ),\n  showcase = bsicons::bs_icon(\"piggy-bank\"),\n  theme = \"success\"\n)\n\\dontshow{\\}) # examplesIf}"
        }
    }
}